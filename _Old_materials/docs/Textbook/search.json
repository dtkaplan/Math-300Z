[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lessons in Statistical Thinking",
    "section": "",
    "text": "Lessons in Statistical Thinking is an update and reconsideration of the concepts and methods needed to extract information from data. Such an update is needed because the canon of traditional introductory statistics texts has long been obsolescent and fails to address the needs of the contemporary data scientist and decision-maker. That canon stems from an influential 1925 book, Ronald Fisher’s Statistical Methods for Research Workers. Research workers of that era typically ran small benchtop or field experiments with a dozen or fewer observations on each of two treatments. A first task with such small data is to rule out the possibility that calculated differences might reflect only the accidental arrangement of numbers into groups.\nPerhaps emblematic of the current dissatisfaction with small-data methods is the controversy over “statistical significance.” Although situated at the core of many statistics textbooks, significance testing has little to do with the everyday meaning of “significant” as “important” or “relevant.” This article in the prestigious science journal Nature details the controversy. figure 18.1 reproduces a cartoon from that article that puts the shortcomings of “statistical significance” in a historical context.\nObsession with the mantra, “Correlation is not causation,” is another sign of the obsolescence of the traditional introduction to statistics. Around 1910, the pioneers of statistics were the first to emphasize an important innovation in scientific method: the randomized controlled trial (RCT). Adoption of the RCT in the twentieth century put several branches of science on a new footing. And many statistics instructors see “correlation is not causation” as a slogan pointing to the genuine importance of RCTs. Unfortunately, the mantra has been over-interpreted to mean the impossibility of causal knowledge without RCTs, as in the following cartoon (figure 18.2):\nNowadays, when data are used to inform policy decisions in many areas, the being statistically literate includes the need to make justifiable conclusions about causality. One approach to this was highlighted by the 2019 Nobel Prize in economics; breaking down complex issues of global poverty into smaller, more manageable questions where an RCT is feasible. Another approach, using “natural experiments” was honored by the 2021 Nobel Prize. There is no Nobel in computer science. The equivalent is the Turing Award, which in 2011 was awarded for “fundamental contributions to … probabilistic and causal reasoning.” That such prestigious awards are being given in the last decade demonstrates how recent and how important causal reasoning is and why the fundamentals of causality ought to be part a modern introduction to statistics, and consequently why they are a major theme in these Lessons."
  },
  {
    "objectID": "index.html#statistical-thinking",
    "href": "index.html#statistical-thinking",
    "title": "Lessons in Statistical Thinking",
    "section": "Statistical thinking",
    "text": "Statistical thinking\nThe work of today’s data scientists is often to discover novel connections among multiple variables and to guide decision-making. It is common for data to be available in large masses from observations rather than experiments. One common purpose is “prediction,” which might be as simple as the uses of medical screening tests or as breathtaking as machine-learning techniques of “artificial intelligence.” Another pressing need from data analysis is to understand possible causal connections between variables.\nThe twenty lessons that follow describe a way of thinking that is historically novel, unfamiliar to most otherwise well-educated people, and incredibly useful for making sense of the world and what data can tell us about the world."
  },
  {
    "objectID": "index.html#for-reference-important-word-pairs",
    "href": "index.html#for-reference-important-word-pairs",
    "title": "Lessons in Statistical Thinking",
    "section": "For reference: Important word pairs",
    "text": "For reference: Important word pairs\nMany of the vocabulary terms used in statistical thinking come in pairs. We list several such pairs below, in roughly the order they first appear in the Lessons. The pairs can be a reference while reading, but it is also helpful to return to this list to sharpen your understanding of the distinctions.\nExplanatory vs response variables. Models (in these Lessons) always involve a single response variable*. In contrast, models can have zero or more explanatory variables.\nVariable vs covariate. “Covariate” is another word for an explanatory variable. The word “covariate” signals that the variable is not itself of direct interest to the modeler but puts another explanatory variable in a correct context.\nCategorical vs quantitative variables. Always be aware of whether a model’s response variable is categorical or quantitative. When categorical, expect to use zero_one() to convert it to quantitative before modeling. In contrast, explanatory variables can be either categorical or quantitative.\nRegression model vs classifier. A regression model always has a quantitative response variable. A classifier has a categorical response variable. In these Lessons, as in much professional use of data, our categorical response variables will have two levels (e.g., healthy or sick, up or down, yes or no). In this situation, regression techniques suffice to build classifiers.\nModel vs model function. By “model,” we will almost always mean “regression model.” A regression model, typically constructed by the lm() function, contains various information useful to summarize the model. The “model function” provides the mechanism for one important task, calculating from values from the explanatory variables the corresponding model output.\nModel coefficient vs effect size. Model coefficients are numerical parameters. Training determines the appropriate values for the coefficients. In contrast, an effect size describes the relationship between the response variable and a selected explanatory variable.\nPoint estimate vs interval estimate. A point estimate is a single number. For instance, a model coefficient is a point estimate, as is the output from a model function. In contrast, interval estimates involve two numbers; one specifies the lower end of the interval and the other number specifies the upper end.\nPrediction interval vs confidence interval. A prediction interval describes the anticipated range of the actual result for which we have made a prediction, e.g., “tomorrow’s wind will be between 5 and 10 mph.” A confidence interval is often used to express the uncertainty in a coefficient or effect size."
  },
  {
    "objectID": "bogus.html",
    "href": "bogus.html",
    "title": "1  Bogus",
    "section": "",
    "text": "Foobar"
  },
  {
    "objectID": "Reading-notes-lesson-19.html#statistical-thinking",
    "href": "Reading-notes-lesson-19.html#statistical-thinking",
    "title": "19  Preliminaries",
    "section": "Statistical thinking",
    "text": "Statistical thinking\nThese lessons are about “statistical thinking,” a phrase which includes habits of mind, routine questions to ask, and understanding of which statistical measures are informative—and which not—in different contexts. The goal of statistical thinking is to understand “how and when we can draw valid inferences from data.” [Source] The word “valid” means several things at once: faithful to the data, consistent with the process used to assemble the data, and informative for the uses to which the inferences are to be directed.\nEvery person has a natural ability to think. We train our thinking skills by observing and emulating the logic and language of people and sources deemed authoritative. We have resources spanning several millennia to hone our ability to think. However, statistical thinking is a comparatively recent arrival on the intellectual scene, germinating and developing over only the last 150 years. As a result, hardly anything that we hear or read exemplifies statistical thinking.\nIn general, effective thinking requires us to grasp various intellectual tools, for example, logic. Our mode of logical thinking was promulgated by Aristotle (384–322 BC) and, to quote the Stanford Encyclopedia of Philosophy, “has had an unparalleled influence on the history of Western thought.” In the 2500 years since Aristotle’s time, the use of Aristotelian logic has been so pervasive that we expect any well-educated person to be able to identify logical thinking. For example, the statement “John’s car is red” has implications. Which of these two statements are among those implications? “That red car is necessarily John’s,” or “The blue car is not John’s car.” Not so hard!\nThe intellectual tools needed for statistical thinking are, by and large, unfamiliar and non-intuitive. These Lessons are intended to provide the tools you will need to engage in effective statistical thinking.\nTo get started, consider this headline from The Economist, a well-reputed international news magazine: “The pandemic’s indirect effects on small children could last a lifetime.” As support for this claim, the headlined article provides more detail. For instance:\n\n“Stress and distraction made some patients more distant. LENA, a charity in Colorado, has for years used wearable microphones to keep track of how much chatter babies and the care-givers exchange. During the pandemic the number of such \"conversations\" declined. ….”[g]etting lots of interaction in the early years of life is essential for healthy development, so these kinds of data \"are a red flag\".” The article goes on to talk of “children starved of stimulation at home …..”\n\nThis short excerpt might raise some questions. Think about it briefly and note what questions come to mind.\nFor those already along the road toward statistical thinking, the phrase, “the number of such conversations declined” might prompt this question: “By how much?” Similarly, reading the claim that “getting lots of interactions … is essential for healthy development,” your mind might insist on these questions: How much is “lots?” How does the decline in the number compare to “lots?”\nNot finding the answer to these questions in the article’s text, it would be sensible to look for the primary source of the information. In our Internet age, that’s comparatively easy to do. The LENA website includes an article, “COVID-era infants vocalize less and experience fewer conversational turns, says LENA research team.” The article contains two graphs. (?fig-lena-two-graphs)\n\nknitr::include_graphics(\"www/Lena-fig1.png\")\nknitr::include_graphics(\"www/Lena-fig2.png\")\n\n\n\n\n\n\nFigure 19.1: Graphics from the LENA website. The left is captioned, “Children from the COVID-era sample produced significantly fewer vocalizations than their pre-COVID peers.” The right, “The differences in vocalizations and turns were greatest among children from families in the lowest SES [socio-economic status] quartile.”\n\n\n\n\n\n\n\nFigure 19.2: Graphics from the LENA website. The left is captioned, “Children from the COVID-era sample produced significantly fewer vocalizations than their pre-COVID peers.” The right, “The differences in vocalizations and turns were greatest among children from families in the lowest SES [socio-economic status] quartile.”\n\n\n\n\n\n\nTo make any proper sense of the graphs in ?fig-lena-two-graphs, you need some basic technical knowledge. For example, what do the vertical bars in the graph mean? And the subcaptions, “t(628) = 3.03, p = 0.003” and “t(216)= 2.13, p=0.002”: What do they mean, if anything? Turning back to the text of The Economist, do these graphs justify raising a “red flag?” More basically, are these graphs the “data,” or is there more data behind the graphs? What would that data show?\nThe LENA article does not link to supporting data, that is, what lies behind the graphs in ?fig-lena-two-graphs. But the LENA article does point to other publications.\n\n“These findings from LENA support a growing body of evidence that babies born during the COVID pandemic are, on average, experiencing developmental delays. For example, researchers from the COMBO (COVID-19 Mother Baby Outcomes) consortium at Columbia University published findings in the January 2022 issue of JAMA Pediatrics showing that children born during the pandemic achieved significantly lower gross motor, fine motor, and personal-social scores at six months of age.”\n\nTo the statistical thinker, phrases like “red flag,” “growing body of evidence,” and “significantly lower” are weasel words, that is, terms “used in order to evade or retreat from a direct or forthright statement or position.” [Source] In ordinary thinking, such evasiveness or lack of forthrightness would naturally prompt concern about the reliability of the claim. It makes sense to look deeper, for instance, by checking out the JAMA article. Many people would be hesitant to do this, anticipating that the article would be incomprehensible and filled with jargon. An important reason to study statistical thinking is to tear down barriers to substantiating or debunking claims. In fact, the JAMA article contains very little that requires knowledge of pediatrics or the meaning of “gross motor, fine motor, and personal-social scores,” but a lot that depends on understanding statistical notation and convention and—more critical —the reasoning behind the conventions.\nThe tools of statistical thinking are the tools for making sense of data. Evaluating data is essential to determine whether to rely on claims supposedly based on those data. In the words of eminent engineer and statistician W. Edwards Demming: “In God we trust. All others must bring data.” And former President Ronald Reagan famously quoted a Russian proverb: “Trust, but verify.” Unfortunately, until you have the statistical thinking tools needed to interpret data reliably, all you can do is trust, not verify."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#defining-statistical-thinking",
    "href": "Reading-notes-lesson-19.html#defining-statistical-thinking",
    "title": "19  Preliminaries",
    "section": "Defining statistical thinking",
    "text": "Defining statistical thinking\nLearning a new way of thinking is genuinely hard. As you learn statistical thinking, it may help to have a concise definition. The following definition captures much of the essence of statistical thinking:\n\nStatistic thinking is the accounting for variation in the context of what remains unaccounted for.\n\nImplicit in this definition is a pathway for learning to think statistically:\n\nLearn how to measure variation;\nLearn how to account for variation;\nLearn how to measure what remains unaccounted for.\n\nThe next three sections briefly touch on each of these three topics."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#variation",
    "href": "Reading-notes-lesson-19.html#variation",
    "title": "19  Preliminaries",
    "section": "Variation",
    "text": "Variation\n\n\nVariation itself is nature’s only irreducible essence. Variation is the hard reality, not a set of imperfect measures for a central tendency. Means and medians are the abstractions. —– Stephen Jay Gould (1941- 2002), paleontologist and historian of science.\n\n\nTo illustrate variation, let’s consider a process fundamental to human life: gestation. We all know that human pregnancy “typically” lasts around nine-months, but that the duration isn’t known in advance.\nfigure 19.3 shows data from the Gestation data frame. In this data frame, each of the 1200 rows is one pregnancy and birth about which several measurements were made. The gestation variable records the length of the pregnancy (in days).\n\n\nCode\nGestation <- Gestation %>% \n  mutate(parity = ifelse(parity==0, \"first-time\", \"previous-preg\")) \nPlot1 <- Gestation %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) \nPlot1\n\n\n\n\n\nFigure 19.3: Gestational period for first-time mothers and mothers with a previous pregancy.\n\n\n\n\nfigure 19.3 divides the 1200 births in the Gestation data frame according to the variable parity, which describes whether or not the pregnancy is the mother’s first.\nThe variation in gestation is evident directly from the dots in the graph. One strategy for describing variation is to specify an interval: the span between a lower and an upper value. For instance,\n\nThe large majority of pregancies last between 250 and 300 days. Or,\nThe majority of pregnancies are between 275 and 290 days.\n\nA more subtle description avoids setting hard bounds in favor of saying which durations are common and which not. This common-or-not description is called a “distribution.” The “histogram” is a famous style of presentation of a distribution. Even elementary-school students are introduced to histograms; they are easy to draw. But we have more important concerns; we want to be able to show relationships between variables and we want, whenever possible, to put the graphical summaries of data as a layer on top of the data themselves. And we have the computer as a tool for making graphics. Consequently, our preferred format for displaying distributions is a smooth shape, oriented along the vertical axis. The width of the shape expresses how common is the corresponding region of the vertical axis. figure 19.4 shows the density display layered on top of the pregnancy data. For reasons that may be evident, this sort of display is called a “violin plot.”\n\n\nCode\nPlot1 +\n  geom_violin(aes(group=parity), fill=\"blue\", alpha=0.65, color=NA)\n\n\n\n\n\nFigure 19.4: A violin plot. The long axis of the violin-like shape is oriented along the response-variable axis (that is, the vertical axis in our standard format). The width of the violin for each possible value of the response variable is proportional to the density of data near that value.\n\n\n\n\nThe shapes of the two violins in figure 19.4 are similar, suggesting that the variation in the duration of pregnancy is about the same for first-time mothers as for mothers in a second or later pregnancy.\nThere is a strong link between the interval descriptions of variation and the density display. Suppose you specify the fraction of cases that you want to include in an interval description, say 50% or 80%. In terms of the violin, that fraction is a proportion of the overall area of the violin. For instance, the 50% interval would include the central 50% of the area of the violin, leaving 25% out at the bottom and another 25% out at the top. The 80% interval would leave out only 10% of the area at the top and bottom of the violin. This suggests that the interval style of describing variation really involves three numbers; the top and bottom of the interval as well as the selected percentage (say, 50% or 80%) used to find the location of the top and bottom.\nYet another style for describing variation—one that will take primary place in these Lessons—uses only a single-number. Perhaps the simplest way to imagine how a single number can capture variation is to think about the spread or distance between the top and bottom of an interval description. In taking such a distance as the measure of variation, we are throwing out some information. Taken together, the top and bottom of the interval describe two things: the location of the values and the spread among the values. These are both important, but it is the spread that gives a pure description of variation.\nEarly pioneers of statistics took some time to agree on a standard way of measuring the spread. For instance, should it be the spread between the top and bottom of a 50% interval or an 80% interval, or something else. In the end, the selected standard focussed on something more basic: the differences between pairs of individual values.\nIt works like this. For a data frame with \\(n=2\\) rows, the spread in a variable can be measured simply as the difference between the two values. For instance, suppose the gestation variable had only two entries, say, 267 and 293 days. The spread or distance between these is \\(293-267 = 26\\) days. Of course, we don’t intend to measure spread with a negative number. One solution is to use the absolute value of the difference. However, for subtle mathematical reasons relating to—of all things!—the Pythagorean theorem, we avoid the possibility of a negative spread by using the square of the difference, that is, \\((293 - 267)^2 = 676\\) days-squared.\nTo extend this very simple measure of variation to data with \\(n > 2\\) is simple: look at the square difference between every possible pair of values, then average. For instance, for \\(n=3\\) with values 267, 293, 284, look at the differences \\((267-293)^2, (267-284)^2\\) and \\((293-284)^2\\) and average them! This simple way of measuring variation is called the “modulus” and dates from at least 1885. Since then, statisticians have standardized on a closely related measure, the “variance,” which is the modulus divided by \\(\\sqrt{2}\\). Either one would work, but there are advantages to standardizing on one: the variance.\nCalculating the variance is straightforward, Here’s the variance of gestation:\n\nGestation %>%\n  summarize(variance = var(gestation))\n\n\n\n \n  \n    variance \n  \n \n\n  \n    256.887 \n  \n\n\n\n\nA consequence of the use of squaring in defining the variance is the units of the result. gestation is measured in days, so var(gestation) is measured in days2. The advantage to this will only become clear later in these Lessons. For now, you might prefer to think about the square-root of the variance, which has been given the name “standard deviation.”\n\nGestation %>%\n  summarize(standard_deviation = sd(gestation))\n\n\n\n \n  \n    standard_deviation \n  \n \n\n  \n    16.02769"
  },
  {
    "objectID": "Reading-notes-lesson-19.html#sec-accounting-for-variation",
    "href": "Reading-notes-lesson-19.html#sec-accounting-for-variation",
    "title": "19  Preliminaries",
    "section": "Accounting for variation",
    "text": "Accounting for variation\nThe word “account” has several related meanings.1\n\nTo “account for something” means “to be the explanation or cause of something.” [Oxford Languages]\nAn “account of something” is a story, a description, or an explanation, as in the Biblical account of the creation of the world.\nTo “take account of something” means “to consider particular facts, circumstances, etc. when making a decision about something.”\n\nSynonyms for “account” include “description,”report,” “version,” “story,” “statement,” “explanation,” “interpretation,” “sketch,” and “portrayal.” “Accountants” and their “account books” keep track of where money comes from and goes to.\nThese various nuances of meaning, from a simple arithmetical tallying up to an interpretation or version serve the purposes of statistical thinking well. When we “account for variation,” we are telling a story that tries to explain where the variation might have come from. An accounting of variation is not necessarily definitive, true, or helpful. Just as witnesses of an event can have different accounts, so there can be many accounts of the variation even of the same variable in the same data frame.\nThere are many formats for stories, many ways of organizing facts and data, and many ways of accounting for variance. In these Lessons, we will use regression modeling almost exclusively as our method of accounting. Here, for example, are two different accounts of gestation:\n\nlm(gestation ~ 1, data=Gestation) %>% coef()\n\n(Intercept) \n   279.3385 \n\nlm(gestation ~ parity, data = Gestation) %>% coef()\n\n        (Intercept) parityprevious-preg \n         281.261981           -2.585058 \n\n\nIn the R language, expressions like gestation ~ 1 and gestation ~ parity are called “tilde expressions.” They are the means by which the modeler specifies the structure of the model that is to be built. Training (or “fitting”) translates the model specification into an arithmetic formula that involves the explanatory variables and numerical coefficients.\nThe coefficients from a regression model are part of an accounting for variation. Learning how to read them is an important skill in statistical thinking. For instance, the coefficient from a model in the form y ~ 1 is always the average value of variable y. In contrast, in a model like y ~ x, the “intercept” is a baseline value and the x-coefficient describes what part of the variation in y can be credited to x.\n\n\n\n\n\n\nThe RESPEX graphics format\n\n\n\nfigure 19.3 is an example of what we call the RESPEX graphics style. Each RESPEX graphic is made to coordinate with aregression model of the data. Every regression model has a response variable. Likewise, every RESPEX graphic shows the response variable on the vertical axis. Similarly, RESPEX graphics place an explanatory variable on the horizontal axis. If there is more than one explanatory variable, they are encoded graphically using color then faceting.\nRESPEX stands for “RESPonse versus EXplanatory,” but you might like to think of it as data graphics drawn with “respect” to a model.\n\n\nRegression models always have a quantitative response variable, although explanatory variables can be either quantitative or categorical. But, often, the modeling situation calls for a response variable that is categorical. Expert modelers can use specialized modeling methods to handle such situations. However, some of the power of these specialized methods is available to the beginning modeler by a little trick. When categorical response variables have just two levels, e.g., Alive/Dead, Promoted/Not, or Win/Loss, they can be transformed to a numerical representation using 0 for one level and 1 for the other.\nWe will identify the such variables as being of type “yes/no” or, equivalently, “zero-one” variables. With the zero-one encoding\nThis numerical “0/1 encoding” is directly suited for regression modeling and enables us to extend the scope of regression models. The output of the regression model is always numerical. Nothing in the regression technique restricts those outputs to exactly zero or one, even when the response variable is of the yes/no type. Usually, the modeler interprets such numerical output as probabilities or, more generally, as measures to be converted to probabilities.\n\n\n\n\n\n\nR technique: zero_one().\n\n\n\nThe zero_one() function converts a yes/no variable to the numerical zero-one format. zero_one() allows you to specify which of the two levels is represented by 1.\nTo illustrate, consider the mosaicData::Whickham data frame, which records a 1972-1974 survey, part of a study of the relationship between smoking and mortality. Twenty years after the initial survey, a follow-up established whether or not each person was still alive. Here are a few rows from the data frame:\n\n\n\n\n \n  \n    outcome \n    smoker \n    age \n  \n \n\n  \n    Alive \n    Yes \n    23 \n  \n  \n    Alive \n    Yes \n    18 \n  \n  \n    Dead \n    Yes \n    71 \n  \n  \n    Alive \n    No \n    67 \n  \n  \n    Alive \n    No \n    64 \n  \n  \n    Alive \n    Yes \n    38 \n  \n\n\n\n\nThe outcome variable in Whickham records the result of the follow-up survey. It is a categorical variable with levels “Alive” and “Dead.” To examine what the data have to say about the relationship between smoking and mortality, we construct a model with outcome as the response variable and smoking as an explanatory variable. Before doing so, we translate outcome into a zero-one format. Like this:\n\nWhickham %>% \n  mutate(alive = zero_one(outcome, one=\"Alive\"))\n\n\n\n\n\n \n  \n    outcome \n    smoker \n    age \n    alive \n  \n \n\n  \n    Alive \n    Yes \n    23 \n    1 \n  \n  \n    Alive \n    Yes \n    18 \n    1 \n  \n  \n    Dead \n    Yes \n    71 \n    0 \n  \n  \n    Alive \n    No \n    67 \n    1 \n  \n  \n    Alive \n    No \n    64 \n    1 \n  \n  \n    Alive \n    Yes \n    38 \n    1 \n  \n\n\n\n\nNote the correspondence between the outcome and the newly created alive variable."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#variation-unaccounted-for",
    "href": "Reading-notes-lesson-19.html#variation-unaccounted-for",
    "title": "19  Preliminaries",
    "section": "Variation unaccounted for",
    "text": "Variation unaccounted for\nA model typically accounts for only some of the variation in a response variable. The remaining variation is called “residual variation.”\nConsider the model gestation ~ parity. In the next lines of code we build this model, training it with the Gestation data. Then we evaluate the model on the trained data. This amounts to using the model coefficients to generate a model output for each row in the training data, and can be accomplished with the model_eval() R function.\n\nModel <- lm(gestation ~ parity, data = Gestation)\nEvaluated <- model_eval(Model)\n\nUsing training data as input to model_eval().\n\n\n\n\nUsing training data as input to model_eval().\n\n\n\n\n \n  \n      \n    .response \n    parity \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    1218 \n    270 \n    previous-preg \n    278.6769 \n    -8.6769231 \n    247.2800 \n    310.0738 \n  \n  \n    1219 \n    275 \n    first-time \n    281.2620 \n    -6.2619808 \n    249.8322 \n    312.6917 \n  \n  \n    1220 \n    265 \n    previous-preg \n    278.6769 \n    -13.6769231 \n    247.2800 \n    310.0738 \n  \n  \n    1221 \n    291 \n    previous-preg \n    278.6769 \n    12.3230769 \n    247.2800 \n    310.0738 \n  \n  \n    1222 \n    281 \n    first-time \n    281.2620 \n    -0.2619808 \n    249.8322 \n    312.6917 \n  \n  \n    1223 \n    297 \n    previous-preg \n    278.6769 \n    18.3230769 \n    247.2800 \n    310.0738 \n  \n\n\n\n\n\n\n\n\n\n\nThe .response variable\n\n\n\nThe output from model_eval() repeats some columns from the data used for evaluation. For example, the explanatory variables are listed by name. (Here, the only explanatory variable is parity.) The response variable is also included, but given a generic name, .response to make it easy to distinguish it from the explanatory variables.\n\n\nTo see where the .output comes from, let’s look again at the model coefficients:\n\nModel %>% coef()\n\n        (Intercept) parityprevious-preg \n         281.261981           -2.585058 \n\n\nThe baseline value is 281.3 days. This applies to first-time mothers. For the other mothers, those with a previous pregnancy, the coefficient indicates that the model value is 2.6 days less than the baseline, or 279.7 days.\nThe output from model_eval() includes other columns of importance. For us, here, those are. the response variable itself (gestation, which has been given a generic name, .response) and the residuals from the model (.resid). There is a simple relationship between .response, .output and .resid:\n\\[\\mathtt{.response} = \\mathtt{.output} + \\mathtt{.resid}\\]\n\n\n\n\n\n\nDemonstration: Why the variance?\n\n\n\nThe subtle mathematical reasoning behind the choice of variance to measure variation is illuminated when we compute the variances of the three quantities in the previous equation.\n\nEvaluated %>%\n  summarize(var_response = var(.response),\n            var_output = var(.output),\n            var_resid  = var(.resid))\n\n\n\n \n  \n    var_response \n    var_output \n    var_resid \n  \n \n\n  \n    256.887 \n    1.273587 \n    255.6134 \n  \n\n\n\n\nThe variances of the output and residuals add up to equal, exactly, the variance of the response variable! This isn’t true for the standard deviations:\n\nEvaluated %>%\n  summarize(sd_response = sd(.response),\n            sd_output = sd(.output),\n            sd_resid  = sd(.resid))\n\n\n\n \n  \n    sd_response \n    sd_output \n    sd_resid \n  \n \n\n  \n    16.02769 \n    1.128533 \n    15.98791"
  },
  {
    "objectID": "Reading-notes-lesson-19.html#appendix-software-guide",
    "href": "Reading-notes-lesson-19.html#appendix-software-guide",
    "title": "19  Preliminaries",
    "section": "Appendix: Software guide",
    "text": "Appendix: Software guide\nThese Lessons use about a dozen new R functions. Some of these are used frequently in examples and exercises and are worth mastering. Others appear only in demonstrations.\n\n\n\n\n\n\nDemonstrations\n\n\n\nThese lessons contain demonstrations illustrating statistical concepts or data analysis strategies. We will place these in a distinctive box, of which this is an example.\nThe demonstrations will often contain new computer commands that perform tasks used in teaching statistics. However, readers are not expected to be able to construct such commands on their own.\n\n\n\nTraining models with data\n\nlm() arguments: i. tilde expression, ii. data= data frame.\nOccasionally, you will be directed to use glm() or model_train(), which work similarly to lm() but are specialized for models whose output is a probability.\nzero_one() converts a two-level categorical variable to a 0/1 encoding.\n\nSummarizing models. These invariably take as input a model produced by lm() (or glm()) and generate a summary report about that model.\n\ncoef(): displays model coefficients. Each coefficient is a single number.\nconf_interval(): displays model coefficients as an interval with a lower and upper value.\nrsquared() calculates the R2 of a model, and some related measures.\nregression_summary(), like conf_interval(), but with more detail.\n\nEvaluating a model on inputs\n\nmodel_eval() takes a trained model (as produced by lm()) and calculates the model output in both a point form and an interval form. model_eval() can also display the residuals from training or evaluation data.\n\nGraphics\n\nmodel_plot() draws a graphic of a model’s function optionally with prediction or confidence intervals.\ngeom_violin() is a modern alternative to geom_boxplot().\n\nDAGs (directed, acyclic graphs)\n\nsample() collects simulated data from a DAG\ndag_draw() draws a picture of a DAG showing how the variables are connected.\n\nUsed within the summarize() data wrangling function:\n\nvar() computes the variance of a single variable.\n\n\n\n\n\n\n\n\nDemonstration\n\n\n\nHere are some of the command structures that appear in demonstrations. These explanations give a general idea of the tasks they perform.\n\ndo(10) * { command } causes the command to be executed repeatedly the indicated number of times. Such repetitions are useful when the command is a trial of a random process such as sampling, resampling, or shuffling.\nfunction(arguments) { set of commands } packages in a single unit a set of one or more commands. The packaging facilitates using them over and over again with specified arguments.\ngeom_errorbar() works much like geom_point() but draws vertical bars instead of dots. Bar-shaped glyphs depict intervals such as confidence or prediction intervals.\ngeom_ribbon() is like geom_line() but for intervals.\neffect_size() calculates the strength and direction of the input-output relationship between the response variable of a model and a selected one of the explanatory variables."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#directed-acyclic-graphs",
    "href": "Reading-notes-lesson-20.html#directed-acyclic-graphs",
    "title": "20  Simulation and sampling variation",
    "section": "Directed Acyclic Graphs",
    "text": "Directed Acyclic Graphs\nA core tool in thinking about causal connections is a mathematical structure called a “directed acyclic graph” (DAG, for short). DAGs are one of the most popular ways for statistical thinkers to express their ideas about what might be happening in the real world. Despite the long name, DAGs are very accessible to a broad audience.\nDAGs, despite the G for “graph,” are not about data graphics. The “graph” in DAG is a mathematical term of art; a suitable synonym is “network.” Mathematical graphs consist of a set of “nodes” and a set of “edges” connecting the nodes. For instance, figure 20.1 shows three different graphs, each with five nodes labeled A, B, C, D, and E.\n\n\n\n\n\n\n\n(a) undirected graph\n\n\n\n\n\n\n\n(b) directed but cyclic\n\n\n\n\n\n\n\n(c) directed acyclic graph\n\n\n\n\nFigure 20.1: Graphs of various types\n\n\nThe nodes are the same in all three graphs of figure 20.1, but each graph is different from the others. It is not just the nodes that define a graph; the edges (drawn as lines) are part of the definition as well.\nThe left-most graph in figure 20.1 is an “undirected” graph; there is no suggestion that the edges run one way or another. In contrast, the middle graph has the same nodes and edges, but the edges are directed. An excellent way to think about a directed graph is that each node is a pool of water; each directed edge shows how the water flows between pools. This analogy is also helpful in thinking about causality: the causal influences flow like water.\nLook more carefully at the middle graph. There is a couple of loops; the graph is cyclic. In one loop, water flows from E to C to D and back again to E. The other loop runs B, C, D, E, and back to B. Such a flow pattern cannot exist without pumps pushing the water back uphill.\nThe rightmost graph reverses the direction of some of the edges. This graph has no cycles; it is acyclic. Using the flowing and pumped water analogy, an acyclic graph needs no pumps; the pools can be arranged at different heights to create a flow exclusively powered by gravity. The node-D pool will be the highest, E lower. C has to be lower than E for gravity to pull water along the edge from E to C. The node-B pool is the lowest, so water can flow in from E, C, and A.\nDirected acyclic graphs represent causal influences; think of “A causes B,” meaning that causal “water” flows naturally from A to B. In a DAG, a node can have multiple outputs, like D and E, and it might have multiple inputs, like B and C. In terms of causality, a node—like B—having multiple inputs means that more than one factor is responsible for the value of that node. A real-world example: the rising sun causes a rooster to crow, but so can another intruder to the coop.\nOften, nodes do not have any inputs. These are called “exogenous factors”at least by economists. The “genous” means “originates from.” “Exo” means “outside.” The value of an exogenous node is determined by something, just not something that we are interested in (or perhaps capable of) modeling. No edges are directed into an exogenous node since none of the other nodes influence its value.\nFor simulating data, we go beyond drawing a graph of causal connections to outfit DAGs with specific formulas representing the mechanism imbued in each node. DAGs equipped with formulas can be used to generate simulated data.1 Training a model on those data leads to a model function that we can compare to the DAG’s formulas. Then check whether the formulas and the model function match. This practice helps us learn what can go right or wrong in building a model, just as practice in an aircraft simulator trains pilots to handle real-world situations in real aircraft.\nWe start with a simple example, dag08. The dag_draw() command draws a picture of the graph. Printing the dag displays the formulas that set the values of the nodes.\n\ndag_draw(dag08)\n\n\n\n\nThe graph shows that both c and x contribute to y.\n\nprint(dag08)\n\nc ~ exo()\nx ~ c + exo()\ny ~ x + c + 3 + exo()\n\n\nThe formulas show that x and c contribute equally to y, with coefficients of 1. To what extent can regression modeling recover this relationship from data?\nTo find out, we can generate simulated data using the sample() function. For instance,\n\nsample(dag08, size=5)\n\n\n\n \n  \n    c \n    x \n    y \n  \n \n\n  \n    -0.3260365 \n    0.8479298 \n    4.048341 \n  \n  \n    0.5524619 \n    1.1712517 \n    3.928869 \n  \n  \n    -0.6749438 \n    -0.7876782 \n    2.965133 \n  \n  \n    0.2143595 \n    1.1313877 \n    2.878928 \n  \n  \n    0.3107692 \n    0.0875099 \n    3.161596 \n  \n\n\n\n\nEach row in the sample is one trial; in each trial, the node’s formula sets the value for that node. For example, the formula might use the values of other nodes as input. Alternatively, the formula might specify that the node is exogenous, without input from any other nodes.\nModels can be trained on the simulated data using the same techniques as for any other data. To illustrate, here we generate a sample of size \\(n=50\\), then fit the model specification c ~ a + b and summarize by taking the coefficients.\n\nsample(dag08, size=50) %>% \n  lm(y ~ c + x, data = .) %>%\n  coef()\n\n(Intercept)           c           x \n  2.9451445   1.2606473   0.8235923 \n\n\nThe coefficients, including the intercept, are close, but not exactly right.\nIn Lessons 21 and ?sec-lesson-22 we will figure out how close we can expect the coefficients to be to the precise values implemented in the simulation."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#samples-summaries-of-samples-and-samples-of-summaries-of-samples",
    "href": "Reading-notes-lesson-20.html#samples-summaries-of-samples-and-samples-of-summaries-of-samples",
    "title": "20  Simulation and sampling variation",
    "section": "Samples, summaries of samples, and samples of summaries (of samples)",
    "text": "Samples, summaries of samples, and samples of summaries (of samples)\nBeginners sometimes think that each row in a data frame is a sample. Better to say that each row is a “specimen.” A “sample” is a collection of specimens, the set of rows in a data frame.\nThe “sample size” is the number of rows. “Sampling” is the process of collecting the specimens to be put into the data frame.\nThe following command illustrates computing a summary of a sample from dag08.\n\nsample(dag08, size=10000) %>% \n  lm(y ~ c + x, data = .) %>%\n  coef()\n\n(Intercept)           c           x \n  3.0070253   1.0100177   0.9934592 \n\n\nAn essential question in statistics is how the summary depends on the incidental specifics of a particular sample. DAGs provide a convenient way to address this question since we can generate multiple samples from the same DAG, summarize each, and compare those summaries.\nTo generate a sample of summaries, re-run many trials of the summary. The do() function automates this process, accumulating the results from the trials in a single data frame: a “sample of summaries.” We will use do() mostly in demonstrations.\n\n\n\n\n\n\nDemonstration: Conducting many trials with do()\n\n\n\nIn this demonstration, we will revisit a model used earlier in this Lesson to see how much the coefficients vary from one sample to another. Each trial consists of drawing a sample from dag08, training a model, and summarizing with the model coefficients. Curly braces ({ and }) surround the commands needed for an individual trial.\nPreceding the curly braces, we have placed do(5) *. This instruction causes the trial to be repeated five times.\n\ndo(5) * {\n  sample(dag08, size=50) %>% \n    lm(y ~ c + x, data = .) %>%\n    coef()\n}\n\n\n\n \n  \n    Intercept \n    c \n    x \n  \n \n\n  \n    3.019112 \n    0.6794641 \n    1.3353393 \n  \n  \n    3.006728 \n    0.9042066 \n    0.8406397 \n  \n  \n    2.966061 \n    1.1619847 \n    0.9307029 \n  \n  \n    2.866499 \n    1.0881640 \n    1.0769612 \n  \n  \n    3.080889 \n    1.1088753 \n    1.0009938 \n  \n\n\n\n\nThe five trials are collected together by do() into the five rows of a single data frame. Such a data frame can be considered a “sample of summaries.”\n\n\nOne of the things we will do with a “sample of summaries” is to … wait for it … summarize it. For instance, in the following code chunk, a sample of 40 summaries is stored under the name Trials. Then we will summarize Trials, in this case, to see how much the values of the a and b coefficients vary from trial to trial.\n\nTrials <- do(40) * {\n  sample(dag08, size=50) %>% \n    glm(y ~ c + x, data = .) %>%\n    coef()\n} \nTrials %>% \n  summarize(mean_c_coef = mean(c), spread_a = sd(c), \n            mean_x_coef = mean(x), spread_b = sd(c))\n\n\n\n \n  \n    mean_c_coef \n    spread_a \n    mean_x_coef \n    spread_b \n  \n \n\n  \n    0.9858736 \n    0.2215985 \n    1.022228 \n    0.2215985 \n  \n\n\n\n\nThe result of summarizing the trials is a “summary of a sample of summaries.” This phrase is admittedly awkward, but we will use this technique often: summarizing trials, where each trial is a “summary of a sample” Often, the clue will be the use of do(), which repeats trials as many times as you ask."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#causal-inference",
    "href": "Reading-notes-lesson-20.html#causal-inference",
    "title": "20  Simulation and sampling variation",
    "section": "Causal inference",
    "text": "Causal inference\nOften, but not always, our interest in studying data is to reveal or exploit the causal connections between variables. Understanding causality is essential, for instance, if we are planning to intervene in the world and want to anticipate the consequences. Interventions are things like “increase the dose of medicine,” “stop smoking!”, “lower the budget,” “add more cargo to a plane (which will increase fuel consumption and reduce the range).”\nHistorically, mainstream statisticians were hostile to using data to explore causal relationships. (The one exception was experiment, which gathers data from an actual intervention in the world. See Lesson 24.) Statistics teachers encouraged students to use phrases like “associated with” or “correlated with” and reminded them that “correlation is not causation.”\nRegrettably, this attitude made statistics irrelevant to the many situations where intervention is the core concern and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists saw evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, “smoking is associated with lung cancer.” He speculated that smoking and lung cancer might have a common cause, perhaps genetic. Fisher argued that establishing causation requires running an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result.\nFortunately, around 1960, a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. Cornfield’s work was an important step in the development of a new area in statistics: “causal inference.”\nCausal inference is not about proving that one thing causes another but about formal ways to say something about how the world works that can be used, along with data, to make responsible conclusions about causal relationships.\nAs you will see in Lesson 23, DAGs are a major tools in causal inference, allowing you not only to represent a hypothesis about causal relationships, but to deduce what sorts of models will be able to reveal causal mechanisms.\nThe point of a DAG is to make a clear statement of a hypothesis about causation. Drawing a DAG does not mean that the hypothesis is correct, just that we believe the hypothesis is, in some sense, a possibility. Different people might have different beliefs about what causes what in real-world systems. Comparing their different DAGs can help, sometimes, to discuss and resolve the disagreement.\nWe are going to use DAGs for two distinct purposes. One purpose is to inform responsible conclusions from data about what causes what. The data on its own is insufficient to demonstrate the causal connections. However, data combined with a DAG can tell us something. Sometimes a DAG includes a causal connection that should create an association between variables. The DAG is incomplete if the association does not appear in the data.\nDAGs are also valuable aids for building models. For example, analysis of the paths in a DAG, as in Lesson 23, can tell us which explanatory variables to include and which to exclude from a model if our modeling goal is to represent the hypothetical causal connections.\nIn these Lessons, we have a second, entirely different, use for DAGs: learning modeling technique. Our approach will be to ::: {.callout-warning} ## Reality check: DAGs and data\nDAGs represent hypotheses about the connections between variables in the real world. They are a kind of scratchpad for constructing alternative scenarios and, as seen in Lesson 22, thinking about how models might go wrong in the face of a plausible alternative causal mechanism.\nIn this book, we extend the use of DAGs beyond their scope in professional statistics; we use them as simulations from which we can generate data. Such simulations provide one way to learn about statistical methodology.\nDAGs are aides to reasoning, scratchpads that help us play out the consequences of our hypotheses about possible real-world mechanisms. However, take caution to distinguish data from DAG simulations from data from reality.\nFinding out about the real world requires collecting data from the real world. The proper role of DAGs in real work is to guide model building from real data.\nIn this course, we sample from DAGs to learn statistical techniques. But never to make claims about real-world phenomena. :::"
  },
  {
    "objectID": "Reading-notes-lesson-21.html#sec-signal-and-noise",
    "href": "Reading-notes-lesson-21.html#sec-signal-and-noise",
    "title": "21  Signal and noise",
    "section": "Signal and noise",
    "text": "Signal and noise\nTo illustrate the statistical problem of signal and noise, let us turn to a DAG simulation: dag01. Here’s a sample from dag01:\n\nTiny <- sample(dag01, size=2)\n\n\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.3260365 \n    2.836001 \n  \n  \n    0.5524619 \n    5.043052 \n  \n\n\n\n\nThe DAG simulation implements a relationship between x and y. In statistics, this relationship is the signal.\n\nLook at the 2-row sample (?tbl-tiny-dag01) from the DAG and guess what the relationship might be.\n\nAny of an infinite number of possible relationships could account for the x and y data. The noise reduction problem of statistics is to make a guess that is as good as possible. Unfortunately, for a sample with \\(n=2\\), as “good as possible” is not very good!\nMore data—a bigger sample—gives us a better shot at revealing the relationship hidden by the noise. ?tbl-small-dag01 shows a sample of size \\(n=10\\):\n\nSmall <- sample(dag01, size=10)\n\n\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.7859732 \n    1.8888204 \n  \n  \n    0.0547389 \n    4.1153256 \n  \n  \n    -1.1725603 \n    2.3632792 \n  \n  \n    -0.1673128 \n    6.3287614 \n  \n  \n    -1.8650316 \n    0.9329524 \n  \n  \n    -0.1204402 \n    2.9310384 \n  \n  \n    0.8259787 \n    5.6981878 \n  \n  \n    1.1901595 \n    5.9006170 \n  \n  \n    -1.0914519 \n    2.1314570 \n  \n  \n    -0.3751124 \n    4.2296648 \n  \n\n\n\n\nA careful perusal of the Small sample suggests some patterns. x is never larger than about 2 in magnitude and can be positive or negative. y is always positive. Furthermore, when x is negative, the corresponding y value is relatively small compared to the y values for positive x.\nA sample of size \\(n=10\\) provides more information than a sample of \\(n=2\\), so we can make a more informed guess about the relationship between variables x and y.\nHuman cognition is not well suited to looking at long columns of numbers. Often, we can make better use of our natural human talents by translating the sample into a graphic:\n\n\n\n\n\nCollecting more data can make the relationship clearer. figure 21.1 displays an \\(n=10,000\\) sample.\n\nLarge <- sample(dag01, size=10000)\n\n\n\n\n\n\nFigure 21.1: With \\(n=10,000\\) rows, the relationship between x and y is evident graphically. (The original Small sample is shown in orange.)\n\n\n\n\nThere are many possible ways to describe the x-y relationship in figure 21.1. For instance, we can see that when x is positive, y is almost always greater than 4, but for negative x, the value of y tends to be less than 4. Such a description might be apt for some purposes, but in these Lessons, we describe relationships by fitting models to data.\nThe following command uses the small sample (n=10) as training data for a model y ~ x that accounts for y on the basis of x:\n\nlm(y ~ x, data = Small) %>% coef()  # n = 10 sample\n\n(Intercept)           x \n   4.262846    1.741758 \n\n\nThe coefficients provide the information needed to construct the model function: \\[y = 4.26 + 1.74 x\\ .\\] This mathematical formula is a guess of the signal—the relationship between the two variables in dag01. Unfortunately, the formula tells us nothing about the noise obscuring the signal nor how good the guess is.\nThe model coefficients produced by training the model on a much larger sample will presumably be a better guess:\n\nlm(y ~ x, data = Large) %>% coef() #  n = 10,000 sample\n\n(Intercept)           x \n   4.008928    1.495904 \n\n\nUnfortunately, we cannot tell from the coefficients how good the guess is.\nLuckily for us, since the data are a simulation from a DAG, we can see what the coefficients should be as well as the origin of the noise mixing in with the signal.\n\nprint(dag01)\n\nx ~ exo()\ny ~ 1.5 * x + 4 + exo()\n\n\nThe Large sample produced coefficients much closer than the Small sample to the mechanism in the DAG. The idea that larger samples lead to better accuracy has been appreciated since the 16th century and now has the prestige of being a “Law”: the Law of Large Numbers.\nHowever, “better accuracy” does not tell us whether the accuracy suffices for any given purpose. The model filters out some of the noise. However, the model coefficients still display a noisy legacy.\nThe challenge of real-world data is that we cannot open the black box that generated the data; all we have is the data! So how can we tell whether the data at hand are sufficient for giving a usefully accurate description of the actual relationships?\nThe key to the puzzle is the variation within the sample."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#measuring-variation",
    "href": "Reading-notes-lesson-21.html#measuring-variation",
    "title": "21  Signal and noise",
    "section": "Measuring variation",
    "text": "Measuring variation\nLesson 19 introduced the standard way to measure variation in a single variable: the variance or its square root, the standard deviation. For instance, we can measure the variation in the variables from the Large sample using sd() and var():\n\nLarge %>%\n  summarize(sx = sd(x), sy = sd(y), vx = var(x), vy = var(y))\n\n\n\n \n  \n    sx \n    sy \n    vx \n    vy \n  \n \n\n  \n    0.9830639 \n    1.779003 \n    0.9664146 \n    3.164851 \n  \n\n\n\n\nAccording to the standard deviation, the size of the x variation is about 1. The size of the y variation is about 1.7.\nLook again at the formulas that compose dag01:\n\nprint(dag01)\n\nx ~ exo()\ny ~ 1.5 * x + 4 + exo()\n\n\nThe formula for x shows that x is endogenous, its values coming from a random number generator, exo(), which, unless otherwise specified, generates noise of size 1.\nAs for y, the formula includes two sources of variation:\n\nThe part of y determined by x, that is \\(y = \\mathbf{1.5 x} + \\color{gray}{4 + \\text{exo()}}\\)\nThe noise added directly into y, that is \\(y = \\color{gray}{\\mathbf{1.5 x} + 4} + \\color{black}{\\mathbf{exo(\\,)}}\\)\n\nThe 4 in the formula does not add any variation to y; it is just a number.\nWe already know that exo() generates random noise of size 1. So the amount of variation contributed by the + exo() term in the DAG formula is 1. The remaining variation is contributed by 1.5 * x. The variation in x is 1 (coming from the exo() in the formula for x). A reasonable guess is that 1.5 * x will have 1.5 times the variation in x. So, the variation contributed by the 1.5 * x component is 1.5. The overall variation in y is the sum of the variations contributed by the individual components. This suggests that the variation in y should be \\[\\underbrace{1}_\\text{from exo()} + \\underbrace{1.5}_\\text{from 1.5 x} = \\underbrace{2.5}_\\text{overall variation in y}.\\] Simple addition! Unfortunately, the result is wrong. In the previous summary of the Large, we measured the overall variation in y as about 1.72.\nThe variance will give a better accounting than the standard deviation. Recall that exo() generates variation whose standard deviation is 1, so the variance from exo() is \\(1^2 = 1\\). Since x comes entirely from exo(), the variance of x is 1. So is the variance of the exo() component of y.\nTurn to the 1.5 * x component of y. Since variances involve squares, the variance of 1.5 * x works out to be \\(1.5^2\\, \\text{var(}\\mathit{x}\\text{)} = 2.25\\). Adding up the variances from the two components of y gives\n\\[\\text{var(}\\mathit{y}\\text{)} = \\underbrace{2.25}_\\text{from 1.5 exo()} + \\underbrace{1}_\\text{from exo()} = 3.25\\]\nThis result that the variance of y is 3.25 closely matches what we found in summarizing the y data generated by the DAG.\nThe lesson here: When adding two sources of variation, the variances of the individual sources add to form the overall variance of the sum. Just like \\(A^2 + B^2 = C^2\\) in the Pythagorean Theorem."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#dags-from-data",
    "href": "Reading-notes-lesson-21.html#dags-from-data",
    "title": "21  Signal and noise",
    "section": "DAGs from data",
    "text": "DAGs from data\nIn modeling data from dag01 we could recover a good approximation to the formula for y.\n\nLarge %>%\n  lm(y ~ x, data = .) %>%\n  coef()\n\n(Intercept)           x \n   4.008928    1.495904 \n\n\nA DAG describes the causal links between variables. Data modeling reveals the formula implementing the causal link in dag01. Nevertheless, it is wrong to think we can determine the DAG that generated the data from the data alone. Only if we already know the structure of the data-generation DAG can we recover the mechanism inside that DAG. For instance, another statistical thinker might believe that the causal mechanism behind the data is y causing x. Based on this assumption, she also can find the mechanism inside her hypothesized DAG:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .) %>%\n  coef()\n\n(Intercept)           y \n -1.8261448   0.4559782 \n\n\nA DAG is a hypothesis, a statement that might or might not be true. DAGs are part of the statistical apparatus for thinking responsibly about causality. Use a DAG—or, potentially, multiple DAGs—when the issue of what causes what is relevant to the purpose behind the work.\nWhen there are only two variables involved in the system under consideration—we will call them X and Y for simplicity—there are only two possible DAGs:\n\\[X \\rightarrow Y\\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\ X \\leftarrow Y\\] Our understanding of the world sometimes allows us to focus on one of these and not the other. Example: Does the rooster crowing cause the sun to rise, or does the rising sun cause the rooster to crow?\nBeyond the two DAGs \\(X \\rightarrow Y\\) and \\(X \\leftarrow Y\\), additional DAG possibilities can account for the relationship between X and Y. For instance, if we introduce another variable, C, located between X and Y, four other DAGs need to be considered:\n\\[X \\rightarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\leftarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\rightarrow C \\leftarrow Y\\]\nThere are many other DAG configurations involving three variables. To keep things simple, we will restrict things to DAGs where X might or might not cause Y, but Y never causes X.1 figure 21.2 shows the ten configurations of 3-variable DAGs where Y does not cause X.\n\n\n\n\n\nFigure 21.2: Ten DAG configurations involving three variables X, Y, and C.\n\n\n\n\nWith the conceptual tool of DAGs, the statistical thinker can consider multiple possibilities for what might cause what. Sometimes she can discard some of the possibilities based on common sense. (Think: roosters and the sun.) However, in other settings, there may be possibilities that she does not favor but might be plausible to other people. In Lesson 22, we will explore how each configuration of DAG has implications for which model specifications can or cannot reveal the hypothesized causal mechanism."
  },
  {
    "objectID": "Reading-notes-lesson-28.html#all-other-things-being-equal",
    "href": "Reading-notes-lesson-28.html#all-other-things-being-equal",
    "title": "22  Covariates",
    "section": "All other things being equal",
    "text": "All other things being equal\nThe common phrase “all other things being equal” is a critical qualifier in describing relationships. To illustrate: A simple claim in economics is that a high price for a commodity reduces the demand. For example, increasing the price of heating fuel will reduce demand as people turn down thermostats to save money. Nevertheless, the claim can be considered obvious only with the qualifier all other things being equal. For instance, the fuel price might have increased because winter weather has increased the demand for heating compared to summer. Thus, higher prices may be associated with higher demand. Therefore, increased price may not be associated with lower demand unless holding other variables, such as weather conditions, constant.\nIn economics, the Latin equivalent of “all other things being equal” is sometimes used: “ceteris paribus”. So, the economics claim would be, “higher prices are associated with lower demand, ceteris paribus.”\nAlthough the phrase “all other things being equal” has a logical simplicity, it is impractical to implement “all.” So instead of the blanket “all other things,” it is helpful to consider just “some other things” to be held constant, being explicit about what those things are. Other phrases along the same lines are “taking into account …” and “controlling for ….” Those additional variables that are to be considered are called “covariates.\n\n\n\n\n\n\nExample: Covariates and Death\n\n\n\nThis news report appeared in 2007:\n\nHeart Surgery Drug Carries High Risk, Study Says. A drug widely used to prevent excessive bleeding during heart surgery appears to raise the risk of dying in the five years afterward by nearly 50 percent, an international study found. The researchers said replacing the drug—aprotinin, sold by Bayer under the brand name Trasylol—with other, cheaper drugs for a year would prevent 10,000 deaths worldwide over the next five years.\nBayer said in a statement that the findings are unreliable because Trasylol tends to be used in more complex operations, and the researchers’ statistical analysis did not fully account for the complexity of the surgery cases. The study followed 3,876 patients who had heart bypass surgery at 62 medical centers in 16 nations. Researchers compared patients who received aprotinin to patients who got other drugs or no antibleeding drugs. Over five years, 20.8 percent of the aprotinin patients died, versus 12.7 percent of the patients who received no antibleeding drug. [This is a 64% increase in the death rate.] When researchers adjusted for other factors, they found that patients who got Trasylol ran a 48 percent higher risk of dying in the five years afterward. The other drugs, both cheaper generics, did not raise the risk of death significantly.  The study was not a randomized trial, meaning that it did not randomly assign patients to get aprotinin or not. In their analysis, the researchers took into account how sick patients were before surgery, but they acknowledged that some factors they did not account for may have contributed to the extra deaths. - Carla K. Johnson, Associated Press, 7 Feb. 2007\n\nThe report involves several variables. Of primary interest is the relationship between (1) the risk of dying after surgery and (2) the drug used to prevent excessive bleeding during surgery. Also potentially important are (3) the complexity of the surgical operation and (4) how sick the patients were before surgery. Bayer disputes the published results of the relationship between (1) and (2) holding (4) constant, saying that it is also essential to hold variable (3) constant.\nThe total relationship involves a death rate of 20.8 percent of patients who got aprotinin versus 12.7 percent for the patients taking the generic drugs: an increase in the death rate by a factor of 1.64. However, when the researchers looked at a partial relationship (holding constant patient sickness before the operation), the effect size of aprotinin on mortality was less: a factor of 1.48. In other words, the model death ~ aprotinin shows a 64% increase in the death rate, but the model death ~ aprotinin + sickness shows a slightly smaller increase in death rate: 48%. The difference between the two estimates reflects doctors being more likely to give aprotinin to sicker patients.\nThe story’s last paragraph states that the choice of patients receiving aprotinin versus the generic drugs was not made at random. Some readers may find this reassuring. Why in the world would anyone prescribe a drug at random? The point, however, is to select randomly who gets which drug among the patients for whom the drugs would be appropriate. The phrase “randomized trial” used in the paragraph means specifically an experiment in which one treatment or the other—aprotinin versus the generic drugs—is assigned at random. The virtues of experiment and the vital role of random assignment are detailed in Lesson 24.\n\n\n“Significant” has a specialized meaning in statistical language. It is not a synonym for “important.” See Lessons 36 through 38"
  },
  {
    "objectID": "Reading-notes-lesson-28.html#letting-things-change-as-they-will",
    "href": "Reading-notes-lesson-28.html#letting-things-change-as-they-will",
    "title": "22  Covariates",
    "section": "Letting things change as they will",
    "text": "Letting things change as they will\nUsing covariates in models enables the relationship between a response and an explanatory variable to be described ceteris paribus, that is, “all other things being equal.” Another phrase used in news stories is “after adjusting for ….” This is appropriate since the all in “all other things” is, in reality, refers only to those particular factors used as the covariates in the model. So, Dr. Meyer’s foot width results might be stated in everyday language as, “After adjusting for foot width, she found no difference in the widths of girls’ and boys’ feet.”\nNot including covariates in a model amounts to “letting other things change as they will.” In Latin, this is “mutatis mutandis.” In the foot-width example, the model width ~ sex looks at the differences in foot width for the two sexes. However, sex is not the only thing associated with foot width. The model width ~ sex ignores all other factors than sex; it compares boys and girls mutatis mutandis, that is, letting other things change as they will. In this case, comparing boys and girls involves not just the possible differences in foot width but also the differences in other factors such as foot length and body weight.\n\n\n\n\n\n\nExample: One change can bring another\n\n\n\nI was once involved in a budget committee that recommended employee health benefits for the college where I worked. At the time, college employees who belonged to the college’s insurance plan received a generous subsidy for their health insurance costs. Employees who did not belong to the plan received no subsidy but were given a modest monthly cash payment. After the stock market crashed in 2000, the college needed to cut budgets. One proposal called for eliminating the cash payment to employees who did not belong to the insurance plan. Proponents of the plan claimed that this would save money without reducing health benefits. I argued that this claim was an “all other things being equal” analysis: how expenditures would change assuming the number of people belonging to the insurance plan remained constant. In reality, however, the policy change would play out mutatis matandis; the loss of the cash payment would cause some employees, who currently received health benefits through their spouse’s health plan, to switch to the college’s health plan. That is what happened, contributing to an overall increase in healthcare expenses.\n\n\n\n\n\n\n\n\nExample: Spending and student performance\n\n\n\nTo illustrate how covariates set context, consider an issue of interest to public policy-makers in many societies: How much money to spend on children’s education? State lawmakers in the US are understandably concerned with the quality of public education provided. However, they also have other concerns and constraints and constituencies who give budget priority to other matters.\nIn evaluating their various trade-offs, lawmakers could benefit by knowing how increased educational spending will shape educational outcomes. What can available data tell us? Unfortunately, there are various political constraints that work against states adopting and publishing data on a standard, genuine measure of educational outcome. Instead, we have high-school graduation rates, student grades, and other non-standardized data. These data might have some meaning but can also reflect system gaming by administrators and teachers, for which there is little systematic data.\nAlthough imperfect, college admissions tests such as the ACT and SAT provide consistent data between states. For example, figure 22.1 shows the average SAT score in 2010 in each state versus expenditures per pupil in public elementary and secondary schools. Layered on top of the data is a flexible linear model (and its confidence band) of SAT score versus expenditure.\nThe overall impression given by the model is that the relationship is negative, with lower expenditures corresponding to higher SAT scores. However, the confidence band is broad; it is possible to find a smooth path with almost zero slope through the confidence band. Either way, this graph does not support the conventional wisdom that higher spending produces better school outcomes.\n\n\n\n\n\nFigure 22.1: State by state data (from 2010) on average SAT college admissions test scores and expenditures for public education.\n\n\n\n\nOf course, other factors play a role in shaping education outcomes: for instance, poverty levels, parental education, and how the educational money is spent (higher pay for teachers or smaller class sizes? administrative bloat?).\nAt first glance, it is tempting to ignore these additional factors. For instance, we may not have data on them. Moreover, as our interest is in understanding the relationship between expenditures and education outcomes, we are not directly concerned with the additional factors. However, the lack of direct concern does not imply that we should ignore the factors but that we should do what we can to “hold them constant”.\nTo illustrate, consider the fraction of eligible students (those in their last year of high school) who take the college admission test. This fraction varies widely from state to state. In a poor state where few students go to college, the fraction can be tiny (Alabama 8%, Arkansas 5%, Mississippi 4%, Louisiana 8%). In some other states, the large majority of students take the SAT (Maine 93%, Massachusetts 89%, New York 89%). In states with low SAT participation rates, the students who take the test tend to be those applying to schools with competitive admissions. Such strong students will get high scores. In contrast, the scores in states with high participation rates reflect both strong and weak students. Consequently, the scores will be lower on average than in the low-participation states.\nPutting the relationship between expenditure and SAT scores in the context of the fraction taking the SAT is accomplished with the model SAT ~ expenditure + fraction rather than just SAT ~ expenditure. figure 22.2 shows a model with fraction as a covariate.\n\n\n\n\n\nFigure 22.2: The model of SAT score versus expenditures, including as a covariate the fraction of eligible students in the state who take the SAT.\n\n\n\n\nNote that the effect size of spending on SAT scores is positive when the expenditure level is less than $10,000 per pupil. Notice as well that when the fraction taking the SAT is tiny, the average scores do not depend on expenditure. This flat relationship suggests that, among elite students, state expenditure does not make a discernible difference. Perhaps the college-bound students in such states have other educational resources to draw on.\nThe relationship shown in figure 22.1 is genuine. However, so is the very different relationship seen in figure 22.2. How can the same data be consistent with two utterly different displays? The answer, perhaps unexpectedly, has to do with the connections among the explanatory variables. Whatever the relationship between an individual explanatory variable and the response variable, the appearance of that relationship will depend on which covariates the modeler chooses to include."
  },
  {
    "objectID": "Reading-notes-lesson-30.html#block-that-path",
    "href": "Reading-notes-lesson-30.html#block-that-path",
    "title": "23  Confounding",
    "section": "Block that path!",
    "text": "Block that path!\nLet us look more generally at the possible causal connections among three variables, which we will call X, Y, and C. We will stipulate that X points causally toward Y and that C is a possible covariate. Like all DAGs, there cannot be a cycle of causation. These conditions leave three distinct DAGs that do not have a cycle, shown in figure 23.2.\n\n\n\n\n\n\n\n(a) C is a confounder.\n\n\n\n\n\n\n\n(b) C is a mechanism.\n\n\n\n\n\n\n\n(c) C is a consequence.\n\n\n\n\nFigure 23.2: Three different DAGs connecting X, Y, and C.\n\n\nC plays a different role in each of the three dags. In sub-figure (a), C causes both X and Y. In (b), part of the way that X influences Y is through C. We say, in this case, “C is a mechanism by which X causes Y. In sub-figure (c), C does not cause either X or Y. Instead, C is a consequence of both X and Y.1\nTo understand how a DAG informs whether or not to include a covariate, It will help to give general names to some of the sub-structures seen in the figure 23.2 DAGs. ?fig-dag-paths shows some of these sub-structures, removing other links that are not part of the structure.\n\n\n\n\n\n\n\n(a) Direct causal link from X to Y\n\n\n\n\n\n\n\n(b) Causal path from X through C to Y\n\n\n\n\n\n\n\n(c) Correlating path connecting X and Y via C\n\n\n\n\n\n\n\n(d) C as a consequence of X and Y\n\n\n\n\nFigure 23.3: Sub-structures seen in figure 23.2.\n\n\n\nA “direct causal link” between X and Y. There are no intermediate nodes.\nA “causal path” from C to X and on to Y. A causal path is one where, starting at the originating node, flow along the arrows can get to the terminal node, passing through all intermediate nodes.\nA “correlating path” from Y through X to C. Correlating paths are distinct from causal paths because, in a correlating path, there is no way to get from one end to the other by following the flows.\nA “collider” wealth. In other words, both X and Y are causes of C.\n\nLook back to figure 23.2(a), where wealth is a confounder. A confounder is always an intermediate node in a correlating path.\nIncluding a covariate either blocks or opens the pathway on which that covariate lies. Which it will be depends on the kind of pathway. A causal path, as in figure 23.3(b), is blocked by including the covariate. Otherwise, it is open. A correlating path (?fig-dags-path(c)) is similar: the path is open unless the covariate is included in the model. A colliding path, as in figure 23.3(d), is blocked unless the covariate is included—the opposite of a causal path.\nOften, covariates are selected to block all paths except the direct link between the explanatory and response variable. This means do include the covariate if it is on a correlating path and do not include it if the covariate is at the collision point.\nAs for a causal path, the choice depends on what is to be studied. Consider the DAG drawn in figure 23.2(b), reproduced here for convenience:\n\n\n\n\n\ngrass influences illness through two distinct paths:\n\nthe direct link from grass to illness.\nthe causal pathway from grass through wealth to illness.\n\nAdmittedly, it is far-fetched that choosing to green the grass makes a household wealthier, but focus on the topology of the DAG and not the unlikeliness of this specific causal scenario.\nThere is no way to block a direct link from an explanatory variable to a response. If there were a reason to do this, the modeler probably selected the wrong explanatory variable.\nBut there is a genuine choice to be made about whether to block pathway (ii). If the interest is the purely biochemical link between grass-greening chemicals and illness, then block pathway (ii). However, if the interest is in the total effect of grass and illness, including both biochemistry and the sociological reasons why wealth influences illness, then leave the pathway open.\n\n\n\n\n\n\nIn draft: Some resources\n\n\n\nhttps://towardsdatascience.com/causal-effects-via-dags-801df31da794\nhttps://towardsdatascience.com/causal-effects-via-the-do-operator-5415aefc834a"
  },
  {
    "objectID": "Reading-notes-lesson-32.html#replication",
    "href": "Reading-notes-lesson-32.html#replication",
    "title": "24  Experiment and random assignment",
    "section": "Replication",
    "text": "Replication\nTo understand some of the contribution that statistical thinking can make to experiment, recall our earlier definition:\n\nStatistic thinking is the explanation/description of variation in the context of what remains unexplained/undescribed.\n\nA key concept that statistical thinking brings to experiment is the idea of variation. Simply put, a good experiment should involve some variation. The simplest way to create variation is to repeat each experimental trial multiple times. This is called “replication.”"
  },
  {
    "objectID": "Reading-notes-lesson-32.html#example-replicated-bed-net-trials",
    "href": "Reading-notes-lesson-32.html#example-replicated-bed-net-trials",
    "title": "24  Experiment and random assignment",
    "section": "Example: Replicated bed net trials",
    "text": "Example: Replicated bed net trials\nOne way to improve the simple experiment bed net described above is to carry out many trials. One reason is that the results from any single trial might be shaped by accidental or particular circumstances: the weather in the trial area was less favorable to mosquito reproduction; another government agency decided to help out by spraying pesticides broadly, and so on. Setting up trials in different areas can help to balance out these influences.\nReplicated trials also allow us to estimate the size of the variability caused by the accidental or particular factors. To illustrate, suppose a single trial is done and the rate of malarial illness goes down by 5 percentage points. What can we conclude? The result is promising but we can’t rule out that it is due to accidental factors other than bed nets. Why not? Because we have no idea how much unexplained variation is in play.\n\n\n\n:::: {.column-margin}\n\n\n\n?tbl-bed-net shows data from four imagined trials of the effect of bed nets. (Reduction by a negative number, like -1, is an increase.) The mean reduction is 3 percentage points, but this number is not much use unless we can put it in the context of sampling variation. Conducting multiple trials gives us a handle on the amount of sampling variation. By We can easilyNow we know something about the amount of variation due to site-to-site factors. The replication introduces observed variation in results, the observed variation can be quantified and used to place the overall trend in context.\nUsing the regression framework makes it easy to estimate the amount of sampling variation. The mean reduction corresponds to the coefficient from the model reduction ~ 1.\n\nlm(reduction ~ 1, \n     data=Bed_net_data) %>% \n  coef()\n\n(Intercept) \n          3 \n\n\n\nlm(reduction ~ 1, \n     data=Bed_net_data) %>% \n  conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    1 \n    5 \n  \n\n\n\n\nThe observed 3 percentage point mean reduction in the incidence of malaria does stand out from the noise: the confidence interval does not include zero. In these (imagined) data, we have confidence that we have seen a signal."
  },
  {
    "objectID": "Reading-notes-lesson-32.html#control",
    "href": "Reading-notes-lesson-32.html#control",
    "title": "24  Experiment and random assignment",
    "section": "Control",
    "text": "Control\nHowever, there is still a problem with the design of the imagined bed-net experiment. What if the year the experiment was done was unusually dry, reducing the mosquito population and, with it, the rate of malaria infection? Then we don’t know whether the observed 3 point reduction is due to the weather or the bed nets, or even something else, e.g. better nutrition due to a drop in international prices for rice.\nWe need to measure what the change in malarial infection would have been without the bed-net intervention. Care needs to be taken here. If the trial sites were rural, it would not be appropriate to look at malarial rates in urban areas where there was no bed-net program. We want to compare the trial sites with non-trial sites where the intervention was not carried out, so-called “control” sites. The With_controls data frame imagines what data might look like if in half the sites no bed-net program was involved.\n\n\n\n\n\n\n\nTable 24.1:  With_controls \n \n  \n    site \n    reduction \n    nets \n  \n \n\n  \n    A \n    2 \n    control \n  \n  \n    B \n    8 \n    treatment \n  \n  \n    C \n    4 \n    treatment \n  \n  \n    D \n    1 \n    treatment \n  \n  \n    E \n    -1 \n    control \n  \n  \n    F \n    -2 \n    control \n  \n  \n    G \n    0 \n    control \n  \n  \n    H \n    2 \n    treatment \n  \n  \n    I \n    3 \n    treatment \n  \n  \n    J \n    2 \n    control \n  \n\n\n\n\n\nThe proper regression model for the With_controls data is reduction ~ treatment:\n\n\nlm(reduction ~ nets, \n       data=With_controls) %>% \n  coef() \n\n  (Intercept) netstreatment \n          0.2           3.4 \n\nlm(reduction ~ nets, \n       data=With_controls) %>% \n  conf_interval() \n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    -2.200 \n    2.6 \n  \n  \n    netstreatment \n    0.058 \n    6.7 \n  \n\n\n\n\nThe effect of the bed nets is summarized by the netstreatment coefficient, which compares the reduction between the treatment and control groups. In this new (imagined) data frame, the confidence interval on netstreatment touches very close to zero; the signal is just barely discernible from the noise.\nThe reader might wonder why, in moving to the controlled design, the ten sites were not all treated with nets and another ten or so sites found to use as the control. Perhaps, even, the control sites could be selected as villages nearby to the bed net villages.\nOne reason is pragmatic: the larger study would require more effort and money. The larger study might be worthwhile; larger \\(n\\) would presumably narrow the confidence interval. Another reason, to be expanded on in the next section, is that the treatment and control sites should be as similar as possible. This can be surprising hard to achieve. Other factors such as the enthusiasm or skepticism of the town leaders toward public-health interventions might be behind the choice of the original sites for the bed-net program. The control sites might be towns that turned down the original offer of the bed-net program and, accordingly, have different attitudes toward public health."
  },
  {
    "objectID": "Reading-notes-lesson-32.html#example-testing-the-salk-polio-vaccine",
    "href": "Reading-notes-lesson-32.html#example-testing-the-salk-polio-vaccine",
    "title": "24  Experiment and random assignment",
    "section": "Example: Testing the Salk polio vaccine",
    "text": "Example: Testing the Salk polio vaccine\nToday, most children are vaccinated against polio, though a smaller fraction than in previous years. This might be because symptomatic polio is very rare, lessening the perceived urgency of protecting against it. Partly, the reduction reflects the growth in the “anti-vax” movement, which became especially notable with the advent of COVID-19.\nThe first US polio epidemic occurred in 1916, just two years before the COVID-like “Spanish flu” pandemic.1 Up through the early 1950s, polio injured or killed hundreds of thousands of people, particularly children. Anxiety about the disease was similar to that seen in the first year of the COVID-19 pandemic.\nThere were many attempts to develop a vaccine against polio. Jonas Salk created the first really promising vaccine, the promise being based on laboratory tests. To establish the safety and effectiveness of the Salk vaccine, it needed to be tried in the field, with people. Two organizations, the US Public Health Service and the National Foundation for Infantile Paralysis got together to organize a clinical field trial which, all told, involved two-million students in grades 1 through 3.\nThe two studies involved both a treatment and a control group. In some school districts, students in grades 1 and 3 were held as controls. The treatment group was students in grade 2 whose parents gave consent. We will call this “Study 1.” In other school districts, the study design was different: the parents of all students in all three grades were asked for consent. The students with parental consent were then randomly split into two groups: a treatment and a control. Call this “Study 2.”\nThe Study 2 design might seem inefficient; it reduced the number of children receiving the vaccine because half of the children with parental consent were left unvaccinated. On the other hand, it might be that children from families who consent to be given a vaccine are different in a systematic way from children whose families refuse, just as today’s anti-vax families might be different from “pro-vax” families.\nAs reported in Freedman (1998)2, the different risk of symptomatic polio between children from consent versus refuse families became evident in the study. table 24.2 shows the study results from the school districts which used half the consent group as controls.\nThe difference between treatment and control groups is very evident: a reduction from 71 cases per 100,000 children to 28 cases per 100,000. The no-consent children had a rate between the two, 46 per 100,000. Since both the “control” and “no consent” groups did not get the vaccine, one might expect those rates to be similar. That they are not shows that the “no-consent” children are systematically different from those children whose parents gave consent.\n\nIn the other branch of the study, Study 1, where no-consent 2nd-graders were used as control and vaccine was given to all whose parents did consent, the results (table 24.3) were different because of confounding between treatment and consent.\n\n\n\n\nTable 24.2:  Results from Study 2. \n \n  \n    vaccine \n    size \n    rate \n  \n \n\n  \n    Treatment \n    200000 \n    28 \n  \n  \n    Control \n    200000 \n    71 \n  \n  \n    No consent \n    350000 \n    46 \n  \n\n\n\n\n\n\n\n\n\nTable 24.3:  Results from Study 1 \n \n  \n    vaccine \n    size \n    rate \n  \n \n\n  \n    Treatment \n    225000 \n    25 \n  \n  \n    No consent \n    125000 \n    44 \n  \n\n\n\n\n\nThe effect of the vaccine from Study 1 under-estimated the biological link between vaccination and reduction of polio risk."
  },
  {
    "objectID": "Reading-notes-lesson-32.html#random-assignment",
    "href": "Reading-notes-lesson-32.html#random-assignment",
    "title": "24  Experiment and random assignment",
    "section": "Random assignment",
    "text": "Random assignment\nThe example of the Salk vaccine trial is a chastening reminder that care must be taken when assigning treatment or control to the units in an experiment. Without such care, confounding enters into the picture. Merely the possibility of confounding is damaging to the experiment’s result; it invites skepticism and doubt.\n\n\n\n\n\n\nFigure 24.1: A DAG for the polio vaccine experiment.\n\n\n\nIt is illuminating to look at the vaccine trial as a DAG. The essential situation is diagrammed in figure 24.1. The socio_economic node represents the idea that socio-economic status has an influence on susceptibility to symptomatic polio3 and also is a factor in shaping a family’s decision about giving consent.\nThe DAG in figure 24.1 has two pathways between treatment and polio that can produce confounding:\n\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\rightarrow \\mathtt{polio}\\)\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\leftarrow \\mathtt{socio.economic} \\rightarrow \\mathtt{polio}\\)\n\n\n\n\n\n\n\nFigure 24.2: The DAG when consent \\(\\equiv\\) vaccine.\n\n\n\nThe approach emphasized in Lesson 23 to avoid such confounding is to block the relevant pathways. Both can be blocked by including consent as a covariate. However, in Study 1, assignment to vaccine was purely a matter of consent; consent and treatment are essentially the same variable. figure 24.2 shows the corresponding DAG, where consent and treatment are merged into a single variable. Holding consent constant deprives the system of the explanatory variable and still introduces confounding through socio_economic.\nIn Study 2, all the children participating had parents give consent. This means that consent is not actually a variable; it doesn’t vary! The corresponding DAG, without consent as a factor, is drawn in figure 24.3. This Study 2 DAG is unfolded; there are no confounding pathways! Thus the model polio ~ treatment is appropriate.\n\n\n\n\n\n\nFigure 24.3: The Study 2 DAG.\n\n\n\nThe assignment to treatment or control in figure 24.3 is made by the people running the study. Although the DAG doesn’t show any inputs to assignment, the involvement of people in making the assignment opens up a possibility that their assignment of treatment or control might have been influenced by other factors, such as socio-economic status. To guard against this, or even skepticism raised by the possibility, experimentalists have developed a simple safeguard: “random assignment.” In random assignment, assignment is made by a computer generating random numbers. Nobody believes that the computer algorithm is influenced by socio-economic status or any other factor that might be connected to polio in any way.\n\n\n\n\n\n\nUnder construction"
  },
  {
    "objectID": "Reading-notes-lesson-32.html#blocking",
    "href": "Reading-notes-lesson-32.html#blocking",
    "title": "24  Experiment and random assignment",
    "section": "Blocking",
    "text": "Blocking"
  }
]
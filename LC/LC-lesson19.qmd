---
title: "Learning Checks Lesson 19"
---

## Setup

The `math300` package will be needed for lessons 20 through 39.

```{r message=FALSE}
library(math300)
library(moderndive)
library(NHANES)
```


0. Introduce the NULL model. Evaluate the model 

```{r}
mod <- lm(mpg ~ 1, data = mtcars)
model_eval(mod) %>% summarize(explained = var(.output), resid=var(.resid))

```


1.  One of these pipeline commands will work and the other won't. Which one will work? Explain why the other one doesn't work.

    ```{r eval=FALSE}
    lm(net ~ age, data = TenMileRace)
    TenMileRace %>% lm(net ~ age)
    ```

2.  An example from the *OpenIntro* book uses data on promotions. Some data wrangling commands that might be relevant are these:

    ```{r}
    promotions %>% tally()
    promotions %>% group_by(decision) %>% tally()
    promotions %>% group_by(gender) %>% tally()
    promotions %>% group_by(gender, decision) %>% tally()
    ```

You could use such wrangling to compare groups. For instance, you can use the results of the last command to calculate separately the proportion of males who were promoted and, similarly, the proportion of females.

a\. **What are those proportions?**

The following wrangling command will calculate the proportions for you, but it is a bit complicated:

```{r eval=FALSE}
promotions %>%
  group_by(gender) %>%
  summarize(prop_promoted = sum(decision=="promoted") / n())
```

b\. **Use the above command to check your calculations in (a).**

c\. In the regression paradigm, the comparison of proportions between the two groups is done directly in `lm()`, like this:

```{r}
promotions %>%
  mutate(promoted = zero_one(decision, one="promoted")) %>%
  lm(promoted ~ gender, data = .) %>%
  coef()
```

We'll explain the purpose of `zero_one()` in Lesson 19, but putting that matter aside for a moment, compare the two coefficients in the regression model to the proportion results you got from wrangling.

i.  **What does the value of the intercept coefficient correspond to in the wrangling results?**

ii. **What does the `genderfemale` coefficient correspond to in the wrangling results?** (Hint: you will have to do a bit of arithmetic on the wrangling results.)





## 19.1

Consider the `moderndive::evals` data that records students' evaluations (`score`, on a 1-5 scale) of the professors in each of several courses (the course `ID`), as well as the `age`, "average beauty rating" (`bty_avg`) of the professor, enrollment in the course (`cls_students`) and the level o the course (`cls_level`). Each row in the data frame is an individual course section.


```{r echo=FALSE}
set.seed(101)
sample_n(moderndive::evals %>% select(ID, score, age, bty_avg, cls_students, cls_level ),
         size=10) %>% knitr::kable()
```

The following commands model `score` versus `age` and plots the data as a point plot.

```{r}
lm(score ~ age, data = moderndive::evals) %>% coef()
openintro::evals %>% gf_point(score ~ age, alpha=0.2 )
```
1. Explain why some of the dots are darker than others?

::: {.callout-note}
## Solution

All the ages have integer values---e.g., 43, 44, 45---so the dots line up in vertical lines.

Similarly, the scores have values only to one decimal place---e.g., 3.1, 3.2, 3.3---so the dots line up in horizontal lines. If there are two or more rows in `evals` that have the same age and score, the dots will be plotted over one another. Since transparency (`alpha = 0.2`) is being used, points where there is a lot of overplotting will appear darker.
:::

2. Remake the plot, but using `gf_jitter()` instead of `gf_point()`. Explain what's different about the jittered plot. (Hint: Almost all of the dots are the same lightness.)

::: {.callout-note}
## Solution

```{r}
openintro::evals %>% gf_jitter(score ~ age, alpha=0.2 )
```

"Jittering" means to shift each dot by a small random amount. This reduces the number of instances where dots are overplotted.
:::

3. Now make a jitter plot of score versus class level (`cls_level`).
    a. What do the tick-mark labels on the horizontal axis describe? Are they numerical?
    b. To judge from the plot, are their more lower-level than upper-level courses? Explain briefly what graphical feature lets you answer this question at a glance.

::: {.callout-note}
## Solution

```{r}
openintro::evals %>% gf_jitter(score ~ cls_level)
```

a. The tick-mark labels are the levels of the categorical variable `cls_level`. The are words, not numbers.
b. There are many more dots in the right column than in the left. Since `lower` level class are shown in the left column, there are fewer lower-level courses than upper-level courses. 
:::

4. The two columns of points in the plot you made in (3) are not separated by very much empty space. You can fix this by giving `gf_jitter()` an argument `width=0.2`. Try different numerical values for `width` and report which one you find most effective at making the two columns clearly separated while avoiding overplotting.

5. Are the scores, on average, different for the lower- vs upper-level classes? It's hard to get more than a rough idea of the distribution of scores by looking at the "density" of points. The reason is that the number of points differs in the two columns. But there is an easy fix: add a layer to the graphic that shows the distribution (more or less like a histogram displays a distribution of values). You can do this by piping the jitter plot layer into a geom called a "violin," like this:

```{r}
openintro::evals %>% 
  gf_jitter(score ~ cls_level) %>%
  gf_violin(fill="blue", alpha=0.2, color=NA)
```

Explain how to read the violins.

--------

## 19.2

The `openintro::promotions` data comes the the 1970s and records the gender of 38 people along with the result of a decision to promote (or not) the person. =

Chapter 2 of ModernDive suggests graphically depicting `decision` versus `gender` by using a bar plot. There are two ways to make the bar plot, depending on which variable you assign to the horizontal axis and which to the fill color.

```{r}
#| label: fig-promotion-bars
#| fig-cap: "Two different ways to plot promotion outcome and gender"
promotions %>% gf_bar(~ decision, fill=~ gender)
promotions %>% gf_bar(~ gender, fill=~decision)
```

Plots like those in @fig-promotion-bars might be attractive or not, depending on your taste. What they don't accomplish is to make sure which is the response variable and which the explanatory variable.

The choice of response and explanatory variables depends, of course, on what you are trying to display. But everyday English gives a big hint. For instance, you might describe the question at hand as, "Does gender affect promotion decisions." Here, the variable doing the affecting is `gender`, and the outcome is the `decision`.

Modeling decision as a function of gender is easy once you convert the response variable to a zero-one variable. Like this:

```{r}
mod <- lm(zero_one(decision, one="promoted") ~ gender, data = promotions)
coefficients(mod)
mosaicModel::mod_eval(mod)
```

1. Explain what is the relationship between the model coefficients and the model outputs.

::: {.callout-note}
## Solution

The coefficients tell how to calculate the model output. These coefficients say that the model output will be 0.875, but subtract 0.292 if the person is female.

The model outputs give the probability of being promoted for each of the two genders.
:::

2. Make this plot and explain what the red lines show. (We don't expect you to be able to write the command to generate such plots on your own, but we do expect you to be able to interpret them.)

```{r eval=FALSE}
promotions %>% 
  gf_jitter(zero_one(decision) ~ gender, height=0.2, width=0.2) %>%
  gf_errorbar(model_output + model_output ~ gender, data=mod_eval(mod), 
              color="red", inherit=FALSE) %>%
  label_zero_one()
```

::: {.callout-note}
## Solution

The red lines show the proportion of the people in each gender group who were promoted. The y-axis scale on the left refers to the zero-one encoding of `decision`, while the y-axis labels on the right make it easier to read off the numerical value of the proportion.
:::

--------

## 19.3

The `mosaicData::Whickham` data from comes from a survey of a thousand or so nurses in the UK in the 1970s. The data record the `age` of each nurse along with whether the nurse was still alive in a follow-up survey 20 years later (`outcome`).

Make this graph from the `Whickham` data:

```{r}
gf_jitter(zero_one(outcome) ~ age, data = Whickham, alpha=0.3, height=0.1) %>% 
  label_zero_one() 
 
```
1. Explain in everyday language what the graph shows about the lives of humans.

2. Make the graph again, but leave out the `%>% label_zero_one()`. Then explain what `label_zero_one()` does.

::: {.callout-note}
## Solution

1. The graph shows that young nurses tended to be alive at the 20-year follow-up, older nurses not so much.

2. `%>% label_zero_one()` adds an axis on the left of the graph showing that in the zero-one tranform of `outcome`, "Alive" is assigned the value 1 and "Dead" the value 0.
:::

### Solution

--------

## 19.4

About the summarization of models. Pipe the model fit into any of four functions:

i. `%>% coef()`
ii. `%>% regression_summary()`
iii. `%>% rsquared()`
iv. `%>% confint()`

REDO `confint()` so that the columns are named `lower`, `middle`, `upper`

### Solution


--------

## 19.5 (Obj. 19.3)

Calculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We'll use the `CO2city` variable in the `math300::MPG` data frame. The basic calculation using the `mosaic` package is:


```{r kangaroo-freeze-candy-1, echo = TRUE}
df_stats( ~ CO2city, data = math300::MPG, coverage(0.95))
```

The following figure shows a violin plot of `CO2city` which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.

1. 50% coverage interval -A- (c)
2. 75% coverage interval -A- (e)
3. 90% coverage interval -A- (g)
4. 100% coverage interval -A- (i). This extends from the min to the max, so you could have figured this out just from the figure.

```{r kangaroo-freeze-candy-2, echo = FALSE}
Res <- NULL
levels <- c(0.2, 0.4, 0.5, 0.6, 0.75, 0.80, 0.90, 0.95, 1.00)
letters <- base::letters[1:length(levels)]
for (k in 1:length(levels)) {
  new <- df_stats( ~ CO2city, data = math300::MPG, coverage(!!levels[k]))
  new$id <- paste0("(", letters[k], ")")
  new$x <- 0.75 + k/20
  Res <- bind_rows(Res, new)
}
gf_violin(CO2city ~ 1, data = math300::MPG, alpha = 0.25, fill = "blue", color = NA) %>%
  gf_errorbar(lower + upper ~ x, data = Res, inherit=FALSE) %>%
  gf_text(upper ~ x, label = ~ id, data = Res, nudge_y = 30) %>%
  gf_lims(x = c(.5, 1.5))
```


--------

## 19.6 (Obj 19.3)

The two jitter + violin graphs below show the distribution of two  different variables, X and Y. Which variable has more variability?



```{r goat-take-linen-1, echo = FALSE, fig.show = "hold", out.width = "50%"}
set.seed(101)
Data <- data.frame(x = rnorm(500, mean = 50, sd = 10)) %>%
  mutate(y = 7 * round(x /  7), pos = " ")
gf_jitter(x ~ pos, data = Data, seed = 400, width = 0.2, alpha = 0.5) %>%
  gf_violin(fill = "gray", color = NA, alpha = 0.5) %>%
  gf_labs(y = "Variable A", x = "", title = "(a)") %>%
  gf_lims(y = c(15, 85))
gf_jitter(y ~ pos, data = Data, height = 1,  seed = 400, alpha = .5) %>%
  gf_violin(fill = "gray", color = NA, alpha = 0.5, bw = 2) %>%
  gf_labs(y = "Variable B", x = "", title = "(b)")   %>%
  gf_lims(y = c(15, 85))
```

::: {.callout-note}
## Solution

There is about  the same level of variability in variable A and variable B. This surprises some people. Remember, the amount of variability has to do with the spread of *values* of the variable. In variable B, those values are have a 95% prediction interval of about 30 to 65,  about the same as for variable A. There are two things about plot (b) that  suggest to many people that there is more variability in  variable B. 

1. The larger horizontal spread of the dots. Note that variable B is shown along the vertical axis. The horizontal spread imposed by  jittering is completely arbitrary: the only values that count are on the y axis.  
2.  The  scalloped, irregular edges of the violin plot. 

On the other hand, some people look at the clustering of the data points in graph (b) into several discrete values, creating empty spaces in between. To them, this clustering implies less variability. And, in a way, it does. But the *statistical* meaning of variability has to do with  the overall spread of the  points, not whether they are restricted to discrete values.   

:::

--------

## 19.7 (Objs. 19.3 & 19.4)

The graphs below show a violin plot of body mass index (BMI) for adults and children. One of the graphs shows a correct 95% coverage interval on BMI, the other does not.

Identify the incorrect graph and say what feature of the graph led to your answer.


```{r goat-hurt-painting-1, echo = FALSE}
Dat <- NHANES %>%
  dplyr::select(BMI, Age) %>%
  mutate(age_group = ifelse(Age < 18, "Child", "Adult")) %>%
  na.omit() 
Intervals <- df_stats(BMI ~ age_group, data = Dat, 
                      ci = coverage(0.95))
P <- gf_violin(BMI ~ age_group, data = Dat, alpha = 0.2, fill = ~ age_group, color = NA) %>%
  gf_theme(legend.position="none",axis.text = element_text(size = 20), 
           plot.title = element_text(size = 30)) %>%
  gf_labs(y = "Body Mass Index", x = "")
Bogus_intervals <- Intervals %>% mutate(age_group = ifelse(age_group == "Child", "Adult", "Child"))

P %>% 
  gf_errorbar(ci_lower + ci_upper ~ age_group, data = Bogus_intervals,
              width = 0.2, size = 2, color = ~ age_group, inherit=FALSE) %>%
  gf_labs(title = "(a)")
P %>% 
  gf_errorbar(ci_lower + ci_upper ~ age_group, data = Intervals,
              width = 0.2, size = 2, color = ~ age_group, inherit=FALSE) %>%
  gf_labs(title = "(b)")
```

::: {.callout-note}
## Solution
Graph (b) is correct. In graph (a), you can see that the interval fails to include a lot of the low BMI children and extends too high. For adults, the graph (a) interval extends too far low and doesn't go high enough.
:::

--------

## 19.E

There are two equivalent formats describing an interval numerically that are widely used:

i. Specify the lower and upper endpoints of the interval, e.g. 7 to 13.
ii. Specify  the center and half-width of the interval, e.g. 10 ± 3, which is  just the same as 7 to 13.

Complete the following table to show the equivalences between the two notations.

```{r rabbit-put-pen-1, echo=FALSE}
This_table <- tribble(
~ Interval, ~ "bottom-to-top", ~ "plus-or-minus",
"(a)", "3 to 11", "",
"(b)",  "" , "108 ± 10",
"(c)", "", "30 ± 1",
"(d)",  "97 to  100"  , "",
"(e)",  "-4 to  16"   , "",
)
knitr::kable(This_table)
```

::: {.callout-note}
## Solution

a. 7 ± 4
b. 98 to 118
c. 29 to 31
d. 98.5 ± 1.5
e. 6 ± 10

It's a matter of judgement which format to use. The bottom-to-top notation highlights the range of the interval  while the plus-or-minus notation emphasizes the center of the interval. As a rule of thumb, I suggest this:

* If the first two digits are different between the top and bottom of the interval, use the bottom-to-top notation. So,  write 387 to 393.  If the first two digits are the same, use plus-or-minus. For instancer, the ratio of the mass of the Earth to that of the Moon is 81.3005678 ± 0.0000027. This is easier to take in at a glance than the equivalent 81.3005651 - 81.3005708

:::

--------

## 19.F

::: {.callout-warning}
## Still in draft

Suppose there are other explanatory variables to be displayed. In that case, we will use color and faceting. If there are *no* explanatory variables, as in `y ~ 1`, we will jitter the data horizontally to avoid overplotting.]
:::


--------

## Demonstration: Predicting calorie content

Starbucks is a famous coffee-shop franchise with more than 30,000 branches (as of 2021). People go to Starbucks for coffee, but they often buy something to eat as well. In this demonstration, we will look at the calorie content of Starbucks' food offerings. As always, when conducting a statistical analysis, it is helpful to have in mind the motivation for the task. So we will imagine, tongue in cheek, that we want to make food recommendations for the calorie-conscious consumer.

First, a **point summary** of the calories in the different types of food products available at Starbucks:

```{r echo=FALSE}
point_summary <- 
  df_stats(calories ~ type, 
           data = openintro::starbucks, mean)
```

```{r}
df_stats(calories ~ type, 
         data = openintro::starbucks, mean)
```

This summary supports the sensible advice to choose salads or smaller portions (type "petite") to avoid calories. One might go further, for example, concluding that a sandwich is a poor choice (in terms of calorie content), so lean toward parfaits or hot breakfasts. We can even imagine someone concluding from this summary that a bistro box is a better calorie-conscious choice than a sandwich.

@fig-starbucks-food shows the point summary, using the raw data to put things in context.

```{r}
#| label: fig-starbucks-food
#| fig-cap: "Calories of the various food items sold by Starbucks, annotated with point and interval summaries."
#| fig-cap-location: margin
#| code-fold: true
openintro::starbucks %>% 
  ggplot(aes(x=type, y=calories)) +
  geom_jitter(width=0.2, alpha=0.5) +
  geom_errorbar(data=point_summary, aes(ymin=mean, ymax=mean), 
                y=NA, color="blue") +
  geom_point(data=point_summary, aes(y=mean), color="red")
```

Plotting the point summary in the context of the raw data shows at a glance that the point summary is not of any genuine use. For instance, using the point summary without the data, we might conclude that hot breakfasts are better than sandwiches. However, the data display suggests otherwise; there is just one low-calorie breakfast. The others are much like sandwiches.

A point summary is compact but cannot represent the *variation* within each food type. An interval summary, as in @fig-starbucks-food2, does show this variation. 

```{r}
#| label: fig-starbucks-food2
#| fig-cap: "Calories of the various food items sold by Starbucks, annotated with point and interval summaries."
#| fig-cap-location: margin
#| code-fold: true
openintro::starbucks %>% 
  ggplot(aes(x=type, y=calories)) +
  geom_jitter(width=0.2, alpha=0.5) +
  geom_errorbar(data=point_summary, aes(ymin=mean, ymax=mean), 
                y=NA, color="blue") 
```

Unlike point summaries, interval summaries can overlap. Such overlap indicates that the groups are not all that different. Here, the interval summary indicates an appropriate conclusion; "Don't make your diet choices based on food type. Look at the calorie content of individual items before choosing."

Admittedly, in this simple setting the data themselves would lead to the conclusion. However, as we move into more complicated settings, it will become infeasible to see patterns quickly straight from the data. 




----------


-----------

# Original scatter notes




Here's a graph of the data in our standard response-vs-explanatory graphic frame:

```{r}
#| column: margin
Whickham %>%
  ggplot(aes(x = smoker, y = outcome)) +
  geom_jitter(width=0.2, height=0.2, alpha=0.5)
```

The graph suggests that non-smokers were more likely than smokers to be dead at the follow-up interview. But it's hard to calculate proportions from such a graph. It's reasonable to argue that for the purpose of showing the fraction of smokers and of non-smokers who died, a bar chart would be better.

```{r warning=FALSE}
#| label: fig-smoker-barplot
#| fig-cap: "Barplots of the `Whickham`smoking and survival data."
#| fig-subcap:
#|   - "counts"
#|   - "proportions"
Whickham %>%
  ggplot(aes(x=smoker, fill=outcome)) +
  geom_bar()
Whickham %>%
  ggplot(aes(x=smoker, fill=outcome)) +
  geom_bar(position = "fill") +
  ylab("Proportions")
```

The left barplot, showing counts, suggests that a higher proportion of non-smokers died than of smokers. But its easy to instruct the `geom_bar()` to graph proportions rather than counts, as done in the left plot. This makes it easy to conclude at a glance that a higher proportion of non-smokers have died.

The important question here, "Does smoking affect mortality?" translates well into the response/explanatory paradigm: `outcome` is the response variable while `smoker` is the explanatory variable. In the jitter-plot presentation of the data, these assignments are clearly indicated in the computer commands, which set `x=smoker, y=outcome`. In the barplot, a different notation is used: `x=smoker, fill=outcome`.

Unfortunately, neither of the graphic styles---jitter or boxplot---answers the important question. At best they provide a description of the nurses in the `Whickham` data frame.

To answer the important question, we need to invoke statistical thinking. In particular, we need an *interval summary* of the proportion who died, not the point summary produced by the barplot. 

This doesn't mean that we can't easily calculate the proportions from the categorical response variable: we just have to use the right commands. for instance:

```{r}
Whickham %>%
  df_stats(outcome ~ smoker, prop, ci.prop)
```

The point summary---the `prop_Alive` column---suggests an obvious difference between the smokers and non-smokers. The interval summary---columns `lower` and `upper`---tempers this conclusion a little: the intervals almost touch.

Although regression is our go-to technique for modeling relationships between variables, we can't use it directly on a categorical response variable. 

::: {.callout-warning}
Here's what happens if we try:

```{r}
lm(outcome ~ smoker, data = Whickham) %>% confint()
```

The computer's warning message is a reminder that the response variable is categorical. (The message uses the phrase "factor response," which is just computerese for "categorical response.")
:::

To use regression with a two-level categorical response variable, transform it into a zero-one encoding. In the following, we'll use 1 to represent `"Alive"` and 0 to represent `"Dead"`, although we can equally well do things the other way around.

```{r}
lm(zero_one(outcome, one="Alive") ~ smoker, data = Whickham) %>% 
  confint()
```

You don't yet know enough to interpret this interval summary. That will have to wait until Lesson 24. The significant^[In lesson 38 you'll learn to be wary whenever a statistician uses the word "significant."] feature of the interval on `smokerYes` is that it does not include zero. In everyday terms, the interval means, "Smokers are 3 to 12 percentage points more likely to survive for 20 years than are non-smokers."

Using interval summaries instead of point summaries is an important aspect of statistical thinking, but there are other aspects that need to be taken into account. A simple, but important, question is whether the nurses recorded in the `Whickham` data frame are good representatives of all smokers. (It turns out that the nurses in `Whickham` are all women interviewed in the 1970s. At that moment of history, women were very different than men when it comes to smoking, and the Whickham smokers were also very different from today's female smokers. We'll say more about this in the demonstration below.)

Statistical thinking also leads one to ask another sort of question: What else might be going on other than smoking? In technical language, the other-goings-on are called "**covariates**," the topic of Lessons 28 & 29.

For instance, you might wonder about the overall result from our brief examination of the `Whickham` data. Is it really the case that the smokers were more likely to survive than the non-smokers? The answer is "yes," as we have demonstrated from the previous analysis. But this answer is completely misleading. Tobacco companies worked hard to mislead people into thinking that smoking was not dangerous. They knew full well the negative health consequence of smoking, but they used statistical-sounding claims to hide this knowledge from the public.

In the following demonstration, we'll look at the `Whickham` data again using the power of regression models to incorporate covariates.

::: {.callout-note}
## Demonstration: Smoking with covariates

*Remember that you are not expected to master the calculations in these demonstrations. Focus your attention on the output from the calculations.*

It goes without saying that smoking is not the only thing that kills people. There are other risky behaviors such as heavy drinking, there's environmental exposure to pollutants, and there's disease (other than the smoking induced ones of lung cancer, emphysema, and high blood pressure). But there's one risk factor for death that everyone knows about but nobody is doing anything about: getting old.

In virtually every public health or clinical study, the participant's age is taken into account. Not doing so can produce a completely misleading view of the situation. This is also the case with smoking and mortality in the `Whickham` study.

Regression techniques enable us to take multiple explanatory variables into account. In this demonstration, we'll use regression to study `outcome` as a function of `smoker` and, importantly, `age`.

To get started, we need to remember to convert the categorical `outcome` variable into a zero-one encoding. After that, building the model is not so hard.

```{r}
survival_model <- model_train(zero_one(outcome, one="Alive") ~ age + smoker,
                              data=Whickham)
```

From this model, we can read off an interval summary of the effect of smoking on survival:

```{r message=FALSE}
survival_model %>% confint()
```

A full understanding of this interval summary will need to wait until Lessons 22 through 24. For the present, we'll simply point out that the summary interval on `smokerYes` includes zero, so `Whickham` provides no support for the mistaken conclusion that smoking improves survival. But seeing this requires taking into account `age`. A graphic may help explain why:

```{r}
Model_output <- model_eval(survival_model, interval="confidence")
Model_output %>%
  ggplot(aes(y = .response, ymin=.lwr, ymax=.upr, x=age, color=smoker, fill=smoker)) +
  geom_jitter(height=.1, width=0, alpha=0.2) +
  geom_ribbon(alpha=0.2) 
```

The interval summary in the graph shows how the probability of survival changes for different ages. The intervals for non-smokers and smokers entirely overlap. For both groups, 20-year survival goes down with greater initial age. So why did the model `outcome ~ smoker` suggest that smokers have a higher survival? The reason relates to the proportion of smokers with initial age 70+. In the 1970s, life expectancy was such that people 70+ were unlikely to survive 20 years. This pulls down the survival rate at that age. Notice that the 70+ nurses were unlikely to have been smokers compared to younger nurses. The 70+ nurses grew up in an era when social conventions caused smoking to be uncommon for women (even though it was very common for men).

:::



::: {.callout-warning}
## In draft

Make a jitter plot of gestation period versus smoking status. Then find the mean for each smoking status show this table. Then the ci.mean() as a table and graphed as an interval.

```{r}
values <- lm(gestation ~ smoke, data = Gestation) %>% 
  model_eval(interval="confidence", skeleton=TRUE)
Gestation %>% ggplot(aes(x = smoke, y = gestation)) +
  geom_jitter(width=0.2, alpha=0.1) +
  geom_errorbar(data = values, aes(x=smoke, ymin=.lwr, ymax=.upr), y=NA)
```
:::
    
## Objectives

While in draft, see `Objectives/Obj-lesson-19.qmd`. Those will be copied over here.

## A standard format for data graphics

A "data graphic" is one that displays each of the rows of a data frame. Graphics are fundamentally two-dimensional, so the data graphic has a frame that maps one variable to the vertical axis ("the y aesthetic") and another to the horizontal axis ("the x aesthetic"). 

Models are important to extracting information from data. Models always have a **response variable** and one or more **explanatory variables**. So our standard format for data graphics will put the response variable on the vertical axis and the one of the explanatory variables on the horizontal axis.

### Example: Scottish hill racing

A popular competitive sport in Scotland is hill racing. This is a running race that involves ascending a hill rather than running on the flat. The `math300::Hill_racing` data frames records about 2000 winning performances in hill races. (See `?Hill_racing` for the documentation.)

1. A simple model of a hill race performance is `time ~ distance`. 

    a. Which is the response variable and which is the explanatory variable in the model `time ~ distance`?
    
    **ANSWER**:
    

    b. Make a data graphic from `Hill_racing` that is consistent with this choice of explanatory and response variables.
    
```{r}
# complete the code
# Hill_racing %>%
#  ggplot(aes(x=_____, y=______)) +
#  geom_point()
```


::: {.callout-note}
## Solution
```{r}
# complete the code
Hill_racing %>%
 ggplot(aes(x=distance, y=time)) +
 geom_point() 
```
:::

### The "Null" model

The basis of the techniques you'll learn in this second half of the course is building a model that uses explanatory variables to account for the response variable.

We'll extract various quantities from the models we build and, in particular, use those quantities to compare one model to another, the point being to see SAY WHAT.

Surprisingly, an important model for starting the chain of comparisons *has no explanatory variable*. OR RATHER, WE MAKE UP AN EXPLANATORY variable. SHOW MEAN is a coefficient from such a model. Model formula: `y ~ 1`

Let's use the standard data-graphic format to display to display `y ~ 1`,

```{r}
Whickham %>% mutate(group = "all") %>% ggplot(aes(y=age, x=group)) + geom_jitter()
```


2. A somewhat more complex model is `time ~ distance + climb`. In this model there are two explanatory variables. Only one of them can be mapped to the horizontal axis, the other will need to be mapped to some other aesthetic or to faceting.

```{r}
# complete the code
# Hill_racing %>%
#  ggplot(aes(x=_____, y=______)) +
#  geom_point()
```

------------




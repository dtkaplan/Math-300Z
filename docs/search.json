[
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "",
    "text": "The entire box is 14.43 “inches” long. This should be the total of the left, right, and middle measurements, but people tended to round.\n\nThirds <- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT_asFV5LD312bYaGgHK3F91kgLVSiaQpNhggDilfPKAiDBNz9iueOiYWKgAtRRwkFlOz6U9znbiMGK/pub?gid=0&single=true&output=csv\")\n\nRows: 16 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Student_initials\ndbl (3): left, middle, right\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nLong_form <- tidyr::pivot_longer(Thirds, !Student_initials, names_to = \"position\")\nLong_form |> group_by(Student_initials) |> summarize(tot = sum(value), v = var(value))\n\n# A tibble: 16 × 3\n   Student_initials   tot        v\n   <chr>            <dbl>    <dbl>\n 1 AMA               13.8 0.0208  \n 2 BAK               14.5 0.271   \n 3 DQS               14.4 0.416   \n 4 EDS               16   0.646   \n 5 EFSF              14.1 0.00750 \n 6 EJD               13.8 0.271   \n 7 HAM               14.2 0.188   \n 8 HJB               14.3 0.000833\n 9 IMW               14   0.396   \n10 JK                14.2 0       \n11 JTSF              14.8 0.271   \n12 KDL               14.2 0.0625  \n13 KZC               14   0.583   \n14 RHP               14.2 0       \n15 RRP               14.2 0.188   \n16 SJA               14.5 1.65    \n\nLong_form |> summarize(vmeas = var(value))\n\n# A tibble: 1 × 1\n  vmeas\n  <dbl>\n1 0.240\n\nlm(value ~ position, data=Long_form) |> conf_interval()\n\n# A tibble: 3 × 4\n  term             .lwr .coef  .upr\n  <chr>           <dbl> <dbl> <dbl>\n1 (Intercept)     4.29  4.50  4.70 \n2 positionmiddle  0.392 0.678 0.964\n3 positionright  -0.126 0.159 0.445\n\nggplot(Long_form, aes(y=value, x=position)) + geom_jitter(width=0.2, alpha=0.5)"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#variation",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#variation",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Variation",
    "text": "Variation\nRemember that statistics focus on variation in the characteristics of a set of multiple specimens. The characteristics of each individual specimen are recorded in a row of a data frame. The data frame itself, with its multiple rows, represents the set of specimens. Each characteristic is arranged as a column in the data frame. We call such columns “variables,” a name that emphasizes that our particular interest is to understand/explain/account-for the variation of the values stored in the column.\nIn a regression model, we attempt to understand/explain/account-for the variation in a single variable, called the “response variable.” We accomplish this explanation by associating the variation in the response with the simultaneous variation in other variables called “explanatory variables.”\nThe lm() model-building function does the work of quantifying the associations. Your task in model building is to provide data for training and to specify which are the explanatory variables you want to use to account for the variation in the response variable. The specification takes the form of a tilde expression listing the response and explanatory variables. All these variables must be in the data frame used for training. We say such variables are “observed.”\nThere are usually other characteristics that are relevant to the system being studied that are not observed, that is, they are not in the data frame. It’s a bad idea to ignore such things."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#starting-lesson-20",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#starting-lesson-20",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Starting Lesson 20",
    "text": "Starting Lesson 20\nToday is a meta-day. It is about tools for learning about statistical methods and gaining insight into why certain kinds of questions/techniques come up over and over again as you work on genuine statistical problems.\nThe two kinds of tools for learning are:\n\nTools for thinking and communicating about hypotheses about causal connections.\n\nDiagrams called “DAGs” for sketching out causation.\nGenerating random, simulated data consistent with the mechanism described by a DAG.\n\nWays to automate the process of random trials. This is purely a labor-saving measure. You are not responsible to generate the code for this automation, but you should learn to read the code to be able to say what’s going on."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#causation-examples",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#causation-examples",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Causation examples",
    "text": "Causation examples\n\nSystolic blood pressure in the elderly:\n\nExperiment shows that lowering SBP reduces mortality.\nObservation shows that lower SBP is associated with increased mortality.\n\nCongressional elections\n\nAmong incumbents, higher election spending is associated with worse vote outcomes.\n\nVitamin D and disease\n\nLow vitamin linked to adverse outcomes in many diseases\nIll people go outside less often so are less exposed to sunlight AND Vitamin D is an acute phase reactant and declines with the inflammatory cytokine rise in acute and chronic diseases AND No evidence from randomized trials that vitamin D supplementation lessens mortality risks in such conditions.\nBring up article"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#directed-acyclic-graphs-dags",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#directed-acyclic-graphs-dags",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Directed acyclic graphs (DAGs)",
    "text": "Directed acyclic graphs (DAGs)\nA DAG is a format for writing down which characteristics, either observed or unobserved, are important in the operation of a system.\nA good dictionary definition of “system” is:\n\nA set of things working together as parts of a mechanism or an interconnecting network.\n\nGraphs, Directed, Acyclic"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#sampling-from-dags",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#sampling-from-dags",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Sampling from DAGs",
    "text": "Sampling from DAGs\nIn Math 300Z, DAGs have been augmented with a simulation mechanism. This consists of formulas that are invoked to create each variable in the DAG."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#activity-life-savers",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#activity-life-savers",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Activity: Life Savers",
    "text": "Activity: Life Savers"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#repeating-trials",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#repeating-trials",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Repeating trials",
    "text": "Repeating trials\n\nfoo <- do(100000)*sum(runif(10))\nggplot(foo, aes(x=\" \", y = sum)) + geom_violin(alpha=0.5)\n\n\n\n\n\nTrials <- do(100) * {lm(x ~ y, data=sample(dag03, size=5)) |> R2()}"
  },
  {
    "objectID": "Day-by-day/Lesson-22/counts-and-waiting-times.html#counts-and-waiting-times",
    "href": "Day-by-day/Lesson-22/counts-and-waiting-times.html#counts-and-waiting-times",
    "title": "Spring 2023 Math 300Z",
    "section": "Counts and waiting times",
    "text": "Counts and waiting times\nGENERATE SEQUENCES FROM POISSON and EXPONENTIAL.\nFirst, generate a sample of size 1: find the largest and smallest across the class.\nThen generate a sample of size 20:\n\nfind the largest and smallest. are they pretty consistent across the class?\n\nfind the variance. Is that pretty consistent across the class?\n\nfind the mean:\n\nis that as spread out as the largest and smallest?\nhow spread out is it? (variance)\n\n\nKeep a table\n\n\n\nsample size\nvariance\nstandard deviation\n\n\n\n\n20\n\n\n\n\n20\n\n\n\n\n20\n\n\n\n\n40\n\n\n\n\n80"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "",
    "text": "Open your Z-section project in Posit.cloud. Use get_lesson_worksheet(22) to bring in today’s worksheet.\n\nOpen the worksheet and leave it for later in class.\nCreate a new Rmd file, say, \"Scratch22.Rmd\". We’ll do scratch work there."
  },
  {
    "objectID": "Day-by-day/Lesson-24/with-respect-to.html#with-respect-to",
    "href": "Day-by-day/Lesson-24/with-respect-to.html#with-respect-to",
    "title": "Spring 2023 Math 300Z",
    "section": "With respect to …",
    "text": "With respect to …\nBuild a model with a single quantitative explanatory variable. Maybe palmerpenguins::penguins\n- What is the effect size?\n- What are the units? Is it a rate or a difference?\nBuild a model with a single categorical explanatory variable.\n- What is the effect size?\n- What are the units? Is it a rate or a difference?\nCombine the two and calculate effect sizes again.\n- Do the effect sizes change?\n- Do the units change? \n- Is it still a rate?\nInclude many explanatory variables: Find the effect sizes with respect to each."
  },
  {
    "objectID": "Day-by-day/Lesson-24/Teaching-notes-24.html",
    "href": "Day-by-day/Lesson-24/Teaching-notes-24.html",
    "title": "Teaching notes, Lesson 24",
    "section": "",
    "text": "Our emphasis will be on the data-analysis techniques that are useful for decision-making. What this means will become evident as we move through the semester, but I’ll give two settings for decision-making:\n\nIntervention in a system. (Lesson 24) There’s a system that you are working with, perhaps aircraft design. You want to intervene in the system, to change something. You need to know how the change you impose will change the outcome.\n\nUsually, we are interested in predictions that cover most of the likely outcomes.\nSometimes, we want to predict the probabilities of extreme events, for instance thousand-year floods.\n\nPrediction. (Lessons 25 and 26)"
  },
  {
    "objectID": "Day-by-day/Lesson-23/got-you-covered.html#got-you-covered",
    "href": "Day-by-day/Lesson-23/got-you-covered.html#got-you-covered",
    "title": "Spring 2023 Math 300Z",
    "section": "Got you covered",
    "text": "Got you covered\nSimulate from a DAG, use the data to train a model that has all the same explanatory variables as in the formula for y in the DAG, and calculate the confidence interval @ 95%\nDid everybody’s interval include the parameter from the DAG?\nMove to an 80% interval. About 4 students should not cover the parameter.\nMove to a 50% interval. About 10 students should not cover the parameter.\nMove to a 100% interval. What do the results tell you?\nFrederick the Great said, “To defend everything is to defend nothing.” paraphrase as “To try to cover everything is to cover nothing.”"
  },
  {
    "objectID": "textbooks.html",
    "href": "textbooks.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "For Lessons 1-17, the textbook is Statistical Inference via Data Science (“ModernDive”)"
  },
  {
    "objectID": "textbooks.html#lessons-19-onward",
    "href": "textbooks.html#lessons-19-onward",
    "title": "Spring 2023 Math 300Z",
    "section": "Lessons 19 onward …",
    "text": "Lessons 19 onward …\n\n\nFor Lessons 19-39, the textbook is Lessons in Statistical Thinking"
  },
  {
    "objectID": "math300-software.html",
    "href": "math300-software.html",
    "title": "R package for Math 300Z",
    "section": "",
    "text": "Most students will want to use POSIT.cloud. Use the “Z-Section” project."
  },
  {
    "objectID": "math300-software.html#accessing-the-daily-worksheet",
    "href": "math300-software.html#accessing-the-daily-worksheet",
    "title": "R package for Math 300Z",
    "section": "Accessing the daily worksheet",
    "text": "Accessing the daily worksheet\nAlmost all class days there will be a worksheet in the form of an Rmd file.\nFrom within your POSIT.cloud project, download the Rmd file using this command, substituting the number of the relevant Lesson:\nmath300::get_lesson_worksheet(19)\n\nMake a habit of reading each day’s Rmd file before class. This way you can note what doesn’t yet make sense so that you can be receptive to the topic in class.\nComplete the worksheet after class."
  },
  {
    "objectID": "math300-software.html#math-300z-r-commands",
    "href": "math300-software.html#math-300z-r-commands",
    "title": "R package for Math 300Z",
    "section": "Math 300Z R commands",
    "text": "Math 300Z R commands\nThere will be only a dozen commands that you will be using in the second half of Math 300Z. Almost all of them involve constructing or summarizing models.\nAs a reminder, here are some of the commands/syntax that should be familiar to you from the first half of the course.\n\nlm() fits (or “trains”) a “linear model” on data from a data frame.\nggplot() sets things up for a new graphic. Use aes() as an argument.\ngraphics layers to add onto the output of ggplot():\n\ngeom_point(), geom_jitter(alpha=0.5)\n\nfilter(), mutate() and summarize() are basic data-wrangling commands we will use often.\n\nNew commands in the second half of the course:\n\nsummarize a model: conf_intervals(), R2()\nevaluate a model: model_eval()\ngraphic of a model: model_plot() (This replaces the geom_smooth() used in the first half of the course.)\nvariance of a variable in a data frame: DF %>% summarize(NM = var(VAR))\ndraw a DAG: dag_draw()\nsample from a DAG: sample(DAG, size=100)\n\nA few other commands will be used occasionally in examples and demonstrations. You should know what they do, but typically there will be a reminder of the syntax: zero_one(), shuffle(), do() * {}, dag_intervene(), tibble(), regression_summary(), anova_summary()."
  },
  {
    "objectID": "math300-software.html#running-rrstudio-on-your-laptop",
    "href": "math300-software.html#running-rrstudio-on-your-laptop",
    "title": "R package for Math 300Z",
    "section": "Running R/RStudio on your laptop?",
    "text": "Running R/RStudio on your laptop?\nInstall the {math300} and other packages with these two commands:\ninstall.packages(c(\"mosaic\", \"ggplot\", \"dplyr\", \"openintro\", \"moderndive\", \"nycflights13\", \"knitr\"))\n# additional package for Math 300Z\nremotes::install_github(\"dtkaplan/math300\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300Z",
    "section": "",
    "text": "Take-aways from the most recent class\n\n\n\nOn Math 300 Blog\nMath 300Z is the prototype for scheduled revisions to Math 300. The revisions apply only to Lessons 19 and up; the first 18 lessons come from Math 300."
  },
  {
    "objectID": "index.html#day-by-day-plan",
    "href": "index.html#day-by-day-plan",
    "title": "Math 300Z",
    "section": "Day-by-day plan",
    "text": "Day-by-day plan\nMar 3 Lesson 19: Topic: Variation Reading, Activity: Measuring by eye, Worksheet Solns, take-aways\nMar 7 Lesson 20: Topic: DAGs and simulation Reading, Activity: Life savers?, Worksheet Solns, take-aways\nMar 9 Lesson 21: Topic: Signal and noise Reading, Activity: Membrane channels Handout, Worksheet Solns, take-aways class video\nMar 13 Lesson 22: Topic: Sampling variation Reading, Activity: Counts and rates, Worksheet Solns, take-aways class video\nMar 15 Lesson 23: Topic: Confidence intervals Reading, Activity: Got you covered!, Worksheet Solns, take-aways class video\nMar 17 Lesson 24: Topic: Effect size Reading, Activity: With respect to …, Worksheet Solns, take-aways\nMar 21 Lesson 25: Topic: Prediction mechanics Reading, Activity: To be determined, Worksheet Solns, take-aways\nMar 23 Lesson 26: Topic: Prediction intervals Reading, Activity: Intervals by eye, Handout, Worksheet Solns, take-aways\nSPRING BREAK. Keep in mind that Problem Set 5 will be due April 6.\nApr 3 Lesson 28: Topic: Covariates :: Reading, Activity: TBA, Worksheet Solns, take-aways\nAPRIL 6: Problem Set 5 due. See Z-section Problem Set 5 on posit.com\nApr 5 Lesson 29: Topic: Covariates eat variance :: Reading, Activity: TBA. Homework assignment due. (link to come)\nApr 7 Lesson 30: Topic: Confounding :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 11 Lesson 31: Topic: Spurious correlation :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 13 Lesson 32: Topic: Experiment & random assignment :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 17 Lesson 33: Topic: Measuring and accumulating risk :: Reading, Activity: TBA, Worksheet Solns, take-aways\nAPRIL 19: GR 3\nApr 24 Lesson 34: Topic: Constructing a classifier :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 27 Lesson 35: Topic: Accounting for prevalence :: Reading, Activity: TBA, Worksheet Solns, take-aways\nMay 1 Lesson 36: Topic: Hypothesis testing :: Reading, Activity: TBA, Worksheet Solns, take-aways\nMay 3 Lesson 37: Topic: Calculating a p-value :: Reading, Activity: TBA, Worksheet Solns, take-aways\nMAY 4, PS6 due\nMay 5 Lesson 38: Topic: False discovery with hypothesis testing :: Reading, Activity: TBA, Worksheet Solns, take-aways\nMay 9 Lesson 39: REVIEW of lessons 28-38 :: Reading\nMay 11 Lesson 40: Review of entire course"
  },
  {
    "objectID": "Day-by-day/Lesson-34/Teaching-notes-34.html#in-draft",
    "href": "Day-by-day/Lesson-34/Teaching-notes-34.html#in-draft",
    "title": "Instructor Teaching Notes for Lesson 34",
    "section": "IN DRAFT",
    "text": "IN DRAFT\nTell the story of the Chinese spy balloon. After it was detected, the Air Force (according to news reports) increased the sensitivity of radars. This led to an increase in detection and a week-long rash of high-altitude detections, two of which were shot down. Eventually it was realized that there is a surprising amount of stuff floating around at high altitude. https://www.nytimes.com/live/2023/02/16/us/biden-china-balloon-ufo?smid=nytcore-ios-share&referringSource=articleShare"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "",
    "text": "You have been learning some basics of data wrangling and visualization, along with what ModernDive calls “basic regression” and “multiple regression.” These are tools which you will continue to use in the second half of the semester.\nSuch tools are necessary but usually not sufficient. Data only occasionally speak for themselves. Most often, we need to interpret data in the context of what we already know or believe about the system under study.\nExample: As you’ve seen, merely fitting a regression model does not demonstrate that there is a causal relationship between the explanatory variables and the response variable. We will need some new concepts to encode our ideas (and speculations) about causal relationships and to use regression modeling to inform (or contradict) our ideas.\nExample: We’ll see how to avoid seeing patterns and relationships for which the evidence in unpersuasive. Example: We will see how detection thresholds can be set to reflect our opinions about the costs and benefits of different ways of getting it right or wrong and our prior knowledge of the frequency of different kinds of events."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#regression-models",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#regression-models",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Regression models",
    "text": "Regression models\nThe point of regression modeling is to detect and quantify patterns of relationship between variables. Sometimes simple data graphics are enough to display a pattern. For instance, let’s look at some Department of Transportation data on models of cars stored in the MPG data frame. We will start by looking at the link between fuel economy and CO_2_ production.\n\nggplot(MPG, aes(x = fuel_year, y = CO2_year)) +\n  geom_jitter(alpha=.3)\n\n\n\n\nThis is a very strong pattern. fuel_year and CO2_year are practically the same thing.\n\nWhy?\nWhy are there some points off of the straight line describing the large majority of points?\n\nOther times, patterns are hidden by extreme variability in the data. For instance, here are data on the effect of kindergarden class size on student outcomes.\n\nggplot(STAR, aes(x=classtype, y=g4math)) + geom_jitter() + geom_violin(alpha=.5, fill=\"blue\")\n\n\n\n\n\n\n\n\nRegression modeling is a technique for looking for simple forms of patterns in the relationships among variables. It is not the only such technique, but it is by far the most widely used in practice across diverse fields.\nWe will use only regression modeling (and allied methods) in this course. You may have heard about other methods such as “deep learning” or “neural networks,” but regression modeling is the basis for most of the others.\nIt’s critically important that you understand the framework for regression modeling.\n\nIn any one model, there is one variable that is identified as the response variable.\n\nThis identification depends on the purpose behind your work. You’ll learn it mostly by example.\n\nOther variables in the model are cast in the role of explanatory variables. There might be one explanatory variable or there might be other. There’s even a use for the case where there are no explanatory variables, but we don’t need to worry about that now.\nIn fitting a model to data (sometimes called “training a model on data”) the computer does the heavy lifting of finding a relationship between the response and explanatory variables that stays close to the data. Often, the “shape” of the relationship is very simple, e.g. a straight-line relationship or, more generally, a linear combination. That’s where the l comes in the function lm() that you will be using again and again in this course.\nIn using lm(), you specify which variables you want in the explanatory role and which single variable you have selected to be the response variable. The computer language syntax is very simple:\n\nresponse ~ var1 + var2 + ...\nThe name of the response variable is always on the left-hand side of the TILDE.\nThe explanatory variables are listed by name on the right side of the TILDE.\nThe + between the names of explanatory variables is mostly just punctuation. You can read it as “and”.\n\nThe TILDE character is usually just pronounced “tilde,” but English-language equivalents are\n\n“as explained by”\n“as accounted for by”\n“as modeled by”\n“versus”"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#data-graphics",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#data-graphics",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Data graphics",
    "text": "Data graphics\nSince the distinction between the response and the explanatory variables is so central, we are going to enforce a graphical style that reflects the distinction.\n\nThe response variable will always be on the vertical axis.\nOne of the explanatory variables will be on the horizontal axis.\nIf there is a second explanatory variable, we will use color.\nWhen we need a third or fourth explanatory variable, we will use faceting.\n\nIn the ggplot2 graphics system, this policy will appear like this:\nggplot(Dataframe, aes(y=response, x=var1, color=var2)) + geom_jitter() or geom_point() and so on."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#models",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#models",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Models",
    "text": "Models\nWe have been working a lot with data frames. Now we are going to add a new type of R object, which we can call a “model”. A model is NOT a data frame, it is a different kind of thing with different properties and different operations.\nMaking a model:\n\nmod1 <- lm(sat~ expend, data=SAT)\n\nOperations we will perform on models:\n\nGraph the model (and usually the data used for training)\n::: {.cell}\nmodel_plot(mod1)\n::: {.cell-output-display}  ::: :::\n\nBeing able to use multiple explanatory variables allows us to see patterns that may be subtle.\n\nmod2 <- lm(sat ~ expend + frac, data=SAT)\nmodel_plot(mod2)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\n\n\n\n\nModel summaries, especially conf_interval() and R2()\n\n\nmod2 |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef    .upr\n  <chr>        <dbl>  <dbl>   <dbl>\n1 (Intercept) 950.   994.   1038.  \n2 expend        3.79  12.3    20.8 \n3 frac         -3.28  -2.85   -2.42\n\nmod2 |> R2()\n\n   n k  Rsquared        F     adjR2 p df.num df.denom\n1 50 2 0.8194726 106.6741 0.8117906 0      2       47\n\n\n\nEvaluate the model at each of the rows of the training data.\n::: {.cell}\nmodel_eval(mod2) |> head()\n::: {.cell-output .cell-output-stderr} Using training data as input to model_eval(). :::\n::: {.cell-output .cell-output-stdout} .response expend frac   .output     .resid     .lwr      .upr  1      1029  4.405    8 1025.1463   3.853661 958.2677 1092.0250  2       934  8.963   47  969.9621 -35.962052 900.0065 1039.9176  3       944  4.778   27  975.5616 -31.561556 909.1283 1041.9948  4      1005  4.459    6 1031.5117 -26.511670 964.6071 1098.4162  5       902  4.992   45  926.8741 -24.874145 860.0437  993.7046  6       980  5.443   29  978.0302   1.969768 912.0035 1044.0570 ::: :::"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#todays-lesson",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#todays-lesson",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Today’s Lesson",
    "text": "Today’s Lesson\nA definition of “statistical thinking” from the book:\n\nStatistic thinking is the accounting for variation in the context of what remains unaccounted for.\n\nImplicit in this definition is a pathway for learning to think statistically:\n\nLearn how to measure variation;\nLearn how to account for variation;\nLearn how to measure what remains unaccounted for.\n\nToday: How to measure variation.\nConsider some closely related words: variable, variation, vary, various, variety, variant. The root is vari.\nOur preferred way to measure the amount of variation numerically: the variance, a single number, always positive.\n\nVariance always involves a single variable; it is about the variation in that variable.\nCalculate the variance is with var() within summarize() r DF |> summarize(NM = var(VAR))\nA good way to conceptualize the variance is as the average squared pairwise difference between values\nThe units of the variance are the square of the units of the variable.\nWhy square the pairwise differences? It’s a convention. Experience has shown this convention simplifies many operations we do with models. Underlying mathematics: Pythagorean theorem and formula for bell-shaped curve.\nOften people talk about the “standard deviation.” This is merely the square-root of the variance. But the variance is more fundamental mathematically. Using standard deviations introduces square roots in many calculations that don’t need to be there if we use variance."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#activity",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#activity",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Activity",
    "text": "Activity"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#administration",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#administration",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Administration",
    "text": "Administration\n\nUse “300Z Section” under Teams.\nClone the Z-section project on posit.cloud. We’ll use this project for the rest of the semester.\nThere will be a “worksheet” almost every day.\n\nThe worksheet is in the form of an Rmd file. To access it, go into the Z-section project and give a command like this: get_lesson_worksheet(19) or whatever the lesson number is.\nAn effective way to prepare for a class is to look at the worksheet before class. Just read it and take note of what doesn’t make sense to you. That way you can be attentive to those things in class.\nComplete the worksheet after class.\nCome with unresolved questions about the worksheet for the next class or for EI.\n\nMost days there will also be a group activity.\nThere will be a couple of problem sets that will be graded.\nThere will be one GR about half-way through the rest of the semester. And a final GR."
  },
  {
    "objectID": "Worksheet-20.html",
    "href": "Worksheet-20.html",
    "title": "Lesson 20: Worksheet",
    "section": "",
    "text": "20.1 [Technical] Collect a sample from a DAG simulation.\n20.2 [Technical] Examine the formulas behind a DAG simulation and compare to the results of a regression model trained on a sample from the DAG simulation.\n20.3 [Conceptual] Recognize properties of a DAG. i. Identify exogenous nodes. ii. Identify all pathways between two specified end nodes. iii. On a given pathway, is there causal flow from one end node to another? iv. On a given pathway, is there a causal flow from some node on the pathway to both end nodes?"
  },
  {
    "objectID": "Worksheet-20.html#part-1-samples-from-dags",
    "href": "Worksheet-20.html#part-1-samples-from-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 1: Samples from DAGs",
    "text": "Part 1: Samples from DAGs\n\nUse dag_draw() to draw a picture of the dag08 directed acyclic graph. From this graph, explain why node c is exogenous and why x and y are not.\n\nANSWER:\n\nUse print() to view the formulas used by dag08 to simulate data. What about the formula for y indicates that it’s receives inputs from x and c.\n\nANSWER:\n\nThere are three coefficients in the formula for y: an intercept, an x coefficient, and a c coefficient. (There is also some random input from an exogenous source unrelated to c or x.) What are the numerical values of the three coefficients?\n\nANSWER:\n\nCollect a sample of size \\(n=100\\) from dag08 and use it to train the model with specification y ~ x. Do the coefficients reported match those you found in part (c)? (If you are not sure, use a bigger sample size, say \\(n=1000\\) or even bigger.)\n\nANSWER:\nThe model says the x coefficient is about 1.5, not the same as in the DAG formula for y.\n\nSimilar to (4), but use the specification y ~ x + c. How do the coefficients for this model compare to those you found in (3)?\n\nANSWER:"
  },
  {
    "objectID": "Worksheet-20.html#part-2-paths-in-dags",
    "href": "Worksheet-20.html#part-2-paths-in-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 2: Paths in DAGs",
    "text": "Part 2: Paths in DAGs\n\nIn dag08 there are two paths connecting x andy. One path is direct, \\(X \\longrightarrow Y\\). The other path is indirect, \\(X \\longleftarrow C \\longrightarrow Y\\).\n\nAlong the indirect path, is there a causal flow from x to y?\nAlong the indirect path, is there a causal flow from any node on the graph that reaches both endpoints, x and y?\n\n\nANSWER:\n\ndag_school2 is a highly simplistic model of the relationship between expenditures on schools and student outcomes in terms of, say, standardized test scores.\n\n\ndag_draw(dag_school2, vertex.label.cex=1, vertex.size=40)\n\n\n\n\nThere is a direct pathway from expenditure to outcome as well as another, indirect pathway.\n\nAre there any exogenous nodes in the graph?\nOn the indirect pathway, is there a causal flow from expenditure to outcome?\nIs there a causal flow from any node on the indirect pathway to both expenditure and outcome? Which one?\n\nANSWER:"
  },
  {
    "objectID": "Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "href": "Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "title": "Lesson 20: Worksheet",
    "section": "Part 3: Are expenditures good for school outcomes?",
    "text": "Part 3: Are expenditures good for school outcomes?\n\nLook at the formulas for dag_school2. Is a higher expenditure connected to a higher outcome?\n\nANSWER:\n\nGenerate a simple of size 1000 from dag_school2 and use it to train the model outcome ~ expenditure. Is the coefficient on expenditure consistent with what you found in (1)? (If you aren’t sure, use a larger sample size, say 10,000.) What about the coefficient on expenditure leads to your conclusion?\n\nANSWER:\n\nSpeculate on what might be the origin of the evident inconsistency between (1) and (2)?\n\nANSWER:"
  },
  {
    "objectID": "Worksheet-20.html#part-4-constructing-a-dag",
    "href": "Worksheet-20.html#part-4-constructing-a-dag",
    "title": "Lesson 20: Worksheet",
    "section": "Part 4: Constructing a DAG",
    "text": "Part 4: Constructing a DAG\nIn this task, you will construct DAGs using dag_make() and draw them using dag_draw().\nA DAG is defined by a series of tilde expressions, one for each node in the graph. The tilde expression for a node has the node’s name on the left-hand side of the tilde. The right-hand side contains the nodes which serve as inputs to the node named on the left-hand side. If there are no inputs, write exo().\nFor example, consider a DAG with three nodes: one, two, and three. To define a DAG where node two receives input from node one, and node three receives input from nodes one and two, use make_dag() with three tilde expressions:\n\nexample_dag <- dag_make(\n  one ~ exo(),\n  two ~ one,\n  three ~ two + one\n)\ndag_draw(example_dag)\n\n\n\n\nThe right-hand side of a formula can be any arithmetic expression involving the node names, but we will keep it simple: just use + to separated the node names. If a node receives no inputs, the right-hand side should be simply exo() to mark that node as exogenous.\n\nWhat happens if node one, instead of being exogenous, takes as input one of the other two nodes in example_dag?\n\nANSWER:\n\nCreate and draw a DAG that has the same arrangement of causal connections as “Professor Butts and the Self-Operating Napkin,” illustrated below:\n\n\nProfessor Butts and the Self-Operating Napkin (1931). Soup_spoon (A) is raised to mouth, pulling string (B) and thereby jerking ladle (C), which throws cracker (D) past toucan (E). Toucan jumps after cracker and perch (F) tilts, upsetting seeds (G) into pail (H). Extra weight in pail pulls cord (I), which opens and ignites lighter (J), setting off skyrocket (K), which causes sickle (L) to cut string_m (M), allowing pendulum with attached napkin to swing back and forth, thereby wiping_chin.\nWatch your spelling of node names! Use this command to draw your napkin_dag:\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)\nANSWER:"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html",
    "href": "Worksheets/Worksheet-19.html",
    "title": "Lesson 19: Worksheet",
    "section": "",
    "text": "19.1. [Conceptual] Master the use and units of variance and standard deviation in measuring variability.\n19.2. [Conceptual] Understand the equivalence between mean and proportion on a zero-one variable.\n19.3. [Technical] Use var() and sd() within summarize()\n19.4. [Technical] Use model_plot() to graph models with one or two explanatory variables.\n19.5. [Technical] Use zero_one() with mutate() to create a zero-one variable."
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#preliminaries-how-we-will-work-with-r.",
    "href": "Worksheets/Worksheet-19.html#preliminaries-how-we-will-work-with-r.",
    "title": "Lesson 19: Worksheet",
    "section": "Preliminaries: How we will work with R.",
    "text": "Preliminaries: How we will work with R.\nIn the first half of Math 300Z, the daily student notes were largely structured around “scaffolded” R code, which often involved filling in the blanks. In this second half of 300Z, we will start to use a new way of helping you construct appropriate R command. We call this “command patterns. For instance,\nDF %>% summarize(NM=var(VAR)) \nis a command pattern.\nOne reason for the shift to the command-pattern style is that there will be only a handful of new patterns in the second half of the course that you’ll be using over and over again. Another reason is to help you develop “finger memory” for the most common patterns. An analogy: scaffolding is like GPS navigation which certainly makes it easier to drive but harder to get to know the town. Command patterns are like a paper map, there to help you when you need it.\nThere is a specific notation for command patterns, which you should memorize. Instead of the blanks used in a scaffold, the command pattern uses a CAPITALIZED abbreviation for the **kind of thing* that should be put in the position. Common kinds of thing are\n\nDF: a data frame, almost always referred to by name.\nVAR: a variable in a data frame. Many command patterns involve multiple variables, each of which is referred to by VAR. You will replace each VAR with the appropriate variable name.\nVARS: one or more variable names. When these are the right-hand side of a tilde expression, separate the names with + punctuation. When we mean to indicate that there is only one variable, we use VAR instead of VARS. If we want to say, “use two variables,” we would write VAR + VAR.\nMODEL refers to the name of a model that you have previously constructed with lm().\nNM means a name that you will be calling something by. For instance, NM <- lm(VAR ~ VARS, data=DF). Another occasion for using NM is as part of an argument to summarize() or mutate().\n[, MORE] means that you can have multiple additional arguments of the same form as the previous argument.\nVALUE a number, quoted string (e.g., \"red\"), or multiple values inside c( ).\nMODSPEC is a model specification, which could equally well be written VAR ~ VARS\n\nAnything in a command pattern that is not a CAPITALIZED abbreviation is a specific part of the command to be used as-is. For instance, lm(VAR ~ VARS, data=DF) refers explicitly to the lm() function whose first argument is a tilde expression and whose second argument is named data.\nOccasionally, you will refer to a data frame by naming the package from which it comes. For example, the moderndive package includes (among many others) the amazon_books data frame. Think of amazon_books as a first name, and moderndive as a family name. When you see PACKAGE::DF it is meant to indicate, for instance, moderndive::amazon_books. (Note that the :: in the command pattern is to be taken literally; there are two successive colons separating the package name from the name of the data frame.)"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#part-1",
    "href": "Worksheets/Worksheet-19.html#part-1",
    "title": "Lesson 19: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nCommand patterns:\n\nDF %>% summarize(NM = var(VAR)) Calculate variance of a variable in a data frame.\n`DF %>% summarize(NM1 = var(VAR1), NM2 = var(VAR2) [, MORE])\nPACKAGE::DF The name of a data frame within a package.\n\n\nIn the mosaicData::Galton data frame, find the variance of mother and father. Give both the numerical value and the units.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmosaicData::Galton |> \n  summarize(vmother = var(mother), vfather = var(father))\n\n   vmother  vfather\n1 5.322365 6.102164\n\n\nThe units for both are `inches-squared” since the variables themselves have units “inches.”\n\n\n\nIn the moderndive::amazon_books data frame, find the variance of list_price and num_pages. Give both the numerical value and the units.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  summarize(vprice = var(list_price), vpages = var(num_pages))\n\n# A tibble: 1 × 2\n  vprice vpages\n   <dbl>  <dbl>\n1     NA     NA\n\n\nThe units of list_price are dollars, so the variance has units “square-dollars”.\nnum_pages is dimensionless, so the variance is also dimensionless.\n\n\n\nCalculate the variance of sex from Galton. If something goes wrong, explain why.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nGalton |> summarize(vsex = var(sex))\n\nError in `summarize()`:\n! Problem while computing `vsex = var(sex)`.\nCaused by error in `stats::var()`:\n! Calling var(x) on a factor x is defunct.\n  Use something like 'all(duplicated(x)[-1L])' to test for a constant vector.\n\n\nsex is a categorical variable. There’s no such thing as the variance of a categorical variable."
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#part-2",
    "href": "Worksheets/Worksheet-19.html#part-2",
    "title": "Lesson 19: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nCommand patterns:\n\nNM <- lm(VAR ~ VARS, data = DF)\nlm(VAR ~ VARS, data=DF) %>% conf_interval()\nlm(MODSPEC, data=DF) %>% conf_interval() means the same as (b).\n\n\n(Easy, no computing needed.) What kind of a thing is conf_interval(). (Hint: It’s the same kind of thing as lm().)\n\n\n\n\n\n\n\nANSWER:\n\n\n\nconf_interval() is a function.\n\n\n\nUsing the moderndive::amazonbooks data frame, fit the model list_price ~ num_pages:\n\nWhat are the units of the “(Intercept)” coefficient?\nReport the coefficient on num_pages. Give both the numerical bounds and the units.\n\n\n\n\n\n\n\n\nANSWER:\n\n\n\nThe intercept always has the same units as the response variable. Here, that’s dollars.\n\nlm(list_price ~ num_pages, data = moderndive::amazon_books) |>\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept) 8.33   11.8    15.4   \n2 num_pages   0.0105  0.0199  0.0293\n\n\nCoefficient on num_pages: .02 dollars per page. Multiplying the coefficient by the number of pages will give dollars: the units of the response variable.\n\n\n\nSimilar to (2) but with the model list_price ~ numpages + hard_paper\n\nWhat does the term hard_paperH refer to?\nAccording to the coefficients, is a hardcover book any more expensive (on average) than a softcover book?\n\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nlm(list_price ~ num_pages + hard_paper, data = moderndive::amazon_books) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept) 7.04   10.6    14.2   \n2 num_pages   0.0102  0.0196  0.0289\n3 hard_paperH 1.56    4.96    8.36  \n\n\nhard_paperH refers to the H level of the hard_paper variable. According to the model, a hardback costs $4.96 more than a paperback, on average.\n\n\n\nStore the model you created in (3) under the name mod3. We’ll use it in the next part. For your answer, put the R command you used to store the model as mod3.\n\n\n\n\n\n\n\nANSWER:\n\n\n\nNote that we are asked to store the model itself, not the confidence interval.\n\nmod3 <- lm(list_price ~ num_pages + hard_paper, data = moderndive::amazon_books)"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#graphics-review",
    "href": "Worksheets/Worksheet-19.html#graphics-review",
    "title": "Lesson 19: Worksheet",
    "section": "Graphics review",
    "text": "Graphics review\nCommand patterns:\n\nggplot(DF, aes(x=VAR, y=VAR)) + geom_jitter()\nggplot(DF, aes(x=VAR, y=VAR)) + geom_jitter() + geom_violin(fill=\"blue\", alpha=0.3)\nggplot(DF, aes(x=\"all\", y=VAR)) + geom_jitter()\nmodel_plot(MODEL, x=VAR)\nmodel_plot(MODAL, x=VAR, color=VAR)\n\n\nMake a jitter plot of list_price ~ hard_paper from moderndive::amazon_books.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  ggplot(aes(x=hard_paper, y = list_price)) + \n  geom_jitter(alpha=0.5)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nUsing your command from (1), add a new layer: + geom_violin(fill=\"blue\", alpha=0.3)\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  ggplot(aes(x=hard_paper, y = list_price)) + \n  geom_jitter(alpha=0.5) +\n  geom_violin(fill=\"blue\", alpha=0.3)\n\nWarning: Removed 1 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nUse model_plot() to draw a picture of mod3. Set x=hard_paper and num_pages=200. What do you think the horizontal line segments refer to?\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmodel_plot(mod3, x=hard_paper, num_pages=200)\n\n\n\n\nThe vertical position of the horizontal lines indicates the model output for books with 200 pages.\n\n\n\nRepeat (3), but remove num_pages = 200. Instead, set x=num_pages and color=hard_paper. Explain the meaning of the line segments in everyday terms.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmodel_plot(mod3, x=hard_paper, color=num_pages)\n\n\n\n\nThe parallel line segments in each column show the model output for books with 200, 400, and 600 pages respectively."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html",
    "href": "Worksheets/Worksheet-20.html",
    "title": "Lesson 20: Worksheet",
    "section": "",
    "text": "20.1 [Technical] Collect a sample from a DAG simulation.\n20.2 [Technical] Examine the formulas behind a DAG simulation and compare to the results of a regression model trained on a sample from the DAG simulation.\n20.3 [Conceptual] Recognize properties of a DAG. i. Identify exogenous nodes. ii. Identify all pathways between two specified end nodes. iii. On a given pathway, is there causal flow from one end node to another? iv. On a given pathway, is there a causal flow from some node on the pathway to both end nodes?"
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-1-samples-from-dags",
    "href": "Worksheets/Worksheet-20.html#part-1-samples-from-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 1: Samples from DAGs",
    "text": "Part 1: Samples from DAGs\n\nUse dag_draw() to draw a picture of the dag08 directed acyclic graph. From this graph, explain why node c is exogenous and why x and y are not.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\ndag_draw(dag08)\n\n\n\n\nNode C is exogenous because it has no incoming arrows.\n\n\n\nUse print() to view the formulas used by dag08 to simulate data. What about the formula for y indicates that it’s receives inputs from x and c.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nprint(dag08)\n\nc ~ exo()\nx ~ c + exo()\ny ~ x + c + 3 + exo()\n\n\nThe right-hand side of the formula for y says that y will be calculated as the sum of x and c (plus 3 plus some random noise). That is, x and c directly shape the value of y.\n\n\n\nThere are three coefficients in the formula for y: an intercept, an x coefficient, and a c coefficient. (There is also some random input from an exogenous source unrelated to c or x.) What are the numerical values of the three coefficients?\n\n\n\n\n\n\n\nANSWER\n\n\n\nFrom the formula for y in the dag, the coefficients are 1 for x, 1 for c, and 3 for the intercept.\n\n\n\nCollect a sample of size \\(n=100\\) from dag08 and use it to train the model with specification y ~ x. Do the coefficients reported match those you found in part (c)? (If you are not sure, use a bigger sample size, say \\(n=1000\\) or even bigger.)\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag08, size=1000)\nlm(y ~ x, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept)  2.83  2.90  2.98\n2 x            1.36  1.41  1.47\n\n\n\n\nThe model says the x coefficient is about 1.5, not the same as in the DAG formula for y.\n\nSimilar to (4), but use the specification y ~ x + c. How do the coefficients for this model compare to those you found in (3)?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag08, size=1000)\nlm(y ~ x + c, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept) 2.90  2.97   3.03\n2 x           0.919 0.983  1.05\n3 c           0.901 0.989  1.08\n\n\nWhen we include both x and c in the model specification, the coefficients work out to match those of the DAG formula for y."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-2-paths-in-dags",
    "href": "Worksheets/Worksheet-20.html#part-2-paths-in-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 2: Paths in DAGs",
    "text": "Part 2: Paths in DAGs\n\nIn dag08 there are two paths connecting x andy. One path is direct, \\(X \\longrightarrow Y\\). The other path is indirect, \\(X \\longleftarrow C \\longrightarrow Y\\).\n\nAlong the indirect path, is there a causal flow from x to y?\nAlong the indirect path, is there a causal flow from any node on the graph that reaches both endpoints, x and y?\n\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nYes, the flow runs directly from x to y.\nYes. The flow runs from c to each of x and y.\n\n\n\n\ndag_school2 is a highly simplistic model of the relationship between expenditures on schools and student outcomes in terms of, say, standardized test scores.\n\n\ndag_draw(dag_school2, vertex.label.cex=1, vertex.size=40)\n\n\n\n\nThere is a direct pathway from expenditure to outcome as well as another, indirect pathway.\n\nAre there any exogenous nodes in the graph?\nOn the indirect pathway, is there a causal flow from expenditure to outcome?\nIs there a causal flow from any node on the indirect pathway to both expenditure and outcome? Which one?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nCulture is an exogenous node; there are no incoming arrows to culture.\nYes.\nFrom Culture there is a flow to outcome through expenditure. There is also a flow from Culture to Outcome via Participation."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "href": "Worksheets/Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "title": "Lesson 20: Worksheet",
    "section": "Part 3: Are expenditures good for school outcomes?",
    "text": "Part 3: Are expenditures good for school outcomes?\n\nLook at the formulas for dag_school2. Is a higher expenditure connected to a higher outcome?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nprint(dag_school2)\n\nculture ~ unif(-1, 1)\nexpenditure ~ 12000 + 4000 * culture + exo(1000)\nparticipation ~ (50 + 30 * culture + exo(15)) %>% pmax(0) %>% \n    pmin(100)\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + exo(50)\n\n\nThe formula for Outcome has a positive coefficient (0.01) on expenditure. So when expenditure goes up, so will outcome. The magnitude of the coefficient is neither here nor there. Remember that there are always units associated with a coefficient. It’s impossible to say whether a magnitude is large or small unless you know the units.\n\n\n\nGenerate a simple of size 1000 from dag_school2 and use it to train the model outcome ~ expenditure. Is the coefficient on expenditure consistent with what you found in (1)? (If you aren’t sure, use a larger sample size, say 10,000.) What about the coefficient on expenditure leads to your conclusion?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag_school2, size=1000)\nlm(outcome ~ expenditure, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term             .lwr     .coef      .upr\n  <chr>           <dbl>     <dbl>     <dbl>\n1 (Intercept) 1191.     1215.     1239.    \n2 expenditure   -0.0179   -0.0160   -0.0140\n\n\nThe coefficient on Expenditure is negative in contrast to the known positive coefficient in the DAG formula for Outcome.\n\n\n\nSpeculate on what might be the origin of the evident inconsistency between (1) and (2)?\n\n\n\n\n\n\n\nANSWER\n\n\n\nIn the DAG, Outcome is influenced negatively by Participation. And Expenditure is influenced positively by Participation. The two effects of Participation combine to produce an overall negative link between Expenditure and Outcome. By overall, we mean the combination of the direct Expenditure to Outcome link and the indirect path from Expenditure to Outcome via Participation."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-4-constructing-a-dag",
    "href": "Worksheets/Worksheet-20.html#part-4-constructing-a-dag",
    "title": "Lesson 20: Worksheet",
    "section": "Part 4: Constructing a DAG",
    "text": "Part 4: Constructing a DAG\nIn this task, you will construct DAGs using dag_make() and draw them using dag_draw().\nA DAG is defined by a series of tilde expressions, one for each node in the graph. The tilde expression for a node has the node’s name on the left-hand side of the tilde. The right-hand side contains the nodes which serve as inputs to the node named on the left-hand side. If there are no inputs, write exo().\nFor example, consider a DAG with three nodes: one, two, and three. To define a DAG where node two receives input from node one, and node three receives input from nodes one and two, use make_dag() with three tilde expressions:\n\nexample_dag <- dag_make(\n  one ~ exo(),\n  two ~ one,\n  three ~ two + one\n)\ndag_draw(example_dag)\n\n\n\n\nThe right-hand side of a formula can be any arithmetic expression involving the node names, but we will keep it simple: just use + to separated the node names. If a node receives no inputs, the right-hand side should be simply exo() to mark that node as exogenous.\n\nWhat happens if node one, instead of being exogenous, takes as input one of the other two nodes in example_dag?\n\n\n\n\n\n\n\nANSWER\n\n\n\nThe graph would become cyclic, hence not a DAG. Notice that by using a node on the right-hand side of a tilde expression only when it has already been created by a previous tilde expression, you guarantee that the graph will be acyclic.\n\n\n\nCreate and draw a DAG that has the same arrangement of causal connections as “Professor Butts and the Self-Operating Napkin,” illustrated below:\n\n\nProfessor Butts and the Self-Operating Napkin (1931). Soup_spoon (A) is raised to mouth, pulling string (B) and thereby jerking ladle (C), which throws cracker (D) past toucan (E). Toucan jumps after cracker and perch (F) tilts, upsetting seeds (G) into pail (H). Extra weight in pail pulls cord (I), which opens and ignites lighter (J), setting off skyrocket (K), which causes sickle (L) to cut string_m (M), allowing pendulum with attached napkin to swing back and forth, thereby wiping_chin.\nWatch your spelling of node names! Use this command to draw your napkin_dag:\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)\n\n\n\n\n\n\nANSWER\n\n\n\n\nnapkin_dag <- dag_make(\n  soup_spoon ~ exo(),\n  string ~ soup_spoon,\n  ladle ~ string,\n  cracker ~ ladle,\n  toucan ~ cracker,\n  perch ~ toucan,\n  seeds ~ perch,\n  pail ~ seeds,\n  cord ~ pail,\n  lighter ~ cord,\n  skyrocket ~ lighter,\n  sickle ~ skyrocket,\n  string_m ~ sickle,\n  wiping_chin ~ string_m\n)\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html",
    "href": "Worksheets/Worksheet-21.html",
    "title": "Lesson 21: Worksheet",
    "section": "",
    "text": "Command patterns:"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#basic-regression-patterns",
    "href": "Worksheets/Worksheet-21.html#basic-regression-patterns",
    "title": "Lesson 21: Worksheet",
    "section": "Basic regression patterns",
    "text": "Basic regression patterns\nEvery regression model involves a response variable, which Lessons in Statistical Thinking always plots on the vertical axis. Most of the regression models we will consider in these Lessons have one or two explanatory variables, although sometimes there will be more than two and sometimes none at all.\nIt is worth memorizing the forms of the tilde-expression specifications of the zero-, one-, and two-explanatory models, as well as their shapes. For this purpose, we’ll write the forms using five generic variable names. In practice, you will replace these generic names with specific names from the data frame of interest.\n\ny — a quantitative response variable (which might be the result of a zero-one transformation).\nx and z — quantitative explanatory variables\ng and h — categorical explanatory variables.\n\n\n\n\n\n\n\n\nModel specification\nShape\n\n\n\n\ny ~ 1\nA line with slope zero.\n\n\ny ~ x\nA line with possibly non-zero slope.\n\n\ny ~ g\nA value for each level of g.\n\n\ny ~ x + g\nSeparate lines for each level of g, all with the same slope.\n\n\ny ~ x + z\nParallel, evenly spaced lines.\n\n\ny ~ g + h\nFor each level of g, a set of spaced values, one for each level of h. The h-spacing will be the same for every level of g.\n\n\n\nNote: It doesn’t matter what order the explanatory variables are given in. The name of the response variable is always on the left-hand side of the tilde expression."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-1",
    "href": "Worksheets/Worksheet-21.html#part-1",
    "title": "Lesson 21: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nDo Exercise 21.4."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-2",
    "href": "Worksheets/Worksheet-21.html#part-2",
    "title": "Lesson 21: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nBy fitting a regression model, we divide the response variable into two components: a signal component and a noise component. The model specification tells what sort of signal to look for. For instance, the Clock_auction data frame records the sales price of antique grandfather clocks sold at auction. Presumably, the price reflects some feature of the clock itself as well as the market conditions. We have only the variables age and bidders to represent the the value of the clock and the market conditions.\nUsing lm() with the specification price ~ age directs the computer to look for a signal in the form of a straight-line relationship between age and price. The estimated noise is the difference between the response variable values (price) and the signal.\n\nHow much clock-to-clock variation is there in price? (Use the variance to measure variation.)\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nClock_auction |> summarize(vprice = var(price), sd=sqrt(vprice))\n\n# A tibble: 1 × 2\n   vprice    sd\n    <dbl> <dbl>\n1 134203.  366.\n\n\nSince the price is in dollars, the variance of price has units of “square dollars.” This is a unit that’s hard to get your head around. That’s why many people prefer the “standard deviation”, which is the square root of the variance.\n\n\n\nFit a model price ~ age, then plot with model_plot(). Describe the pattern between price and age you see in the plot.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 <- lm(price ~ age, data=Clock_auction)\nmodel_plot(mod1)\n\n\n\n\nAccording to the model, price goes up with age. A 25 year increase in age corresponds to about a $200 increase in price.\n\n\n\nUse model_eval() to find the model output for each clock for the model you constructed in (2).\n\nWhat’s the variance of the model .output? How does it compare to the variance of the response variable price?\nThe amount of noise can be measured with the variance of .resid. How much noise is there for price ~ age?\nDemonstrate arithmetically the relationship between the variance of the response variable, the variance of the model .output, and the variance of the noise.\n\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nValues <- model_eval(mod1)\n\nUsing training data as input to model_eval().\n\nValues |> summarize(voutput = var(.output))\n\n   voutput\n1 66352.26\n\n\nThe variance of the model output (note the dot used in the name .output) is 66-thousand square dollars. (Why “square dollars?” Because the response variable is price which is in dollars. The model output always has the same units as the response variable. And the variance of a variable always has units that are the square of the variable’s units.) The variance of the price variable is about 134-thousand square dollars, so the model output has about half the variance of the response.\n\nValues |> summarize(vresid = var(.resid))\n\n    vresid\n1 67850.62\n\n\nThe variance of the residuals is about 68-thousand square dollars. (The residuals always have the same units as the response variable.)\nTo show that the sum of the variances of the model output and residuals equals the variance of the response variable, just add them up and compare:\n\n66352.26 + 67850.62\n\n[1] 134202.9\n\n\nThe result exactly matches the variance of the price variable (calculated above).\n\n\n\nUse R2() to summarize the model you constructed in (2). Demonstrate arithmetically the relationship between R2 and variances of the response variable and the model .output.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 |> R2()\n\n   n k  Rsquared       F     adjR2            p df.num df.denom\n1 32 1 0.4944176 29.3375 0.4775648 5.914169e-06      1       30\n\n\nR2 is 0.49. This exactly matches the quotient of the variance of the model output divided by the variance of the response variable:\n\nValues |> summarize(ratio = var(.output) / var(.response))\n\n      ratio\n1 0.4944176\n\n\n\n\n\nThe quantity 1 - R2 describes the amount of noise. Arithmetically, how does 1 - R2 correspond to the variance of the .resid from part (3)?\n\n\n\n\n\n\n\nANSWER\n\n\n\nFirst, the value of 1 - R2, then the ratio of the variance of the residuals to the variance of the response variable:\n\n1 - 0.4944176\n\n[1] 0.5055824\n\nValues |> summarize(ratio = var(.resid) / var(.response))\n\n      ratio\n1 0.5055824"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-3-r2",
    "href": "Worksheets/Worksheet-21.html#part-3-r2",
    "title": "Lesson 21: Worksheet",
    "section": "Part 3: R2",
    "text": "Part 3: R2\ndag10 has a simple structure, with nodes a through f each contributing to the value of y. Use sample() to generate a sample of size 1000. Using your sample, construct several models and calculate the R2 statistic.\n\ny ~ 1.\ny ~ a\ny ~ b\ny ~ a + b\nand so on.\n\n\nWhich of the models gives the smallest value of R2? Explain why that particular model gives such a small R2.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nOur_sample <- sample(dag10, size=1000) \nlm(y ~ 1, data=Our_sample) |> R2()\n\n     n k Rsquared   F adjR2   p df.num df.denom\n1 1000 0        0 NaN     0 NaN      0      999\n\nlm(y ~ a, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 1 0.1370083 158.4421 0.1361435 0      1      998\n\nlm(y ~ b, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 1 0.3093868 447.0925 0.3086948 0      1      998\n\nlm(y ~ a + b, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 2 0.4159078 354.9612 0.4147361 0      2      997\n\n\nThe model y ~ 1 has R2=0 because the pseudo-variable 1 has zero variance and cannot “explain” any of the variance in the response variable y.\n\n\n\nAs you add more terms to the model specification, does R2 ever go down?\n\n\n\n\n\n\n\nANSWER\n\n\n\nModel specifications like y ~ a are actually shorthand for y ~ 1 + a. So y ~ a has an additional explanatory variable in addition to the pseudo-variable 1. Whenever you add a new explanatory variable to an existing model specification, the R2 will increase (or, more precisely, cannot decrease).\n\n\n\nWhat effect does the order of terms in the model have on R2? (For instance, y ~ a + b + c versus y ~ c + a + b.)\n\n\n\n\n\n\n\nANSWER\n\n\n\nCompare, for instance, y ~ a + b + c to y ~ c + a + b\n\nlm(y ~ a + b + c, data=Our_sample) |> R2()\n\n     n k Rsquared        F   adjR2 p df.num df.denom\n1 1000 3 0.446557 267.8811 0.44489 0      3      996\n\nlm(y ~ c + a + b, data=Our_sample) |> R2()\n\n     n k Rsquared        F   adjR2 p df.num df.denom\n1 1000 3 0.446557 267.8811 0.44489 0      3      996\n\n\nThe order of explanatory variables does not matter at all. It’s the collection that matters."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-4-concept-check",
    "href": "Worksheets/Worksheet-21.html#part-4-concept-check",
    "title": "Lesson 21: Worksheet",
    "section": "Part 4: Concept check",
    "text": "Part 4: Concept check\nWrite a sentence or two explaining what each of the following terms refers to.\n\n“Levels of a categorical variable”\n“Zero-one transformation”\n“Model specification”\n“Tilde expression”\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nLevels of a categorical variable. The values of a categorical variable are named. For instance, a household_pets variable would take on values like “cat,” “dog,” “turtle,” “parakeet,” …. The set of possible values for the categorical variable is called the “levels” of the variable.\nZero-one transformation. A means to translate a categorical variable with two levels into a quantitative variable. One of the levels is translated to the numerical value 1, the other to 0. The advantage of this particular scheme is that means or models where the zero-one variable is used for the response variable will have model outputs that can be interpreted as probabilities.\nModel specification. When constructing a regression model, the modeler has to provide two different kinds of inputs: (1) a data frame for training the model, (2) a statement about which variable from the data frame to use as the response variable and which other variables to use as explanatory variables. This statement (2) is called the “model specification.”\nTilde expression. Tilde expressions are an element of the syntax (or “grammar”) of R. They always involve the tilde character (~) which has no other legitimate use in R. Any valid R expression can be used on the right-side of tilde. The left side (if present, as will be the case when used with lm()) can also be any valid R expression. The most prominant role for tilde expressions in Math 300Z is to hold the model specification for use by lm(). In this use, the response variable’s name always goes on the left side of the tilde. Explanatory variable names go on the right side, usually separated by + as punctuation.\nFor the computer-science oriented …. Tilde expressions are a way to represent symbolic expressions such as fragments of code. In ordinary use, R tries to evaluate every code fragment, replacing names with their values and invoking any functions used. Symbolic expressions are taken literally as a code fragment, without any evaluation. This is valuable as a means to pass code fragments to a function which can then parse or otherwise evaluate the fragment in a particular context."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-5-curvy-models",
    "href": "Worksheets/Worksheet-21.html#part-5-curvy-models",
    "title": "Lesson 21: Worksheet",
    "section": "Part 5: Curvy models",
    "text": "Part 5: Curvy models\nHere’s a model of human height versus age based on the NHANES::NHANES data frame. (The package NHANES has the data frame which itself is called NHANES, so the full name is NHANES::NHANES.)\n\nmod1 <- lm(Height ~ Age, data = NHANES::NHANES)\nmodel_plot(mod1, data_alpha=0.05)\n\nWarning: Removed 353 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nDo you think the model gives a good description of the relationship between Age and Height? Explain using simple biological terms what the problem is with the straight-line model.\n\n\n\n\n\n\n\nANSWER\n\n\n\nHumans and other animals grow in a non-linear manner: rapid growth during gestation, infancy, and youth and comparative stasis later in life. With a model specification like Height ~ Age, lm() will look for a purely linear function, as in the straight line in the above graphic. The linear function doesn’t capture the age-dependent growth rate (fast during youth, slow or nil in adulthood), since a straight line has the same slope at all values of the explanatory variable.\nOne of the signs of the ill-fit of a linear model is that the response values tend to be cluster mostly below or mostly above the line for different regions of the explanatory variable. For instance, in the graph above, for ages younger than 10, the height values are systematically below the straight-line function.\n\n\nThere are several modeling techniques for constructing models that are more flexible than a straight line. We won’t be using them in Math 300, but we want to point out that they exist. Try this one:\n\nmod2 <- lm(Height ~ splines::ns(Age,5) * Gender, data = NHANES::NHANES)\nmodel_plot(mod2, data_alpha=0.05)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\nWarning: Removed 353 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nCalculate R2 for the straight-line model and for the curvy model.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 9647 1 0.2117647 2591.193 0.2116829 0      1     9645\n\nmod2 |> R2()\n\n     n  k  Rsquared        F    adjR2 p df.num df.denom\n1 9647 11 0.8720171 5968.044 0.871871 0     11     9635\n\n\nThe R2 for the rigid, straight-line model is substantially lower than for the more flexible, curvy model. Another way of saying this is that the curvy model stays closer (on average) to the data than the straight-line model.\nAn important theoretical question in statistical modeling is when to prefer a curvy model to a straight-line model. We haven’t yet encountered the statistical concepts that address this question."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "",
    "text": "We think about data with multiple variables as depicting a system: an interconnected network of components. Some components appear as variables in the data frame; other components may be unmeasured but given a name for reference.\nUse “directed acyclic graphs” (DAGs) to draw a picture of the system. Each system component is a node of the DAG. When one component has a causal connection with another, an arrow is drawn between those nodes.\nBy analysis of the DAG—using techniques we haven’t covered yet—you can figure out which variables to include in your model.\nMany DAGs are provided with the {math300} package, with names like dag01 through dag12. Here’s an example:\n\n\nprint(dag03)\n\ng ~ exo()\nx ~ 1 * g + exo()\ny ~ 1 * g + exo()\n\ndag_draw(dag03)\n\n\n\n\nNotice that there is no direct flow between nodes x and y. Still, there is an indirect connection: node g influences both x and y.\n\nWe can collect a sample from a DAG, for instance:\n\n\nMy_data <- sample(dag03, size=10000)\nMy_data |> head(3)\n\n\n\n \n  \n    g \n    x \n    y \n  \n \n\n  \n    0.0029971 \n    1.5327673 \n    0.0231254 \n  \n  \n    -0.1386110 \n    -0.4274596 \n    3.5090740 \n  \n  \n    -0.1543428 \n    1.0009144 \n    0.3116392 \n  \n\n\n\n\n\nDepending on the structure of the DAG, different model specifications will reveal different aspects of the DAG. For instance,\n\n\nmod1 <- lm(y ~ x, data=My_data)\nmod1 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0431319 \n    -0.0194309 \n    0.0042701 \n  \n  \n    x \n    0.4715121 \n    0.4883234 \n    0.5051348 \n  \n\n\n\n\nwill show if there is any kind of connection between x and y. But another specification will, in this case, show that the connection from x to y is via the connection provided by node g.\n\nmod2 <- lm(y ~ x + g, data=My_data)\nmod2 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0386313 \n    -0.0193601 \n    -0.0000888 \n  \n  \n    x \n    -0.0257069 \n    -0.0064618 \n    0.0127834 \n  \n  \n    g \n    0.9838257 \n    1.0115210 \n    1.0392163 \n  \n\n\n\n\nWe can look at the coefficients to see that in mod1 there is a non-zero connection between y and x, but in mod2 there is no non-zero connection of x on y.\n\nmod1 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0431 \n    -0.0194 \n    0.00427 \n  \n  \n    x \n    0.4720 \n    0.4880 \n    0.50500 \n  \n\n\n\nmod2 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0386 \n    -0.01940 \n    -8.88e-05 \n  \n  \n    x \n    -0.0257 \n    -0.00646 \n    1.28e-02 \n  \n  \n    g \n    0.9840 \n    1.01000 \n    1.04e+00 \n  \n\n\n\n\nOne way we will use DAGs to help us learn statistics is to compare the coefficients of models to the (known) mechanism of the DAG. We can see, for instance, that the g coefficient on y is 1 and the x coefficient is zero. Only mod2 reveals this.\n\nThe sampling and analysis in points (4) and (5) are an example of a “random trial” or “simulation.” We will use random trials to look at the properties of models fit to samples, especially with an eye to understanding the role of the sample size \\(n\\)."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#mathematical-functions-through-data",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#mathematical-functions-through-data",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Mathematical functions through data",
    "text": "Mathematical functions through data\nLet’s collect a small (\\(n=10\\)) sample from dag01:\n\nset.seed(103)\nSmall <- sample(dag01, size=10)\nhead(Small, 3)\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.7860 \n    1.89 \n  \n  \n    0.0547 \n    4.12 \n  \n  \n    -1.1700 \n    2.36 \n  \n\n\n\n\nWe can easily plot the data points. Less obviously, we can find any number of mathematical functions that are consistent with the data.\n\n\n\n\n\nFigure 1: Three of the infinite number of functions that can be drawn through the data ?@tbl-small-dag01.\n\n\n\n\n\nWhat don’t you like about these functions? Why do they insult your intuition?\n\nThey show details that are in no way suggested by the data.\nIf we were to collect more data, the function shapes could be entirely different. (Go back and change the random seed used for sampling from dag01.)"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#close-but-not-on-the-data",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#close-but-not-on-the-data",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "“Close” but not “on” the data",
    "text": "“Close” but not “on” the data\nIn general, we don’t insist that model functions go exactly through the data points. Instead, we imagine that the response variable involves some random noise that we don’t need to “capture” with our model. Doing things this way lets us fit model functions that are much simpler in shape, like this one:\n\n\n\n\n\nFigure 2: The straight-line function (blue) that goes through the data points as closely as possible. The noise is estimated as the difference (red for negative noise, black for positive noise) between the actual data points and the function.\n\n\n\n\nConstructing such a model divides the “explanation” of the response variable values into two parts:\n\nModel values, that is, the output of the model function (blue) at each of the values of the explanatory variable(s).\nWhat’s left over, the residuals, the vertical deviation of the actual response value from the model value.\n\nThe signal is the model values. The noise is the residuals.\nWhen we “fit” (or, “train”) a model, we take an aggressive stance. We look for the particular function of the shape implied by the model specification that will produce the smallest residuals. As usual, we measure the size of a residual by its square."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#measuring-signal-and-noise",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#measuring-signal-and-noise",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Measuring signal and noise",
    "text": "Measuring signal and noise\nWe depict the “size” of the signal as the amount of variability in the model values. As always, we measure variability using the variance.\nSimilarly, the “size” of the noise is the amount of variability in the residuals.\nThe model_eval() function is convenient for figuring out the model value and the residual for each row in the training data.\n\n\nThe training data\n\nSmall\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.790 \n    1.90 \n  \n  \n    0.055 \n    4.10 \n  \n  \n    -1.200 \n    2.40 \n  \n  \n    -0.170 \n    6.30 \n  \n  \n    -1.900 \n    0.93 \n  \n  \n    -0.120 \n    2.90 \n  \n  \n    0.830 \n    5.70 \n  \n  \n    1.200 \n    5.90 \n  \n  \n    -1.100 \n    2.10 \n  \n  \n    -0.380 \n    4.20 \n  \n\n\n\n\n\n\n\n\n\nOutput of model_eval()\n\nPts <- model_eval(mod)\n\n\n\n\n\n \n  \n    .response \n    x \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    1.90 \n    -0.790 \n    2.9 \n    -1.0000 \n    0.37 \n    5.4 \n  \n  \n    4.10 \n    0.055 \n    4.4 \n    -0.2400 \n    1.80 \n    6.9 \n  \n  \n    2.40 \n    -1.200 \n    2.2 \n    0.1400 \n    -0.38 \n    4.8 \n  \n  \n    6.30 \n    -0.170 \n    4.0 \n    2.4000 \n    1.50 \n    6.5 \n  \n  \n    0.93 \n    -1.900 \n    1.0 \n    -0.0810 \n    -1.80 \n    3.8 \n  \n  \n    2.90 \n    -0.120 \n    4.1 \n    -1.1000 \n    1.50 \n    6.6 \n  \n  \n    5.70 \n    0.830 \n    5.7 \n    -0.0033 \n    3.00 \n    8.4 \n  \n  \n    5.90 \n    1.200 \n    6.3 \n    -0.4400 \n    3.50 \n    9.2 \n  \n  \n    2.10 \n    -1.100 \n    2.4 \n    -0.2300 \n    -0.22 \n    4.9 \n  \n  \n    4.20 \n    -0.380 \n    3.6 \n    0.6200 \n    1.10 \n    6.1 \n  \n\n\n\n\n\n\nHow big is …\n\nThe response variable.\nThe signal.\nThe noise.\n\n\nPts |> summarize(i. = var(.response),\n                 ii. = var(.output),\n                 iii. = var(.resid)) \n\n\n\n \n  \n    i. \n    ii. \n    iii. \n  \n \n\n  \n    3.55 \n    2.6 \n    0.949 \n  \n\n\n\n\n\n\n\n\n\n\nSomething special about the variance\n\n\n\nFor every lm() model you build, the variance of the response variable is exactly equal to the sum of the variance of the model values and the variance of the residuals.\nIn other words, lm() splits the response variable into two parts: the sum of those parts equals the whole.\nR2 is the variance of the model values divided by the variance of the response variable.\n\nmod |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    10 \n    1 \n    0.733 \n    21.9 \n    0.699 \n    0.000863 \n    1 \n    8 \n  \n\n\n\nPts |> \n  summarize(i. = var(.response),\n            ii. = var(.output)) |>\n  mutate(R2 = ii. / i. )\n\n\n\n \n  \n    i. \n    ii. \n    R2 \n  \n \n\n  \n    3.55 \n    2.6 \n    0.733"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#activity-identifying-signal",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#activity-identifying-signal",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Activity: Identifying signal",
    "text": "Activity: Identifying signal"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#five-six-simple-models",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#five-six-simple-models",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Five Six simple models",
    "text": "Five Six simple models\nModels can have any number of explanatory variables. In Math 300, we will be mainly concerned with models with a single explanatory variable or with two explanatory variable. Since an explanatory variable can be either categorical or quantitative, there are six “shapes” of models:\nSingle explanatory variable\n\nCategorical explanatory variable.\nQuantitative explanatory variable.\n\nTwo explanatory variables\n\nCategorical & Categorical\nCategorical & Quantitative\nQuantitative & Quantitative\n\nSometimes, we will use models with Zero explanatory variables\n\nNo explanatory variables.\n\nGraphs of (i) through (v), with (iv) shown in two different modes."
  },
  {
    "objectID": "Day-by-day/Lesson-22/counts-and-waiting-times.html",
    "href": "Day-by-day/Lesson-22/counts-and-waiting-times.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "Measurements are a combination of signal and noise. This activity aims to help understand how much data is needed to reduce the noise to an acceptable level.\n\n\nOften, measurements are made of how many random events of a particular type happen in a fixed interval of time or a given area of space. Examples:\n\nHow many inches of snow fall per year in a given location?\nHow many traffic accidents happen per year at a particular intersection?\n\nMaking such measurements can be straightforward. The challenge comes in interpreting them. For instance, how different do the measurements of snowfall in two different areas need to be to support a claim that one area is snowier than the other? Similarly, how many years of observation are required to know if one intersection is more dangerous than another?\nCommon sense tells us that one region can be snowier than another or that one intersection can be more dangerous than another. Yet the individual events—a snow storm of a given magnitude or a pedestrian hit by an automobile—are random. That is, there is both signal and noise in the measurements.\nThis activity is about how much data we should collect in order to reduce the noise sufficiently that signals can be read clearly.\n\n\n\n\nEach of the people in your group will create his or her own signal, that is, a single number specifying the rate at which events happen.\n\n\nDo not look at your signal until directed to.\nGenerate your signal with this command:\n\n\nsignal <- 50*(5 + runif(1)) # Don't peek\n\nOnce all of your group members have their own signal, you are going to start to generate noisy data. The idea is to compare the observations you make to those from the other group members to decide the extent to which the signals differ from one another.\n\nGenerate a single (\\(n=1\\)) measurement this way:\n\n\nrpois(n=1, lambda=signal)\n\nCompare your measurement to those of each of the other members of the group. For each of those members, decide among these choices, making sure not to look at the numerical value of signal.\n\nMy signal is certainly higher.\nMy signal is likely to be higher.\nCan’t tell whether my signal is different than the other person’s.\nMy signal is likely to be lower.\nMy signal is certainly lower.\n\nRecord your conclusion for each of your group partners.\n\nRepeat the process in (2), but generate \\(n=2\\) observations. You will have two numbers, which you will compare to the other person’s two numbers. You’ll have to decide how you want to do this.\n\nMake a note of the method you choose do the comparison and record your conclusion (i)-(v) for each of your group partners.\n\nRepeat (3), but this time with \\(n=4\\).\n\nAre you still happy with the method you chose in (3)? If not, figure out a new method that can handle the comparison of 4 numbers to another set of 4 numbers.\nOnce again, record your conclusion (i)-(v) of your data against each of your group partners.\n\nRepeat the following for each of your partners separately. Try higher values of \\(n\\) until the two of you agree that your signals are certainly different.\n\n\n\n\nStarting in June 1944, the Germans engaged in missile attacks against London. At first, the V-1 cruise missile was used. Later, the V-2 ballistic missile was added to the attack.\nThe London City Council kept records of V-1 impact sites within their jurisdiction. Figure 1 shows the locations.\n\n\n\n\n\nFigure 1: Map showing impact sites of V-1 missiles in the area administered by the London City Council. The Thames river is shown in blue. The map does not show impacts outside the LCC region.\n\n\n\n\nAn analysis published in 1946 looked at 537 V-1 impacts within the area outlined in red: a region of 144 km2. The area was divided into 576 boxes each of area 0.25 km2. Five-hundred thirty-seven V-1 impacts fell within the rectangle; suggesting that there might be about 1 impact per box.\nTo be more precise, the signal is 537 / 576 = 0.93 impact per box.\nThere is a mathematical theory of how the measured number of impacts will vary from one box to another. The theory is called the Poisson distribution. In the activity, when you used rpois(n=1, lambda=signal) the computer was using this theory to generate the measurement, that is, signal plus noise.\nThese are the results of comparing the theory to the actual observations of how many boxes had no impact, 1 impact, 2 impacts, and so on.\n\n\n\n\n\nFigure 2: The number of boxes (out of 576, total) with different numbers of impacts. The Poisson theory result is in the middle column. (Copied from the 1946 analysis paper.\n\n\n\n\nThis article gives a more detailed account."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#motivating-problem-1-dags",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#motivating-problem-1-dags",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Motivating problem 1: DAGs",
    "text": "Motivating problem 1: DAGs\nYou propose a DAG to describe a specific situation and want to see how well it matches the available data.\nCommon sense suggests that two variables y and x may not have any causal connection between them.\nIn such a case, we anticipate that the model y ~ x + ... will have a coefficient of zero on x. (The + ... stands for other possible explanatory variables, which we call “covariates.”)\nThis lesson is about two closely related things:\n\nWhen is a coefficient small enough that we can regard it as zero?\nWhen we have a coefficient generated by fitting a model to data, how do we describe how precisely we know it?"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#motivating-problem-2-sustainable-fisheries",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#motivating-problem-2-sustainable-fisheries",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Motivating problem 2: Sustainable fisheries",
    "text": "Motivating problem 2: Sustainable fisheries\nDesigning an enforcement regime for limits on scallop fisheries.\n\n\n\n\n\nFigure 1: Life cycle of a scallop\n\n\n\n\nFisheries are regulated by states and the Federal government in order to avoid collapse due to over-fishing. Often, the regulations attempt to protect juveniles—animals that have not yet reached reproductive age. If the juveniles are harvested, their potential progeny are annihilated. There are various ways to do this, for instance restricting fishing to months where adults are most prevalent, closing fisheries to provide an opportunity for the reproductive stock to recover, and so on.\nIn the 1990s, one of the ways the Federal government regulated scallop fisheries was by setting a minimum acceptable size for harvested scallops. For practical reasons, rather than monitoring individual scallops, the government monitored the average per-scallop weight of each boat’s catch. For the sake of the example, imagine that the minimum acceptable weight is 1/30 pound.\nA fishing boat might have 10,000 or more bags of scallops, which can be handled individually: weigh the bag, then count the number of scallops to get the average weight per scallop.\nDiscussion questions:\n\nHow many bags should be sampled? Should this depend on the number of bags in the cargo. For instance, should a cargo of 1000 bags be sampled differently than a cargo of 10,000 bags.\nWhat should be the threshold for declaring the whole cargo below minimum size? (The whole catch is confiscated in such a case.)\n\nIn this section of the course, you’ll learn some statistical concepts and methods that allow the above questions to be answered to produce a regulation that is protective and fair to the fishermen.\nOne idea is very simple: sampling variation. This is about how much the average per-scallop weight will vary from one bag to another.\nAnother idea is very subtle: What you can say about the whole cargo based on a sample of \\(n\\) bags."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#vocabulary-sample-vs-sampling",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#vocabulary-sample-vs-sampling",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Vocabulary: “Sample” vs “sampling”",
    "text": "Vocabulary: “Sample” vs “sampling”\nThe vocabulary here can be confusing, because similar sounding terms refer to different things.\nA sample is a collection, just as a data frame is a collection of rows. The individual items in the collection—the individual rows—are “specimens,” or “cases,” or “rows,” or “units of observation,” or “observations,” or even “tuples.”\n“Sampling” is the process of collecting a sample.\nStatisticians use the phrase “sample statistic” to refer to a summary calculated from a sample. For instance, if your summary is the variance of a variable, this could properly be called the “sample variance.”\nThe terms “sampling variation” and “sampling variance.” “Sampling variation” is a theoretical concept: how much a model coefficient or other sample statistic would vary from one randomly collected sample to another. Our measurement of sampling variance is often accomplished with our usual tool for measuring variation: the variance.\nYou can’t directly see sampling variation in a single sample; however, we can use the theory of sampling variation to estimate from a single sample how much other samples might differ from the sample at hand. In this Lesson, we will simulate sampling variation in order to understand its properties, particularly how it depends on the sample size \\(n\\).\nTechnical vocabulary:\n\n“Confidence interval” (or “CI”) a range indicating the sampling variation of a sample statistic. Example: [19.1, 21.5]. Every model coefficient comes with a confidence interval. The conf_interval() model summary calculates the CI. Example:\n\n\nlm(mpg ~ hp, data=mtcars) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) 26.8    30.1    33.4   \n2 hp          -0.0889 -0.0682 -0.0476\n\n\n\n“Standard error”: the square-root of the “sampling variance.” It might make more sense to use “sampling standard deviation” for the square root of the “sampling variance.”\n\n“Margin of error” is the plus-or-minus part \\(20.3 \\pm 1.2\\) of another format for writing the CI.\n“Confidence level” is a number between 0 and 1. Almost always this is set to be 95%, which is what we will use. It is used to calculate the margin of error, which is a multiple of the standard error. For 95%, the multiplier is about 2. (If you took AP statistics, you may remember the number 1.96, which is their way of saying “two.”)"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#class-activity",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#class-activity",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Class Activity",
    "text": "Class Activity\nPoisson data"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#digression",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#digression",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Digression",
    "text": "Digression\nIs process of fitting like the process of learning in animals? Here’s a report from The Economist:\n\nNever before … has the whole brain of such a complex organism—spanning some 548,000 connections between 3,016 neurons in the case of the fruit-fly larva—been mapped.\n\n\nThe latest work, published in Science, marks the culmination of over a decade’s worth of effort ….\n\n\nThe connectome of the fruit-fly larva has already provided insights. For example, regions of the creature’s brain associated with learning had more loops in their circuitry, with downstream neurons connecting back to those close behind them, than other regions of the brain. This suggested some repeat processing of signals. One proposed explanation is that such loops encode predictions, and that the creatures learn by comparing these with actual experiences.\n\n\nInformation about the taste of a leaf, for example, might enter a neuron simultaneously with a prediction based on previous meals. If the taste differs from prediction, the neuron may secrete dopamine, a chemical capable of rewiring the circuitry to create a new memory."
  },
  {
    "objectID": "Worksheets/Worksheet-22.html",
    "href": "Worksheets/Worksheet-22.html",
    "title": "Lesson 22: Worksheet",
    "section": "",
    "text": "22.1 Describe the logical origin of sampling variation as the variation between multiple samples from the same source.\n22.2 Recognize the several formats in which we describe sampling variation—sampling variance, standard error, margin of error, confidence interval—and show how they are related.\n22.3 Using repeated sampling trials, observe how sampling variance scales with sample size \\(n\\)."
  },
  {
    "objectID": "Worksheets/Worksheet-22.html#overview",
    "href": "Worksheets/Worksheet-22.html#overview",
    "title": "Lesson 22: Worksheet",
    "section": "Overview",
    "text": "Overview\nIt is critical to keep in mind that a sample is a collection, just as a data frame is a collection of rows. The variation we account for by regression models is row-by-row variation in the training data frame.\n“Sampling variation” is a theoretical concept: how much a model coefficient or other sample statistic would vary from one randomly collected sample to another.\nYou can’t directly see sampling variation in a single sample; however, we can use the theory of sampling variation to estimate from a single sample how much other samples might differ from the sample at hand. In this Lesson, we will simulate sampling variation in order to understand its properties, particularly how it depends on the sample size \\(n\\)."
  },
  {
    "objectID": "Worksheets/Worksheet-22.html#part-1",
    "href": "Worksheets/Worksheet-22.html#part-1",
    "title": "Lesson 22: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nUsing dag02, obtain a sample of size 25 and show the values of y.\n\ndag02sample = sample(dag02, size=25)\n\ndag02sample%>%\n  select(y)\n\n# A tibble: 25 × 1\n         y\n     <dbl>\n 1  6.06  \n 2  3.71  \n 3  3.01  \n 4  1.28  \n 5  0.440 \n 6 11.0   \n 7  7.24  \n 8  6.74  \n 9  7.81  \n10 -0.0512\n# … with 15 more rows\n\n\nCompute the mean those 25 values of y in two different, but entirely equivalent ways. (1) Use data wrangling. (2) Construct a model y ~ 1 report the intercept coefficient. Show that these give the same answer.\n\n\n\n\n\n\nANSWER\n\n\n\n\ndag02sample%>%\n  summarize(mean(y))\n\n# A tibble: 1 × 1\n  `mean(y)`\n      <dbl>\n1      4.76\n\ndag02sample %>% \n  lm(y~1,data=.)%>%\n  conf_interval()\n\n# A tibble: 1 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept)  3.32  4.76  6.20"
  },
  {
    "objectID": "Worksheets/Worksheet-22.html#part-2",
    "href": "Worksheets/Worksheet-22.html#part-2",
    "title": "Lesson 22: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nCreate a new chunk that repeats the generation of a sample from dag02 the the two methods for calculating the mean of the y values. Run the new chunk and observe that the calculated value of the mean differs somewhat from that you found in Part 1. Repeat running the chunk over and over again; the mean value will differ each time.\nTask 2.1. Each time you run the chunk, you are performing a new sampling trial. Run a dozen or so trials, observing the calculated value of the mean of y in order to get a sense for how much it varies from trial to trial. Then summarizing your observations by giving a rough interval for the range of the mean of y across the trials.\n\n\n\n\n\n\nANSWER\n\n\n\n\n\n\nWe are going to automate the process of performing sampling trials so that we can run hundreds of them.\nUsing the do operator, calculate the sampling variance for a set of trials from dag02. The following code chunk shows how to run 500 trials, in each of which the mean of y is calculated using the y ~ 1 method and reporting the intercept coefficient. These will be collected into a data frame named dag02trials25.\n\ndag02trials25 <- do(500) * {\n  sample(dag02, size=25) |> \n  lm(y ~ 1, data=_) |>\n  conf_interval()\n}\n\nTask 2.2. Run the chunk above to create dag02trials25. Then use data wrangling commands to compute three summaries of the trials: i. The mean of the coefficient across the trials. ii. The variance of the coefficient across the trials. iii. The standard deviation of the coefficient across the trials.\n\n\n\n\n\n\nANSWER\n\n\n\nThe coefficient for each of the 500 trials is stored in the .coef column of dag02trials25. Simple data wrangling provides the summary.\n\ndag02trials25%>%\n  summarize(mean_of_means = mean(.coef),\n            sampling_variance = var(.coef),\n            standard_error = sd(.coef))\n\n# A tibble: 1 × 3\n  mean_of_means sampling_variance standard_error\n          <dbl>             <dbl>          <dbl>\n1          5.01             0.443          0.665\n\n\nNotice that the names—sampling_variance and standard_error—we used for the different summaries correspond to the standard statistical nomenclature for these quantities.\n\n\nTask 2.3. Repeat (2) with four different sample sizes (try 50, 100, 200, and 400). Fill in the table below. What do you notice about the standard error as sample size increases?\n\n\n\nSample size\nSampling variance\nStandard error\n\n\n\n\nn=25\n0.439\n0.663\n\n\nn=50\n0.253\n0.503\n\n\nn=100\n0.129\n0.359\n\n\nn=200\n0.059\n0.243\n\n\nn=400\n0.032\n0.178\n\n\n\n\n\n\n\n\n\nANSWER\n\n\n\nThe sampling variance gets smaller as the sample size increases. Specifically, doubling the sample size tends to halve the sampling variance. The standard error—which is just the square-root of the sampling variance—also gets smaller as \\(n\\) increases. As in the nature of square roots, to halve the standard error, the sample size must be doubled twice."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#essential-take-home-points",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#essential-take-home-points",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Essential take-home points",
    "text": "Essential take-home points\n\nThe sampling variance depends on the sample size. Larger n leads to smaller sampling variance. The dependence is simple: sampling variance goes as \\(1/n\\).\nThe width of the confidence interval depends on the square-root of the sampling variance. Consequently, the width of the confidence interval goes as \\(1/\\sqrt{n}\\).\n\nStated more simply: Under normal conditions, more data means shorter confidence intervals."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#work-though-worksheet-22",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#work-though-worksheet-22",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Work though worksheet 22",
    "text": "Work though worksheet 22\nThis is a simulation demonstration of the essential take-home points listed above. In Lesson 23 we’ll see how it’s possible to do the calculations in real data."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#confidence-level",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#confidence-level",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Confidence level",
    "text": "Confidence level\nNot a big deal in our course. Use 0.95, which is the default for conf_interval().\nA lower confidence level produces a shorter CI, a higher confidence level produces a longer CI.\n\nlm(mpg ~ hp, data=mtcars) |> conf_interval(level=0.80) |> filter(term==\"hp\")\n\n# A tibble: 1 × 4\n  term     .lwr   .coef    .upr\n  <chr>   <dbl>   <dbl>   <dbl>\n1 hp    -0.0815 -0.0682 -0.0550\n\nlm(mpg ~ hp, data=mtcars) |> conf_interval(level=0.90) |> filter(term==\"hp\")\n\n# A tibble: 1 × 4\n  term     .lwr   .coef    .upr\n  <chr>   <dbl>   <dbl>   <dbl>\n1 hp    -0.0854 -0.0682 -0.0511\n\nlm(mpg ~ hp, data=mtcars) |> conf_interval(level=1.00) |> filter(term==\"hp\")\n\n# A tibble: 1 × 4\n  term   .lwr   .coef  .upr\n  <chr> <dbl>   <dbl> <dbl>\n1 hp     -Inf -0.0682   Inf"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "",
    "text": "Basic procedure in statistical interpretation of data: collect data then calculate one or more sample statistics from those data. Example: Galton’s data on heights and an analysis of whether a child’s height is related to the parents’.\n\n\nlm(height ~ mother + father + sex, data=Galton) |> coefficients()\n\n(Intercept)      mother      father        sexM \n 15.3447600   0.3214951   0.4059780   5.2259513 \n\n\nQuestion: Are fathers more influential on the children’s height than mothers?\n\nBut statisticians don’t regard the sample statistics in (1) as completely informative because we know, in our hearts, that the particular sample we worked with is just one of many, many samples that we might have collected. Each of those hypothetical samples has its own sample statistic, which likely differs from the sample statistic we calculated in (1). This is what we mean by sampling variability.\nWe can say that sampling variability implies that our sample statistics are noisy, that they are not absolutely precise.\nWe describe the precision of a sample statistic via a confidence interval, which can be in either of two forms, e.g.:\n\n\\([56.3, 57.1]\\) or \\(56.7 \\pm 0.4\\)"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#lesson-23",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#lesson-23",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "Lesson 23",
    "text": "Lesson 23\nThere are mathematical techniques that allow us to get a handle on the precision of a sample statistic using just the sample at hand. Here is the central mathematical fact:\n\nWidth of confidence interval is proportional to \\(1/\\sqrt{n}\\).\n\nLet’s demonstrate this using simulations (via DAGs).\n\nsample(dag06, size=400) |> \n  lm(d ~ c + a, data=_) |>\n  conf_interval() |>\n  mutate(width = .upr - .lwr) \n\n# A tibble: 3 × 5\n  term          .lwr   .coef   .upr width\n  <chr>        <dbl>   <dbl>  <dbl> <dbl>\n1 (Intercept) -0.119 -0.0144 0.0902 0.209\n2 c            0.916  0.987  1.06   0.142\n3 a            0.836  0.969  1.10   0.267\n\n\nHere, the width of each term is subject, like everything else, to sampling variation.\nWe can average over many trials to reduce the sampling variation.\n\n{do(500) * {\n  sample(dag06, size=100) |> \n  lm(d ~ c + a, data=_) |>\n  conf_interval() |>\n  mutate(width = .upr - .lwr)} \n  } |>\n  group_by(term) %>% \n  summarize(ave_width = mean(width))\n\n# A tibble: 3 × 2\n  term        ave_width\n  <chr>           <dbl>\n1 (Intercept)     0.401\n2 a               0.495\n3 c               0.285\n\n\nTry this for different sample sizes: 25, 100, 400.\n\nWhat will be the width of the confidence interval when the sample size is 1600?"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#how-to-find-the-confidence-interval-when-we-have-only-one-sample",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#how-to-find-the-confidence-interval-when-we-have-only-one-sample",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "How to find the confidence interval when we have only one sample",
    "text": "How to find the confidence interval when we have only one sample\n\nConstruct many trials of subsamples, say 1/50th the size of the data.\n\n\nlm(height ~ mother + father + sex, data=sample(Galton, size=18)) |>\n  conf_interval() |>\n  select(term, .coef)\n\n# A tibble: 4 × 2\n  term         .coef\n  <chr>        <dbl>\n1 (Intercept) 36.3  \n2 mother       0.216\n3 father       0.193\n4 sexM         6.21 \n\n\n\n{do(500) * {\n  lm(height ~ mother + father + sex, data=sample(Galton, size=18)) |>\n  conf_interval() |>\n  select(term, .coef)\n}} |> \n  group_by(term) |>\n  summarize(var_coef = var(.coef))\n\n# A tibble: 4 × 2\n  term        var_coef\n  <chr>          <dbl>\n1 (Intercept) 560.    \n2 father        0.0644\n3 mother        0.0684\n4 sexM          1.19  \n\n\nWe can look at any of the coefficients. Let’s use sexM for the example.\nThe sampling variance of the sexM coefficient for a sample of size \\(n=18\\) is 1.14 inches2.\n\nThe “standard error” corresponding to this is \\(\\sqrt{1.14} = 1.07\\) inches.\nThe width of the confidence interval for a sample of size 18 will be four times the standard error—that’s part of the definition of the confidence interval, that is 4.28.\nFour the whole sample, \\(n=898\\), the width of the confidence interval will be \\(4.28 \\times \\sqrt{\\frac{18}{898}} = 0.606\\).\n\nCheck this against the calculation:\n\nlm(height ~ mother + father + sex, data=Galton) |>\n  conf_interval() %>%\n  mutate(width=.upr - .lwr)\n\n# A tibble: 4 × 5\n  term         .lwr  .coef   .upr  width\n  <chr>       <dbl>  <dbl>  <dbl>  <dbl>\n1 (Intercept) 9.95  15.3   20.7   10.8  \n2 mother      0.260  0.321  0.383  0.123\n3 father      0.349  0.406  0.463  0.115\n4 sexM        4.94   5.23   5.51   0.565\n\n\n\nWhat does this report tell you about whether fathers contribute more to children’s height than mothers? Compare the confidence interval on mother and father."
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#activity",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#activity",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "Activity",
    "text": "Activity\nGot you covered"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#precision-versus-accuracy",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#precision-versus-accuracy",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "Precision versus accuracy",
    "text": "Precision versus accuracy\nIn the case of our model y ~ x + a on data simulated from dag02 we can compare the model coefficients to the formula for y in the DAG.\n\ndag_draw(dag06)\n\n\n\nprint(dag06)\n\na ~ exo()\nb ~ a + exo()\nc ~ b + exo()\nd ~ c + a + exo()\n\nsample(dag06, size=25) |> \n  lm(d ~ c + a, data=_) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) -0.748 -0.327 0.0930\n2 c            0.677  0.981 1.28  \n3 a            0.673  1.28  1.88  \n\n\nThe coefficients are a close match to the DAG formula.\nIf I repeat this simulation many times, roughly one time in twenty the actual formula coefficient will be outside the confidence interval. (Do the trials.)\nMake the sample size very large so that we get a very narrow confidence interval to demonstrate the accuracy.\n\nFrom the above, predict what sample size is needed to get a confidence interval that is about 0.01 wide. (The CI on x for example, is has width about 1. To get 0.01 we need 1002 as much data.)\n\n\nsample(dag06, size=250000) |> \n  lm(d ~ c + a, data=_) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term            .lwr     .coef    .upr\n  <chr>          <dbl>     <dbl>   <dbl>\n1 (Intercept) -0.00493 -0.000993 0.00295\n2 c            0.996    0.999    1.00   \n3 a            0.995    1.00     1.00   \n\n\nMore data buys better precision. But accuracy is a different matter altogether. The above model is both precise and accurate.\nHere is a model that is very precise, but not at all accurate:\n\nsample(dag06, size=250000) |> \n  lm(d ~ c, data=_) |>\n  conf_interval()\n\n# A tibble: 2 × 4\n  term            .lwr    .coef    .upr\n  <chr>          <dbl>    <dbl>   <dbl>\n1 (Intercept) -0.00454 0.000515 0.00557\n2 c            1.33    1.33     1.33   \n\n\n\nWhat is it that tells you the precision of the coefficients?\n\n\nWhat is it that tells you the coefficient on c is not accurate?"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#going-further",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#going-further",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "Going further",
    "text": "Going further\nStatistics texts tend to feature simple models without covariates.\nObviously, the choice of covariates or other model terms is not an issue. Sample statistics are designed mathematically so that they are accurate. This is called being unbiased.\nBut in cases of actual interest, where we are interested in causal connections, we generally cannot know from our data whether the sample statistic is accurate.\nCollecting more data will not help us determine the accuracy; more data improves precision but not accuracy.\nLater, in Lesson 30, we can return to accuracy. There, we’ll look at accuracy with respect to a DAG, choosing covariates so that the model would be accurate if the data were indeed generated by a mechanism well represented by our DAG."
  },
  {
    "objectID": "Worksheets/Worksheet-23.html",
    "href": "Worksheets/Worksheet-23.html",
    "title": "Lesson 23: Worksheet",
    "section": "",
    "text": "You are going to work with data collected in the 1970s to examine the effects of smoking and exposure to second-hand smoke on pulmonary functions in youths. The data frame is FEV and is included in the {math300} package.\nThe response variable that we will study is also called FEV, standing for the “forced expiratory volume” measured in the participants in the study. In general, higher forced expiratory volume is considered a sign of better respiratory health."
  },
  {
    "objectID": "Worksheets/Worksheet-23.html#task-2",
    "href": "Worksheets/Worksheet-23.html#task-2",
    "title": "Lesson 23: Worksheet",
    "section": "Task 2",
    "text": "Task 2\n\nWhat is the width of the confidence interval on smokersmoker?\n\n\n\n\n\n\n\nANSWER\n\n\n\nYou can find the length of the confidence interval simply by subtracting the .lwr limit of the interval from the .upr limit. Here that’s 0.927 - 0.494 = 0.433.\n\n\n\nTask 3.\nJust for pedagogical purposes, we are going to explore how the width of the confidence interval would change if we had more or less data. You already have heard the theoretical relationship of the width of the confidence interval as a function of sample size \\(n\\).\n\nWhat is the size of the sample contained in FEV?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nnrow(FEV)\n\n[1] 654\n\n\n\n\n\nUsing the theoretical relationship with \\(n\\), what do you think the width of the confidence interval would be if only \\(n=150\\) rows of data were available?\n\n\n\n\n\n\n\nANSWER\n\n\n\n150 is about one-quarter the sample size of FEV. So a confidence interval calculated on a sample of \\(n=150\\) will be about \\(\\sqrt{4}\\) times larger. That is, the sample size 150 will lead to a confidence interval about twice as wide as the confidence interval from the full sample.\n\n\n\nWe can easily simulate working with a sample of \\(n=150\\). To do this, fit a model (and calculate the confidence interval on smokersmoker), but rather than using the argument data=FEV use this instead: data=sample(FEV, size=150). Compare the width of confidence interval you get in this way to your theoretical prediction in (2).\nThis will be surprising, but we can actually simulate what would happen if we had a larger sample size. (This is just a simulation, and just for pedagogical purposes. This is not a way to collect a genuine sample of a larger size.)\n\nTo create a (simulated) sample of size, say, \\(n=2500\\) set the data argument to lm() this way: data=resample(FEV, size=2500). (NOTE: The function being used here is not sample() but the closely related resample(), with an re in front.)\nCalculate the width of the (simulated) confidence interval on smokersmoker for the sample size of 2500.\n\n\n\n\n\n\nANSWER\n\n\n\n\nlm(FEV ~ smoker, data=resample(FEV, size=2500)) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr .coef  .upr\n  <chr>        <dbl> <dbl> <dbl>\n1 (Intercept)  2.54  2.58  2.61 \n2 smokersmoker 0.629 0.742 0.854\n\n\nThe width of the confidence interval on `smokersmoker is about 0.20.\n\n\n\n\nTask 3\nWere you surprised to see in Task 1 that smoking is associated with a higher FEV than non-smoking? Since a higher FEV is considered healthier, does this mean that smoking is healthy? The answer is “no,” but let’s consider it from the perspective of accuracy versus precision.\nThe confidence interval on smokersmoker in the model FEV ~ smoker was 0.50 to 0.93 liters. This precision is good enough to rightfully claim that the smokersmoker coefficient is not zero or negative.\nBut precision is different from accuracy. One of the major potential determinants of FEV is age.\n\nBuild a model FEV ~ age and construct the confidence interval on age. Explain whether your model is consistent or not with the idea that FEV depends on age.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nlm(FEV ~ age, data=FEV) |> conf_interval()\n\n# A tibble: 2 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept) 0.279 0.432 0.585\n2 age         0.207 0.222 0.237\n\n\nThe age coefficient is about 0.2 liters per year. In other words, FEV increases with age.\n\n\n\nIt also happens that smoking is associated with age. The younger kids don’t smoke. We can demonstrate this with a model of smoker ~ age. Since smoker is a categorical variable, we need to convert it to a zero-one variable before fitting the model. Here’s a chunk to do so, assigning the smokers to have a value of 1:\n\n\nFEV |> \n  mutate(smoke = zero_one(smoker, one=\"smoker\")) |>\n  lm(smoke ~ age, data = _) |>\n  conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) -0.381  -0.308  -0.234 \n2 age          0.0338  0.0410  0.0481\n\n\nInterpret the coefficient as a rate of probability: how the probability that a participant smokes changes per year of age.\n\nIs the age coefficient consistent with the idea that older kids are more likely to smoke?\nNow the point about accuracy and precision being different things. FEV increases with age and so does the probability of being a smoker. That means that smoker is also related to age. In fact, to some extent smoker is a proxy for age. As an exercise, draw on paper a DAG where age influences FEV, and age influences smoking status, and also smoking status influences FEV.\n\nFor the DAG just described, an accurate model to estimate the direct effect of smoking on FEV is FEV ~ age + smoker. Fit this model and use the confidence interval on smoker to make an accurate statement about smoking and FEV.\n::: {.callout-note} ## ANSWER\n\nlm(FEV ~ age + smoker, data = FEV) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr  .coef    .upr\n  <chr>         <dbl>  <dbl>   <dbl>\n1 (Intercept)   0.207  0.367  0.527 \n2 age           0.215  0.231  0.247 \n3 smokersmoker -0.368 -0.209 -0.0504\n\n\nThe confidence interval on smokersmoker is entirely negative. Negative means that smoking is associated with smaller FEV. This sort of thing would be summarized as, “After adjusting for age, smoking is associated with smaller FEV.”"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300Z",
    "section": "",
    "text": "Math 300Z is the prototype for scheduled revisions to Math 300. The revisions apply only to Lessons 19 and up; the first 18 lessons come from Math 300.\nMar 3 Lesson 19: Topic: Variation Reading, Activity: Measuring by eye\nMar 7 Lesson 20: Topic: DAGs and simulation Reading, Activity: Life savers?\nMar 9 Lesson 21: Topic: Signal and noise Reading, Activity: Membrane channels Handout\nMar 13 Lesson 22: Topic: Sampling variation Reading, Activity: Counts and waiting times\nMar 15 Lesson 23: Topic: Confidence intervals Reading, Activity: Got you covered!\nMar 17 Lesson 24: Topic: Effect size Reading, Activity: With respect to …\nMar 21 Lesson 25: Topic: Prediction mechanics Reading, Activity:\nMar 23 Lesson 26: Topic: Prediction intervals Reading, Activity: Intervals by eye, Handout\nApr 3 Lesson 27: REVIEW of lessons 19-26 :: Reading\nApr 5 Lesson 28: Topic: Covariates :: Reading, Activity:\nApr 7 Lesson 29: Topic: Covariates eat variance :: Reading, Activity:\nApr 11 Lesson 30: Topic: Confounding :: Reading, Activity:\nApr 13 Lesson 31: Topic: Spurious correlation :: Reading, Activity:\nApr 17 Lesson 32: Topic: Experiment & random assignment :: Reading, Activity:\nApr 19 Lesson 33: Topic: Measuring and accumulating risk :: Reading, Activity:\nApr 24 Lesson 34: Topic: Constructing a classifier :: Reading, Activity:\nApr 27 Lesson 35: Topic: Accounting for prevalence :: Reading, Activity:\nMay 1 Lesson 36: Topic: Hypothesis testing :: Reading, Activity:\nMay 3 Lesson 37: Topic: Calculating a p-value :: Reading, Activity:\nMay 5 Lesson 38: Topic: False discovery with hypothesis testing :: Reading, Activity:\nMay 9 Lesson 39: REVIEW of lessons 28-38 :: Reading\nMay 11 Lesson 40: Review of entire course"
  },
  {
    "objectID": "index.html#course-textbooks",
    "href": "index.html#course-textbooks",
    "title": "Math 300Z",
    "section": "Course textbooks",
    "text": "Course textbooks\n\n\nFor Lessons 19-39, the textbook is Lessons in Statistical Thinking"
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Math 300Z",
    "section": "Software",
    "text": "Software\nThe POSIT.cloud workspaces will already have the packages installed. If you are running RStudio on your laptop (that is, not in a browser), the following command will install the packages.\ninstall.packages(c(\"mosaic\", \"ggplot\", \"dplyr\", \"openintro\", \"moderndive\", \"nycflights13\", \"knitr\"))\n# addition package for Math 300Z\nremotes::install_github(\"dtkaplan/math300\")\n\nA note on computing summaries of data"
  },
  {
    "objectID": "index.html#first-half",
    "href": "index.html#first-half",
    "title": "Math 300Z",
    "section": "First half of Math 300, Jan. and Feb. 2023",
    "text": "First half of Math 300, Jan. and Feb. 2023\n\n\nFor Lessons 1-18, the textbook is Statistical Inference via Data Science\n\n\n\n\nData, graphics, wrangling\nReading: SIDS Chapters 1 through 4\n\nData with R\nScatterplots\nLinegraphs, histograms, facets\nBoxplots and barcharts\nfilter and summarize\ngroup_by, mutate, arrange\njoin, select, rename, & top n\nImporting data\nCase study/review\nGR1 (chapters 1-4)\n\nRegression\nReadings 11. SLR: Continuous x 12. SLR: Discrete x 13. SLR: Related topics 14. Multiple regression: Numerical & discrete 15. Multiple regression: Two numerical 16. Multiple regression: Related topics 17. Multiple regression: Conclusion/review 18. GR 2 (chapters 5-6)"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Ruler-activity-handout.html",
    "href": "Day-by-day/Lesson-19/Ruler-activity-handout.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "In this activity, you will be given two strips of paper printed with:\n\nAn empty rectangular box\nA ruler\n\n\n\n\nWithout using the ruler at all, subdivide by eye the rectangular box into three equal-sized sections, like this:\n\n\n\n\n\n\n\nNow the ruler comes into play. Measure the lengths of your three subdivisions using the ruler.\nRecord your three measurements in this spreadsheet. Also create an ID for yourself, for example your initials or the initials of your favorite aunt, baseball player, or whatever.\n\n\n\nLink to a spreadsheet for data entry\n\nOnce everyone has entered their measurements, copy the following statement into the console in Posit.cloud and run it to create a data frame named Thirds.\n\n\nThirds <- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT_asFV5LD312bYaGgHK3F91kgLVSiaQpNhggDilfPKAiDBNz9iueOiYWKgAtRRwkFlOz6U9znbiMGK/pub?gid=0&single=true&output=csv\")\n\n\nFollow the instructions given in class. These will have you\n\nCalculate the variance of the three measurements for each student separately. Use mutate() to calculate modulus <- ((left-middle)^2 + (left-right)^2 + (middle-right)^2)/3, storing the result back in Thirds as a variable named modulus.\nAnalyze how good you and your colleagues are at sub-dividing evenly by eye.\nCreate a new dataframe that is Thirds re-arranged into “long” format.\n\n Long_form <- tidyr::pivot_longer(Thirds, !Student_initials, names_to = \"position\")\n\nGroup Long_form by student initials and calculate the variance of value. Compare these values to those stored under modulus in Thirds.\n\n\n\n\n\n\nWhy is it not useful, for the purposes of measuring the quality of the subdivision, to calculate the mean of the left, middle, and right segment lengths?\nWhat did you look for in Step (ii)?"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "",
    "text": "You have been learning some basics of data wrangling and visualization, along with what ModernDive calls “basic regression” and “multiple regression.” These are tools which you will continue to use in the second half of the semester.\nSuch tools are necessary but usually not sufficient. Data only occasionally speak for themselves. Most often, we need to interpret data in the context of what we already know or believe about the system under study.\nExample: As you’ve seen, merely fitting a regression model does not demonstrate that there is a causal relationship between the explanatory variables and the response variable. We will need some new concepts to encode our ideas (and speculations) about causal relationships and to use regression modeling to inform (or contradict) our ideas.\nExample: We’ll see how to avoid seeing patterns and relationships for which the evidence in unpersuasive. Example: We will see how detection thresholds can be set to reflect our opinions about the costs and benefits of different ways of getting it right or wrong and our prior knowledge of the frequency of different kinds of events."
  },
  {
    "objectID": "Day-by-day/Lesson-26/Intervals-by-eye.html#intervals-by-eye",
    "href": "Day-by-day/Lesson-26/Intervals-by-eye.html#intervals-by-eye",
    "title": "Spring 2023 Math 300Z",
    "section": "Intervals by eye",
    "text": "Intervals by eye\n\nIn this activity, you are given some point plots of data: y vs x. Your job is to\n\nsketch in an appropriate model fitted (by eye) to the data.\nadd a prediction band showing for each value of x what is the prediction interval\ntransform the prediction band into a confidence band.\n\nTIPS:\n\nThe fitted model will be a line or curve, or in the case of Model 3, two lines.\nThe bounds of the prediction band will be more-or-less parallel to the fitted model, but should include roughly 95% of the \\(n\\) points in the plot.\nThe confidence band is narrower than the prediction band by a factor of \\(1/\\sqrt{n}\\).\n\n\n\n\n\nModel 1: A straight-line model y ~ x\n\n\n\n\n\n\n\n\nModel 2: A sine-wave model, y ~ sin(x)\n\n\n\n\n\n\n\n\n\nModel 3: A function of two variables, y ~ x + group"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Patch-clamping.html#membrane-channels",
    "href": "Day-by-day/Lesson-21/Patch-clamping.html#membrane-channels",
    "title": "Spring 2023 Math 300Z",
    "section": "Membrane channels",
    "text": "Membrane channels\nElectrical signaling is one of the means of cell-to-cell communication in organisms. Familiar examples are nerve cells and muscle cells. The electrical activity is mediated by assemblies of a few proteins—called “membrane channels”—that penetrate the cell membrane and switch minute flows of electrical current on and off depending on conditions in the cell and the influence of neighboring cells.\nAmazingly, even though this activity involves only thousands of atoms, it is possible to record the on-again-off-again activity of a single channel. (The 1991 Nobel Prize in Physiology or Medicine was awarded to E. Neher and B. Sakmann for their invention of the measurement technique, which is now widely used in electrophysiology research.)\nThe figure shows 16 recordings of the activity of different channels. Each of the recordings is a combination of signal and noise.\nTASK: In three recordings of your choice,\n\nIdentify the signal by drawing it over the recording.\nMeasure the typical amplitude of the noise. For this purpose, use the guide at the lower-right corner of the figure: a short vertical line marks the amplitude of 5 pA, that is, 5 pico-Amps.\n\nTo identify the signal, you need to know something about what the signal looks like. In our work in Math 300Z, the “signal” corresponds to a relationship between two variables or more. Here, one of the variables is time, the other is electrical current. Usually in Math 300Z we will be interested in “linear” relationships between the variables and we seek to identify the signal using only that limited piece of information. To define the signal in the single-channel recordings, use this information: the channels open and close to current, occasionally staying open (or closed) for the better part of a second, but also opening (or closing) for a much shorter time, say 0.01 second.\n\n\n\n\n\nSource of image: Kawano, R., Tsuji, Y., Sato, K. et al. (2013) “Automated Parallel Recordings of Topologically Identified Single Ion Channels”. Scienfic Reports **3(#1995) https://doi.org/10.1038/srep01995"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Life-savers.html#saving-lives",
    "href": "Day-by-day/Lesson-20/Life-savers.html#saving-lives",
    "title": "Spring 2023 Math 300Z",
    "section": "Saving Lives",
    "text": "Saving Lives\nA tourniquet is a belt-like device used to cut off the blook supply to a damaged and severely bleeding limb. A 2014 study of 1413 US casualities in Afghanistan and Iraq concluded that “those who received tourniquets had survival rates similar to those of comparable, transfused casualties who did not receive tourniquets.” That study was careful to take into account injury severity when comparing the casualties with tourniquets to those without. (JF Kragh et al. (2014) “Transfusion for Shock in US Military War Casualties With and Without Tourniquet Use” Annals of Emergency Medicine 65(3) link)\nThe study authors pointed out a potential bias in the collection of data. Only those soldiers who survived up to arrival at the hospital were included.\nConsider these four factors:\n\nInjury SEVERITY\nTourniquet USE (at the battle location)\nADMISSION, that is, arrival at the hospital\nPost-Admission SURVIVAL\n\nTASK: Construct a directed acyclic graph with a node for each of these factors. Draw directed causal links between each pair of nodes that you think are likely to be connected. For each link that you draw, make sure to show the direction of causation, giviving a few words of explanation. Similarly, when there is a pair of nodes without a direct connection, explain why not.\n \n \n \nNote: The direct paths you draw may create longer, indirect paths. For instance, \\(\\mathbb{A} \\longrightarrow \\mathbb{B} \\longrightarrow \\mathbb{C}\\) has direct paths between \\(\\mathbb{A}\\) and \\(\\mathbb{B}\\) as well as between \\(\\mathbb{B}\\) and \\(\\mathbb{C}\\). However, there is no direct connection between \\(\\mathbb{A}\\) and \\(\\mathbb{C}\\).\n\nReference: J Pearl and D Mackenzie (2018) The Book of Why pp343-7"
  },
  {
    "objectID": "Worksheets/Worksheet-20.html",
    "href": "Worksheets/Worksheet-20.html",
    "title": "Lesson 20: Worksheet",
    "section": "",
    "text": "20.1 [Technical] Collect a sample from a DAG simulation.\n20.2 [Technical] Examine the formulas behind a DAG simulation and compare to the results of a regression model trained on a sample from the DAG simulation.\n20.3 [Conceptual] Recognize properties of a DAG. i. Identify exogenous nodes. ii. Identify all pathways between two specified end nodes. iii. On a given pathway, is there causal flow from one end node to another? iv. On a given pathway, is there a causal flow from some node on the pathway to both end nodes?"
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-1-samples-from-dags",
    "href": "Worksheets/Worksheet-20.html#part-1-samples-from-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 1: Samples from DAGs",
    "text": "Part 1: Samples from DAGs\n\nUse dag_draw() to draw a picture of the dag08 directed acyclic graph. From this graph, explain why node c is exogenous and why x and y are not.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\ndag_draw(dag08)\n\n\n\n\nNode C is exogenous because it has no incoming arrows.\n\n\n\nUse print() to view the formulas used by dag08 to simulate data. What about the formula for y indicates that it’s receives inputs from x and c.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nprint(dag08)\n\nc ~ exo()\nx ~ c + exo()\ny ~ x + c + 3 + exo()\n\n\nThe right-hand side of the formula for y says that y will be calculated as the sum of x and c (plus 3 plus some random noise). That is, x and c directly shape the value of y.\n\n\n\nThere are three coefficients in the formula for y: an intercept, an x coefficient, and a c coefficient. (There is also some random input from an exogenous source unrelated to c or x.) What are the numerical values of the three coefficients?\n\n\n\n\n\n\n\nANSWER\n\n\n\nFrom the formula for y in the dag, the coefficients are 1 for x, 1 for c, and 3 for the intercept.\n\n\n\nCollect a sample of size \\(n=100\\) from dag08 and use it to train the model with specification y ~ x. Do the coefficients reported match those you found in part (c)? (If you are not sure, use a bigger sample size, say \\(n=1000\\) or even bigger.)\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag08, size=1000)\nlm(y ~ x, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept)  2.96  3.04  3.11\n2 x            1.46  1.51  1.56\n\n\n\n\nThe model says the x coefficient is about 1.5, not the same as in the DAG formula for y.\n\nSimilar to (4), but use the specification y ~ x + c. How do the coefficients for this model compare to those you found in (3)?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag08, size=1000)\nlm(y ~ x + c, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept) 2.97  3.03   3.09\n2 x           0.889 0.952  1.01\n3 c           0.998 1.09   1.18\n\n\nWhen we include both x and c in the model specification, the coefficients work out to match those of the DAG formula for y."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-2-paths-in-dags",
    "href": "Worksheets/Worksheet-20.html#part-2-paths-in-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 2: Paths in DAGs",
    "text": "Part 2: Paths in DAGs\n\nIn dag08 there are two paths connecting x andy. One path is direct, \\(X \\longrightarrow Y\\). The other path is indirect, \\(X \\longleftarrow C \\longrightarrow Y\\).\n\nAlong the indirect path, is there a causal flow from x to y?\nAlong the indirect path, is there a causal flow from any node on the graph that reaches both endpoints, x and y?\n\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nYes, the flow runs directly from x to y.\nYes. The flow runs from c to each of x and y.\n\n\n\n\ndag_school2 is a highly simplistic model of the relationship between expenditures on schools and student outcomes in terms of, say, standardized test scores.\n\n\ndag_draw(dag_school2, vertex.label.cex=1, vertex.size=40)\n\n\n\n\nThere is a direct pathway from expenditure to outcome as well as another, indirect pathway.\n\nAre there any exogenous nodes in the graph?\nOn the indirect pathway, is there a causal flow from expenditure to outcome?\nIs there a causal flow from any node on the indirect pathway to both expenditure and outcome? Which one?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nCulture is an exogenous node; there are no incoming arrows to culture.\nYes.\nFrom Culture there is a flow to outcome through expenditure. There is also a flow from Culture to Outcome via Participation."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "href": "Worksheets/Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "title": "Lesson 20: Worksheet",
    "section": "Part 3: Are expenditures good for school outcomes?",
    "text": "Part 3: Are expenditures good for school outcomes?\n\nLook at the formulas for dag_school2. Is a higher expenditure connected to a higher outcome?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nprint(dag_school2)\n\nculture ~ unif(-1, 1)\nexpenditure ~ 12000 + 4000 * culture + exo(1000)\nparticipation ~ (50 + 30 * culture + exo(15)) %>% pmax(0) %>% \n    pmin(100)\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + exo(50)\n\n\nThe formula for Outcome has a positive coefficient (0.01) on expenditure. So when expenditure goes up, so will outcome. The magnitude of the coefficient is neither here nor there. Remember that there are always units associated with a coefficient. It’s impossible to say whether a magnitude is large or small unless you know the units.\n\n\n\nGenerate a simple of size 1000 from dag_school2 and use it to train the model outcome ~ expenditure. Is the coefficient on expenditure consistent with what you found in (1)? (If you aren’t sure, use a larger sample size, say 10,000.) What about the coefficient on expenditure leads to your conclusion?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag_school2, size=1000)\nlm(outcome ~ expenditure, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term             .lwr     .coef      .upr\n  <chr>           <dbl>     <dbl>     <dbl>\n1 (Intercept) 1152.     1176.     1201.    \n2 expenditure   -0.0150   -0.0130   -0.0110\n\n\nThe coefficient on Expenditure is negative in contrast to the known positive coefficient in the DAG formula for Outcome.\n\n\n\nSpeculate on what might be the origin of the evident inconsistency between (1) and (2)?\n\n\n\n\n\n\n\nANSWER\n\n\n\nIn the DAG, Outcome is influenced negatively by Participation. And Expenditure is influenced positively by Participation. The two effects of Participation combine to produce an overall negative link between Expenditure and Outcome. By overall, we mean the combination of the direct Expenditure to Outcome link and the indirect path from Expenditure to Outcome via Participation."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-4-constructing-a-dag",
    "href": "Worksheets/Worksheet-20.html#part-4-constructing-a-dag",
    "title": "Lesson 20: Worksheet",
    "section": "Part 4: Constructing a DAG",
    "text": "Part 4: Constructing a DAG\nIn this task, you will construct DAGs using dag_make() and draw them using dag_draw().\nA DAG is defined by a series of tilde expressions, one for each node in the graph. The tilde expression for a node has the node’s name on the left-hand side of the tilde. The right-hand side contains the nodes which serve as inputs to the node named on the left-hand side. If there are no inputs, write exo().\nFor example, consider a DAG with three nodes: one, two, and three. To define a DAG where node two receives input from node one, and node three receives input from nodes one and two, use make_dag() with three tilde expressions:\n\nexample_dag <- dag_make(\n  one ~ exo(),\n  two ~ one,\n  three ~ two + one\n)\ndag_draw(example_dag)\n\n\n\n\nThe right-hand side of a formula can be any arithmetic expression involving the node names, but we will keep it simple: just use + to separated the node names. If a node receives no inputs, the right-hand side should be simply exo() to mark that node as exogenous.\n\nWhat happens if node one, instead of being exogenous, takes as input one of the other two nodes in example_dag?\n\n\n\n\n\n\n\nANSWER\n\n\n\nThe graph would become cyclic, hence not a DAG. Notice that by using a node on the right-hand side of a tilde expression only when it has already been created by a previous tilde expression, you guarantee that the graph will be acyclic.\n\n\n\nCreate and draw a DAG that has the same arrangement of causal connections as “Professor Butts and the Self-Operating Napkin,” illustrated below:\n\n\nProfessor Butts and the Self-Operating Napkin (1931). Soup_spoon (A) is raised to mouth, pulling string (B) and thereby jerking ladle (C), which throws cracker (D) past toucan (E). Toucan jumps after cracker and perch (F) tilts, upsetting seeds (G) into pail (H). Extra weight in pail pulls cord (I), which opens and ignites lighter (J), setting off skyrocket (K), which causes sickle (L) to cut string_m (M), allowing pendulum with attached napkin to swing back and forth, thereby wiping_chin.\nWatch your spelling of node names! Use this command to draw your napkin_dag:\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)\n\n\n\n\n\n\nANSWER\n\n\n\n\nnapkin_dag <- dag_make(\n  soup_spoon ~ exo(),\n  string ~ soup_spoon,\n  ladle ~ string,\n  cracker ~ ladle,\n  toucan ~ cracker,\n  perch ~ toucan,\n  seeds ~ perch,\n  pail ~ seeds,\n  cord ~ pail,\n  lighter ~ cord,\n  skyrocket ~ lighter,\n  sickle ~ skyrocket,\n  string_m ~ sickle,\n  wiping_chin ~ string_m\n)\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html",
    "href": "Worksheets/Worksheet-19.html",
    "title": "Lesson 19: Worksheet",
    "section": "",
    "text": "19.1. [Conceptual] Master the use and units of variance and standard deviation in measuring variability.\n19.2. [Conceptual] Understand the equivalence between mean and proportion on a zero-one variable.\n19.3. [Technical] Use var() and sd() within summarize()\n19.4. [Technical] Use model_plot() to graph models with one or two explanatory variables.\n19.5. [Technical] Use zero_one() with mutate() to create a zero-one variable."
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#preliminaries-how-we-will-work-with-r.",
    "href": "Worksheets/Worksheet-19.html#preliminaries-how-we-will-work-with-r.",
    "title": "Lesson 19: Worksheet",
    "section": "Preliminaries: How we will work with R.",
    "text": "Preliminaries: How we will work with R.\nIn the first half of Math 300Z, the daily student notes were largely structured around “scaffolded” R code, which often involved filling in the blanks. In this second half of 300Z, we will start to use a new way of helping you construct appropriate R command. We call this “command patterns. For instance,\nDF %>% summarize(NM=var(VAR)) \nis a command pattern.\nOne reason for the shift to the command-pattern style is that there will be only a handful of new patterns in the second half of the course that you’ll be using over and over again. Another reason is to help you develop “finger memory” for the most common patterns. An analogy: scaffolding is like GPS navigation which certainly makes it easier to drive but harder to get to know the town. Command patterns are like a paper map, there to help you when you need it.\nThere is a specific notation for command patterns, which you should memorize. Instead of the blanks used in a scaffold, the command pattern uses a CAPITALIZED abbreviation for the **kind of thing* that should be put in the position. Common kinds of thing are\n\nDF: a data frame, almost always referred to by name.\nVAR: a variable in a data frame. Many command patterns involve multiple variables, each of which is referred to by VAR. You will replace each VAR with the appropriate variable name.\nVARS: one or more variable names. When these are the right-hand side of a tilde expression, separate the names with + punctuation. When we mean to indicate that there is only one variable, we use VAR instead of VARS. If we want to say, “use two variables,” we would write VAR + VAR.\nMODEL refers to the name of a model that you have previously constructed with lm().\nNM means a name that you will be calling something by. For instance, NM <- lm(VAR ~ VARS, data=DF). Another occasion for using NM is as part of an argument to summarize() or mutate().\n[, MORE] means that you can have multiple additional arguments of the same form as the previous argument.\nVALUE a number, quoted string (e.g., \"red\"), or multiple values inside c( ).\nMODSPEC is a model specification, which could equally well be written VAR ~ VARS\n\nAnything in a command pattern that is not a CAPITALIZED abbreviation is a specific part of the command to be used as-is. For instance, lm(VAR ~ VARS, data=DF) refers explicitly to the lm() function whose first argument is a tilde expression and whose second argument is named data.\nOccasionally, you will refer to a data frame by naming the package from which it comes. For example, the moderndive package includes (among many others) the amazon_books data frame. Think of amazon_books as a first name, and moderndive as a family name. When you see PACKAGE::DF it is meant to indicate, for instance, moderndive::amazon_books. (Note that the :: in the command pattern is to be taken literally; there are two successive colons separating the package name from the name of the data frame.)"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#part-1",
    "href": "Worksheets/Worksheet-19.html#part-1",
    "title": "Lesson 19: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nCommand patterns:\n\nDF %>% summarize(NM = var(VAR)) Calculate variance of a variable in a data frame.\n`DF %>% summarize(NM1 = var(VAR1), NM2 = var(VAR2) [, MORE])\nPACKAGE::DF The name of a data frame within a package.\n\n\nIn the mosaicData::Galton data frame, find the variance of mother and father. Give both the numerical value and the units.\n\nANSWER:\n\nIn the moderndive::amazon_books data frame, find the variance of list_price and num_pages. Give both the numerical value and the units.\n\nANSWER:\n\nCalculate the variance of sex from Galton. If something goes wrong, explain why.\n\nANSWER:"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#part-2",
    "href": "Worksheets/Worksheet-19.html#part-2",
    "title": "Lesson 19: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nCommand patterns:\n\nNM <- lm(VAR ~ VARS, data = DF)\nlm(VAR ~ VARS, data=DF) %>% conf_interval()\nlm(MODSPEC, data=DF) %>% conf_interval() means the same as (b).\n\n\n(Easy, no computing needed.) What kind of a thing is conf_interval(). (Hint: It’s the same kind of thing as lm().)\n\nANSWER:\n\nUsing the moderndive::amazonbooks data frame, fit the model list_price ~ num_pages:\n\nWhat are the units of the “(Intercept)” coefficient?\nReport the confidence interval on num_pages. Give both the numerical bounds and the units.\nDoes the confidence interval of either of the terms include zero?\n\n\nANSWER:\n\nSimilar to (2) but with the model list_price ~ numpages + hard_paper\n\nWhat does the term hard_paperH refer to?\nAccording to the coefficients, is a hardcover book any more expensive (on average) than a softcover book?\nDoes the confidence interval of any of the terms include zero?\n\n\nANSWER:\n\nStore the model you created in (3) under the name mod3. We’ll use it in the next part. For your answer, put the R command you used to store the model as mod3.\n\nANSWER:"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#graphics-review",
    "href": "Worksheets/Worksheet-19.html#graphics-review",
    "title": "Lesson 19: Worksheet",
    "section": "Graphics review",
    "text": "Graphics review\nCommand patterns:\n\nggplot(DF, aes(x=VAR, y=VAR)) + geom_jitter()\nggplot(DF, aes(x=VAR, y=VAR)) + geom_jitter() + geom_violin(fill=\"blue\", alpha=0.3)\nggplot(DF, aes(x=\"all\", y=VAR)) + geom_jitter()\nmodel_plot(MODEL, x=VAR)\nmodel_plot(MODAL, x=VAR, color=VAR)\n\n\nMake a jitter plot of list_price ~ hard_paper from moderndive::amazon_books.\n\nANSWER:\n\nUsing your command from (1), add a new layer: + geom_violin(fill=\"blue\", alpha=0.3)\n\nANSWER:\n\nUse model_plot() to draw a picture of mod3. Set x=hard_paper. What do you think the horizontal line segments refer to?\n\nANSWER:\n\nRepeat (3), but now set x=num_pages, color=hard_paper. Explain the meaning of the line segments in everyday terms.\n\nANSWER:"
  },
  {
    "objectID": "Worksheets/Worksheet-25.html",
    "href": "Worksheets/Worksheet-25.html",
    "title": "Lesson 25: Worksheet",
    "section": "",
    "text": "Count the points above and below the prediction band. What fraction of the points are within the band.\nPull out the values in one category and use them for prediction directly."
  },
  {
    "objectID": "Worksheets/Worksheet-24.html",
    "href": "Worksheets/Worksheet-24.html",
    "title": "Lesson 24: Worksheet",
    "section": "",
    "text": "USE model_eval() to evaluate the function. Calculate the effect size by hand. Is it a slope or a difference?\nCompare the effect size to a model coefficient."
  },
  {
    "objectID": "Worksheets/Worksheet-23.html",
    "href": "Worksheets/Worksheet-23.html",
    "title": "Lesson 23: Worksheet",
    "section": "",
    "text": "Use models to calculate a confidence interval.\nCompare the band on a model_plot() view to the numbers in the confidence interval for a slope. Do they correspond.\nMaybe have them add a geom_abline() to the plot to compare to the slope of the top and bottom of the confidence band."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html",
    "href": "Worksheets/Worksheet-21.html",
    "title": "Lesson 21: Worksheet",
    "section": "",
    "text": "Command patterns:"
  },
  {
    "objectID": "Day-by-day/Lesson-22/counts-and-waiting-times.html#counts-and-waiting-times",
    "href": "Day-by-day/Lesson-22/counts-and-waiting-times.html#counts-and-waiting-times",
    "title": "Spring 2023 Math 300Z",
    "section": "Counts and waiting times",
    "text": "Counts and waiting times\nGENERATE SEQUENCES FROM POISSON and EXPONENTIAL.\nFirst, generate a sample of size 1: find the largest and smallest across the class.\nThen generate a sample of size 20:\n\nfind the largest and smallest. are they pretty consistent across the class?\n\nfind the variance. Is that pretty consistent across the class?\n\nfind the mean:\n\nis that as spread out as the largest and smallest?\nhow spread out is it? (variance)\n\n\nKeep a table\n\n\n\nsample size\nvariance\nstandard deviation\n\n\n\n\n20.\n\n\n\n\n20\n\n\n\n\n20\n\n\n\n\n40\n\n\n\n\n80\n\n\n\n\n\nMaybe add a parameter or distribution argument to sample.dag_system()"
  },
  {
    "objectID": "Day-by-day/Lesson-24/with-respect-to.html#with-respect-to",
    "href": "Day-by-day/Lesson-24/with-respect-to.html#with-respect-to",
    "title": "Spring 2023 Math 300Z",
    "section": "With respect to …",
    "text": "With respect to …\nBuild a model with a single quantitative explanatory variable. Maybe palmerpenguins::penguins\n- What is the effect size?\n- What are the units? Is it a rate or a difference?\nBuild a model with a single categorical explanatory variable.\n- What is the effect size?\n- What are the units? Is it a rate or a difference?\nCombine the two and calculate effect sizes again.\n- Do the effect sizes change?\n- Do the units change? \n- Is it still a rate?\nInclude many explanatory variables: Find the effect sizes with respect to each."
  },
  {
    "objectID": "Day-by-day/Lesson-23/got-you-covered.html#got-you-covered",
    "href": "Day-by-day/Lesson-23/got-you-covered.html#got-you-covered",
    "title": "Spring 2023 Math 300Z",
    "section": "Got you covered",
    "text": "Got you covered\nSimulate from a DAG and calculate the confidence interval @ 95%\nDid everybody’s interval include the parameter from the DAG?\nMove to an 80% interval. About 4 students should not cover the parameter.\nMove to a 50% interval. About 10 students should not cover the parameter.\nMove to a 100% interval. What do the results tell you?\nFrederick the Great said, “To defend everything is to defend nothing.” paraphrase as “To try to cover everything is to cover nothing.”"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#regression-models",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#regression-models",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Regression models",
    "text": "Regression models\nThe point of regression modeling is to detect and quantify patterns of relationship between variables. Sometimes simple data graphics are enough to display a pattern. For instance, let’s look at some Department of Transportation data on models of cars stored in the MPG data frame. We will start by looking at the link between fuel economy and CO_2_ production.\n\nggplot(MPG, aes(x = fuel_year, y = CO2_year)) +\n  geom_jitter(alpha=.3)\n\n\n\n\nThis is a very strong pattern. fuel_year and CO2_year are practically the same thing.\n\nWhy?\nWhy are there some points off of the straight line describing the large majority of points?\n\nOther times, patterns are hidden by extreme variability in the data. For instance, here are data on the effect of kindergarden class size on student outcomes.\n\nggplot(STAR, aes(x=classtype, y=g4math)) + geom_jitter() + geom_violin(alpha=.5, fill=\"blue\")\n\n\n\n\n\n\n\n\nRegression modeling is a technique for looking for simple forms of patterns in the relationships among variables. It is not the only such technique, but it is by far the most widely used in practice across diverse fields.\nWe will use only regression modeling (and allied methods) in this course. You may have heard about other methods such as “deep learning” or “neural networks,” but regression modeling is the basis for most of the others.\nIt’s critically important that you understand the framework for regression modeling.\n\nIn any one model, there is one variable that is identified as the response variable.\n\nThis identification depends on the purpose behind your work. You’ll learn it mostly by example.\n\nOther variables in the model are cast in the role of explanatory variables. There might be one explanatory variable or there might be other. There’s even a use for the case where there are no explanatory variables, but we don’t need to worry about that now.\nIn fitting a model to data (sometimes called “training a model on data”) the computer does the heavy lifting of finding a relationship between the response and explanatory variables that stays close to the data. Often, the “shape” of the relationship is very simple, e.g. a straight-line relationship or, more generally, a linear combination. That’s where the l comes in the function lm() that you will be using again and again in this course.\nIn using lm(), you specify which variables you want in the explanatory role and which single variable you have selected to be the response variable. The computer language syntax is very simple:\n\nresponse ~ var1 + var2 + ...\nThe name of the response variable is always on the left-hand side of the TILDE.\nThe explanatory variables are listed by name on the right side of the TILDE.\nThe + between the names of explanatory variables is mostly just punctuation. You can read it as “and”.\n\nThe TILDE character is usually just pronounced “tilde,” but English-language equivalents are\n\n“as explained by”\n“as accounted for by”\n“as modeled by”\n“versus”"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#data-graphics",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#data-graphics",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Data graphics",
    "text": "Data graphics\nSince the distinction between the response and the explanatory variables is so central, we are going to enforce a graphical style that reflects the distinction.\n\nThe response variable will always be on the vertical axis.\nOne of the explanatory variables will be on the horizontal axis.\nIf there is a second explanatory variable, we will use color.\nWhen we need a third or fourth explanatory variable, we will use faceting.\n\nIn the ggplot2 graphics system, this policy will appear like this:\nggplot(Dataframe, aes(y=response, x=var1, color=var2)) + geom_jitter() or geom_point() and so on."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#models",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#models",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Models",
    "text": "Models\nWe have been working a lot with data frames. Now we are going to add a new type of R object, which we can call a “model”. A model is NOT a data frame, it is a different kind of thing with different properties and different operations.\nMaking a model:\n\nmod1 <- lm(sat~ expend, data=SAT)\n\nOperations we will perform on models:\n\nGraph the model (and usually the data used for training)\n::: {.cell}\nmodel_plot(mod1)\n::: {.cell-output-display}  ::: :::\n\nBeing able to use multiple explanatory variables allows us to see patterns that may be subtle.\n\nmod2 <- lm(sat ~ expend + frac, data=SAT)\nmodel_plot(mod2)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\n\n\n\n\nModel summaries, especially conf_interval() and R2()\n\n\nmod2 |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef    .upr\n  <chr>        <dbl>  <dbl>   <dbl>\n1 (Intercept) 950.   994.   1038.  \n2 expend        3.79  12.3    20.8 \n3 frac         -3.28  -2.85   -2.42\n\nmod2 |> R2()\n\n   n k  Rsquared        F     adjR2 p df.num df.denom\n1 50 2 0.8194726 106.6741 0.8117906 0      2       47\n\n\n\nEvaluate the model at each of the rows of the training data.\n::: {.cell}\nmodel_eval(mod2) |> head()\n::: {.cell-output .cell-output-stderr} Using training data as input to model_eval(). :::\n::: {.cell-output .cell-output-stdout} .response expend frac   .output     .resid     .lwr      .upr  1      1029  4.405    8 1025.1463   3.853661 958.2677 1092.0250  2       934  8.963   47  969.9621 -35.962052 900.0065 1039.9176  3       944  4.778   27  975.5616 -31.561556 909.1283 1041.9948  4      1005  4.459    6 1031.5117 -26.511670 964.6071 1098.4162  5       902  4.992   45  926.8741 -24.874145 860.0437  993.7046  6       980  5.443   29  978.0302   1.969768 912.0035 1044.0570 ::: :::"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#todays-lesson",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#todays-lesson",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Today’s Lesson",
    "text": "Today’s Lesson\nA definition of “statistical thinking” from the book:\n\nStatistic thinking is the accounting for variation in the context of what remains unaccounted for.\n\nImplicit in this definition is a pathway for learning to think statistically:\n\nLearn how to measure variation;\nLearn how to account for variation;\nLearn how to measure what remains unaccounted for.\n\nToday: How to measure variation.\nConsider some closely related words: variable, variation, vary, various, variety, variant. The root is vari.\nOur preferred way to measure the amount of variation numerically: the variance, a single number, always positive.\n\nVariance always involves a single variable; it is about the variation in that variable.\nCalculate the variance is with var() within summarize() r DF |> summarize(NM = var(VAR))\nA good way to conceptualize the variance is as the average squared pairwise difference between values\nThe units of the variance are the square of the units of the variable.\nWhy square the pairwise differences? It’s a convention. Experience has shown this convention simplifies many operations we do with models. Underlying mathematics: Pythagorean theorem and formula for bell-shaped curve.\nOften people talk about the “standard deviation.” This is merely the square-root of the variance. But the variance is more fundamental mathematically. Using standard deviations introduces square roots in many calculations that don’t need to be there if we use variance."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#activity",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#activity",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Activity",
    "text": "Activity"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#administration",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#administration",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Administration",
    "text": "Administration\n\nUse “300Z Section” under Teams.\nClone the Z-section project on posit.cloud. We’ll use this project for the rest of the semester.\nThere will be a “worksheet” almost every day.\n\nThe worksheet is in the form of an Rmd file. To access it, go into the Z-section project and give a command like this: get_lesson_worksheet(19) or whatever the lesson number is.\nAn effective way to prepare for a class is to look at the worksheet before class. Just read it and take note of what doesn’t make sense to you. That way you can be attentive to those things in class.\nComplete the worksheet after class.\nCome with unresolved questions about the worksheet for the next class or for EI.\n\nMost days there will also be a group activity.\nThere will be a couple of problem sets that will be graded.\nThere will be one GR about half-way through the rest of the semester. And a final GR."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#basic-regression-patterns",
    "href": "Worksheets/Worksheet-21.html#basic-regression-patterns",
    "title": "Lesson 21: Worksheet",
    "section": "Basic regression patterns",
    "text": "Basic regression patterns\nEvery regression model involves a response variable, which Lessons in Statistical Thinking always plots on the vertical axis. Most of the regression models we will consider in these Lessons have one or two explanatory variables, although sometimes there will be more than two and sometimes none at all.\nIt is worth memorizing the forms of the tilde-expression specifications of the zero-, one-, and two-explanatory models, as well as their shapes. For this purpose, we’ll write the forms using five generic variable names. In practice, you will replace these generic names with specific names from the data frame of interest.\n\ny — a quantitative response variable (which might be the result of a zero-one transformation).\nx and z — quantitative explanatory variables\ng and h — categorical explanatory variables.\n\n\n\n\n\n\n\n\nModel specification\nShape\n\n\n\n\ny ~ 1\nA line with slope zero.\n\n\ny ~ x\nA line with possibly non-zero slope.\n\n\ny ~ g\nA value for each level of g.\n\n\ny ~ x + g\nSeparate lines for each level of g, all with the same slope.\n\n\ny ~ x + z\nParallel, evenly spaced lines.\n\n\ny ~ g + h\nFor each level of g, a set of spaced values, one for each level of h. The h-spacing will be the same for every level of g.\n\n\n\nNote: It doesn’t matter what order the explanatory variables are given in. The name of the response variable is always on the left-hand side of the tilde expression."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-1",
    "href": "Worksheets/Worksheet-21.html#part-1",
    "title": "Lesson 21: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nDo Exercise 21.4."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-2",
    "href": "Worksheets/Worksheet-21.html#part-2",
    "title": "Lesson 21: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nBy fitting a regression model, we divide the response variable into two components: a signal component and a noise component. The model specification tells what sort of signal to look for. For instance, the Clock_auction data frame records the sales price of antique grandfather clocks sold at auction. Presumably, the price reflects some feature of the clock itself as well as the market conditions. We have only the variables age and bidders to represent the the value of the clock and the market conditions.\nUsing lm() with the specification price ~ age directs the computer to look for a signal in the form of a straight-line relationship between age and price. The estimated noise is the difference between the response variable values (price) and the signal.\n\nHow much clock-to-clock variation is there in price? (Use the variance to measure variation.)\n\nANSWER:\n\nFit a model price ~ age, then plot with model_plot(). Describe the pattern between price and age you see in the plot.\n\nANSWER:\n\nUse model_eval() to find the model output for each clock for the model you constructed in (2).\n\nWhat’s the variance of the model .output? How does it compare to the variance of the response variable price?\nThe amount of noise can be measured with the variance of .resid. How much noise is there for price ~ age?\nDemonstrate arithmetically the relationship between the variance of the response variable, the variance of the model .output, and the variance of the noise.\n\n\nANSWER:\n\nUse R2() to summarize the model you constructed in (2). Demonstrate arithmetically the relationship between R2 and variances of the response variable and the model .output.\n\nANSWER:\n\nThe quantity 1 - R2 describes the amount of noise. Arithmetically, how does 1 - R2 correspond to the variance of the .resid from part (3)?\n\nANSWER:"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-3-r2",
    "href": "Worksheets/Worksheet-21.html#part-3-r2",
    "title": "Lesson 21: Worksheet",
    "section": "Part 3: R2",
    "text": "Part 3: R2\ndag_10 has a simple structure, with nodes a through f each contributing to the value of y. Use sample() to generate a sample of size 1000. Using your sample, construct several models and calculate the R2 statistic.\n\ny ~ 1.\ny ~ a\ny ~ b\ny ~ a + b\nand so on.\n\n\nWhich of the models gives the smallest value of R2? Explain why that particular model gives such a small R2.\n\nANSWER:\n\nAs you add more terms to the model specification, does R2 ever go down?\n\nANSWER:\n\nWhat effect does the order of terms in the model have on R2? (For instance, y ~ a + b + c versus y ~ c + a + b.)\n\nANSWER:"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-4-concept-check",
    "href": "Worksheets/Worksheet-21.html#part-4-concept-check",
    "title": "Lesson 21: Worksheet",
    "section": "Part 4: Concept check",
    "text": "Part 4: Concept check\nWrite a sentence or two explaining what each of the following terms refers to.\n\n“Zero-one transformation”\n“Levels of a categorical variable”\n“Model specification”\n“Tilde expression”"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-5-curvy-models",
    "href": "Worksheets/Worksheet-21.html#part-5-curvy-models",
    "title": "Lesson 21: Worksheet",
    "section": "Part 5: Curvy models",
    "text": "Part 5: Curvy models\nHere’s a model of human height versus age based on the NHANES::NHANES data frame. (The package NHANES has the data frame which itself is called NHANES, so the full name is NHANES::NHANES.)\n\nmod1 <- lm(Height ~ Age, data = NHANES::NHANES)\nmodel_plot(mod1, data_alpha=0.05)\n\nWarning: Removed 353 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nDo you think the model gives a good description of the relationship between Age and Height? Explain using simple biological terms what the problem is with the straight-line model.\n\nANSWER:\nThere are several modeling techniques for constructing models that are more flexible than a straight line. We won’t be using them in Math 300, but we want to point out that they exist. Try this one:\n\nmod2 <- lm(Height ~ splines::ns(Age,5) * Gender, data = NHANES::NHANES)\nmodel_plot(mod2, data_alpha=0.05)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\nWarning: Removed 353 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nCalculate R2 for the straight-line model and for the curvy model.\n\nANSWER:"
  },
  {
    "objectID": "Day-by-day/Lesson-34/L-34-instructor-notes.html",
    "href": "Day-by-day/Lesson-34/L-34-instructor-notes.html",
    "title": "Instructor Notes: Math 300Z",
    "section": "",
    "text": "Tell the story of the Chinese spy balloon. After it was detected, the Air Force (according to news reports) increased the sensitivity of radars. This led to an increase in detection and a week-long rash of high-altitude detections, two of which were shot down. Eventually it was realized that there is a surprising amount of stuff floating around at high altitude. https://www.nytimes.com/live/2023/02/16/us/biden-china-balloon-ufo?smid=nytcore-ios-share&referringSource=articleShare"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "",
    "text": "Motivating problem: Designing an enforcement regime for limits on scallop fisheries.\n\n\n\n\n\nFigure 1: Life cycle of a scallop\n\n\n\n\nFisheries are regulated by states and the Federal government in order to avoid collapse due to over-fishing. Often, the regulations attempt to protect juveniles—animals that have not yet reached reproductive age. If the juveniles are harvested, their potential progeny are annihilated. There are various ways to do this, for instance restricting fishing to months where adults are most prevalent, closing fisheries to provide an opportunity for the reproductive stock to recover, and so on.\nIn the 1990s, one of the ways the Federal government regulated scallop fisheries was by setting a minimum acceptable size for harvested scallops. For practical reasons, rather than monitoring individual scallops, the government monitored the average per scallop weight of each boat’s catch. For the sake of the example, imagine that the minimum acceptable weight is 1/30 pound.\nA fishing boat might have 10,000 or more bags of scallops, which can be handled individually: weigh the bag, then count the number of scallops to get the average weight per scallop.\nDiscussion questions:\n\nHow many bags should be sampled? Should this depend on the number of bags in the cargo. For instance, should a cargo of 1000 bags be sampled differently than a cargo of 10,000 bags.\nWhat should be the threshold for declaring the whole cargo below minimum size? (The whole catch is confiscated in such a case.)\n\nIn this section of the course, you’ll learn some statistical concepts and methods that allow the above questions to be answered to produce a regulation that is protective and fair to the fishermen.\nOne idea is very simple: sampling variation. This is about how much the average per-scallop weight will vary from one bag to another.\nAnother idea is very subtle: What you can say about the whole cargo based on a sample of \\(n\\) bags."
  },
  {
    "objectID": "Day-by-day/Lesson-24/Teaching-notes-24.html",
    "href": "Day-by-day/Lesson-24/Teaching-notes-24.html",
    "title": "Teaching notes, Lesson 24",
    "section": "",
    "text": "Our emphasis will be on the data-analysis techniques that are useful for decision-making. What this means will become evident as we move through the semester, but I’ll give two settings for decision-making:\n\nIntervention in a system. (Lesson 24) There’s a system that you are working with, perhaps aircraft design. You want to intervene in the system, to change something. You need to know how the change you impose will change the outcome.\n\nUsually, we are interested in predictions that cover most of the likely outcomes.\nSometimes, we want to predict the probabilities of extreme events, for instance thousand-year floods.\n\nPrediction. (Lessons 25 and 26)"
  },
  {
    "objectID": "textbooks.html",
    "href": "textbooks.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "For Lessons 1-17, the textbook is Statistical Inference via Data Science (“ModernDive”)"
  },
  {
    "objectID": "textbooks.html#lessons-19-onward",
    "href": "textbooks.html#lessons-19-onward",
    "title": "Spring 2023 Math 300Z",
    "section": "Lessons 19 onward …",
    "text": "Lessons 19 onward …\n\n\nFor Lessons 19-39, the textbook is Lessons in Statistical Thinking"
  },
  {
    "objectID": "math300-software.html",
    "href": "math300-software.html",
    "title": "R package for Math 300Z",
    "section": "",
    "text": "Most students will want to use POSIT.cloud. Use the “Z-Section” project."
  },
  {
    "objectID": "math300-software.html#accessing-the-daily-worksheet",
    "href": "math300-software.html#accessing-the-daily-worksheet",
    "title": "R package for Math 300Z",
    "section": "Accessing the daily worksheet",
    "text": "Accessing the daily worksheet\nAlmost all class days there will be a worksheet in the form of an Rmd file.\nFrom within your POSIT.cloud project, download the Rmd file using this command, substituting the number of the relevant Lesson:\nmath300::get_lesson_worksheet(19)\n\nMake a habit of reading each day’s Rmd file before class. This way you can note what doesn’t yet make sense so that you can be receptive to the topic in class.\nComplete the worksheet after class."
  },
  {
    "objectID": "math300-software.html#math-300z-r-commands",
    "href": "math300-software.html#math-300z-r-commands",
    "title": "R package for Math 300Z",
    "section": "Math 300Z R commands",
    "text": "Math 300Z R commands\nThere will be only a dozen commands that you will be using in the second half of Math 300Z. Almost all of them involve constructing or summarizing models.\nAs a reminder, here are some of the commands/syntax that should be familiar to you from the first half of the course.\n\nlm() fits (or “trains”) a “linear model” on data from a data frame.\nggplot() sets things up for a new graphic. Use aes() as an argument.\ngraphics layers to add onto the output of ggplot():\n\ngeom_point(), geom_jitter(alpha=0.5)\n\nfilter(), mutate() and summarize() are basic data-wrangling commands we will use often.\n\nNew commands in the second half of the course:\n\nsummarize a model: conf_intervals(), R2()\nevaluate a model: model_eval()\ngraphic of a model: model_plot() (This replaces the geom_smooth() used in the first half of the course.)\nvariance of a variable in a data frame: DF %>% summarize(NM = var(VAR))\ndraw a DAG: dag_draw()\nsample from a DAG: sample(DAG, size=100)\n\nA few other commands will be used occasionally in examples and demonstrations. You should know what they do, but typically there will be a reminder of the syntax: zero_one(), shuffle(), do() * {}, dag_intervene(), tibble(), regression_summary(), anova_summary()."
  },
  {
    "objectID": "math300-software.html#running-rrstudio-on-your-laptop",
    "href": "math300-software.html#running-rrstudio-on-your-laptop",
    "title": "R package for Math 300Z",
    "section": "Running R/RStudio on your laptop?",
    "text": "Running R/RStudio on your laptop?\nInstall the {math300} and other packages with these two commands:\ninstall.packages(c(\"mosaic\", \"ggplot\", \"dplyr\", \"openintro\", \"moderndive\", \"nycflights13\", \"knitr\"))\n# additional package for Math 300Z\nremotes::install_github(\"dtkaplan/math300\")"
  }
]
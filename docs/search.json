[
  {
    "objectID": "NTI/Math300R-Lesson21.html",
    "href": "NTI/Math300R-Lesson21.html",
    "title": "Math 300R NTI Lesson 21",
    "section": "",
    "text": "[Note in draft: Replace these with reference to a central repo for objectives.]\n\nDetermine whether a proposed graph is directed and acyclic.\nRead notation to identify response variable, explanatory variable, covariates, and effect sizes.\nCharacterize the magnitude of random noise.\nGenerate data from simulations and summarize variables individually."
  },
  {
    "objectID": "NTI/Math300R-Lesson21.html#reading",
    "href": "NTI/Math300R-Lesson21.html#reading",
    "title": "Math 300R NTI Lesson 21",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/Math300R-Lesson21.html#lesson",
    "href": "NTI/Math300R-Lesson21.html#lesson",
    "title": "Math 300R NTI Lesson 21",
    "section": "Lesson",
    "text": "Lesson\nRemember that you will be running this more like a lab than a lecture. You want them using R and answering questions. Have them open the notes rmd and work through it together.\n\nProlog: Review notation for DAGs and noise. Show how to generate data use the run_dag() function.\n\n\n\n\n\n\n\nRunning a DAG simulation\n\n\n\n\nrun_dag(RTM)\nshow_dag(RTM)\n\n\n\n\nWork through the learning checks LCA.1 - LCA.2.\nA DAG for “regression to the mean.”\n\nAsk what “regression” means in everyday English\nProvide some history of the idea and point out that this is where the name “regression” comes from for the method of fitting a linear model linking a response variable to explanatory variable(s).\nShow this DAG and ask what students think the consequences will be: what’s the relationship between the parents’ height and the (adult) child’s height.\n\n\n\\[\\epsilon \\rightarrow parent \\longleftarrow GENES \\longrightarrow child \\leftarrow \\nu\\]\n\n\n\n\n\n\nSimulation: Regression to the mean\n\n\n\n\nrtm <- tibble(genes = 64 + rnorm(1000), \n              parent = genes + 0.5*rnorm(1000), \n              child  = genes + 0.5*rnorm(1000))\nmod <- lm(child ~ parent, data = rtm) \nmod\n\n\nCall:\nlm(formula = child ~ parent, data = rtm)\n\nCoefficients:\n(Intercept)       parent  \n    11.1719       0.8258  \n\n# Convert this to ggplot2\ngf_point(child ~ parent, data = rtm) %>%\n  gf_lm(interval=\"confidence\") %>%\n  gf_abline(intercept = 0, slope = 1, color=\"red\")\n\n\n\n\nInterpret the coefficient and the graph.\n\nWhat’s the mean for the children and for the parents? [64 inches for each]\nThe red line runs along the diagonal. It reflects the statement that the children are, on average, the same as the parents.\nThe blue line is the regression model. The children are, on average, closer to the mean than the parents.\n\n\n\n\n\nLC A.1 (Objective 2)\n[NOTE in DRAFT: These are the learning objectives from Bradley Warner’s notes, retained here simply to remind me of the format for the NTIs.]\nLC XX.XX Say you have a normal distribution with mean \\(\\mu=6\\) and standard deviation \\(\\sigma=3\\). What proportion of the area under the normal curve is less than 3? Greater than 12? Between 0 and 12?\nSolution:\n\nLess than 3: 3 is one standard deviation less than the mean of 6, since \\(\\frac{3 - \\mu}{\\sigma} = \\frac{3-6}{3} = -1\\). Thus we compute the area to the left of z = -1 in Figure A.2: 0.15% + 2.35% + 13.5% = 16%.\n\n\npnorm(3,mean=6,sd=3)\n\n[1] 0.1586553\n\n\n\nGreater than 12: 12 is two standard deviations greater than the mean of 6, since \\(\\frac{12 - \\mu}{\\sigma} = \\frac{12-6}{3} = 2\\), we compute the area to the right of z = 2 in Figure A.2: 2.35% + 0.15% = 2.5%.\n\n\npnorm(12,mean=6,sd=3,lower.tail = FALSE)\n\n[1] 0.02275013\n\n\n\nBetween 0 and 12: 0 is two standard deviations less than the mean of 6, since \\(\\frac{0 - \\mu}{\\sigma} = \\frac{0-6}{3} = -2\\). Thus we compute the area between z = -2 and z = +2 in Figure A.2: 13.5% + 34% + 34% + 13.5% = 95%.\n\n\npnorm(12,mean=6,sd=3)-pnorm(0,mean=6,sd=3)\n\n[1] 0.9544997\n\n\n\n\nLC A.2 (Objective 2)\n`r LC XX.YY (Same normal distribution.) What is the 2.5th percentile of the area under the normal curve? The 97.5th percentile? The 100th percentile?\nSolution:\n\n2.5th percentile: Starting from the left of Figure A.2, since 0.15% + 2.35% = 2.5%, the 2.5th percentile is z = - 2. However, this is in standard units. Thus we need the value in the normal distribution that is two standard deviations lower than the mean: \\(\\mu - 2 \\cdot\\sigma = 6 - 2 \\cdot 3 = 0\\).\n\n\nqnorm(.025,mean=6,sd=3)\n\n[1] 0.120108\n\n\nThis is not 0 since the figure is an approximation.\n\n97.5th percentile: Starting from the left of Figure A.2, since 0.15% + 2.35% + 13.5% + 34% + 34% + 13.5% = 97.5%, the 97.5th percentile is z = +2. However, this is in standard units. Thus we need the value in the normal distribution that is two standard deviations higher than the mean: \\(\\mu + 2 \\cdot\\sigma = 6 + 2 \\cdot 3 = 12\\).\n\n\nqnorm(.975,mean=6,sd=3)\n\n[1] 11.87989\n\n\n\n100th percentile: \\(+\\infty\\). In other words, 100% of values will be less than positive infinity.\n\n\nqnorm(1,mean=6,sd=3)\n\n[1] Inf"
  },
  {
    "objectID": "NTI/Math300R-Lesson21.html#documenting-software",
    "href": "NTI/Math300R-Lesson21.html#documenting-software",
    "title": "Math 300R NTI Lesson 21",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-09-24\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/Math300R-Lesson20.html#reading",
    "href": "NTI/Math300R-Lesson20.html#reading",
    "title": "Math 300R NTI Lesson 20",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/Math300R-Lesson20.html#lesson",
    "href": "NTI/Math300R-Lesson20.html#lesson",
    "title": "Math 300R NTI Lesson 20",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nIn-class example\n\n\n\n\n3 + 2\n\n[1] 5\n\n\n\n\n\n\n\n\n\n\nIn-class activity\n\n\n\n\ndate()\n\n[1] \"Sat Sep 24 16:44:39 2022\"\n\n\n\n\nThis is just a template."
  },
  {
    "objectID": "NTI/Math300R-Lesson20.html#learning-challenges",
    "href": "NTI/Math300R-Lesson20.html#learning-challenges",
    "title": "Math 300R NTI Lesson 20",
    "section": "Learning Challenges",
    "text": "Learning Challenges\n\n\n\n\n\n\nLC 20.1 (Objective 2)\n\n\n\nThis is just a template.\n\n\n\n\n\n\nSolution\n\n\n\nThe solution will go here."
  },
  {
    "objectID": "NTI/Math300R-Lesson20.html#documenting-software",
    "href": "NTI/Math300R-Lesson20.html#documenting-software",
    "title": "Math 300R NTI Lesson 20",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-09-24\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/Math300R-Lesson22.html#reading",
    "href": "NTI/Math300R-Lesson22.html#reading",
    "title": "Math 300R NTI Lesson 22",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/Math300R-Lesson22.html#lesson",
    "href": "NTI/Math300R-Lesson22.html#lesson",
    "title": "Math 300R NTI Lesson 22",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nIn-class example\n\n\n\n\n3 + 2\n\n[1] 5\n\n\n\n\n\n\n\n\n\n\nIn-class activity\n\n\n\n\ndate()\n\n[1] \"Sat Sep 24 16:44:42 2022\"\n\n\n\n\nThis is just a template."
  },
  {
    "objectID": "NTI/Math300R-Lesson22.html#learning-challenges",
    "href": "NTI/Math300R-Lesson22.html#learning-challenges",
    "title": "Math 300R NTI Lesson 22",
    "section": "Learning Challenges",
    "text": "Learning Challenges\n\n\n\n\n\n\nLC 19.1 (Objective 2)\n\n\n\nThis is just a template.\n\n\n\n\n\n\nSolution\n\n\n\nThe solution will go here."
  },
  {
    "objectID": "NTI/Math300R-Lesson22.html#documenting-software",
    "href": "NTI/Math300R-Lesson22.html#documenting-software",
    "title": "Math 300R NTI Lesson 22",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-09-24\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/Math300R-Lesson23.html#reading",
    "href": "NTI/Math300R-Lesson23.html#reading",
    "title": "Math 300R NTI Lesson 23",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/Math300R-Lesson23.html#lesson",
    "href": "NTI/Math300R-Lesson23.html#lesson",
    "title": "Math 300R NTI Lesson 23",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nIn-class example\n\n\n\n\n3 + 2\n\n[1] 5\n\n\n\n\n\n\n\n\n\n\nIn-class activity\n\n\n\n\ndate()\n\n[1] \"Sat Sep 24 16:44:44 2022\"\n\n\n\n\nThis is just a template."
  },
  {
    "objectID": "NTI/Math300R-Lesson23.html#learning-challenges",
    "href": "NTI/Math300R-Lesson23.html#learning-challenges",
    "title": "Math 300R NTI Lesson 23",
    "section": "Learning Challenges",
    "text": "Learning Challenges\n\n\n\n\n\n\nLC 19.1 (Objective 2)\n\n\n\nThis is just a template.\n\n\n\n\n\n\nSolution\n\n\n\nThe solution will go here."
  },
  {
    "objectID": "NTI/Math300R-Lesson23.html#documenting-software",
    "href": "NTI/Math300R-Lesson23.html#documenting-software",
    "title": "Math 300R NTI Lesson 23",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-09-24\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/Math300R-Lesson24.html#reading",
    "href": "NTI/Math300R-Lesson24.html#reading",
    "title": "Math 300R NTI Lesson 24",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/Math300R-Lesson24.html#lesson",
    "href": "NTI/Math300R-Lesson24.html#lesson",
    "title": "Math 300R NTI Lesson 24",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nIn-class example\n\n\n\n\n3 + 2\n\n[1] 5\n\n\n\n\n\n\n\n\n\n\nIn-class activity\n\n\n\n\ndate()\n\n[1] \"Sat Sep 24 16:44:46 2022\"\n\n\n\n\nThis is just a template."
  },
  {
    "objectID": "NTI/Math300R-Lesson24.html#learning-challenges",
    "href": "NTI/Math300R-Lesson24.html#learning-challenges",
    "title": "Math 300R NTI Lesson 24",
    "section": "Learning Challenges",
    "text": "Learning Challenges\n\n\n\n\n\n\nLC 19.1 (Objective 2)\n\n\n\nThis is just a template.\n\n\n\n\n\n\nSolution\n\n\n\nThe solution will go here."
  },
  {
    "objectID": "NTI/Math300R-Lesson24.html#documenting-software",
    "href": "NTI/Math300R-Lesson24.html#documenting-software",
    "title": "Math 300R NTI Lesson 24",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-09-24\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/Math300R-Lesson25.html#reading",
    "href": "NTI/Math300R-Lesson25.html#reading",
    "title": "Math 300R NTI Lesson 25",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/Math300R-Lesson25.html#lesson",
    "href": "NTI/Math300R-Lesson25.html#lesson",
    "title": "Math 300R NTI Lesson 25",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nIn-class example\n\n\n\n\n3 + 2\n\n[1] 5\n\n\n\n\n\n\n\n\n\n\nIn-class activity\n\n\n\n\ndate()\n\n[1] \"Sat Sep 24 16:44:49 2022\"\n\n\n\n\nThis is just a template."
  },
  {
    "objectID": "NTI/Math300R-Lesson25.html#learning-challenges",
    "href": "NTI/Math300R-Lesson25.html#learning-challenges",
    "title": "Math 300R NTI Lesson 25",
    "section": "Learning Challenges",
    "text": "Learning Challenges\n\n\n\n\n\n\nLC 19.1 (Objective 2)\n\n\n\nThis is just a template.\n\n\n\n\n\n\nSolution\n\n\n\nThe solution will go here."
  },
  {
    "objectID": "NTI/Math300R-Lesson25.html#documenting-software",
    "href": "NTI/Math300R-Lesson25.html#documenting-software",
    "title": "Math 300R NTI Lesson 25",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-09-24\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/Math300R-Lesson19.html",
    "href": "NTI/Math300R-Lesson19.html",
    "title": "Math 300R NTI Lesson 19",
    "section": "",
    "text": "Distinguish between the two settings for decision-making:\n\nPrediction: predict an outcome for an individual\nRelationship: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\nGiven a research question, identify whether it corresponds to a prediction setting or a relationship setting."
  },
  {
    "objectID": "NTI/Math300R-Lesson19.html#reading",
    "href": "NTI/Math300R-Lesson19.html#reading",
    "title": "Math 300R NTI Lesson 19",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/Math300R-Lesson19.html#lesson",
    "href": "NTI/Math300R-Lesson19.html#lesson",
    "title": "Math 300R NTI Lesson 19",
    "section": "Lesson",
    "text": "Lesson\nThis lesson marks the beginning of a new phase of the course. Thus far, we’ve worked with techniques* for data wrangling, graphics, and regression modeling. Now we address the question of what a regression model (and other information that we might have) can tell us about the real world.*\n\n\n\n\n\n\nSetup\n\n\n\nlibrary(mosaicData)\n\n\n\n\n\n\n\n\nGuided activity: House prices\n\n\n\nHave students do the calculations for the first model, answering the questions that follow. After this is complete, have students do the calculations for the second model and answer those questions.\nThe mosaicData::SaratogaHouses data frame contains information about the sales price and various attributes of about 1700 houses. (See ?SaratogaHouses for a detailed description.)\nDo a regression of price ~ bedrooms and explain what the regression coefficients mean.\n\nlm(price ~ bedrooms, data=SaratogaHouses) |> coefficients()\n\n(Intercept)    bedrooms \n   59862.96    48217.81 \n\n\n\nWhat are the units of the intercept and of the bedrooms coefficient? *Intercept in dollars, bedrooms in dollars per bedroom.\nInterpret what the coefficients indicate about the price of houses and bedrooms. Each additional bedroom adds about $50000 to the value of a house.\nAccording to the model, predict what would be the sales price (at the time the data was collected, 2006) of a house with two bedrooms? \\(59863 + 2\\times 48218\\)\n\nOf course, bedrooms are not the only important thing about a house. Let’s include livingArea along with bedrooms in the model.\n\nlm(price ~ livingArea + bedrooms, data=SaratogaHouses) |> coefficients()\n\n(Intercept)  livingArea    bedrooms \n  36667.895     125.405  -14196.769 \n\n\n\nWhat are the units of the livingArea coefficient? dollars per square foot\nWhat does this model say about the value of adding a bedroom? It seems to reduce the value of the house by about $15,000.\n\nBased on exactly the same data, the two models seem to give contradictory statements about the value of an additional bedroom.\n\nIs one model right and the other wrong? If so, which one is right? (Explain your reasoning.) Both models are mathematically correct. But they need to be interpreted in different ways. Each is telling us something different about the real world.\nCould both models be right? If so, explain why the bedroom coefficients have opposite signs. We will need to develop some additional tools and concepts before we can take on this question.\n\nLearning how to interpret models in terms of what they say about the world is a major theme of this second half of Math 300.\nAnother, more technical question that we will address has to do with the precision of coefficients like 125.40 dollars per square foot. Might it actually be $200/ft2? How about $500/ft2? And how seriously should we take the sales value that we calculate by setting numbers for bedrooms and livingArea into the model?"
  },
  {
    "objectID": "NTI/Math300R-Lesson19.html#learning-challenges",
    "href": "NTI/Math300R-Lesson19.html#learning-challenges",
    "title": "Math 300R NTI Lesson 19",
    "section": "Learning Challenges",
    "text": "Learning Challenges\n\n\n\n\n\n\nLC 19.1 (Objective 2)\n\n\n\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\n\n\n\n\n\nSolution\n\n\n\n\nPrediction and (ii) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price.\n\n\n\n\n\n\n\n\n\n\n\nLC 19.2 (Objective 2)\n\n\n\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nPrediction\nRelationship\nRelationship\n\n\n\n::::"
  },
  {
    "objectID": "NTI/Math300R-Lesson19.html#documenting-software",
    "href": "NTI/Math300R-Lesson19.html#documenting-software",
    "title": "Math 300R NTI Lesson 19",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-09-24\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "LC/Learning-checks.html",
    "href": "LC/Learning-checks.html",
    "title": "Learning Checks from Modern Dive",
    "section": "",
    "text": "LC 1.1 Block 1 Day 1\n\n\n\nRepeat the earlier installation steps, but for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package for data wrangling, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for generating easy-to-read tables in R. We’ll use these packages in the next section.\n\n\n\n\n\n\n\n\nLC 1.2 Block 1 Day 1\n\n\n\n“Load” the dplyr, nycflights13, and knitr packages as well by repeating the earlier steps.\n\n\nRun View(flights) in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note the uppercase V in View(). R is case-sensitive, so you’ll get an error message if you run view(flights) instead of View(flights)\n\n\n\n\n\n\nLC 1.3 Block 1 Day 1\n\n\n\nWhat does any ONE row in this flights dataset refer to?\n\nA. Data on an airline\nB. Data on a flight\nC. Data on an airport\nD. Data on multiple flights\n\n\n\n\n\n\n\n\n\nLC 1.4 Block 1 Day 1\n\n\n\nWhat are some other examples in this dataset (flights) of categorical variables? What makes them different than quantitative variables?\n\n\n\n\n\n\n\n\nLC 1.5 Block 1 Day 1\n\n\n\nWhat properties of each airport do the variables lat, lon, alt, tz, dst, and tzone describe in the airports data frame? Take your best guess.\n\n\n\n\n\n\n\n\nLC 1.6 Block 1 Day 1\n\n\n\nProvide the names of variables in a data frame with at least three variables where one of them is an identification variable and the other two are not. Further, create your own tidy data frame that matches these conditions.\n\n\n\n\n\n\n\n\nLC 1.7 Block 1 Day 1\n\n\n\nLook at the help file for the airports data frame. Revise your earlier guesses about what the variables lat, lon, alt, tz, dst, and tzone each describe."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-2-visualization",
    "href": "LC/Learning-checks.html#chapter-2-visualization",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 2: Visualization",
    "text": "Chapter 2: Visualization\n\n\n\n\n\n\nLC 2.1 Block 1 Day 2\n\n\n\nTake a look at both the flights and alaska_flights data frames by running View(flights) and View(alaska_flights). In what respect do these data frames differ? For example, think about the number of rows in each dataset.\n\n\n\n\n\n\n\n\nLC 2.2-2.6 Block 1 Day 2\n\n\n\nWhat are some practical reasons why dep_delay and arr_delay have a positive relationship?\nWhat variables in the weather data frame would you expect to have a negative correlation (i.e., a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here. Hint: Explore the weather dataset by using the View() function.\nWhy do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaska Air flights?\nWhat are some other features of the plot that stand out to you?\nCreate a new scatterplot using different variables in the alaska_flights data frame by modifying the example given.\n\n\n\n\n\n\n\n\nLC 2.7-2.8 Block 1 Day 2\n\n\n\nWhy is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot?\nAfter viewing Figure @ref(fig:alpha), give an approximate range of arrival delays and departure delays that occur most frequently. How has that region changed compared to when you observed the same plot without alpha = 0.2 set in Figure @ref(fig:noalpha)?\n\n\n\n\n\n\n\n\nLC 2.9-2.10 Block 1 Day 3\n\n\n\nLC 2.9 Take a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather). In what respect do these data frames differ?\nLC 2.10 View() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement, whereas the hour variable does not?\n\n\n\n\n\n\n\n\nLC 2.11-2.13 Block 1 Day 3\n\n\n\nLC 2.11 Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis?\nLC 2.12 Why are linegraphs frequently used when time is the explanatory variable on the x-axis?\nLC 2.12 Plot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013.\n\n\n\n\n\n\n\n\nLC 2.18-2.21 Block 1 Day 3\n\n\n\nWhat other things do you notice about this faceted plot? How does a faceted plot help us see relationships between two variables?\nWhat do the numbers 1-12 correspond to in the plot? What about 25, 50, 75, 100?\nFor which types of datasets would faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics.\nLC 2.21 Does the temp variable in the weather dataset have a lot of variability? Why do you say that?\n\n\n\n\n\n\n\n\nLC 2.22-2.25 Boxplots Block 1 Day 4\n\n\n\nLC 2.22 What does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point.\nLC 2.23 Which months have the highest variability in temperature? What reasons can you give for this?\nLC 2.24 We looked at the distribution of the numerical variable temp split by the numerical variable month that we converted using the factor() function in order to make a side-by-side boxplot. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative?\nLC 2.25 Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram?\n\n\n\n\n\n\n\n\nLC 2.26-2.29 Histograms Block 1 Day 4\n\n\n\nLC 2.26 Why are histograms inappropriate for categorical variables?\nLC 2.27 What is the difference between histograms and barplots?\nLC 2.28 How many Envoy Air flights departed NYC in 2013?\nLC 2.29 What was the 7th highest airline for departed flights from NYC in 2013? How could we better present the table to get this answer quickly?\n\n\n\n\n\n\n\n\nLC 2.30-2.31 Pie charts Block 1 Day 4\n\n\n\nLC 2.30 Why should pie charts be avoided and replaced by barplots?\nLC 2.31 Why do you think people continue to use pie charts?\n\n\n\n\n\n\n\n\nLC 2.32-2.37 Block 1 Day 4\n\n\n\nLC 2.32 What kinds of questions are not easily answered by looking at Figure @ref(fig:flights-stacked-bar) (2.23)?\nLC 2.33 What can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights?\nLC 2.34 Why might the side-by-side barplot be preferable to a stacked barplot in this case?\nLC 2.35 What are the disadvantages of using a dodged barplot, in general?\nLC 2.36 Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case?\nLC 2.37 What information about the different carriers at different airports is more easily seen in the faceted barplot?"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-3-wrangling",
    "href": "LC/Learning-checks.html#chapter-3-wrangling",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 3: Wrangling",
    "text": "Chapter 3: Wrangling\n\n\n\n\n\n\nLC 3.1 Block 1 Day 5\n\n\n\nWhat’s another way of using the “not” operator ! to filter only the rows that are not going to Burlington, VT nor Seattle, WA in the flights data frame? Test this out using the previous code.\n\n\n\n\n\n\n\n\nLC 3.2 Block 1 Day 5\n\n\n\nSay a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five-year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach?\n\n\n\n\n\n\n\n\nLC 3.3 Block 1 Day 5\n\n\n\nModify the earlier summarize() function code that creates the summary_temp data frame to also use the n() summary function: summarize(... , count = n()). What does the returned value correspond to?\n\n\n\n\n\n\n\n\nLC 3.4 Block 1 Day 5\n\n\n\nWhy doesn’t the following code work? Run the code line-by-line instead of all at once, and then look at the data. In other words, run summary_temp <- weather %>% summarize(mean = mean(temp, na.rm = TRUE)) first.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nsummary_temp <- weather %>%   \n  summarize(mean = mean(temp, na.rm = TRUE)) %>% \n  summarize(std_dev = sd(temp, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\nLC 3.5 Block 1 Day 6\n\n\n\nRecall from Chapter @ref(viz) when we looked at temperatures by months in NYC. What does the standard deviation column in the summary_monthly_temp data frame tell us about temperatures in NYC throughout the year?\n\n\n\n\n\n\n\n\nLC 3.6 Block 1 Day 6\n\n\n\nWhat code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC?\n\n\n\n\n\n\n\n\nLC 3.7 Block 1 Day 6\n\n\n\nRecreate by_monthly_origin, but instead of grouping via group_by(origin, month), group variables in a different order group_by(month, origin). What differs in the resulting dataset?\n\n\n\n\n\n\n\n\nLC 3.8 Block 1 Day 6\n\n\n\nHow could we identify how many flights left each of the three airports for each carrier?\n\n\n\n\n\n\n\n\nLC 3.9 Block 1 Day 6\n\n\n\nHow does the filter() operation differ from a group_by() followed by a summarize()?\n\n\n\n\n\n\n\n\nLC 3.10 Block 1 Day 6\n\n\n\nWhat do positive values of the gain variable in flights correspond to? What about negative values? And what about a zero value?\n\n\n\n\n\n\n\n\nLC 3.11 Block 1 Day 6\n\n\n\nCould we create the dep_delay and arr_delay columns by simply subtracting dep_time from sched_dep_time and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in flights.\n\n\n\n\n\n\n\n\nLC 3.12 Block 1 Day 76\n\n\n\nWhat can we say about the distribution of gain? Describe it in a few sentences using the plot and the gain_summary data frame values.\n\n\n\n\n\n\n\n\nLC 3.13 Block 1 Day 7\n\n\n\nLooking at Figure @ref(fig:reldiagram), when joining flights and weather (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of year, month, day, hour, and origin, and not just hour?\n\n\n\n\n\n\n\n\nLC 3.14 Block 1 Day 7\n\n\n\nWhat surprises you about the top 10 destinations from NYC in 2013?\n\n\n\n\n\n\n\n\nLC 3.15 Block 1 Day 7\n\n\n\nWhat are some advantages of data in normal forms? What are some disadvantages?\n\n\n\n\n\n\n\n\nLC 3.16 Block 1 Day 7\n\n\n\nWhat are some ways to select all three of the dest, air_time, and distance variables from flights? Give the code showing how to do this in at least three different ways.\n\n\n\n\n\n\n\n\nLC 3.17 Block 1 Day 7\n\n\n\nHow could one use starts_with(), ends_with(), and contains() to select columns from the flights data frame? Provide three different examples in total: one for starts_with(), one for ends_with(), and one for contains().\n\n\n\n\n\n\n\n\nLC 3.18 Block 1 Day 7\n\n\n\nWhy might we want to use the select function on a data frame?\n\n\n\n\n\n\n\n\nLC 3.19 Block 1 Day 7\n\n\n\nCreate a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013.\n\n\n::: {.callout-note icon=false} ## LC 3.20 Block 1 Day 7 Let’s now put your newly acquired data wrangling skills to the test!\nAn airline industry measure of a passenger airline’s capacity is the available seat miles, which is equal to the number of seats available multiplied by the number of miles or kilometers flown summed over all flights.\nFor example, let’s consider the scenario in Figure 1. Since the airplane has 4 seats and it travels 200 miles, the available seat miles are \\(4 \\times 200 = 800\\).\n\n\n\n\n\nFigure 1: Example of available seat miles for one flight.\n\n\n\n\nExtending this idea, let’s say an airline had 2 flights using a plane with 10 seats that flew 500 miles and 3 flights using a plane with 20 seats that flew 1000 miles, the available seat miles would be \\(2 \\times 10 \\times 500 + 3 \\times 20 \\times 1000 = 70,000\\) seat miles.\nUsing the datasets included in the nycflights13 package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints:\n\nCrucial: Unless you are very confident in what you are doing, it is worthwhile not starting to code right away. Rather, first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level pseudocode that is informal yet detailed enough to articulate what you are doing. This way you won’t confuse what you are trying to do (the algorithm) with how you are going to do it (writing dplyr code).\nTake a close look at all the datasets using the View() function: flights, weather, planes, airports, and airlines to identify which variables are necessary to compute available seat miles.\nFigure @ref(fig:reldiagram) showing how the various datasets can be joined will also be useful.\nConsider the data wrangling verbs in Table @ref(tab:wrangle-summary-table) as your toolbox! ::"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-4-tidy",
    "href": "LC/Learning-checks.html#chapter-4-tidy",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 4: Tidy",
    "text": "Chapter 4: Tidy\n::: {.callout-note icon=false} ## LC 4.1 Block 1 Day 8 What are common characteristics of “tidy” data frames? ::\n::: {.callout-note icon=false} ## LC 4.2 Block 1 Day 8 What makes “tidy” data frames useful for organizing data? ::\n::: {.callout-note icon=false} ## LC 4.3 Block 1 Day 8 Take a look at the airline_safety data frame included in the fivethirtyeight data package. Run the following:\n\nairline_safety\n\nAfter reading the help file by running ?airline_safety, we see that airline_safety is a data frame containing information on different airline companies’ safety records. This data was originally reported on the data journalism website, FiveThirtyEight.com, in Nate Silver’s article, “Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?”. Let’s only consider the variables airlines and those relating to fatalities for simplicity:\n\nairline_safety_smaller <- airline_safety %>% \n  select(airline, starts_with(\"fatalities\"))\nairline_safety_smaller\n\n# A tibble: 56 × 3\n   airline               fatalities_85_99 fatalities_00_14\n   <chr>                            <int>            <int>\n 1 Aer Lingus                           0                0\n 2 Aeroflot                           128               88\n 3 Aerolineas Argentinas                0                0\n 4 Aeromexico                          64                0\n 5 Air Canada                           0                0\n 6 Air France                          79              337\n 7 Air India                          329              158\n 8 Air New Zealand                      0                7\n 9 Alaska Airlines                      0               88\n10 Alitalia                            50                0\n# … with 46 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThis data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, in particular so that it has a variable fatalities_years indicating the incident year and a variable count of the fatality counts? ::\n::: {.callout-note icon=false} ## LC 4.4 Block 1 Day 9 Convert the dem_score data frame into a “tidy” data frame and assign the name of dem_score_tidy to the resulting long-formatted data frame. ::\n::: {.callout-note icon=false} ## LC 4.5 Block 1 Day 9 Read in the life expectancy data stored at https://moderndive.com/data/le_mess.csv and convert it to a “tidy” data frame. ::"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-5-regression",
    "href": "LC/Learning-checks.html#chapter-5-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 5: Regression",
    "text": "Chapter 5: Regression\n\n\n\n\n\n\nLC 5.1 Block 2 Day 1\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) being score but with age as the new explanatory variable \\(x\\). Remember, this involves three things:\n\nLooking at the raw data values.\nComputing summary statistics.\nCreating data visualizations.\n\nWhat can you say about the relationship between age and teaching scores based on this exploration?\n\n\n\n\n\n\n\n\nLC 5.2 Block 2 Day 1\n\n\n\nFit a new simple linear regression using lm(score ~ age, data = evals_ch5) where age is the new explanatory variable \\(x\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your earlier exploratory data analysis?\n\n\n\n\n\n\n\n\nLC 5.3 Block 2 Day 1\n\n\n\nGenerate a data frame of the residuals of the model where you used age as the explanatory \\(x\\) variable.\n\n\n\n\n\n\n\n\nLC 5.4 Block 2 Day 2\n\n\n\nConduct a new exploratory data analysis with the same explanatory variable \\(x\\) being continent but with gdpPercap as the new outcome variable \\(y\\). What can you say about the differences in GDP per capita between continents based on this exploration?\n\n\n\n\n\n\n\n\nLC 5.5 Block 2 Day 2\n\n\n\nFit a new linear regression using lm(gdpPercap ~ continent, data = gapminder2007) where gdpPercap is the new outcome variable \\(y\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis?\n\n\n\n\n\n\n\n\nLC 5.6 Block 2 Day 2\n\n\n\nUsing either the sorting functionality of RStudio’s spreadsheet viewer or using the data wrangling tools you learned in Chapter @ref(wrangling), identify the five countries with the five smallest (most negative) residuals? What do these negative residuals say about their life expectancy relative to their continents’ life expectancy?\n\n\n\n\n\n\n\n\nLC 5.7 Block 2 Day 2\n\n\n\nRepeat this process, but identify the five countries with the five largest (most positive) residuals. What do these positive residuals say about their life expectancy relative to their continents’ life expectancy?\n\n\n\n\n\n\n\n\nLC 5.8 Block 2 Day 3\n\n\n\nNote in Figure @fig:three-lines there are 3 points marked with dots and:\n\nThe “best” fitting solid regression line in blue\nAn arbitrarily chosen dotted red line\nAnother arbitrarily chosen dashed green line\n\n\n\n\n\n\nFigure 2: Regression line and two others.\n\n\n\n\nCompute the sum of squared residuals by hand for each line and show that of these three lines, the regression line in blue has the smallest value."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-6-multiple-regression",
    "href": "LC/Learning-checks.html#chapter-6-multiple-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 6: Multiple regression",
    "text": "Chapter 6: Multiple regression\n\n\n\n\n\n\nLC 6.1 Block 2 Day 4\n\n\n\nCompute the observed values, fitted values, and residuals not for the interaction model as we just did, but rather for the parallel slopes model we saved in score_model_parallel_slopes.\n\n\n\n\n\n\n\n\nLC 6.2\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age?\n\n\n\n\n\n\n\n\nLC 6.3\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age?\n\n\n\n\n\n\n\n\nLC 6.4\n\n\n\nFit a new simple linear regression using lm(debt ~ credit_rating + age, data = credit_ch6) where credit_rating and age are the new numerical explanatory variables \\(x_1\\) and \\(x_2\\). Get information about the “best-fitting” regression plane from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis?"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-7-sampling",
    "href": "LC/Learning-checks.html#chapter-7-sampling",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 7: Sampling",
    "text": "Chapter 7: Sampling\n\n\n\n\n\n\nLC 7.1 Block 3 Day 1\n\n\n\nWhy was it important to mix the bowl before we sampled the balls?\n\n\n\n\n\n\n\n\nLC 7.2 Block 3 Day 1\n\n\n\nWhy is it that our 33 groups of friends did not all have the same numbers of balls that were red out of 50, and hence different proportions red?\n\n\n\n\n\n\n\n\nLC 7.3 Block 3 Day 1\n\n\n\nWhy couldn’t we study the effects of sampling variation when we used the virtual shovel only once? Why did we need to take more than one virtual sample (in our case 33 virtual samples)?\n\n\n\n\n\n\n\n\nLC 7.4 Block 3 Day 1\n\n\n\nWhy did we not take 1000 “tactile” samples of 50 balls by hand?\n\n\n\n\n\n\n\n\nLC 7.5 Block 3 Day 1\n\n\n\nLooking at Figure @ref(fig:samplingdistribution-virtual-1000), would you say that sampling 50 balls where 30% of them were red is likely or not? What about sampling 50 balls where 10% of them were red?\n\n\n\n\n\n\n\n\nLC 7.6 Block 3 Day 1\n\n\n\nIn Figure 7.9, we used shovels to take 1000 samples each, computed the resulting 1000 proportions of the shovel’s balls that were red, and then visualized the distribution of these 1000 proportions in a histogram. We did this for shovels with 25, 50, and 100 slots in them. As the size of the shovels increased, the histograms got narrower. In other words, as the size of the shovels increased from 25 to 50 to 100, did the 1000 proportions\n\nA. vary less,\nB. vary by the same amount, or\nC. vary more?\n\n\n\n\n\n\n\n\n\nLC 7.7 Block 3 Day 1\n\n\n\nWhat summary statistic did we use to quantify how much the 1000 proportions red varied?\n\nA. The interquartile range\nB. The standard deviation\nC. The range: the largest value minus the smallest.\n\n\n\n\n\n\n\n\n\nLC 7.8 Block 3 Day 2\n\n\n\nIn the case of our bowl activity, what is the population parameter? Do we know its value?\n\n\n\n\n\n\n\n\nLC 7.9 Block 3 Day 2\n\n\n\nWhat would performing a census in our bowl activity correspond to? Why did we not perform a census?\n\n\n\n\n\n\n\n\nLC 7.10 Block 3 Day 2\n\n\n\nWhat purpose do point estimates serve in general? What is the name of the point estimate specific to our bowl activity? What is its mathematical notation?\n\n\n\n\n\n\n\n\nLC 7.11 Block 3 Day 2\n\n\n\nHow did we ensure that our tactile samples using the shovel were random?\n\n\n\n\n\n\n\n\nLC 7.12 Block 3 Day 2\n\n\n\nWhy is it important that sampling be done at random?\n\n\n\n\n\n\n\n\nLC 7.13 Block 3 Day 2\n\n\n\nWhat are we inferring about the bowl based on the samples using the shovel?\n\n\n\n\n\n\n\n\nLC 7.14 Block 3 Day 2\n\n\n\nWhat purpose did the sampling distributions serve?\n\n\n\n\n\n\n\n\nLC 7.15 Block 3 Day 2\n\n\n\nWhat does the standard error of the sample proportion \\(\\widehat{p}\\) quantify?\n\n\n\n\n\n\n\n\nLC 7.16 Block 3 Day 2\n\n\n\nThe table that follows is a version of Table @ref(tab:comparing-n-2) matching sample sizes \\(n\\) to different standard errors of the sample proportion \\(\\widehat{p}\\), but with the rows randomly re-ordered and the sample sizes removed. Fill in the table by matching the correct sample sizes to the correct standard errors.\nStandard errors of \\(\\hat{p}\\) based on n = 25, 50, 100\n\n\n\nSample size\nStandard error of \\(\\hat{p}\\)\n\n\n\n\n\\(n=\\)\n0.94\n\n\n\\(n=\\)\n0.45\n\n\n\\(n=\\)\n0.69\n\n\n\nFor the following four Learning checks, let the estimate be the sample proportion \\(\\widehat{p}\\): the proportion of a shovel’s balls that were red. It estimates the population proportion \\(p\\): the proportion of the bowl’s balls that were red.\n\n\n\n\n\n\n\n\nLC 7.17 Block 3 Day 2\n\n\n\nWhat is the difference between an accurate and a precise estimate?\n\n\n\n\n\n\n\n\nLC 7.18 Block 3 Day 2\n\n\n\nHow do we ensure that an estimate is accurate? How do we ensure that an estimate is precise?\n\n\n\n\n\n\n\n\nLC 7.19 Block 3 Day 2\n\n\n\nIn a real-life situation, we would not take 1000 different samples to infer about a population, but rather only one. Then, what was the purpose of our exercises where we took 1000 different samples?\n\n\n\n\n\n\n\n\nLC 7.20 Block 3 Day 2\n\n\n\nFigure @ref(fig:accuracy-vs-precision) with the targets shows four combinations of “accurate versus precise” estimates. Draw four corresponding sampling distributions of the sample proportion \\(\\widehat{p}\\), like the one in the leftmost plot in Figure @ref(fig:comparing-sampling-distributions-3).\n\n\n\n\n\n\n\n\nLC 7.21 Block 3 Day 3\n\n\n\nThe Royal Air Force wants to study how resistant all their airplanes are to bullets. They study the bullet holes on all the airplanes on the tarmac after an air battle against the Luftwaffe (German Air Force).\n\n\n\n\n\n\n\n\nLC 7.22 Block 3 Day 3\n\n\n\nImagine it is 1993, a time when almost all households had landlines. You want to know the average number of people in each household in your city. You randomly pick out 500 phone numbers from the phone book and conduct a phone survey.\n\n\n\n\n\n\n\n\nLC 7.23 Block 3 Day 3\n\n\n\nYou want to know the prevalence of illegal downloading of TV shows among students at a local college. You get the emails of 100 randomly chosen students and ask them, “How many times did you download a pirated TV show last week?”.\n\n\n\n\n\n\n\n\nLC 7.24 Block 3 Day 3\n\n\n\nA local college administrator wants to know the average income of all graduates in the last 10 years. So they get the records of five randomly chosen graduates, contact them, and obtain their answers."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-8-confidence-intervals",
    "href": "LC/Learning-checks.html#chapter-8-confidence-intervals",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 8: Confidence intervals",
    "text": "Chapter 8: Confidence intervals\n\n\n\n\n\n\nLC 8.1 Block 3 Day 5\n\n\n\nWhat is the chief difference between a bootstrap distribution and a sampling distribution?\n\n\n\n\n\n\n\n\nLC 8.2 Block 3 Day 5\n\n\n\nLooking at the bootstrap distribution for the sample mean in Figure @ref(fig:one-thousand-sample-means), between what two values would you say most values lie?\n\n\n\n\n\n\n\n\nLC 8.3 Block 3 Day 6\n\n\n\nWhat condition about the bootstrap distribution must be met for us to be able to construct confidence intervals using the standard error method?\n\n\n\n\n\n\n\n\nLC 8.4 Block 3 Day 6\n\n\n\nSay we wanted to construct a 68% confidence interval instead of a 95% confidence interval for \\(\\mu\\). Describe what changes are needed to make this happen. Hint: we suggest you look at Appendix @ref(appendix-normal-curve) on the normal distribution.\n\n\n\n\n\n\n\n\nLC 8.5 Block 3 Day 8\n\n\n\nConstruct a 95% confidence interval for the median year of minting of all US pennies. Use the percentile method and, if appropriate, then use the standard-error method."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-9-hypothesis-testing",
    "href": "LC/Learning-checks.html#chapter-9-hypothesis-testing",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 9: Hypothesis testing",
    "text": "Chapter 9: Hypothesis testing\n\n\n\n\n\n\nLC 9.1 Block 4 Day 2\n\n\n\nWhy does the following code produce an error? In other words, what about the response and predictor variables make this not a possible computation with the infer package?\n\nlibrary(moderndive)\nlibrary(infer)\nnull_distribution_mean <- promotions %>%\n  specify(formula = decision ~ gender, success = \"promoted\") %>% \n  hypothesize(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\n\n\n\n\n\n\n\n\nLC 9.2 Block 4 Day 2\n\n\n\nWhy are we relatively confident that the distributions of the sample proportions will be good approximations of the population distributions of promotion proportions for the two genders?\n\n\n\n\n\n\n\n\nLC 9.3 Block 4 Day 2\n\n\n\nUsing the definition of p-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the promotion rates for males and females.\n\n\n\n\n\n\n\n\nLC 9.4 Block 4 Day 2\n\n\n\nDescribe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between the promotion rate of males and females using this study.\n\n\n\n\n\n\n\n\nLC 9.5 Block 4 Day 3\n\n\n\nWhat is wrong about saying, “The defendant is innocent.” based on the US system of criminal trials?\n\n\n\n\n\n\n\n\nLC 9.6 Block 4 Day 3\n\n\n\nWhat is the purpose of hypothesis testing?\n\n\n\n\n\n\n\n\nLC 9.7 Block 4 Day 3\n\n\n\nWhat are some flaws with hypothesis testing? How could we alleviate them?\n\n\n\n\n\n\n\n\nLC 9.8 Block 4 Day 3\n\n\n\nConsider two \\(\\alpha\\) significance levels of 0.1 and 0.01. Of the two, which would lead to a more liberal hypothesis testing procedure? In other words, one that will, all things being equal, lead to more rejections of the null hypothesis \\(H_0\\).\n\n\n\n\n\n\n\n\nLC 9.9\n\n\n\nConduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating. What was different and what was the same?\n\n\n\n\n\n\n\n\nLC 9.10\n\n\n\nWhat conclusions can you make from viewing the faceted histogram looking at rating versus genre that you couldn’t see when looking at the boxplot?\n\n\n\n\n\n\n\n\nLC 9.11\n\n\n\nDescribe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies.\n\n\n\n\n\n\n\n\nLC 9.12\n\n\n\nWhy are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres?\n\n\n\n\n\n\n\n\nLC 9.13\n\n\n\nUsing the definition of \\(p\\)-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the mean rating of romance to action movies.\n\n\n\n\n\n\n\n\nLC 9.14\n\n\n\nWhat is the value of the \\(p\\)-value for the hypothesis test comparing the mean rating of romance to action movies?\n\n\n\n\n\n\n\n\nLC 9.15\n\n\n\nTest your data wrangling knowledge and EDA skills:\n\nUse dplyr and tidyr to create the necessary data frame focused on only action and romance movies (but not both) from the movies data frame in the ggplot2movies package.\nMake a boxplot and a faceted histogram of this population data comparing ratings of action and romance movies from IMDb.\nDiscuss how these plots compare to the similar plots produced for the movies_sample data."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-10-inference-for-regression",
    "href": "LC/Learning-checks.html#chapter-10-inference-for-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 10: Inference for regression",
    "text": "Chapter 10: Inference for regression\n\n\n\n\n\n\nLC 10.1 Block 4 Day 7\n\n\n\nContinuing with our regression using age as the explanatory variable and teaching score as the outcome variable.\n\nUse the get_regression_points() function to get the observed values, fitted values, and residuals for all 463 instructors.\nPerform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here.\n\n\n\n\n\n\n\n\n\nLC 10.2 Block 4 Day 8\n\n\n\nRepeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of stat = \"correlation\" in the calculate() function of the infer package."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-11-tell-your-story-with-data",
    "href": "LC/Learning-checks.html#chapter-11-tell-your-story-with-data",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 11: Tell your story with data",
    "text": "Chapter 11: Tell your story with data\n\n\n\n\n\n\n\nLC 11.1 Block 4 Day 2\n\n\n\nRepeat the regression modeling in Subsection 11.2.3 and the prediction making you just did on the house of condition 5 and size 1900 square feet in Subsection 12.2.4, but using the parallel slopes model you visualized in Figure 11.6. Show that it’s $524,807!\n\n\n\n\n\n\n\n\nLC 11.2\n\n\n\nWhat date between 1994 and 2003 has the fewest number of births in the US? What story could you tell about why this is the case?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "",
    "text": "This site holds the proposal for the Spring 2023 version of Math 300."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Background",
    "text": "Background\nUp through Spring 2022, Math 300 was organized around the Moore and Notz textbook: Statistics: Concepts and controversies 10/e. This book was designed for a non-technical audience of “consumers of statistics” but is dramatically out of date. For instance, it has absolutely no data science content and introduces only primitive statistical methods. This was deemed inappropriate for cadets going on to be officers who will inevitably have to work with modern data and methods.\nIn Fall 2022, Math 300 switched to a very different book, Ismay and Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. The ModernDive book introduces computing on data in an accessible but modern way. It is the only well-known statistics text based on a data-science perspective. Nonetheless, the statistical inference portions of the book regress to the same sort of primitive statistical methods from Concepts and Controversies.\nTo support the Fall 2022 course using ModernDive, a complete set of roughly 35 Notes to Instructors (NTI) were written by Prof. Bradley Warner along with problem sets and other needed materials and deployed for the course.\nThis proposal is to further transition Math 300, building on the Fall 2022 course but replacing the statistical inference portions of the course with more contemporary and general-purpose inference techniques and support for concepts and methods relevant to decision making.\nIn the following, I will be making reference to three different versions of Math 300:\n\nThe Fall 2022 version of the course, using the ModernDive book will be called Math 300.\nThe previous version of course, as it was taught for several years before Fall 2022, will be called 300CC, which refers to the textbook then used, Concepts & Controversies.\nThe course proposed in this document, a revision of part of Math 300, will be called Math 300R. The R stands for “revised.”"
  },
  {
    "objectID": "index.html#overall-goals-of-math-300r",
    "href": "index.html#overall-goals-of-math-300r",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Overall goals of Math 300R",
    "text": "Overall goals of Math 300R\nThe design of a course revision needs to take into account several factors:\n\nthe target audience’s anticipated technical ability and motivation and therefore the appropriate pedagogy and the balance between theory and practice to use in the course\ninstitutional goals as they relate to the course subject, that is, the prioritization of the many different topics that might be included in the course\nconstraints of class time and internal coherence of the course, that is, using later topics in the course to reinforce student learning of the earlier topics\n\nLater, in the rationale section section of this document, I describe how I came to the following conclusions, but for now a simple statement of the conclusions will suffice.\n\nThe target audience is humanities and social science majors, many of whom will not be confident in the use of calculus but all of whom have had previous exposure to R in the core calculus course.\nInstitutional goals (as revealed by discussion with humanities and social science departments and the wording of the catalog description of Math 300) include a substantial emphasis on data science techniques (data wrangling and visualization) and on the use of statistical concepts and methods to support decision-making."
  },
  {
    "objectID": "index.html#statistical-topics-and-framework",
    "href": "index.html#statistical-topics-and-framework",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Statistical topics and framework",
    "text": "Statistical topics and framework\nThe transition from Math 300CC to Math 300—supported in part by the development of Math 141Z/142Z—has already accomplished many of the goals with respect to data science techniques. The challenge motivating this proposal centers on the statistical topics/methods to be covered and the path through the topics.\nThe class-time demands of the new emphasis on data science techniques in Math 300 (and retained in Math 300R) dictate that the statistical concepts and methods need to be taught in a more compact manner than in Math 300CC. Low-priority, legacy topics from Math 300CC should be dropped, particularly those that do not relate to decision-making. Some guidance here is provided by discussions in the statistical education community and the broad agreement with recommendations of the GAISE report.\nA traditional path for statistical methods starts with descriptive statistics (e.g. standard deviation) then presents “1-sample” statistics (e.g. mean, proportion) and inferential techniques (confidence interval, hypothesis test) in that context. Next comes the inferential techniques for the analogous “2-sample” statistics (difference in mean, difference in proportion) followed by inference techniques for regression.\nFor students who have already seen regression, as in Math 141Z/142Z, this path is unnecessarily long: all of the methods in the traditional path are encompassed by regression.1 Framing statistical inference in the context of regression avoids the need to teach method-specific calculations or cover the variety of formats for non-regression test results. Regression is part of the toolbox of data scientists and relates well to more advanced data techniques such as machine learning. It is already integrated into the ModernDive textbook used in Math 300.\nAdditional streamlining comes from motivating statistical inference using a simulation approach. This is based on two conceptually simple data operations: resampling and permutation (shuffling). This approach is well established in the statistics community and considered by many to be a better pedagogy than the traditional formula-and-distribution presentation of statistical methods. Since Math 300 (and 300R) students will already have worked with wrangling and visualization, they will be well prepared to work with the data generated by repeated trials of simulation.\nAdditional concepts and techniques important to supporting decision making relate to risk, prediction and its close cousin classification, as well as causality and confounding. These are treated minimally in traditional statistics courses, but are an important part of Math 300R. A model for teaching about risk, causation, and confounding is provided by introductory epidemiology courses. I’ve drawn the pedagogy for Math 300R from the epidemiology course I introduced at Macalester. In addition, I have more than a decade of experience teaching causality as part of an introductory course. (See the causation chapter of my Statistical Modeling text.)\nAn important part of the Statistical Modeling pedagogy for causality is simulation based on directed acyclic graphs (DAG). Unlike resampling and permutation, which re-arrange existing data, the DAG simulations generate synthetic data with specified properties (such as effect sizes). This allows a concrete demonstration of the extent to which regression techniques can and cannot recover causal information from data.\nThe DAG-simulation approach lends itself naturally to the demonstration of statistical phenomena such as sampling variation and estimation of prediction error. As an example, consider the statistical fallacy of regression to the mean, as with Galton’s finding that heights of children regress to the mean compared to the heights of their parents. The natural hypothesis that heights are determined by genetic and other factors is represented by this DAG:\n\\[\\epsilon \\rightarrow parent \\longleftarrow GENES \\longrightarrow child \\leftarrow \\nu\\]\nIn this DAG there is no causal mechanism included for “regression to the mean.” However, Galton’s empirical finding is replicated by data from the DAG-simulation."
  },
  {
    "objectID": "index.html#sec-broad-structure",
    "href": "index.html#sec-broad-structure",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Scope of the proposed changes",
    "text": "Scope of the proposed changes\nMath 300R will retain the first 17 lessons Math 300. All teaching materials for this part of the course will be used unaltered. (Exception: revisions to Math 300 the Fall 2022 teaching team deems appropriate. Such revisions are not part of this proposal.)\nThe next 19 lessons will be completely refactored and based on new readings, NTIs, exams, and other materials. Objectives for each of these 19 lessons are itemized here.\n\nThe corresponding ModernDive chapters will not be used.\nThe software will the same as that used in the first half of the ModernDive book, specifically the ggplot2 graphics package and the tidyverse data wrangling packages. However, …\nThe infer package used in the second half of ModernDive will be dropped.\n\nThe theme of the refactored 19 lessons is “informing decisions with data.” Math 300R will introduce concepts and methods needed to predict the impact of interventions, the prediction of individual outcomes, and the quantification and accumulation of risk. One important idea is “effect size,” how much a change in an explanatory variable changes the response variable. This is intimately connected with causation and causation is connected with co-variation. Another important idea is prediction of outcomes and the quantification of uncertainty in the prediction. One important application of this is in the design and operation of “classifiers.”\nTopics to be de-emphasized are the algebra of computing confidence intervals and p-values and the (controversial) role of p-values as a guide to practical “significance.” Typically, about half of a traditional course is given over to the construction of confidence intervals in various settings and, more or less equivalently, the conversion of data into p-values. In the contemporary era, when “observational” data are collected en masse, p-values can become very small (“significant”) even when the the relationship under study is slight and insubstantial.\nConfounding and methods for dealing with it (statistical adjustment, experiment) are treated substantially in Math 300R. Decision-making about interventions often relies on the understanding of causal effects; the possibility of confounding is a major source of skepticism about making causal judgments. In a world where much data is observational in nature, the sweeping principles that “correlation is not causation” and “no causation without experimentation” do not provide support for making responsible conclusions about causal connections. Decision-makers need this support."
  },
  {
    "objectID": "index.html#rationale",
    "href": "index.html#rationale",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Rationale for course revisions",
    "text": "Rationale for course revisions\n\nRelationship to Math 357 and Math 377\nDFMS offers three courses satisfying the statistics component of the Academy’s core requirements: Math 300, Math 357, and Math 377. In designing 300R, attention should be paid to the reasons for supporting three distinct courses. The catalog copy lays out the differences in terms of intended student major, software, mathematical background, and orientation to data science.\nIntended student major: The catalog says, “Math 300 is designed primarily for majors in the Social Sciences and Humanities.” while “Math 356 is primarily designed for cadets in engineering, science, or other technical disciplines. Math majors and Operations Research majors will take Math 377.” Math 377 is also the intended course for prospective Data Science majors, although this is not in the catalog.\nSoftware: The catalog does not describe any software component for either Math 300 or Math 357, but states that, in Math 377, “modern software appropriate for data analysis will be used.” In reality, as of Fall 2022, much the same software is used in all three courses: R with the dplyr package for data wrangling, ggplot2 for data visualization, and “R/Markdown” for creating computationally active documents.\nOne difference between Math 300 and 357/377 relates to computer programming. Both 357 and 377 include content about the underlying structure of the R language, object types, the construction of functions, and arrays and iteration. In contrast, Math 300 is based on a small set of command patterns using data frames. Students see R in Math 300 more or less as an extension of what they learned in 141Z/142Z; what’s added is a few statistical and data-wrangling functions and a handful of new graphics types.\nStudents’ mathematical background: Math 377 explicitly refers to “calculus-based probability.” Math 300 and 357 share identical catalog copy, though in reality Math 357 and Math 377 use the same textbook. Calculus is indeed necessary for the probability topics in Math 357 and 377. My interpretation is that Math 300 should serve as a safe haven for those who lack confidence in their calculus skills. Both the Fall 2022 edition of Math 300 and the proposed Math 300R serve this role as safe haven.\nOrientation to Data Science: Starting with the Fall 2022 edition, Math 300 develops and draws on data-science skills for wrangling and visualization. In this, the new Math 300 is in line with both Math 357 and 377.\nThe above analysis indicates that Math 300 and 300R should diverge from Math 357/377 in these ways:\n\nMath 300R should make little or no use of calculus operations.\nMath 300R should include little consideration of probability distributions or (non-automated) calculations with any but the simplest.\nMath 300R should be computational, but should not draw heavily on computer programming skills such as types of objects, arrays, indexing, and loop-style iteration. Use of R/Markdown documents should be considered as a pedagogical choice, and retained or discontinued based on how it contributes to student success in the other areas of the course.\n\nIn addition, I suggest that …\n\nMath 300R include some work with assembling/curating data using spreadsheets and basic data cleaning with spreadsheets. Awareness of the ubiquity of data errors and a basic understanding of how to deal with such errors is an important component of working with data. (This is not to suggest that data analysis, modeling, and graphical depiction be taught using spreadsheets, which are notoriously unreliable, difficult, and limiting for such purposes. Spreadsheets are, however, appropriate for the phase where non-tabular data is transcribed into a tabular arrangement.)\n\n\n\nInstitutional goals\nIt can be difficult to translate broadly stated institutional goals to apply them to a single course. However, catalog descriptions of programs and individual courses provide some assistance. Here is the catalog copy for Math 300 (which is identical to the catalog description of Math 357).\n\nMath 300. Introduction to Statistics. An introduction in probability and statistics for decision-makers. Topics include basic probability, statistical inference, prediction, data visualization, and data management. This course emphasizes critical thinking among decision-makers, preparing future officers to be critical consumers of data. (Emphasis added.)\n\nI interpret the final sentence as a description of the overall objective of the course:\n\nOverall objective: Prepare officers to use data to inform decisions.\n\nReturning to the idea that the topics listed in the catalog copy ought to be interpreted as serving the overall objective of the course, let’s consider those topics one at a time:\n\ndata management\ndata visualization\nprediction\nstatistical inference\nbasic probability\n\n\nStrictly speaking, as a term of art the phrase “data management” is business jargon describing enterprise-level activities that are unrelated to the other items on the list. It would be unheard of to include it, in this strict sense, in a statistics course. I believe the intent of the phrase to be better served by terms like “data wrangling,” “data cleaning,” “database querying,” and such which make up an important part of “data science.” Data wrangling is a major feature of Math 300 launched and is covered using professional level computing tools well suited to both small and large data. But whatever “data management” might reasonably be taken to mean, it was utterly ignored in Math 300CC.\n“Data visualization” is generally taken to be the process of using graphics to discover and highlight patterns shown in data. Math 300CC included only statistical graphics such as histograms, box-and-whisker plots, and basic “scatter plots.” Math 300 adds to this modern modes of graphics such as transparency, color, and faceting that make it possible to display relationships among multiple variables. The software used in Math 300 is the professional-level ggplot2 which provides the ability to increase the sophistication and generality of data display, using for example density graphics such as violin plots. As such, Math 300 is a big step on the road to rich data visualization. Some of these will be introduced in Math 300R in the second half of the course.\n“Prediction” is a central paradigm used in the important area of “machine learning.” It is also an often used method used to inform decision making and characterize risk, for instance, by indicating the distribution of plausible outcomes. Math 300CC emphasized paradigms such as hypothesis testing and confidence intervals that are not aligned with making and interpreting predictions. Math 300 focuses on these same paradigms. Math 300R will treat prediction as a central statistical path, as well as highlighting its proper use, interpretation, and evaluation.\n“Statistical inference” is traditionally taken to mean the calculation and interpretation of hypothesis tests and confidence intervals in various simple settings. Such settings include the “difference between two means,” the “correlation coefficient,” and the “slope of a regression line.” Math 300CC introduced a handful of such settings, providing distinct formulas for each of them. The “controversies” referred to in the title Concepts and controversies includes the problematic interpretation of “p-values” and the need to use random sampling and/or random assignment in data collection to get “correct” results. Math 300 retains the emphasis on confidence intervals and p-values in the simple settings, but emphasizes a more general and accessible methodology based on bootstrapping and permutation tests.\n\nUnfortunately, appealing to random sampling/assignment is often whistling past the graveyard, since these idealized data collection processes are rarely available. Instead, professionals include “covariates” in their data collection in order to “adjust” for the factors that would have been scrambled into insignificance by random sampling/assignment if it had been available. Math 300R incorporates covariate methods and highlights the importance of identifying appropriate covariates.\n\n“Basic probability” can mean different things to different people. In most introductory statistics courses it refers to the construction, calculation, and study of named distributions such as the binomial, normal, chi-squared, t, etc. Such distributions play an important role in the statistical theory of confidence intervals and hypothesis testing. That is, they are support for statistical inference. Math 300CC followed the traditional pattern of having students memorize which distribution is relevant to which setting and using printed tables for calculation. As described earlier, Math 300 provides a much more natural route to inference through bootstrapping and permutation tests.\n\nWhat’s left out in this conception of basic probability is the support for decision making. Essential to this is the proper use of “conditional probability.” Math 300R emphasizes appropriate use and interpretation of conditional probability, seen most clearly in the “classifiers” part of the course.\n\n\nFaculty opinions\nInsofar as faculty internalize the goals of the institution, their views can point to ways that existing courses do and do not reflect those goals.\nWithin DFMS and other departments, there is a general discontent that Math 300CC was not doing what it ought to. Reasons for this can be seen by examining the textbook used in Math 300CC. The book has clear deficiencies, among which are:\n\nthe material is out of date and does not reflect any of the consensus recommendations (such as GAISE) developed in the last 30 years.\nit does not use data at any level beyond hand calculation.\nit does not deal with decision making at any serious level. (The only decision formally supported is whether or not to reject the Null hypothesis.)\n\nThe opinions of faculty outside DFMS can also be an important guide to institutional priorities. In AY 2021-22 I contacted the departments in the social sciences and humanities. Three of these—history, political science, economics—responded with interest. Discussion with groups of faculty from these departments elucidated a number of points:\n\nThe faculty most highly valued the development of data-science skills such as computing for data wrangling and data visualization.\nThe then-current version of Math 300 did not contribute to the development of such skills.\nMath 357 is not seen as an appropriate alterative to Math 300, both because of perceived difficulty of 357 and because faculty do not value the emphasis on probability distributions seen in 357.\n\nFrom my experience at Macalester and in conducting reviews at many colleges, I am often wary of the motivation of faculty in other departments. These can represent a desire for service courses like Math 300 to cover discipline-specific techniques. However, the faculty I spoke to also had an eye on what their students will need for their post-graduation jobs. Particularly the USAF officers drew on their field experience in areas such as military intelligence.\nBased on these findings, the group of faculty planning for revisions to Math 300 made an easy decision: replace the textbook with one oriented to data science. We selected the ModernDive book, which is unique among introductory statistics textbooks in starting out with data wrangling and visualization. This change of textbook addresses the “use data” part of the course objective stipulated above.\nThe other part of the objective—inform decisions—remains problematic even with the switch in the Math 300 textbook. Discussions I had with the ModernDive authors made clear that their purpose in writing the book was to provide a way to introduce data science into introductory statistics, but that they stuck to the traditional hypothesis-testing/confidence-interval framework in order not to make the change too daunting for instructors thinking of adopting the text. In other words, they were not trying to turn the topic toward decision-making with data, the motivation of the ideas presented in this proposal for Math 300R."
  },
  {
    "objectID": "index.html#plan-of-work",
    "href": "index.html#plan-of-work",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Plan of work",
    "text": "Plan of work\n\nEarly October 2022: Preliminary approval, with appropriate modifications, of the proposed objectives.\nOctober 2022: DTK will draft new day-by-day NTIs for the second half of the course in the same style as the existing NTIs for the first half of the course. In the process of drafting, there will likely be some re-arrangement and modification of the objectives in (1).\nNovember 2022: With the draft NTIs in hand, a faculty team will make a more detailed examination of the proposed objectives. I recommend that this examination be structured as a set of hour-long discussions, one for each of the five divisions described in ?@sec-topics.\nNovember/December 2022: DTK (and others, as interested) will assemble student readings to replace the second half of ModernDive. Much of the content already exists in the form of a draft textbook by DTK. These will be re-arranged to correspond to the day-to-day objectives as determined in (3).\nJanuary/February 2023: The first 18 lessons of 300R will be taught as a repeat of those lessons from Math 300 Fall 2022. DTK will participate mainly as an observer.\nJanuary/February 2023: Revision and refinement will be made of the readings and NTIs in (3) and (4) above.\nMarch/April 2023: Teaching the new lessons. DTK will participate as an instructor for these lessons."
  },
  {
    "objectID": "NTIs.html",
    "href": "NTIs.html",
    "title": "Notes to Instructors",
    "section": "",
    "text": "Links to NTIs will go here and in the day-to-day objectives."
  },
  {
    "objectID": "schedule-of-objectives.html",
    "href": "schedule-of-objectives.html",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "",
    "text": "As done in Fall 2020. Possible revisions to those lessons is not a topic of this proposal."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-19-decisions-with-data-nti",
    "href": "schedule-of-objectives.html#lesson-19-decisions-with-data-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 19: Decisions with data (nti)",
    "text": "Lesson 19: Decisions with data (nti)\n\nDistinguish between the two settings for decision-making:\n\nPrediction: predict an outcome for an individual\nRelationship: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\nGiven a research question, identify whether it corresponds to a prediction setting or a relationship setting."
  },
  {
    "objectID": "schedule-of-objectives.html#gaming-intro",
    "href": "schedule-of-objectives.html#gaming-intro",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 20: Reality versus gaming (nti)",
    "text": "Lesson 20: Reality versus gaming (nti)\n\nUnderstand that gaming is a way of improving our skills and identifying potential opportunities and problems.\nEnumerate the four stages of the games we will use and identify which ones correspond to non-gaming, real-world work with data.\n\n\n\n\n\n\n\nThe Four Stages (to be moved to NTI)\n\n\n\n\nbuilding the deck: Instructors provide a simulation of a mechanism that generates rows of a data frame.\nthe deal: Some of these rows will be dealt to you, constituting the data you have to work with. [real-world]\nthe play: Build models and extract results. [real-world]\nthe reveal: Compare your results from (iii) either to the mechanism given in (i) or to more data generated by the simulation.\n\n\n\n\nDistinguish between a sample, a row, and a sample of samples."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-21-dags-noise-and-simulation-nti",
    "href": "schedule-of-objectives.html#lesson-21-dags-noise-and-simulation-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 21: DAGs, noise, and simulation (nti)",
    "text": "Lesson 21: DAGs, noise, and simulation (nti)\n\nDetermine whether a proposed graph is directed and acyclic.\nRead notation to identify response variable, explanatory variable, covariates, and effect sizes.\nCharacterize the magnitude of random noise.\nGenerate data from simulations and summarize variables individually."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-22-sampling-variation-nti",
    "href": "schedule-of-objectives.html#lesson-22-sampling-variation-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 22: Sampling variation (nti)",
    "text": "Lesson 22: Sampling variation (nti)\n\nImplement on the computer a procedure to generate a sample, calculate a regression model, and produce a summary.\nIterate the procedure and collect the summaries across iterations.\nGraphically display the distribution of summaries and generate a compact numerical description (“confidence interval”) of the sampling distribution."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-23-estimate-sampling-variation-from-a-single-sample-nti",
    "href": "schedule-of-objectives.html#lesson-23-estimate-sampling-variation-from-a-single-sample-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 23: Estimate sampling variation from a single sample (nti)",
    "text": "Lesson 23: Estimate sampling variation from a single sample (nti)\n\nUse bootstrapping to estimate sampling variation.\nInfer sampling variation from a regression table."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-24-effect-size-nti",
    "href": "schedule-of-objectives.html#lesson-24-effect-size-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 24: Effect size (nti)",
    "text": "Lesson 24: Effect size (nti)\n\nEstimate an effect size from a regression model of the two variables.\nConstruct a confidence interval on the effect size.\nEvaluate whether confidence interval indicates that estimated effect size is consistent with simulation.\nUnderstand and use scaling of confidence interval length as a function of \\(n\\)."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-25-mechanics-of-prediction-nti",
    "href": "schedule-of-objectives.html#lesson-25-mechanics-of-prediction-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 25: Mechanics of prediction (nti)",
    "text": "Lesson 25: Mechanics of prediction (nti)\n\nGiven a sample from a DAG simulation, construct a predictor function for a specified response variable.\nUse the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error.\nDistinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-26-constructing-a-prediction-interval",
    "href": "schedule-of-objectives.html#lesson-26-constructing-a-prediction-interval",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 26: Constructing a prediction interval",
    "text": "Lesson 26: Constructing a prediction interval\n\nIdentify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-27-covariates",
    "href": "schedule-of-objectives.html#lesson-27-covariates",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 27: Covariates",
    "text": "Lesson 27: Covariates\n\nShow that including covariates in a prediction model always reduces in-sample mean square residual, but may not reduce residuals out-of-sample.\nGiven regression coefficients, calculate model degrees of freedom and residual degrees of freedom.\nCalculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-28-covariates-eat-variance",
    "href": "schedule-of-objectives.html#lesson-28-covariates-eat-variance",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 28: Covariates eat variance",
    "text": "Lesson 28: Covariates eat variance\n\nConstruct F statistic as ratio of incremental increase in model mean square due to model term(s) divided by residual mean square.\nUse software to construct ANOVA report and correctly interpret F statistics for prediction model term selection."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-29-confounding",
    "href": "schedule-of-objectives.html#lesson-29-confounding",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 29: Confounding",
    "text": "Lesson 29: Confounding\n\nIdentify confounding in a DAG\nChoose whether to include covariate depending on form of DAG"
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-30-non-causal-correlation",
    "href": "schedule-of-objectives.html#lesson-30-non-causal-correlation",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 30: Non-causal correlation",
    "text": "Lesson 30: Non-causal correlation\n\nDistinguish “common cause” and “collider” forms of DAG.\nConstruct appropriate DAG to match a narrative hypothesis."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-31-experiment-and-random-assignment",
    "href": "schedule-of-objectives.html#lesson-31-experiment-and-random-assignment",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 31: Experiment and random assignment",
    "text": "Lesson 31: Experiment and random assignment\n\nProperly use nomenclature of experiment.\nCorrectly re-draw DAG for an ideal experimental intervention.\nUse blocking to set assignment to treatment or control."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-32-measuring-and-accumulating-risk",
    "href": "schedule-of-objectives.html#lesson-32-measuring-and-accumulating-risk",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 32: Measuring and accumulating risk",
    "text": "Lesson 32: Measuring and accumulating risk\n\nDistinguish between absolute and relative risk and identify when a change in risk is being presented as absolute or relative.\nCalculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\nInterpret effect size as stated in log odds."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-33-constructing-a-classifier",
    "href": "schedule-of-objectives.html#lesson-33-constructing-a-classifier",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 33: Constructing a classifier",
    "text": "Lesson 33: Constructing a classifier\n\nBuild a classifier from case-control data.\nCross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\nCalculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-34-accounting-for-prevalence",
    "href": "schedule-of-objectives.html#lesson-34-accounting-for-prevalence",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 34: Accounting for prevalence",
    "text": "Lesson 34: Accounting for prevalence\n\nExplain why case-control data may not give an proper measure of “prevalence.”\nConvert"
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-35-hypothesis-testing",
    "href": "schedule-of-objectives.html#lesson-35-hypothesis-testing",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 35: Hypothesis testing",
    "text": "Lesson 35: Hypothesis testing\n\nUnderstand and use properly hypothesis testing nomenclature: test statistic, sampling distribution under the null, Type-1 and Type-2 error, rejection threshold, p-value\nContrast hypothesis testing versus Bayesian framework."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-36-calculating-a-p-value",
    "href": "schedule-of-objectives.html#lesson-36-calculating-a-p-value",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 36: Calculating a p-value",
    "text": "Lesson 36: Calculating a p-value\n\nThe permutation test\nInterpret correctly from regression/ANOVA reports\nTraditional names for hypothesis tests in different “textbook” settings.\nDistinguish between p-value and effect size, that is, “significance” and “substance.”"
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-37-false-discovery-with-hypothesis-testing",
    "href": "schedule-of-objectives.html#lesson-37-false-discovery-with-hypothesis-testing",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 37: False discovery with hypothesis testing",
    "text": "Lesson 37: False discovery with hypothesis testing\n\nIdentify signs of false discovery in a research paper.\nEstimate how overall p-value should change when study is replicated."
  },
  {
    "objectID": "schedule-of-objectives.html#alternative-1",
    "href": "schedule-of-objectives.html#alternative-1",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Alternative 1",
    "text": "Alternative 1\nTheme: Classifiers: ROC and loss function"
  },
  {
    "objectID": "schedule-of-objectives.html#alternative-2",
    "href": "schedule-of-objectives.html#alternative-2",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Alternative 2",
    "text": "Alternative 2\nTheme: Accumulating risk: Logistic regression"
  }
]
[
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "",
    "text": "The entire box is 14.43 “inches” long. This should be the total of the left, right, and middle measurements, but people tended to round.\n\nThirds <- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT_asFV5LD312bYaGgHK3F91kgLVSiaQpNhggDilfPKAiDBNz9iueOiYWKgAtRRwkFlOz6U9znbiMGK/pub?gid=0&single=true&output=csv\")\n\nRows: 16 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Student_initials\ndbl (3): left, middle, right\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nLong_form <- tidyr::pivot_longer(Thirds, !Student_initials, names_to = \"position\")\nLong_form |> group_by(Student_initials) |> summarize(tot = sum(value), v = var(value))\n\n# A tibble: 16 × 3\n   Student_initials   tot        v\n   <chr>            <dbl>    <dbl>\n 1 AMA               13.8 0.0208  \n 2 BAK               14.5 0.271   \n 3 DQS               14.4 0.416   \n 4 EDS               16   0.646   \n 5 EFSF              14.1 0.00750 \n 6 EJD               13.8 0.271   \n 7 HAM               14.2 0.188   \n 8 HJB               14.3 0.000833\n 9 IMW               14   0.396   \n10 JK                14.2 0       \n11 JTSF              14.8 0.271   \n12 KDL               14.2 0.0625  \n13 KZC               14   0.583   \n14 RHP               14.2 0       \n15 RRP               14.2 0.188   \n16 SJA               14.5 1.65    \n\nLong_form |> summarize(vmeas = var(value))\n\n# A tibble: 1 × 1\n  vmeas\n  <dbl>\n1 0.240\n\nlm(value ~ position, data=Long_form) |> conf_interval()\n\n# A tibble: 3 × 4\n  term             .lwr .coef  .upr\n  <chr>           <dbl> <dbl> <dbl>\n1 (Intercept)     4.29  4.50  4.70 \n2 positionmiddle  0.392 0.678 0.964\n3 positionright  -0.126 0.159 0.445\n\nggplot(Long_form, aes(y=value, x=position)) + geom_jitter(width=0.2, alpha=0.5)"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#variation",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#variation",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Variation",
    "text": "Variation\nRemember that statistics focus on variation in the characteristics of a set of multiple specimens. The characteristics of each individual specimen are recorded in a row of a data frame. The data frame itself, with its multiple rows, represents the set of specimens. Each characteristic is arranged as a column in the data frame. We call such columns “variables,” a name that emphasizes that our particular interest is to understand/explain/account-for the variation of the values stored in the column.\nIn a regression model, we attempt to understand/explain/account-for the variation in a single variable, called the “response variable.” We accomplish this explanation by associating the variation in the response with the simultaneous variation in other variables called “explanatory variables.”\nThe lm() model-building function does the work of quantifying the associations. Your task in model building is to provide data for training and to specify which are the explanatory variables you want to use to account for the variation in the response variable. The specification takes the form of a tilde expression listing the response and explanatory variables. All these variables must be in the data frame used for training. We say such variables are “observed.”\nThere are usually other characteristics that are relevant to the system being studied that are not observed, that is, they are not in the data frame. It’s a bad idea to ignore such things."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#starting-lesson-20",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#starting-lesson-20",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Starting Lesson 20",
    "text": "Starting Lesson 20\nToday is a meta-day. It is about tools for learning about statistical methods and gaining insight into why certain kinds of questions/techniques come up over and over again as you work on genuine statistical problems.\nThe two kinds of tools for learning are:\n\nTools for thinking and communicating about hypotheses about causal connections.\n\nDiagrams called “DAGs” for sketching out causation.\nGenerating random, simulated data consistent with the mechanism described by a DAG.\n\nWays to automate the process of random trials. This is purely a labor-saving measure. You are not responsible to generate the code for this automation, but you should learn to read the code to be able to say what’s going on."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#causation-examples",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#causation-examples",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Causation examples",
    "text": "Causation examples\n\nSystolic blood pressure in the elderly:\n\nExperiment shows that lowering SBP reduces mortality.\nObservation shows that lower SBP is associated with increased mortality.\n\nCongressional elections\n\nAmong incumbents, higher election spending is associated with worse vote outcomes.\n\nVitamin D and disease\n\nLow vitamin linked to adverse outcomes in many diseases\nIll people go outside less often so are less exposed to sunlight AND Vitamin D is an acute phase reactant and declines with the inflammatory cytokine rise in acute and chronic diseases AND No evidence from randomized trials that vitamin D supplementation lessens mortality risks in such conditions.\nBring up article"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#directed-acyclic-graphs-dags",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#directed-acyclic-graphs-dags",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Directed acyclic graphs (DAGs)",
    "text": "Directed acyclic graphs (DAGs)\nA DAG is a format for writing down which characteristics, either observed or unobserved, are important in the operation of a system.\nA good dictionary definition of “system” is:\n\nA set of things working together as parts of a mechanism or an interconnecting network.\n\nGraphs, Directed, Acyclic"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#sampling-from-dags",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#sampling-from-dags",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Sampling from DAGs",
    "text": "Sampling from DAGs\nIn Math 300Z, DAGs have been augmented with a simulation mechanism. This consists of formulas that are invoked to create each variable in the DAG."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#activity-life-savers",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#activity-life-savers",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Activity: Life Savers",
    "text": "Activity: Life Savers"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#repeating-trials",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#repeating-trials",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Repeating trials",
    "text": "Repeating trials\n\nfoo <- do(100000)*sum(runif(10))\nggplot(foo, aes(x=\" \", y = sum)) + geom_violin(alpha=0.5)\n\n\n\n\n\nTrials <- do(100) * {lm(x ~ y, data=sample(dag03, size=5)) |> R2()}"
  },
  {
    "objectID": "Day-by-day/Lesson-22/counts-and-waiting-times.html#counts-and-waiting-times",
    "href": "Day-by-day/Lesson-22/counts-and-waiting-times.html#counts-and-waiting-times",
    "title": "Spring 2023 Math 300Z",
    "section": "Counts and waiting times",
    "text": "Counts and waiting times\nGENERATE SEQUENCES FROM POISSON and EXPONENTIAL.\nFirst, generate a sample of size 1: find the largest and smallest across the class.\nThen generate a sample of size 20:\n\nfind the largest and smallest. are they pretty consistent across the class?\n\nfind the variance. Is that pretty consistent across the class?\n\nfind the mean:\n\nis that as spread out as the largest and smallest?\nhow spread out is it? (variance)\n\n\nKeep a table\n\n\n\nsample size\nvariance\nstandard deviation\n\n\n\n\n20.\n\n\n\n\n20\n\n\n\n\n20\n\n\n\n\n40\n\n\n\n\n80\n\n\n\n\n\nMaybe add a parameter or distribution argument to sample.dag_system()"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "",
    "text": "Motivating problem: Designing an enforcement regime for limits on scallop fisheries.\n\n\n\n\n\nFigure 1: Life cycle of a scallop\n\n\n\n\nFisheries are regulated by states and the Federal government in order to avoid collapse due to over-fishing. Often, the regulations attempt to protect juveniles—animals that have not yet reached reproductive age. If the juveniles are harvested, their potential progeny are annihilated. There are various ways to do this, for instance restricting fishing to months where adults are most prevalent, closing fisheries to provide an opportunity for the reproductive stock to recover, and so on.\nIn the 1990s, one of the ways the Federal government regulated scallop fisheries was by setting a minimum acceptable size for harvested scallops. For practical reasons, rather than monitoring individual scallops, the government monitored the average per scallop weight of each boat’s catch. For the sake of the example, imagine that the minimum acceptable weight is 1/30 pound.\nA fishing boat might have 10,000 or more bags of scallops, which can be handled individually: weigh the bag, then count the number of scallops to get the average weight per scallop.\nDiscussion questions:\n\nHow many bags should be sampled? Should this depend on the number of bags in the cargo. For instance, should a cargo of 1000 bags be sampled differently than a cargo of 10,000 bags.\nWhat should be the threshold for declaring the whole cargo below minimum size? (The whole catch is confiscated in such a case.)\n\nIn this section of the course, you’ll learn some statistical concepts and methods that allow the above questions to be answered to produce a regulation that is protective and fair to the fishermen.\nOne idea is very simple: sampling variation. This is about how much the average per-scallop weight will vary from one bag to another.\nAnother idea is very subtle: What you can say about the whole cargo based on a sample of \\(n\\) bags."
  },
  {
    "objectID": "Day-by-day/Lesson-24/with-respect-to.html#with-respect-to",
    "href": "Day-by-day/Lesson-24/with-respect-to.html#with-respect-to",
    "title": "Spring 2023 Math 300Z",
    "section": "With respect to …",
    "text": "With respect to …\nBuild a model with a single quantitative explanatory variable. Maybe palmerpenguins::penguins\n- What is the effect size?\n- What are the units? Is it a rate or a difference?\nBuild a model with a single categorical explanatory variable.\n- What is the effect size?\n- What are the units? Is it a rate or a difference?\nCombine the two and calculate effect sizes again.\n- Do the effect sizes change?\n- Do the units change? \n- Is it still a rate?\nInclude many explanatory variables: Find the effect sizes with respect to each."
  },
  {
    "objectID": "Day-by-day/Lesson-24/Teaching-notes-24.html",
    "href": "Day-by-day/Lesson-24/Teaching-notes-24.html",
    "title": "Teaching notes, Lesson 24",
    "section": "",
    "text": "Our emphasis will be on the data-analysis techniques that are useful for decision-making. What this means will become evident as we move through the semester, but I’ll give two settings for decision-making:\n\nIntervention in a system. (Lesson 24) There’s a system that you are working with, perhaps aircraft design. You want to intervene in the system, to change something. You need to know how the change you impose will change the outcome.\n\nUsually, we are interested in predictions that cover most of the likely outcomes.\nSometimes, we want to predict the probabilities of extreme events, for instance thousand-year floods.\n\nPrediction. (Lessons 25 and 26)"
  },
  {
    "objectID": "Day-by-day/Lesson-23/got-you-covered.html#got-you-covered",
    "href": "Day-by-day/Lesson-23/got-you-covered.html#got-you-covered",
    "title": "Spring 2023 Math 300Z",
    "section": "Got you covered",
    "text": "Got you covered\nSimulate from a DAG and calculate the confidence interval @ 95%\nDid everybody’s interval include the parameter from the DAG?\nMove to an 80% interval. About 4 students should not cover the parameter.\nMove to a 50% interval. About 10 students should not cover the parameter.\nMove to a 100% interval. What do the results tell you?\nFrederick the Great said, “To defend everything is to defend nothing.” paraphrase as “To try to cover everything is to cover nothing.”"
  },
  {
    "objectID": "textbooks.html",
    "href": "textbooks.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "For Lessons 1-17, the textbook is Statistical Inference via Data Science (“ModernDive”)"
  },
  {
    "objectID": "textbooks.html#lessons-19-onward",
    "href": "textbooks.html#lessons-19-onward",
    "title": "Spring 2023 Math 300Z",
    "section": "Lessons 19 onward …",
    "text": "Lessons 19 onward …\n\n\nFor Lessons 19-39, the textbook is Lessons in Statistical Thinking"
  },
  {
    "objectID": "math300-software.html",
    "href": "math300-software.html",
    "title": "R package for Math 300Z",
    "section": "",
    "text": "Most students will want to use POSIT.cloud. Use the “Z-Section” project."
  },
  {
    "objectID": "math300-software.html#accessing-the-daily-worksheet",
    "href": "math300-software.html#accessing-the-daily-worksheet",
    "title": "R package for Math 300Z",
    "section": "Accessing the daily worksheet",
    "text": "Accessing the daily worksheet\nAlmost all class days there will be a worksheet in the form of an Rmd file.\nFrom within your POSIT.cloud project, download the Rmd file using this command, substituting the number of the relevant Lesson:\nmath300::get_lesson_worksheet(19)\n\nMake a habit of reading each day’s Rmd file before class. This way you can note what doesn’t yet make sense so that you can be receptive to the topic in class.\nComplete the worksheet after class."
  },
  {
    "objectID": "math300-software.html#math-300z-r-commands",
    "href": "math300-software.html#math-300z-r-commands",
    "title": "R package for Math 300Z",
    "section": "Math 300Z R commands",
    "text": "Math 300Z R commands\nThere will be only a dozen commands that you will be using in the second half of Math 300Z. Almost all of them involve constructing or summarizing models.\nAs a reminder, here are some of the commands/syntax that should be familiar to you from the first half of the course.\n\nlm() fits (or “trains”) a “linear model” on data from a data frame.\nggplot() sets things up for a new graphic. Use aes() as an argument.\ngraphics layers to add onto the output of ggplot():\n\ngeom_point(), geom_jitter(alpha=0.5)\n\nfilter(), mutate() and summarize() are basic data-wrangling commands we will use often.\n\nNew commands in the second half of the course:\n\nsummarize a model: conf_intervals(), R2()\nevaluate a model: model_eval()\ngraphic of a model: model_plot() (This replaces the geom_smooth() used in the first half of the course.)\nvariance of a variable in a data frame: DF %>% summarize(NM = var(VAR))\ndraw a DAG: dag_draw()\nsample from a DAG: sample(DAG, size=100)\n\nA few other commands will be used occasionally in examples and demonstrations. You should know what they do, but typically there will be a reminder of the syntax: zero_one(), shuffle(), do() * {}, dag_intervene(), tibble(), regression_summary(), anova_summary()."
  },
  {
    "objectID": "math300-software.html#running-rrstudio-on-your-laptop",
    "href": "math300-software.html#running-rrstudio-on-your-laptop",
    "title": "R package for Math 300Z",
    "section": "Running R/RStudio on your laptop?",
    "text": "Running R/RStudio on your laptop?\nInstall the {math300} and other packages with these two commands:\ninstall.packages(c(\"mosaic\", \"ggplot\", \"dplyr\", \"openintro\", \"moderndive\", \"nycflights13\", \"knitr\"))\n# additional package for Math 300Z\nremotes::install_github(\"dtkaplan/math300\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300Z",
    "section": "",
    "text": "Take-aways from the most recent class\n\n\n\nOn Math 300 Blog\nMath 300Z is the prototype for scheduled revisions to Math 300. The revisions apply only to Lessons 19 and up; the first 18 lessons come from Math 300."
  },
  {
    "objectID": "index.html#day-by-day-plan",
    "href": "index.html#day-by-day-plan",
    "title": "Math 300Z",
    "section": "Day-by-day plan",
    "text": "Day-by-day plan\nMar 3 Lesson 19: Topic: Variation Reading, Activity: Measuring by eye, Worksheet Solns, take-aways\nMar 7 Lesson 20: Topic: DAGs and simulation Reading, Activity: Life savers?, Worksheet Solns, take-aways\nMar 9 Lesson 21: Topic: Signal and noise Reading, Activity: Membrane channels Handout, Worksheet Solns, take-aways\nMar 13 Lesson 22: Topic: Sampling variation Reading, Activity: Counts and waiting times, Worksheet Solns, take-aways\nMar 15 Lesson 23: Topic: Confidence intervals Reading, Activity: Got you covered!, Worksheet Solns, take-aways\nMar 17 Lesson 24: Topic: Effect size Reading, Activity: With respect to …, Worksheet Solns, take-aways\nMar 21 Lesson 25: Topic: Prediction mechanics Reading, Activity: To be determined, Worksheet Solns, take-aways\nMar 23 Lesson 26: Topic: Prediction intervals Reading, Activity: Intervals by eye, Handout, Worksheet Solns, take-aways\nApr 3 Lesson 27: REVIEW of lessons 19-26 :: Reading\nApr 5 Lesson 28: Topic: Covariates :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 7 Lesson 29: Topic: Covariates eat variance :: Reading, Activity: TBA. Homework assignment due. (link to come)\nApr 11 Lesson 30: Topic: Confounding :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 13 Lesson 31: Topic: Spurious correlation :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 17 Lesson 32: Topic: Experiment & random assignment :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 19 Lesson 33: Topic: Measuring and accumulating risk :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 24 Lesson 34: Topic: Constructing a classifier :: Reading, Activity: TBA, Worksheet Solns, take-aways\nApr 27 Lesson 35: Topic: Accounting for prevalence :: Reading, Activity: TBA, Worksheet Solns, take-aways\nMay 1 Lesson 36: Topic: Hypothesis testing :: Reading, Activity: TBA, Worksheet Solns, take-aways\nMay 3 Lesson 37: Topic: Calculating a p-value :: Reading, Activity: TBA, Worksheet Solns, take-aways\nMay 5 Lesson 38: Topic: False discovery with hypothesis testing :: Reading, Activity: TBA, Worksheet Solns, take-aways\nMay 9 Lesson 39: REVIEW of lessons 28-38 :: Reading\nMay 11 Lesson 40: Review of entire course"
  },
  {
    "objectID": "Day-by-day/Lesson-34/Teaching-notes-34.html#in-draft",
    "href": "Day-by-day/Lesson-34/Teaching-notes-34.html#in-draft",
    "title": "Instructor Teaching Notes for Lesson 34",
    "section": "IN DRAFT",
    "text": "IN DRAFT\nTell the story of the Chinese spy balloon. After it was detected, the Air Force (according to news reports) increased the sensitivity of radars. This led to an increase in detection and a week-long rash of high-altitude detections, two of which were shot down. Eventually it was realized that there is a surprising amount of stuff floating around at high altitude. https://www.nytimes.com/live/2023/02/16/us/biden-china-balloon-ufo?smid=nytcore-ios-share&referringSource=articleShare"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "",
    "text": "You have been learning some basics of data wrangling and visualization, along with what ModernDive calls “basic regression” and “multiple regression.” These are tools which you will continue to use in the second half of the semester.\nSuch tools are necessary but usually not sufficient. Data only occasionally speak for themselves. Most often, we need to interpret data in the context of what we already know or believe about the system under study.\nExample: As you’ve seen, merely fitting a regression model does not demonstrate that there is a causal relationship between the explanatory variables and the response variable. We will need some new concepts to encode our ideas (and speculations) about causal relationships and to use regression modeling to inform (or contradict) our ideas.\nExample: We’ll see how to avoid seeing patterns and relationships for which the evidence in unpersuasive. Example: We will see how detection thresholds can be set to reflect our opinions about the costs and benefits of different ways of getting it right or wrong and our prior knowledge of the frequency of different kinds of events."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#regression-models",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#regression-models",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Regression models",
    "text": "Regression models\nThe point of regression modeling is to detect and quantify patterns of relationship between variables. Sometimes simple data graphics are enough to display a pattern. For instance, let’s look at some Department of Transportation data on models of cars stored in the MPG data frame. We will start by looking at the link between fuel economy and CO_2_ production.\n\nggplot(MPG, aes(x = fuel_year, y = CO2_year)) +\n  geom_jitter(alpha=.3)\n\n\n\n\nThis is a very strong pattern. fuel_year and CO2_year are practically the same thing.\n\nWhy?\nWhy are there some points off of the straight line describing the large majority of points?\n\nOther times, patterns are hidden by extreme variability in the data. For instance, here are data on the effect of kindergarden class size on student outcomes.\n\nggplot(STAR, aes(x=classtype, y=g4math)) + geom_jitter() + geom_violin(alpha=.5, fill=\"blue\")\n\n\n\n\n\n\n\n\nRegression modeling is a technique for looking for simple forms of patterns in the relationships among variables. It is not the only such technique, but it is by far the most widely used in practice across diverse fields.\nWe will use only regression modeling (and allied methods) in this course. You may have heard about other methods such as “deep learning” or “neural networks,” but regression modeling is the basis for most of the others.\nIt’s critically important that you understand the framework for regression modeling.\n\nIn any one model, there is one variable that is identified as the response variable.\n\nThis identification depends on the purpose behind your work. You’ll learn it mostly by example.\n\nOther variables in the model are cast in the role of explanatory variables. There might be one explanatory variable or there might be other. There’s even a use for the case where there are no explanatory variables, but we don’t need to worry about that now.\nIn fitting a model to data (sometimes called “training a model on data”) the computer does the heavy lifting of finding a relationship between the response and explanatory variables that stays close to the data. Often, the “shape” of the relationship is very simple, e.g. a straight-line relationship or, more generally, a linear combination. That’s where the l comes in the function lm() that you will be using again and again in this course.\nIn using lm(), you specify which variables you want in the explanatory role and which single variable you have selected to be the response variable. The computer language syntax is very simple:\n\nresponse ~ var1 + var2 + ...\nThe name of the response variable is always on the left-hand side of the TILDE.\nThe explanatory variables are listed by name on the right side of the TILDE.\nThe + between the names of explanatory variables is mostly just punctuation. You can read it as “and”.\n\nThe TILDE character is usually just pronounced “tilde,” but English-language equivalents are\n\n“as explained by”\n“as accounted for by”\n“as modeled by”\n“versus”"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#data-graphics",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#data-graphics",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Data graphics",
    "text": "Data graphics\nSince the distinction between the response and the explanatory variables is so central, we are going to enforce a graphical style that reflects the distinction.\n\nThe response variable will always be on the vertical axis.\nOne of the explanatory variables will be on the horizontal axis.\nIf there is a second explanatory variable, we will use color.\nWhen we need a third or fourth explanatory variable, we will use faceting.\n\nIn the ggplot2 graphics system, this policy will appear like this:\nggplot(Dataframe, aes(y=response, x=var1, color=var2)) + geom_jitter() or geom_point() and so on."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#models",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#models",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Models",
    "text": "Models\nWe have been working a lot with data frames. Now we are going to add a new type of R object, which we can call a “model”. A model is NOT a data frame, it is a different kind of thing with different properties and different operations.\nMaking a model:\n\nmod1 <- lm(sat~ expend, data=SAT)\n\nOperations we will perform on models:\n\nGraph the model (and usually the data used for training)\n::: {.cell}\nmodel_plot(mod1)\n::: {.cell-output-display}  ::: :::\n\nBeing able to use multiple explanatory variables allows us to see patterns that may be subtle.\n\nmod2 <- lm(sat ~ expend + frac, data=SAT)\nmodel_plot(mod2)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\n\n\n\n\nModel summaries, especially conf_interval() and R2()\n\n\nmod2 |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef    .upr\n  <chr>        <dbl>  <dbl>   <dbl>\n1 (Intercept) 950.   994.   1038.  \n2 expend        3.79  12.3    20.8 \n3 frac         -3.28  -2.85   -2.42\n\nmod2 |> R2()\n\n   n k  Rsquared        F     adjR2 p df.num df.denom\n1 50 2 0.8194726 106.6741 0.8117906 0      2       47\n\n\n\nEvaluate the model at each of the rows of the training data.\n::: {.cell}\nmodel_eval(mod2) |> head()\n::: {.cell-output .cell-output-stderr} Using training data as input to model_eval(). :::\n::: {.cell-output .cell-output-stdout} .response expend frac   .output     .resid     .lwr      .upr  1      1029  4.405    8 1025.1463   3.853661 958.2677 1092.0250  2       934  8.963   47  969.9621 -35.962052 900.0065 1039.9176  3       944  4.778   27  975.5616 -31.561556 909.1283 1041.9948  4      1005  4.459    6 1031.5117 -26.511670 964.6071 1098.4162  5       902  4.992   45  926.8741 -24.874145 860.0437  993.7046  6       980  5.443   29  978.0302   1.969768 912.0035 1044.0570 ::: :::"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#todays-lesson",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#todays-lesson",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Today’s Lesson",
    "text": "Today’s Lesson\nA definition of “statistical thinking” from the book:\n\nStatistic thinking is the accounting for variation in the context of what remains unaccounted for.\n\nImplicit in this definition is a pathway for learning to think statistically:\n\nLearn how to measure variation;\nLearn how to account for variation;\nLearn how to measure what remains unaccounted for.\n\nToday: How to measure variation.\nConsider some closely related words: variable, variation, vary, various, variety, variant. The root is vari.\nOur preferred way to measure the amount of variation numerically: the variance, a single number, always positive.\n\nVariance always involves a single variable; it is about the variation in that variable.\nCalculate the variance is with var() within summarize() r DF |> summarize(NM = var(VAR))\nA good way to conceptualize the variance is as the average squared pairwise difference between values\nThe units of the variance are the square of the units of the variable.\nWhy square the pairwise differences? It’s a convention. Experience has shown this convention simplifies many operations we do with models. Underlying mathematics: Pythagorean theorem and formula for bell-shaped curve.\nOften people talk about the “standard deviation.” This is merely the square-root of the variance. But the variance is more fundamental mathematically. Using standard deviations introduces square roots in many calculations that don’t need to be there if we use variance."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#activity",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#activity",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Activity",
    "text": "Activity"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#administration",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#administration",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Administration",
    "text": "Administration\n\nUse “300Z Section” under Teams.\nClone the Z-section project on posit.cloud. We’ll use this project for the rest of the semester.\nThere will be a “worksheet” almost every day.\n\nThe worksheet is in the form of an Rmd file. To access it, go into the Z-section project and give a command like this: get_lesson_worksheet(19) or whatever the lesson number is.\nAn effective way to prepare for a class is to look at the worksheet before class. Just read it and take note of what doesn’t make sense to you. That way you can be attentive to those things in class.\nComplete the worksheet after class.\nCome with unresolved questions about the worksheet for the next class or for EI.\n\nMost days there will also be a group activity.\nThere will be a couple of problem sets that will be graded.\nThere will be one GR about half-way through the rest of the semester. And a final GR."
  },
  {
    "objectID": "Worksheet-20.html",
    "href": "Worksheet-20.html",
    "title": "Lesson 20: Worksheet",
    "section": "",
    "text": "20.1 [Technical] Collect a sample from a DAG simulation.\n20.2 [Technical] Examine the formulas behind a DAG simulation and compare to the results of a regression model trained on a sample from the DAG simulation.\n20.3 [Conceptual] Recognize properties of a DAG. i. Identify exogenous nodes. ii. Identify all pathways between two specified end nodes. iii. On a given pathway, is there causal flow from one end node to another? iv. On a given pathway, is there a causal flow from some node on the pathway to both end nodes?"
  },
  {
    "objectID": "Worksheet-20.html#part-1-samples-from-dags",
    "href": "Worksheet-20.html#part-1-samples-from-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 1: Samples from DAGs",
    "text": "Part 1: Samples from DAGs\n\nUse dag_draw() to draw a picture of the dag08 directed acyclic graph. From this graph, explain why node c is exogenous and why x and y are not.\n\nANSWER:\n\nUse print() to view the formulas used by dag08 to simulate data. What about the formula for y indicates that it’s receives inputs from x and c.\n\nANSWER:\n\nThere are three coefficients in the formula for y: an intercept, an x coefficient, and a c coefficient. (There is also some random input from an exogenous source unrelated to c or x.) What are the numerical values of the three coefficients?\n\nANSWER:\n\nCollect a sample of size \\(n=100\\) from dag08 and use it to train the model with specification y ~ x. Do the coefficients reported match those you found in part (c)? (If you are not sure, use a bigger sample size, say \\(n=1000\\) or even bigger.)\n\nANSWER:\nThe model says the x coefficient is about 1.5, not the same as in the DAG formula for y.\n\nSimilar to (4), but use the specification y ~ x + c. How do the coefficients for this model compare to those you found in (3)?\n\nANSWER:"
  },
  {
    "objectID": "Worksheet-20.html#part-2-paths-in-dags",
    "href": "Worksheet-20.html#part-2-paths-in-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 2: Paths in DAGs",
    "text": "Part 2: Paths in DAGs\n\nIn dag08 there are two paths connecting x andy. One path is direct, \\(X \\longrightarrow Y\\). The other path is indirect, \\(X \\longleftarrow C \\longrightarrow Y\\).\n\nAlong the indirect path, is there a causal flow from x to y?\nAlong the indirect path, is there a causal flow from any node on the graph that reaches both endpoints, x and y?\n\n\nANSWER:\n\ndag_school2 is a highly simplistic model of the relationship between expenditures on schools and student outcomes in terms of, say, standardized test scores.\n\n\ndag_draw(dag_school2, vertex.label.cex=1, vertex.size=40)\n\n\n\n\nThere is a direct pathway from expenditure to outcome as well as another, indirect pathway.\n\nAre there any exogenous nodes in the graph?\nOn the indirect pathway, is there a causal flow from expenditure to outcome?\nIs there a causal flow from any node on the indirect pathway to both expenditure and outcome? Which one?\n\nANSWER:"
  },
  {
    "objectID": "Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "href": "Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "title": "Lesson 20: Worksheet",
    "section": "Part 3: Are expenditures good for school outcomes?",
    "text": "Part 3: Are expenditures good for school outcomes?\n\nLook at the formulas for dag_school2. Is a higher expenditure connected to a higher outcome?\n\nANSWER:\n\nGenerate a simple of size 1000 from dag_school2 and use it to train the model outcome ~ expenditure. Is the coefficient on expenditure consistent with what you found in (1)? (If you aren’t sure, use a larger sample size, say 10,000.) What about the coefficient on expenditure leads to your conclusion?\n\nANSWER:\n\nSpeculate on what might be the origin of the evident inconsistency between (1) and (2)?\n\nANSWER:"
  },
  {
    "objectID": "Worksheet-20.html#part-4-constructing-a-dag",
    "href": "Worksheet-20.html#part-4-constructing-a-dag",
    "title": "Lesson 20: Worksheet",
    "section": "Part 4: Constructing a DAG",
    "text": "Part 4: Constructing a DAG\nIn this task, you will construct DAGs using dag_make() and draw them using dag_draw().\nA DAG is defined by a series of tilde expressions, one for each node in the graph. The tilde expression for a node has the node’s name on the left-hand side of the tilde. The right-hand side contains the nodes which serve as inputs to the node named on the left-hand side. If there are no inputs, write exo().\nFor example, consider a DAG with three nodes: one, two, and three. To define a DAG where node two receives input from node one, and node three receives input from nodes one and two, use make_dag() with three tilde expressions:\n\nexample_dag <- dag_make(\n  one ~ exo(),\n  two ~ one,\n  three ~ two + one\n)\ndag_draw(example_dag)\n\n\n\n\nThe right-hand side of a formula can be any arithmetic expression involving the node names, but we will keep it simple: just use + to separated the node names. If a node receives no inputs, the right-hand side should be simply exo() to mark that node as exogenous.\n\nWhat happens if node one, instead of being exogenous, takes as input one of the other two nodes in example_dag?\n\nANSWER:\n\nCreate and draw a DAG that has the same arrangement of causal connections as “Professor Butts and the Self-Operating Napkin,” illustrated below:\n\n\nProfessor Butts and the Self-Operating Napkin (1931). Soup_spoon (A) is raised to mouth, pulling string (B) and thereby jerking ladle (C), which throws cracker (D) past toucan (E). Toucan jumps after cracker and perch (F) tilts, upsetting seeds (G) into pail (H). Extra weight in pail pulls cord (I), which opens and ignites lighter (J), setting off skyrocket (K), which causes sickle (L) to cut string_m (M), allowing pendulum with attached napkin to swing back and forth, thereby wiping_chin.\nWatch your spelling of node names! Use this command to draw your napkin_dag:\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)\nANSWER:"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html",
    "href": "Worksheets/Worksheet-19.html",
    "title": "Lesson 19: Worksheet",
    "section": "",
    "text": "19.1. [Conceptual] Master the use and units of variance and standard deviation in measuring variability.\n19.2. [Conceptual] Understand the equivalence between mean and proportion on a zero-one variable.\n19.3. [Technical] Use var() and sd() within summarize()\n19.4. [Technical] Use model_plot() to graph models with one or two explanatory variables.\n19.5. [Technical] Use zero_one() with mutate() to create a zero-one variable."
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#preliminaries-how-we-will-work-with-r.",
    "href": "Worksheets/Worksheet-19.html#preliminaries-how-we-will-work-with-r.",
    "title": "Lesson 19: Worksheet",
    "section": "Preliminaries: How we will work with R.",
    "text": "Preliminaries: How we will work with R.\nIn the first half of Math 300Z, the daily student notes were largely structured around “scaffolded” R code, which often involved filling in the blanks. In this second half of 300Z, we will start to use a new way of helping you construct appropriate R command. We call this “command patterns. For instance,\nDF %>% summarize(NM=var(VAR)) \nis a command pattern.\nOne reason for the shift to the command-pattern style is that there will be only a handful of new patterns in the second half of the course that you’ll be using over and over again. Another reason is to help you develop “finger memory” for the most common patterns. An analogy: scaffolding is like GPS navigation which certainly makes it easier to drive but harder to get to know the town. Command patterns are like a paper map, there to help you when you need it.\nThere is a specific notation for command patterns, which you should memorize. Instead of the blanks used in a scaffold, the command pattern uses a CAPITALIZED abbreviation for the **kind of thing* that should be put in the position. Common kinds of thing are\n\nDF: a data frame, almost always referred to by name.\nVAR: a variable in a data frame. Many command patterns involve multiple variables, each of which is referred to by VAR. You will replace each VAR with the appropriate variable name.\nVARS: one or more variable names. When these are the right-hand side of a tilde expression, separate the names with + punctuation. When we mean to indicate that there is only one variable, we use VAR instead of VARS. If we want to say, “use two variables,” we would write VAR + VAR.\nMODEL refers to the name of a model that you have previously constructed with lm().\nNM means a name that you will be calling something by. For instance, NM <- lm(VAR ~ VARS, data=DF). Another occasion for using NM is as part of an argument to summarize() or mutate().\n[, MORE] means that you can have multiple additional arguments of the same form as the previous argument.\nVALUE a number, quoted string (e.g., \"red\"), or multiple values inside c( ).\nMODSPEC is a model specification, which could equally well be written VAR ~ VARS\n\nAnything in a command pattern that is not a CAPITALIZED abbreviation is a specific part of the command to be used as-is. For instance, lm(VAR ~ VARS, data=DF) refers explicitly to the lm() function whose first argument is a tilde expression and whose second argument is named data.\nOccasionally, you will refer to a data frame by naming the package from which it comes. For example, the moderndive package includes (among many others) the amazon_books data frame. Think of amazon_books as a first name, and moderndive as a family name. When you see PACKAGE::DF it is meant to indicate, for instance, moderndive::amazon_books. (Note that the :: in the command pattern is to be taken literally; there are two successive colons separating the package name from the name of the data frame.)"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#part-1",
    "href": "Worksheets/Worksheet-19.html#part-1",
    "title": "Lesson 19: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nCommand patterns:\n\nDF %>% summarize(NM = var(VAR)) Calculate variance of a variable in a data frame.\n`DF %>% summarize(NM1 = var(VAR1), NM2 = var(VAR2) [, MORE])\nPACKAGE::DF The name of a data frame within a package.\n\n\nIn the mosaicData::Galton data frame, find the variance of mother and father. Give both the numerical value and the units.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmosaicData::Galton |> \n  summarize(vmother = var(mother), vfather = var(father))\n\n   vmother  vfather\n1 5.322365 6.102164\n\n\nThe units for both are `inches-squared” since the variables themselves have units “inches.”\n\n\n\nIn the moderndive::amazon_books data frame, find the variance of list_price and num_pages. Give both the numerical value and the units.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  summarize(vprice = var(list_price), vpages = var(num_pages))\n\n# A tibble: 1 × 2\n  vprice vpages\n   <dbl>  <dbl>\n1     NA     NA\n\n\nThe units of list_price are dollars, so the variance has units “square-dollars”.\nnum_pages is dimensionless, so the variance is also dimensionless.\n\n\n\nCalculate the variance of sex from Galton. If something goes wrong, explain why.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nGalton |> summarize(vsex = var(sex))\n\nError in `summarize()`:\n! Problem while computing `vsex = var(sex)`.\nCaused by error in `stats::var()`:\n! Calling var(x) on a factor x is defunct.\n  Use something like 'all(duplicated(x)[-1L])' to test for a constant vector.\n\n\nsex is a categorical variable. There’s no such thing as the variance of a categorical variable."
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#part-2",
    "href": "Worksheets/Worksheet-19.html#part-2",
    "title": "Lesson 19: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nCommand patterns:\n\nNM <- lm(VAR ~ VARS, data = DF)\nlm(VAR ~ VARS, data=DF) %>% conf_interval()\nlm(MODSPEC, data=DF) %>% conf_interval() means the same as (b).\n\n\n(Easy, no computing needed.) What kind of a thing is conf_interval(). (Hint: It’s the same kind of thing as lm().)\n\n\n\n\n\n\n\nANSWER:\n\n\n\nconf_interval() is a function.\n\n\n\nUsing the moderndive::amazonbooks data frame, fit the model list_price ~ num_pages:\n\nWhat are the units of the “(Intercept)” coefficient?\nReport the coefficient on num_pages. Give both the numerical bounds and the units.\n\n\n\n\n\n\n\n\nANSWER:\n\n\n\nThe intercept always has the same units as the response variable. Here, that’s dollars.\n\nlm(list_price ~ num_pages, data = moderndive::amazon_books) |>\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept) 8.33   11.8    15.4   \n2 num_pages   0.0105  0.0199  0.0293\n\n\nCoefficient on num_pages: .02 dollars per page. Multiplying the coefficient by the number of pages will give dollars: the units of the response variable.\n\n\n\nSimilar to (2) but with the model list_price ~ numpages + hard_paper\n\nWhat does the term hard_paperH refer to?\nAccording to the coefficients, is a hardcover book any more expensive (on average) than a softcover book?\n\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nlm(list_price ~ num_pages + hard_paper, data = moderndive::amazon_books) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept) 7.04   10.6    14.2   \n2 num_pages   0.0102  0.0196  0.0289\n3 hard_paperH 1.56    4.96    8.36  \n\n\nhard_paperH refers to the H level of the hard_paper variable. According to the model, a hardback costs $4.96 more than a paperback, on average.\n\n\n\nStore the model you created in (3) under the name mod3. We’ll use it in the next part. For your answer, put the R command you used to store the model as mod3.\n\n\n\n\n\n\n\nANSWER:\n\n\n\nNote that we are asked to store the model itself, not the confidence interval.\n\nmod3 <- lm(list_price ~ num_pages + hard_paper, data = moderndive::amazon_books)"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#graphics-review",
    "href": "Worksheets/Worksheet-19.html#graphics-review",
    "title": "Lesson 19: Worksheet",
    "section": "Graphics review",
    "text": "Graphics review\nCommand patterns:\n\nggplot(DF, aes(x=VAR, y=VAR)) + geom_jitter()\nggplot(DF, aes(x=VAR, y=VAR)) + geom_jitter() + geom_violin(fill=\"blue\", alpha=0.3)\nggplot(DF, aes(x=\"all\", y=VAR)) + geom_jitter()\nmodel_plot(MODEL, x=VAR)\nmodel_plot(MODAL, x=VAR, color=VAR)\n\n\nMake a jitter plot of list_price ~ hard_paper from moderndive::amazon_books.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  ggplot(aes(x=hard_paper, y = list_price)) + \n  geom_jitter(alpha=0.5)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nUsing your command from (1), add a new layer: + geom_violin(fill=\"blue\", alpha=0.3)\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  ggplot(aes(x=hard_paper, y = list_price)) + \n  geom_jitter(alpha=0.5) +\n  geom_violin(fill=\"blue\", alpha=0.3)\n\nWarning: Removed 1 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nUse model_plot() to draw a picture of mod3. Set x=hard_paper and num_pages=200. What do you think the horizontal line segments refer to?\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmodel_plot(mod3, x=hard_paper, num_pages=200)\n\n\n\n\nThe vertical position of the horizontal lines indicates the model output for books with 200 pages.\n\n\n\nRepeat (3), but remove num_pages = 200. Instead, set x=num_pages and color=hard_paper. Explain the meaning of the line segments in everyday terms.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmodel_plot(mod3, x=hard_paper, color=num_pages)\n\n\n\n\nThe parallel line segments in each column show the model output for books with 200, 400, and 600 pages respectively."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html",
    "href": "Worksheets/Worksheet-20.html",
    "title": "Lesson 20: Worksheet",
    "section": "",
    "text": "20.1 [Technical] Collect a sample from a DAG simulation.\n20.2 [Technical] Examine the formulas behind a DAG simulation and compare to the results of a regression model trained on a sample from the DAG simulation.\n20.3 [Conceptual] Recognize properties of a DAG. i. Identify exogenous nodes. ii. Identify all pathways between two specified end nodes. iii. On a given pathway, is there causal flow from one end node to another? iv. On a given pathway, is there a causal flow from some node on the pathway to both end nodes?"
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-1-samples-from-dags",
    "href": "Worksheets/Worksheet-20.html#part-1-samples-from-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 1: Samples from DAGs",
    "text": "Part 1: Samples from DAGs\n\nUse dag_draw() to draw a picture of the dag08 directed acyclic graph. From this graph, explain why node c is exogenous and why x and y are not.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\ndag_draw(dag08)\n\n\n\n\nNode C is exogenous because it has no incoming arrows.\n\n\n\nUse print() to view the formulas used by dag08 to simulate data. What about the formula for y indicates that it’s receives inputs from x and c.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nprint(dag08)\n\nc ~ exo()\nx ~ c + exo()\ny ~ x + c + 3 + exo()\n\n\nThe right-hand side of the formula for y says that y will be calculated as the sum of x and c (plus 3 plus some random noise). That is, x and c directly shape the value of y.\n\n\n\nThere are three coefficients in the formula for y: an intercept, an x coefficient, and a c coefficient. (There is also some random input from an exogenous source unrelated to c or x.) What are the numerical values of the three coefficients?\n\n\n\n\n\n\n\nANSWER\n\n\n\nFrom the formula for y in the dag, the coefficients are 1 for x, 1 for c, and 3 for the intercept.\n\n\n\nCollect a sample of size \\(n=100\\) from dag08 and use it to train the model with specification y ~ x. Do the coefficients reported match those you found in part (c)? (If you are not sure, use a bigger sample size, say \\(n=1000\\) or even bigger.)\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag08, size=1000)\nlm(y ~ x, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept)  2.83  2.90  2.98\n2 x            1.36  1.41  1.47\n\n\n\n\nThe model says the x coefficient is about 1.5, not the same as in the DAG formula for y.\n\nSimilar to (4), but use the specification y ~ x + c. How do the coefficients for this model compare to those you found in (3)?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag08, size=1000)\nlm(y ~ x + c, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept) 2.90  2.97   3.03\n2 x           0.919 0.983  1.05\n3 c           0.901 0.989  1.08\n\n\nWhen we include both x and c in the model specification, the coefficients work out to match those of the DAG formula for y."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-2-paths-in-dags",
    "href": "Worksheets/Worksheet-20.html#part-2-paths-in-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 2: Paths in DAGs",
    "text": "Part 2: Paths in DAGs\n\nIn dag08 there are two paths connecting x andy. One path is direct, \\(X \\longrightarrow Y\\). The other path is indirect, \\(X \\longleftarrow C \\longrightarrow Y\\).\n\nAlong the indirect path, is there a causal flow from x to y?\nAlong the indirect path, is there a causal flow from any node on the graph that reaches both endpoints, x and y?\n\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nYes, the flow runs directly from x to y.\nYes. The flow runs from c to each of x and y.\n\n\n\n\ndag_school2 is a highly simplistic model of the relationship between expenditures on schools and student outcomes in terms of, say, standardized test scores.\n\n\ndag_draw(dag_school2, vertex.label.cex=1, vertex.size=40)\n\n\n\n\nThere is a direct pathway from expenditure to outcome as well as another, indirect pathway.\n\nAre there any exogenous nodes in the graph?\nOn the indirect pathway, is there a causal flow from expenditure to outcome?\nIs there a causal flow from any node on the indirect pathway to both expenditure and outcome? Which one?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nCulture is an exogenous node; there are no incoming arrows to culture.\nYes.\nFrom Culture there is a flow to outcome through expenditure. There is also a flow from Culture to Outcome via Participation."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "href": "Worksheets/Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "title": "Lesson 20: Worksheet",
    "section": "Part 3: Are expenditures good for school outcomes?",
    "text": "Part 3: Are expenditures good for school outcomes?\n\nLook at the formulas for dag_school2. Is a higher expenditure connected to a higher outcome?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nprint(dag_school2)\n\nculture ~ unif(-1, 1)\nexpenditure ~ 12000 + 4000 * culture + exo(1000)\nparticipation ~ (50 + 30 * culture + exo(15)) %>% pmax(0) %>% \n    pmin(100)\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + exo(50)\n\n\nThe formula for Outcome has a positive coefficient (0.01) on expenditure. So when expenditure goes up, so will outcome. The magnitude of the coefficient is neither here nor there. Remember that there are always units associated with a coefficient. It’s impossible to say whether a magnitude is large or small unless you know the units.\n\n\n\nGenerate a simple of size 1000 from dag_school2 and use it to train the model outcome ~ expenditure. Is the coefficient on expenditure consistent with what you found in (1)? (If you aren’t sure, use a larger sample size, say 10,000.) What about the coefficient on expenditure leads to your conclusion?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag_school2, size=1000)\nlm(outcome ~ expenditure, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term             .lwr     .coef      .upr\n  <chr>           <dbl>     <dbl>     <dbl>\n1 (Intercept) 1191.     1215.     1239.    \n2 expenditure   -0.0179   -0.0160   -0.0140\n\n\nThe coefficient on Expenditure is negative in contrast to the known positive coefficient in the DAG formula for Outcome.\n\n\n\nSpeculate on what might be the origin of the evident inconsistency between (1) and (2)?\n\n\n\n\n\n\n\nANSWER\n\n\n\nIn the DAG, Outcome is influenced negatively by Participation. And Expenditure is influenced positively by Participation. The two effects of Participation combine to produce an overall negative link between Expenditure and Outcome. By overall, we mean the combination of the direct Expenditure to Outcome link and the indirect path from Expenditure to Outcome via Participation."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-4-constructing-a-dag",
    "href": "Worksheets/Worksheet-20.html#part-4-constructing-a-dag",
    "title": "Lesson 20: Worksheet",
    "section": "Part 4: Constructing a DAG",
    "text": "Part 4: Constructing a DAG\nIn this task, you will construct DAGs using dag_make() and draw them using dag_draw().\nA DAG is defined by a series of tilde expressions, one for each node in the graph. The tilde expression for a node has the node’s name on the left-hand side of the tilde. The right-hand side contains the nodes which serve as inputs to the node named on the left-hand side. If there are no inputs, write exo().\nFor example, consider a DAG with three nodes: one, two, and three. To define a DAG where node two receives input from node one, and node three receives input from nodes one and two, use make_dag() with three tilde expressions:\n\nexample_dag <- dag_make(\n  one ~ exo(),\n  two ~ one,\n  three ~ two + one\n)\ndag_draw(example_dag)\n\n\n\n\nThe right-hand side of a formula can be any arithmetic expression involving the node names, but we will keep it simple: just use + to separated the node names. If a node receives no inputs, the right-hand side should be simply exo() to mark that node as exogenous.\n\nWhat happens if node one, instead of being exogenous, takes as input one of the other two nodes in example_dag?\n\n\n\n\n\n\n\nANSWER\n\n\n\nThe graph would become cyclic, hence not a DAG. Notice that by using a node on the right-hand side of a tilde expression only when it has already been created by a previous tilde expression, you guarantee that the graph will be acyclic.\n\n\n\nCreate and draw a DAG that has the same arrangement of causal connections as “Professor Butts and the Self-Operating Napkin,” illustrated below:\n\n\nProfessor Butts and the Self-Operating Napkin (1931). Soup_spoon (A) is raised to mouth, pulling string (B) and thereby jerking ladle (C), which throws cracker (D) past toucan (E). Toucan jumps after cracker and perch (F) tilts, upsetting seeds (G) into pail (H). Extra weight in pail pulls cord (I), which opens and ignites lighter (J), setting off skyrocket (K), which causes sickle (L) to cut string_m (M), allowing pendulum with attached napkin to swing back and forth, thereby wiping_chin.\nWatch your spelling of node names! Use this command to draw your napkin_dag:\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)\n\n\n\n\n\n\nANSWER\n\n\n\n\nnapkin_dag <- dag_make(\n  soup_spoon ~ exo(),\n  string ~ soup_spoon,\n  ladle ~ string,\n  cracker ~ ladle,\n  toucan ~ cracker,\n  perch ~ toucan,\n  seeds ~ perch,\n  pail ~ seeds,\n  cord ~ pail,\n  lighter ~ cord,\n  skyrocket ~ lighter,\n  sickle ~ skyrocket,\n  string_m ~ sickle,\n  wiping_chin ~ string_m\n)\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html",
    "href": "Worksheets/Worksheet-21.html",
    "title": "Lesson 21: Worksheet",
    "section": "",
    "text": "Command patterns:"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#basic-regression-patterns",
    "href": "Worksheets/Worksheet-21.html#basic-regression-patterns",
    "title": "Lesson 21: Worksheet",
    "section": "Basic regression patterns",
    "text": "Basic regression patterns\nEvery regression model involves a response variable, which Lessons in Statistical Thinking always plots on the vertical axis. Most of the regression models we will consider in these Lessons have one or two explanatory variables, although sometimes there will be more than two and sometimes none at all.\nIt is worth memorizing the forms of the tilde-expression specifications of the zero-, one-, and two-explanatory models, as well as their shapes. For this purpose, we’ll write the forms using five generic variable names. In practice, you will replace these generic names with specific names from the data frame of interest.\n\ny — a quantitative response variable (which might be the result of a zero-one transformation).\nx and z — quantitative explanatory variables\ng and h — categorical explanatory variables.\n\n\n\n\n\n\n\n\nModel specification\nShape\n\n\n\n\ny ~ 1\nA line with slope zero.\n\n\ny ~ x\nA line with possibly non-zero slope.\n\n\ny ~ g\nA value for each level of g.\n\n\ny ~ x + g\nSeparate lines for each level of g, all with the same slope.\n\n\ny ~ x + z\nParallel, evenly spaced lines.\n\n\ny ~ g + h\nFor each level of g, a set of spaced values, one for each level of h. The h-spacing will be the same for every level of g.\n\n\n\nNote: It doesn’t matter what order the explanatory variables are given in. The name of the response variable is always on the left-hand side of the tilde expression."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-1",
    "href": "Worksheets/Worksheet-21.html#part-1",
    "title": "Lesson 21: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nDo Exercise 21.4."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-2",
    "href": "Worksheets/Worksheet-21.html#part-2",
    "title": "Lesson 21: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nBy fitting a regression model, we divide the response variable into two components: a signal component and a noise component. The model specification tells what sort of signal to look for. For instance, the Clock_auction data frame records the sales price of antique grandfather clocks sold at auction. Presumably, the price reflects some feature of the clock itself as well as the market conditions. We have only the variables age and bidders to represent the the value of the clock and the market conditions.\nUsing lm() with the specification price ~ age directs the computer to look for a signal in the form of a straight-line relationship between age and price. The estimated noise is the difference between the response variable values (price) and the signal.\n\nHow much clock-to-clock variation is there in price? (Use the variance to measure variation.)\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nClock_auction |> summarize(vprice = var(price), sd=sqrt(vprice))\n\n# A tibble: 1 × 2\n   vprice    sd\n    <dbl> <dbl>\n1 134203.  366.\n\n\nSince the price is in dollars, the variance of price has units of “square dollars.” This is a unit that’s hard to get your head around. That’s why many people prefer the “standard deviation”, which is the square root of the variance.\n\n\n\nFit a model price ~ age, then plot with model_plot(). Describe the pattern between price and age you see in the plot.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 <- lm(price ~ age, data=Clock_auction)\nmodel_plot(mod1)\n\n\n\n\nAccording to the model, price goes up with age. A 25 year increase in age corresponds to about a $200 increase in price.\n\n\n\nUse model_eval() to find the model output for each clock for the model you constructed in (2).\n\nWhat’s the variance of the model .output? How does it compare to the variance of the response variable price?\nThe amount of noise can be measured with the variance of .resid. How much noise is there for price ~ age?\nDemonstrate arithmetically the relationship between the variance of the response variable, the variance of the model .output, and the variance of the noise.\n\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nValues <- model_eval(mod1)\n\nUsing training data as input to model_eval().\n\nValues |> summarize(voutput = var(.output))\n\n   voutput\n1 66352.26\n\n\nThe variance of the model output (note the dot used in the name .output) is 66-thousand square dollars. (Why “square dollars?” Because the response variable is price which is in dollars. The model output always has the same units as the response variable. And the variance of a variable always has units that are the square of the variable’s units.) The variance of the price variable is about 134-thousand square dollars, so the model output has about half the variance of the response.\n\nValues |> summarize(vresid = var(.resid))\n\n    vresid\n1 67850.62\n\n\nThe variance of the residuals is about 68-thousand square dollars. (The residuals always have the same units as the response variable.)\nTo show that the sum of the variances of the model output and residuals equals the variance of the response variable, just add them up and compare:\n\n66352.26 + 67850.62\n\n[1] 134202.9\n\n\nThe result exactly matches the variance of the price variable (calculated above).\n\n\n\nUse R2() to summarize the model you constructed in (2). Demonstrate arithmetically the relationship between R2 and variances of the response variable and the model .output.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 |> R2()\n\n   n k  Rsquared       F     adjR2            p df.num df.denom\n1 32 1 0.4944176 29.3375 0.4775648 5.914169e-06      1       30\n\n\nR2 is 0.49. This exactly matches the quotient of the variance of the model output divided by the variance of the response variable:\n\nValues |> summarize(ratio = var(.output) / var(.response))\n\n      ratio\n1 0.4944176\n\n\n\n\n\nThe quantity 1 - R2 describes the amount of noise. Arithmetically, how does 1 - R2 correspond to the variance of the .resid from part (3)?\n\n\n\n\n\n\n\nANSWER\n\n\n\nFirst, the value of 1 - R2, then the ratio of the variance of the residuals to the variance of the response variable:\n\n1 - 0.4944176\n\n[1] 0.5055824\n\nValues |> summarize(ratio = var(.resid) / var(.response))\n\n      ratio\n1 0.5055824"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-3-r2",
    "href": "Worksheets/Worksheet-21.html#part-3-r2",
    "title": "Lesson 21: Worksheet",
    "section": "Part 3: R2",
    "text": "Part 3: R2\ndag10 has a simple structure, with nodes a through f each contributing to the value of y. Use sample() to generate a sample of size 1000. Using your sample, construct several models and calculate the R2 statistic.\n\ny ~ 1.\ny ~ a\ny ~ b\ny ~ a + b\nand so on.\n\n\nWhich of the models gives the smallest value of R2? Explain why that particular model gives such a small R2.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nOur_sample <- sample(dag10, size=1000) \nlm(y ~ 1, data=Our_sample) |> R2()\n\n     n k Rsquared   F adjR2   p df.num df.denom\n1 1000 0        0 NaN     0 NaN      0      999\n\nlm(y ~ a, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 1 0.1370083 158.4421 0.1361435 0      1      998\n\nlm(y ~ b, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 1 0.3093868 447.0925 0.3086948 0      1      998\n\nlm(y ~ a + b, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 2 0.4159078 354.9612 0.4147361 0      2      997\n\n\nThe model y ~ 1 has R2=0 because the pseudo-variable 1 has zero variance and cannot “explain” any of the variance in the response variable y.\n\n\n\nAs you add more terms to the model specification, does R2 ever go down?\n\n\n\n\n\n\n\nANSWER\n\n\n\nModel specifications like y ~ a are actually shorthand for y ~ 1 + a. So y ~ a has an additional explanatory variable in addition to the pseudo-variable 1. Whenever you add a new explanatory variable to an existing model specification, the R2 will increase (or, more precisely, cannot decrease).\n\n\n\nWhat effect does the order of terms in the model have on R2? (For instance, y ~ a + b + c versus y ~ c + a + b.)\n\n\n\n\n\n\n\nANSWER\n\n\n\nCompare, for instance, y ~ a + b + c to y ~ c + a + b\n\nlm(y ~ a + b + c, data=Our_sample) |> R2()\n\n     n k Rsquared        F   adjR2 p df.num df.denom\n1 1000 3 0.446557 267.8811 0.44489 0      3      996\n\nlm(y ~ c + a + b, data=Our_sample) |> R2()\n\n     n k Rsquared        F   adjR2 p df.num df.denom\n1 1000 3 0.446557 267.8811 0.44489 0      3      996\n\n\nThe order of explanatory variables does not matter at all. It’s the collection that matters."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-4-concept-check",
    "href": "Worksheets/Worksheet-21.html#part-4-concept-check",
    "title": "Lesson 21: Worksheet",
    "section": "Part 4: Concept check",
    "text": "Part 4: Concept check\nWrite a sentence or two explaining what each of the following terms refers to.\n\n“Levels of a categorical variable”\n“Zero-one transformation”\n“Model specification”\n“Tilde expression”\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nLevels of a categorical variable. The values of a categorical variable are named. For instance, a household_pets variable would take on values like “cat,” “dog,” “turtle,” “parakeet,” …. The set of possible values for the categorical variable is called the “levels” of the variable.\nZero-one transformation. A means to translate a categorical variable with two levels into a quantitative variable. One of the levels is translated to the numerical value 1, the other to 0. The advantage of this particular scheme is that means or models where the zero-one variable is used for the response variable will have model outputs that can be interpreted as probabilities.\nModel specification. When constructing a regression model, the modeler has to provide two different kinds of inputs: (1) a data frame for training the model, (2) a statement about which variable from the data frame to use as the response variable and which other variables to use as explanatory variables. This statement (2) is called the “model specification.”\nTilde expression. Tilde expressions are an element of the syntax (or “grammar”) of R. They always involve the tilde character (~) which has no other legitimate use in R. Any valid R expression can be used on the right-side of tilde. The left side (if present, as will be the case when used with lm()) can also be any valid R expression. The most prominant role for tilde expressions in Math 300Z is to hold the model specification for use by lm(). In this use, the response variable’s name always goes on the left side of the tilde. Explanatory variable names go on the right side, usually separated by + as punctuation.\nFor the computer-science oriented …. Tilde expressions are a way to represent symbolic expressions such as fragments of code. In ordinary use, R tries to evaluate every code fragment, replacing names with their values and invoking any functions used. Symbolic expressions are taken literally as a code fragment, without any evaluation. This is valuable as a means to pass code fragments to a function which can then parse or otherwise evaluate the fragment in a particular context."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-5-curvy-models",
    "href": "Worksheets/Worksheet-21.html#part-5-curvy-models",
    "title": "Lesson 21: Worksheet",
    "section": "Part 5: Curvy models",
    "text": "Part 5: Curvy models\nHere’s a model of human height versus age based on the NHANES::NHANES data frame. (The package NHANES has the data frame which itself is called NHANES, so the full name is NHANES::NHANES.)\n\nmod1 <- lm(Height ~ Age, data = NHANES::NHANES)\nmodel_plot(mod1, data_alpha=0.05)\n\nWarning: Removed 353 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nDo you think the model gives a good description of the relationship between Age and Height? Explain using simple biological terms what the problem is with the straight-line model.\n\n\n\n\n\n\n\nANSWER\n\n\n\nHumans and other animals grow in a non-linear manner: rapid growth during gestation, infancy, and youth and comparative stasis later in life. With a model specification like Height ~ Age, lm() will look for a purely linear function, as in the straight line in the above graphic. The linear function doesn’t capture the age-dependent growth rate (fast during youth, slow or nil in adulthood), since a straight line has the same slope at all values of the explanatory variable.\nOne of the signs of the ill-fit of a linear model is that the response values tend to be cluster mostly below or mostly above the line for different regions of the explanatory variable. For instance, in the graph above, for ages younger than 10, the height values are systematically below the straight-line function.\n\n\nThere are several modeling techniques for constructing models that are more flexible than a straight line. We won’t be using them in Math 300, but we want to point out that they exist. Try this one:\n\nmod2 <- lm(Height ~ splines::ns(Age,5) * Gender, data = NHANES::NHANES)\nmodel_plot(mod2, data_alpha=0.05)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\nWarning: Removed 353 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nCalculate R2 for the straight-line model and for the curvy model.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 9647 1 0.2117647 2591.193 0.2116829 0      1     9645\n\nmod2 |> R2()\n\n     n  k  Rsquared        F    adjR2 p df.num df.denom\n1 9647 11 0.8720171 5968.044 0.871871 0     11     9635\n\n\nThe R2 for the rigid, straight-line model is substantially lower than for the more flexible, curvy model. Another way of saying this is that the curvy model stays closer (on average) to the data than the straight-line model.\nAn important theoretical question in statistical modeling is when to prefer a curvy model to a straight-line model. We haven’t yet encountered the statistical concepts that address this question."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "",
    "text": "We think about data with multiple variables as depicting a system: an interconnected network of components. Some components appear as variables in the data frame; other components may be unmeasured but given a name for reference.\nUse “directed acyclic graphs” (DAGs) to draw a picture of the system. Each system component is a node of the DAG. When one component has a causal connection with another, an arrow is drawn between those nodes.\nBy analysis of the DAG—using techniques we haven’t covered yet—you can figure out which variables to include in your model.\nMany DAGs are provided with the {math300} package, with names like dag01 through dag12. Here’s an example:\n\n\nprint(dag03)\n\ng ~ exo()\nx ~ 1 * g + exo()\ny ~ 1 * g + exo()\n\ndag_draw(dag03)\n\n\n\n\nNotice that there is no direct flow between nodes x and y. Still, there is an indirect connection: node g influences both x and y.\n\nWe can collect a sample from a DAG, for instance:\n\n\nMy_data <- sample(dag03, size=10000)\n\n\nDepending on the structure of the DAG, different model specifications will reveal different aspects of the DAG. For instance,\n\n\nmod1 <- lm(y ~ x, data=My_data)\n\nwill show if there is any kind of connection between x and y. But another specification will, in this case, show that the connection from x to y is via the connection provided by node g.\n\nmod2 <- lm(y ~ x + g, data=My_data)\n\nWe can look at the coefficients to see that in mod1 there is a non-zero connection between y and x, but in mod2 there is no non-zero connection of x on y.\n\nmod1 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0298 \n    -0.00563 \n    0.0186 \n  \n  \n    x \n    0.4800 \n    0.49700 \n    0.5140 \n  \n\n\n\nmod2 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0120 \n    0.00785 \n    0.0277 \n  \n  \n    x \n    -0.0237 \n    -0.00392 \n    0.0159 \n  \n  \n    g \n    0.9680 \n    0.99700 \n    1.0200 \n  \n\n\n\n\nOne way we will use DAGs to help us learn statistics is to compare the coefficients of models to the (known) mechanism of the DAG. We can see, for instance, that the g coefficient on y is 1 and the x coefficient is zero. Only mod2 reveals this.\n\nThe sampling and analysis in points (4) and (5) are an example of a “random trial” or “simulation.” We will use random trials to look at the properties of models fit to samples, especially with an eye to understanding the role of the sample size \\(n\\)."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#mathematical-functions-through-data",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#mathematical-functions-through-data",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Mathematical functions through data",
    "text": "Mathematical functions through data\nLet’s collect a small (\\(n=10\\)) sample from dag01:\n\nset.seed(103)\nSmall <- sample(dag01, size=10)\nhead(Small, 3)\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.7860 \n    1.89 \n  \n  \n    0.0547 \n    4.12 \n  \n  \n    -1.1700 \n    2.36 \n  \n\n\n\n\nWe can easily plot the data points. Less obviously, we can find any number of mathematical functions that are consistent with the data.\n\n\n\n\n\nFigure 1: Three of the infinite number of functions that can be drawn through the data ?@tbl-small-dag01.\n\n\n\n\n\nWhat don’t you like about these functions? Why do they insult your intuition?\n\nThey show details that are in no way suggested by the data.\nIf we were to collect more data, the function shapes could be entirely different. (Go back and change the random seed used for sampling from dag01.)"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#close-but-not-on-the-data",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#close-but-not-on-the-data",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "“Close” but not “on” the data",
    "text": "“Close” but not “on” the data\nIn general, we don’t insist that model functions go exactly through the data points. Instead, we imagine that the response variable involves some random noise that we don’t need to “capture” with our model. Doing things this way lets us fit model functions that are much simpler in shape, like this one:\n\n\n\n\n\nFigure 2: The straight-line function (blue) that goes through the data points as closely as possible. The noise is estimated as the difference (red for negative noise, black for positive noise) between the actual data points and the function.\n\n\n\n\nConstructing such a model divides the “explanation” of the response variable values into two parts:\n\nModel values, that is, the output of the model function (blue) at each of the values of the explanatory variable(s).\nWhat’s left over, the residuals, the vertical deviation of the actual response value from the model value.\n\nThe signal is the model values. The noise is the residuals.\nWhen we “fit” (or, “train”) a model, we take an aggressive stance. We look for the particular function of the shape implied by the model specification that will produce the smallest residuals. As usual, we measure the size of a residual by its square."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#measuring-signal-and-noise",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#measuring-signal-and-noise",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Measuring signal and noise",
    "text": "Measuring signal and noise\nWe depict the “size” of the signal as the amount of variability in the model values. As always, we measure variability using the variance.\nSimilarly, the “size” of the noise is the amount of variability in the residuals.\nThe model_eval() function is convenient for figuring out the model value and the residual for each row in the training data.\n\n\nThe training data\n\nSmall\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.790 \n    1.90 \n  \n  \n    0.055 \n    4.10 \n  \n  \n    -1.200 \n    2.40 \n  \n  \n    -0.170 \n    6.30 \n  \n  \n    -1.900 \n    0.93 \n  \n  \n    -0.120 \n    2.90 \n  \n  \n    0.830 \n    5.70 \n  \n  \n    1.200 \n    5.90 \n  \n  \n    -1.100 \n    2.10 \n  \n  \n    -0.380 \n    4.20 \n  \n\n\n\n\n\n\n\n\n\nOutput of model_eval()\n\nPts <- model_eval(mod)\n\n\n\n\n\n \n  \n    .response \n    x \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    1.90 \n    -0.790 \n    2.9 \n    -1.0000 \n    0.37 \n    5.4 \n  \n  \n    4.10 \n    0.055 \n    4.4 \n    -0.2400 \n    1.80 \n    6.9 \n  \n  \n    2.40 \n    -1.200 \n    2.2 \n    0.1400 \n    -0.38 \n    4.8 \n  \n  \n    6.30 \n    -0.170 \n    4.0 \n    2.4000 \n    1.50 \n    6.5 \n  \n  \n    0.93 \n    -1.900 \n    1.0 \n    -0.0810 \n    -1.80 \n    3.8 \n  \n  \n    2.90 \n    -0.120 \n    4.1 \n    -1.1000 \n    1.50 \n    6.6 \n  \n  \n    5.70 \n    0.830 \n    5.7 \n    -0.0033 \n    3.00 \n    8.4 \n  \n  \n    5.90 \n    1.200 \n    6.3 \n    -0.4400 \n    3.50 \n    9.2 \n  \n  \n    2.10 \n    -1.100 \n    2.4 \n    -0.2300 \n    -0.22 \n    4.9 \n  \n  \n    4.20 \n    -0.380 \n    3.6 \n    0.6200 \n    1.10 \n    6.1 \n  \n\n\n\n\n\n\nHow big is …\n\nThe response variable.\nThe signal.\nThe noise.\n\n\nPts |> summarize(i. = var(.response),\n                 ii. = var(.output),\n                 iii. = var(.resid)) \n\n\n\n \n  \n    i. \n    ii. \n    iii. \n  \n \n\n  \n    3.55 \n    2.6 \n    0.949 \n  \n\n\n\n\n\n\n\n\n\n\nSomething special about the variance\n\n\n\nFor every lm() model you build, the variance of the response variable is exactly equal to the sum of the variance of the model values and the variance of the residuals.\nIn other words, lm() splits the response variable into two parts: the sum of those parts equals the whole.\nR2 is the variance of the model values divided by the variance of the response variable.\n\nmod |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    10 \n    1 \n    0.733 \n    21.9 \n    0.699 \n    0.000863 \n    1 \n    8 \n  \n\n\n\nPts |> \n  summarize(i. = var(.response),\n            ii. = var(.output)) |>\n  mutate(R2 = ii. / i. )\n\n\n\n \n  \n    i. \n    ii. \n    R2 \n  \n \n\n  \n    3.55 \n    2.6 \n    0.733"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#activity-identifying-signal",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#activity-identifying-signal",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Activity: Identifying signal",
    "text": "Activity: Identifying signal"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#five-six-simple-models",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#five-six-simple-models",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Five Six simple models",
    "text": "Five Six simple models\nModels can have any number of explanatory variables. In Math 300, we will be mainly concerned with models with a single explanatory variable or with two explanatory variable. Since an explanatory variable can be either categorical or quantitative, there are six “shapes” of models:\nSingle explanatory variable\n\nCategorical explanatory variable.\nQuantitative explanatory variable.\n\nTwo explanatory variables\n\nCategorical & Categorical\nCategorical & Quantitative\nQuantitative & Quantitative\n\nSometimes, we will use models with Zero explanatory variables\n\nNo explanatory variables.\n\nGraphs of (i) through (v), with (iv) shown in two different modes."
  }
]
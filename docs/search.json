[
  {
    "objectID": "Worksheets/Worksheet-19.html",
    "href": "Worksheets/Worksheet-19.html",
    "title": "Lesson 19: Worksheet",
    "section": "",
    "text": "19.1. [Conceptual] Master the use and units of variance and standard deviation in measuring variability.\n19.2. [Conceptual] Understand the equivalence between mean and proportion on a zero-one variable.\n19.3. [Technical] Use var() and sd() within summarize()\n19.4. [Technical] Use model_plot() to graph models with one or two explanatory variables.\n19.5. [Technical] Use zero_one() with mutate() to create a zero-one variable."
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#preliminaries-how-we-will-work-with-r.",
    "href": "Worksheets/Worksheet-19.html#preliminaries-how-we-will-work-with-r.",
    "title": "Lesson 19: Worksheet",
    "section": "Preliminaries: How we will work with R.",
    "text": "Preliminaries: How we will work with R.\nIn the first half of Math 300Z, the daily student notes were largely structured around “scaffolded” R code, which often involved filling in the blanks. In this second half of 300Z, we will start to use a new way of helping you construct appropriate R command. We call this “command patterns. For instance,\nDF %>% summarize(NM=var(VAR)) \nis a command pattern.\nOne reason for the shift to the command-pattern style is that there will be only a handful of new patterns in the second half of the course that you’ll be using over and over again. Another reason is to help you develop “finger memory” for the most common patterns. An analogy: scaffolding is like GPS navigation which certainly makes it easier to drive but harder to get to know the town. Command patterns are like a paper map, there to help you when you need it.\nThere is a specific notation for command patterns, which you should memorize. Instead of the blanks used in a scaffold, the command pattern uses a CAPITALIZED abbreviation for the **kind of thing* that should be put in the position. Common kinds of thing are\n\nDF: a data frame, almost always referred to by name.\nVAR: a variable in a data frame. Many command patterns involve multiple variables, each of which is referred to by VAR. You will replace each VAR with the appropriate variable name.\nVARS: one or more variable names. When these are the right-hand side of a tilde expression, separate the names with + punctuation. When we mean to indicate that there is only one variable, we use VAR instead of VARS. If we want to say, “use two variables,” we would write VAR + VAR.\nMODEL refers to the name of a model that you have previously constructed with lm().\nNM means a name that you will be calling something by. For instance, NM <- lm(VAR ~ VARS, data=DF). Another occasion for using NM is as part of an argument to summarize() or mutate().\n[, MORE] means that you can have multiple additional arguments of the same form as the previous argument.\nVALUE a number, quoted string (e.g., \"red\"), or multiple values inside c( ).\nMODSPEC is a model specification, which could equally well be written VAR ~ VARS\n\nAnything in a command pattern that is not a CAPITALIZED abbreviation is a specific part of the command to be used as-is. For instance, lm(VAR ~ VARS, data=DF) refers explicitly to the lm() function whose first argument is a tilde expression and whose second argument is named data.\nOccasionally, you will refer to a data frame by naming the package from which it comes. For example, the moderndive package includes (among many others) the amazon_books data frame. Think of amazon_books as a first name, and moderndive as a family name. When you see PACKAGE::DF it is meant to indicate, for instance, moderndive::amazon_books. (Note that the :: in the command pattern is to be taken literally; there are two successive colons separating the package name from the name of the data frame.)"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#part-1",
    "href": "Worksheets/Worksheet-19.html#part-1",
    "title": "Lesson 19: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nCommand patterns:\n\nDF %>% summarize(NM = var(VAR)) Calculate variance of a variable in a data frame.\n`DF %>% summarize(NM1 = var(VAR1), NM2 = var(VAR2) [, MORE])\nPACKAGE::DF The name of a data frame within a package.\n\n\nIn the mosaicData::Galton data frame, find the variance of mother and father. Give both the numerical value and the units.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmosaicData::Galton |> \n  summarize(vmother = var(mother), vfather = var(father))\n\n   vmother  vfather\n1 5.322365 6.102164\n\n\nThe units for both are `inches-squared” since the variables themselves have units “inches.”\n\n\n\nIn the moderndive::amazon_books data frame, find the variance of list_price and num_pages. Give both the numerical value and the units.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  summarize(vprice = var(list_price), vpages = var(num_pages))\n\n# A tibble: 1 × 2\n  vprice vpages\n   <dbl>  <dbl>\n1     NA     NA\n\n\nThe units of list_price are dollars, so the variance has units “square-dollars”.\nnum_pages is dimensionless, so the variance is also dimensionless.\n\n\n\nCalculate the variance of sex from Galton. If something goes wrong, explain why.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nGalton |> summarize(vsex = var(sex))\n\nError in `summarize()`:\n! Problem while computing `vsex = var(sex)`.\nCaused by error in `stats::var()`:\n! Calling var(x) on a factor x is defunct.\n  Use something like 'all(duplicated(x)[-1L])' to test for a constant vector.\n\n\nsex is a categorical variable. There’s no such thing as the variance of a categorical variable."
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#part-2",
    "href": "Worksheets/Worksheet-19.html#part-2",
    "title": "Lesson 19: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nCommand patterns:\n\nNM <- lm(VAR ~ VARS, data = DF)\nlm(VAR ~ VARS, data=DF) %>% conf_interval()\nlm(MODSPEC, data=DF) %>% conf_interval() means the same as (b).\n\n\n(Easy, no computing needed.) What kind of a thing is conf_interval(). (Hint: It’s the same kind of thing as lm().)\n\n\n\n\n\n\n\nANSWER:\n\n\n\nconf_interval() is a function.\n\n\n\nUsing the moderndive::amazonbooks data frame, fit the model list_price ~ num_pages:\n\nWhat are the units of the “(Intercept)” coefficient?\nReport the coefficient on num_pages. Give both the numerical bounds and the units.\n\n\n\n\n\n\n\n\nANSWER:\n\n\n\nThe intercept always has the same units as the response variable. Here, that’s dollars.\n\nlm(list_price ~ num_pages, data = moderndive::amazon_books) |>\n  conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept) 8.33   11.8    15.4   \n2 num_pages   0.0105  0.0199  0.0293\n\n\nCoefficient on num_pages: .02 dollars per page. Multiplying the coefficient by the number of pages will give dollars: the units of the response variable.\n\n\n\nSimilar to (2) but with the model list_price ~ numpages + hard_paper\n\nWhat does the term hard_paperH refer to?\nAccording to the coefficients, is a hardcover book any more expensive (on average) than a softcover book?\n\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nlm(list_price ~ num_pages + hard_paper, data = moderndive::amazon_books) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept) 7.04   10.6    14.2   \n2 num_pages   0.0102  0.0196  0.0289\n3 hard_paperH 1.56    4.96    8.36  \n\n\nhard_paperH refers to the H level of the hard_paper variable. According to the model, a hardback costs $4.96 more than a paperback, on average.\n\n\n\nStore the model you created in (3) under the name mod3. We’ll use it in the next part. For your answer, put the R command you used to store the model as mod3.\n\n\n\n\n\n\n\nANSWER:\n\n\n\nNote that we are asked to store the model itself, not the confidence interval.\n\nmod3 <- lm(list_price ~ num_pages + hard_paper, data = moderndive::amazon_books)"
  },
  {
    "objectID": "Worksheets/Worksheet-19.html#graphics-review",
    "href": "Worksheets/Worksheet-19.html#graphics-review",
    "title": "Lesson 19: Worksheet",
    "section": "Graphics review",
    "text": "Graphics review\nCommand patterns:\n\nggplot(DF, aes(x=VAR, y=VAR)) + geom_jitter()\nggplot(DF, aes(x=VAR, y=VAR)) + geom_jitter() + geom_violin(fill=\"blue\", alpha=0.3)\nggplot(DF, aes(x=\"all\", y=VAR)) + geom_jitter()\nmodel_plot(MODEL, x=VAR)\nmodel_plot(MODAL, x=VAR, color=VAR)\n\n\nMake a jitter plot of list_price ~ hard_paper from moderndive::amazon_books.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  ggplot(aes(x=hard_paper, y = list_price)) + \n  geom_jitter(alpha=0.5)\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nUsing your command from (1), add a new layer: + geom_violin(fill=\"blue\", alpha=0.3)\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmoderndive::amazon_books |>\n  ggplot(aes(x=hard_paper, y = list_price)) + \n  geom_jitter(alpha=0.5) +\n  geom_violin(fill=\"blue\", alpha=0.3)\n\nWarning: Removed 1 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nUse model_plot() to draw a picture of mod3. Set x=hard_paper and num_pages=200. What do you think the horizontal line segments refer to?\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmodel_plot(mod3, x=hard_paper, num_pages=200)\n\n\n\n\nThe vertical position of the horizontal lines indicates the model output for books with 200 pages.\n\n\n\nRepeat (3), but remove num_pages = 200. Instead, set x=num_pages and color=hard_paper. Explain the meaning of the line segments in everyday terms.\n\n\n\n\n\n\n\nANSWER:\n\n\n\n\nmodel_plot(mod3, x=hard_paper, color=num_pages)\n\n\n\n\nThe parallel line segments in each column show the model output for books with 200, 400, and 600 pages respectively."
  },
  {
    "objectID": "Worksheets/Worksheet-25.html",
    "href": "Worksheets/Worksheet-25.html",
    "title": "Lesson 25: Worksheet",
    "section": "",
    "text": "We are using two different kinds of “intervals” for very different kinds of purposes:\n\nConfidence intervals are used to represent the precision of our estimates of coefficients. This always involves averaging over multiple data points.\nPrediction intervals are used to indicate the range of likely values of the response variable when specifying the explanatory variables.\n\nAlmost always, such intervals are constructed at the “95% level.” Because of this we don’t always mention the level. But other “levels” can be used: 80%, 90%, 99%, and so on.\nIt’s hard to tell from a graph of a confidence interval (or band) what the confidence level is. On the other hand, it’s often straightforward to estimate the “level” for a prediction interval or band. For a 95% prediction level, for instance, about 5% of the data points will be outside the prediction interval, while for a 99% level only about 1% of the data points will be outside the prediction interval.\nHere are graphs of some confidence bands and some prediction bands. For each graph, the sample size \\(n\\) is either 100 or 200. For each graph, say what the sample size is and whether it displays a prediction or a confidence band. If it’s a prediction band, estimate the prediction level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANSWER\n\n\n\nA. These are prediction intervals. Notice that they cover most of the data. There are 200 points and about 40 are outside of the intervals, so the prediction level is (200-40)/200 = 80%.\nB. A confidence band.\nC. Same data as in (B), but the band covers almost all the data. There are about 200 points, of which about 20 are outside the band, so the prediction level is (200-20)/200 = 90%.\nD. A confidence band.\nE. A prediction band. There are about 200 points, with about 2 outside the band, so the prediction level is (200-2)/200 = 1%.\nF. This is a tricky one. It looks like about 80% of the data are outside of the band, so if it’s a prediction band then the level is 80%. Could it be a confidence interval with a very high level (say, 99.99%)? Figure (D) shows the same data with a confidence band. Note the hour-glass shaped band. This is typical of a confidence band, but not of a prediction band.\nG. Confidence intervals.\nH. Prediction band. About 200 points altogether, of which about 20 are outside the band. (You need to look at the intensity of the dots to see the multiple data points being overplotted.) So a prediction level of about 90%."
  },
  {
    "objectID": "Worksheets/Worksheet-24.html",
    "href": "Worksheets/Worksheet-24.html",
    "title": "Lesson 24: Worksheet",
    "section": "",
    "text": "The data frame Clock_auction gives the sales price at auction of antique grandfathers’ clocks. Both the age of the clock and the number of bidders for the clock are presumed to affect the price.\nBuild a linear model price ~ age + bidders.\nFind the effect sizes (i) with respect to price and (ii) with respect to bidders.\n\nDoes age increase or decrease the price? Give units for the effect size.\n\n\n\n\n\n\n\nANSWER\n\n\n\nNote: These answers are more detailed than what’s expected from you.\nFirst, the model and the coefficients:\n\nprice_model <- lm(price ~ age + bidders, data=Clock_auction)\nconf_interval(price_model)\n\n# A tibble: 3 × 4\n  term            .lwr  .coef   .upr\n  <chr>          <dbl>  <dbl>  <dbl>\n1 (Intercept) -1451.   -922.  -392. \n2 age             8.33   11.1   13.8\n3 bidders        37.5    64.0   90.6\n\n\nThis is, as usual for Math 300Z, a model with only linear terms, so the effect size with respect to an explanatory variable is exactly the same as the coefficient.\nThe coefficient on age is 11.1. This is positive, so we know that price increases with age.\nThe units tell us “11.1 what?” For a quantitative explanatory variable like age, the coefficient is always a rate: one quantity divided by another and so the units always read like “____ per ____.” The first blank gets filled in with the units of the response variable. Here, that’s dollars. The second blank is filled in with the units of the explanatory variable. Here, that’s years.\nPutting everything together, the units of the age coefficient are dollars-per-year. 11.1 dollars-per-year means that, according to the model, increasing age by 1 year increases price by about 11.1 dollars.\n\n\n\nDoes bidders increase or decrease the price? Give units for the effect size.\n\n\n\n\n\n\n\nANSWER\n\n\n\nThe bidders coefficient is about 64 (with units). It is positive, so, according to the model, the more bidders the higher the price.\nAs before the units of the coefficient is the units of the response variable divided by the units of the explanatory variable: dollars per bidder.\nInterpretation: According to the model, for every additional bidder on a clock (that is, increased competition), the auction price will go up by about $64.\n\n\n\nWhat can you say about whether the age effect size or the bidders effect size is larger.\n\n\n\n\n\n\n\nANSWER\n\n\n\nThe two effect sizes have different units—dollars-per-year and dollars-per-bidder. There is no direct comparison to be made between quantities of with different units. (For example: Which is bigger? Two feet or a liter of water?)\n\n\n\nBuild a model with an interaction term: price ~ age + bidders and graph the model using model_plot(). Is the age/bidders interaction readily visible in the graph? Explain what you see that informs your answer.\n\n\n\n\n\n\n\nANSWER\n\n\n\nTo include an interaction term, use * instead of + to link the two explanatory variables involved.\n\nprice_model2 <- lm(price ~ age * bidders, data=Clock_auction)\nmodel_plot(price_model2)\n\n\n\n\nThe interaction shows up in the non-parallel slopes of the individual lines of price vs age. Each line corresponds to a different number of bidders. The interaction means that the effect size with respect to age depends on the number of bidders.\nAnother manifestation of the interaction is that the spacing in price between the model values for the different levels bidders (the vertical spacing between the lines) depends on age. The spacing is relatively small for ages near 100 yrs, and somewhat larger for ages near 200 yrs.\nA separate question concerns sampling variation. Because of sampling variation, even if the “social-economic” process behind the auction had no interaction, one might appear in the model because of sampling variation. We haven’t yet talked about how to judge this, but here is a demonstration of how it works, using the confidence intervals on the coefficients.\n\nconf_interval(price_model2)\n\n# A tibble: 4 × 4\n  term             .lwr    .coef   .upr\n  <chr>           <dbl>    <dbl>  <dbl>\n1 (Intercept) -1877.    -513.    851.  \n2 age            -1.23     8.17   17.6 \n3 bidders      -118.      19.9   158.  \n4 age:bidders    -0.662    0.320   1.30\n\n\nI’m looking at the confidence interval on the coefficient of the interaction term (labeled as age:bidders). That interval includes zero, so there is no basis in these data to claim that the interaction is not zero.\nNote also that adding the age:bidders term has caused all the confidence intervals to change, and all of them now include zero. This is an illustration of a general phenomenon: If you try to extract details from data, you sometimes end up with nothing.\nFor those who are concerned that this can happen, I’ll explain what’s going on in this particular case but warn you that this sort of reasoning is not part of what’s expected from you in Math 300Z. The new term, age:bidders, is strongly correlated with the ordinary bidders term.\n\nlm(I(age*bidders) ~ bidders, data = Clock_auction) |> R2()\n\n   n k  Rsquared        F     adjR2            p df.num df.denom\n1 32 1 0.6266754 50.35902 0.6142312 4.742657e-08      1       30\n\n\nThus, the new term and bidders serve as proxies for one another. It becomes highly susceptible to sampling variation how to allocate “credit” to the individual terms: it can be done in any manner of ways giving almost equivalent results. This is a phenomenon called “multi-collinearity.”\nLooking at the model and the confidence bands graphically shows the amount of sampling variability graphically.\n\nmodel_plot(price_model2, interval=\"confidence\")\n\n\n\n\nIt’s possible to place three parallel lines within each of the confidence bands, meaning that the data does not rule out the possibility that the lines are parallel."
  },
  {
    "objectID": "Worksheets/Worksheet-24.html#part-2",
    "href": "Worksheets/Worksheet-24.html#part-2",
    "title": "Lesson 24: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nThe Professional Golfers Association has an “index” used to rank golfers on the basis of the accuracy and distance of their drives. For a statistician, data on this provides an opportunity to “reverse engineer” the index. We can do this by modeling the relationship.\n\ngolf_mod <- lm(index ~ accuracy + dist, PGA_index)\n\nThe following table refers to a model with explanatory variables dist and accuracy. The table shows the .output of the model for each of four combinations of the explanatory variables.\n\nmodel_eval(golf_mod, \n           dist=c(100,200), accuracy = c(50,60)) |> \n  select(-.lwr, -.upr)\n\n  dist accuracy   .output\n1  100       50 -42.14096\n2  200       50 -21.52042\n3  100       60 -39.59864\n4  200       60 -18.97810\n\n\nA. At an accuracy of 50, what is the effect size of the .output with respect to dist? (Be sure to take into account both the difference in .output and the difference in dist.)\n\n\n\n\n\n\nANSWER\n\n\n\nLooking at the two rows with accuracy equal to 50 pts, going from distance 100 to distance 200 increases the output by about 20 index pts. Thus, the effect size with respect to accuracy is $\\(\\frac{-42.1 - 21.5}{100-200} = \\frac{20.4}{100} = 0.20\\) with units of “index pts per yard”. [Thanks to Ruby H. for pointing out my mistake in an earlier edition of this solution.]\n\n\nB. At a distance of 100, what is the effect size of the .output with respect to accuracy?\n\n\n\n\n\n\nANSWER\n\n\n\nLooking at the two rows with distance 100, an increase in accuracy from 50 to 60 corresponds to a change in output of -42.1 to -39.6, a difference of +2.5. The effect size is\n\\[\\frac{-42.1 - 39.6}{50 - 60} = \\frac{-81.7}{10} = -8.2\\] with units index-point per accuracy-point."
  },
  {
    "objectID": "Worksheets/Worksheet-26.html",
    "href": "Worksheets/Worksheet-26.html",
    "title": "Lesson 26: Worksheet",
    "section": "",
    "text": "In this Worksheet you will draw the prediction and confidence intervals called for in the Intervals by Eye class activity.\nBut here you will draw the intervals by software. We will be using DAGs to generate the data. The three DAGs for the three models will be called dag_one, dag_two, and dag_three. They are defined in the next chunk, but the definitions are not important for your work, which will be based on data generated from the DAGs by sample().\nHere we generate the three data frames. There is one for each of the three DAGs. You can read off the size of the samples from the size argument."
  },
  {
    "objectID": "Worksheets/Worksheet-26.html#model-one",
    "href": "Worksheets/Worksheet-26.html#model-one",
    "title": "Lesson 26: Worksheet",
    "section": "Model One",
    "text": "Model One\nThe data in Samp_one represent a straight-line relationship between y and x. An appropriate model specification is therefore y ~ x.\nTrain y ~ x model on Samp_one. Then plot out the model using model_plot() with the argument interval=\"prediction\". Make another similar plot with interval=\"confidence\".\n\n# for your use\n\n\nHow many points are excluded from the prediction band? Since the prediction is being made at a 95% level, only about 5% of the points should be left out. Is this working.\nCompare the width of the confidence band at its narrowest point to the width of the prediction band. A rule of thumb is that these widths should be related by \\(1/\\sqrt{n}\\). Are they?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\nmod_one <- lm(y ~ x, data=Samp_one)\nmodel_plot(mod_one, interval=\"prediction\")\nmodel_plot(mod_one, interval=\"confidence\")\n\n\n\n\n\n\n\n\n\n\n\n\nThere are four data points excluded from the prediction interval. Since the sample size is \\(n=100\\), this is indeed close to the theoretical 5%. (If you repeat the analysis using a different value in set.seed(), you may get a different number.)\nThe “width” (actually, the height, since the interval is always in terms of the response variable) of the confidence interval is about 0.4 at its narrowest point. The width of the prediction interval is about 4. These are indeed related by \\(1/\\sqrt{n=100}\\)."
  },
  {
    "objectID": "Worksheets/Worksheet-26.html#model-2-a-sine-wave",
    "href": "Worksheets/Worksheet-26.html#model-2-a-sine-wave",
    "title": "Lesson 26: Worksheet",
    "section": "Model 2: A sine wave",
    "text": "Model 2: A sine wave\nThe data in Samp_two have a sine-wave pattern. We will not study models for such patterns in detail, even though they are important in a number of areas. For our purposes, use the model specification y ~ cos(x) + sin(x).\nAs in the previous section, train the model on the Samp_two data and use model_plot() to show both the prediction and confidence intervals.\n\nAre about 5% of the points excluded from the prediction band?\nDoes the width of the confidence interval (at it’s narrowest point) have the rule-of-thumb \\(1/\\sqrt{n}\\) relationship to the width of the prediction interval?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\nmod_two <- lm(y ~ sin(x), data=Samp_two)\nmodel_plot(mod_two, interval=\"prediction\")\nmodel_plot(mod_two, interval=\"confidence\")\n\n\n\n\n\n\n\n\n\n\n\n\nI count 18 points outside the prediction interval, although there are a few cases where it’s hard to tell. The sample has size \\(n=400\\). 5% of 400 is 20, which is very close to the number observed.\nThe “width” of the prediction interval is a little more than 3. It’s hard to read from the graph the width of the confidence interval at its narrowest point, but it is much less than one.\n\nWe can calculate the width by using model_eval(). The x value at the narrowest point is \\(\\mathtt{x}=\\pi\\).\n\nmodel_eval(mod_two, x=pi, interval=\"confidence\")\n\n\n\n \n  \n    x \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    3.14 \n    3.04 \n    2.94 \n    3.14 \n  \n\n\n\nmodel_eval(mod_two, x=pi, interval=\"prediction\")\n\n\n\n \n  \n    x \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    3.14 \n    3.04 \n    1.19 \n    4.89 \n  \n\n\n\n\nThe prediction interval has width 3.7, the confidence interval has width 0.2. This is quite close to the value suggested by the rule of thumb, which gives \\(3.7/\\sqrt{400} = 0.185\\)."
  },
  {
    "objectID": "Worksheets/Worksheet-26.html#model-three",
    "href": "Worksheets/Worksheet-26.html#model-three",
    "title": "Lesson 26: Worksheet",
    "section": "Model three",
    "text": "Model three\nUse the model specification y ~ x + group for your model. Train the model using Samp_three then answer the same questions about the number of excluded points from the prediction interval and the relative width of the prediction and confidence intervals.\n\n\n\n\n\n\nAnswer\n\n\n\n\nmod_three <- lm(y ~ x + group, data=Samp_three)\nmodel_plot(mod_three, interval=\"prediction\")\nmodel_plot(mod_three, interval=\"confidence\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe count of excluded points needs to be made separately for the points in group A and the points in group B. It looks like about 4 group A points have been exclude and 3 group B points. Since there are 100 points in each group, this is roughly 5%.\nThe “width” of the prediction interval is about 5.6. (See the calculations below.) The width of the confidence intervals is about 0.5-0.6. With 100 points in each group, this is consistent with the rule of thumb.\n\n\nmodel_eval(mod_three, x=5, group=c(\"group A\", \"group B\"),\n           interval=\"prediction\")\n\n\n\n \n  \n    x \n    group \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    5 \n    group A \n    21 \n    18.00 \n    24 \n  \n  \n    5 \n    group B \n    11 \n    8.02 \n    14 \n  \n\n\n\nmodel_eval(mod_three, x=5, group=c(\"group A\", \"group B\"),\n           interval=\"confidence\")\n\n\n\n \n  \n    x \n    group \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    5 \n    group A \n    21 \n    20.7 \n    21.3 \n  \n  \n    5 \n    group B \n    11 \n    10.7 \n    11.3 \n  \n\n\n\n\nThe prediction interval has width 6, the confidence interval has width 0.6. This is quite close to the value suggested by the rule of thumb, which gives \\(3.7/\\sqrt{100} = 0.185\\)."
  },
  {
    "objectID": "Worksheets/Worksheet-23.html",
    "href": "Worksheets/Worksheet-23.html",
    "title": "Lesson 23: Worksheet",
    "section": "",
    "text": "You are going to work with data collected in the 1970s to examine the effects of smoking and exposure to second-hand smoke on pulmonary functions in youths. The data frame is FEV and is included in the {math300} package.\nThe response variable that we will study is also called FEV, standing for the “forced expiratory volume” measured in the participants in the study. In general, higher forced expiratory volume is considered a sign of better respiratory health."
  },
  {
    "objectID": "Worksheets/Worksheet-23.html#task-2",
    "href": "Worksheets/Worksheet-23.html#task-2",
    "title": "Lesson 23: Worksheet",
    "section": "Task 2",
    "text": "Task 2\n\nWhat is the width of the confidence interval on smokersmoker?\n\n\n\n\n\n\n\nANSWER\n\n\n\nYou can find the length of the confidence interval simply by subtracting the .lwr limit of the interval from the .upr limit. Here that’s 0.927 - 0.494 = 0.433.\n\n\n\nTask 3.\nJust for pedagogical purposes, we are going to explore how the width of the confidence interval would change if we had more or less data. You already have heard the theoretical relationship of the width of the confidence interval as a function of sample size \\(n\\).\n\nWhat is the size of the sample contained in FEV?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nnrow(FEV)\n\n[1] 654\n\n\n\n\n\nUsing the theoretical relationship with \\(n\\), what do you think the width of the confidence interval would be if only \\(n=150\\) rows of data were available?\n\n\n\n\n\n\n\nANSWER\n\n\n\n150 is about one-quarter the sample size of FEV. So a confidence interval calculated on a sample of \\(n=150\\) will be about \\(\\sqrt{4}\\) times larger. That is, the sample size 150 will lead to a confidence interval about twice as wide as the confidence interval from the full sample.\n\n\n\nWe can easily simulate working with a sample of \\(n=150\\). To do this, fit a model (and calculate the confidence interval on smokersmoker), but rather than using the argument data=FEV use this instead: data=sample(FEV, size=150). Compare the width of confidence interval you get in this way to your theoretical prediction in (2).\nThis will be surprising, but we can actually simulate what would happen if we had a larger sample size. (This is just a simulation, and just for pedagogical purposes. This is not a way to collect a genuine sample of a larger size.)\n\nTo create a (simulated) sample of size, say, \\(n=2500\\) set the data argument to lm() this way: data=resample(FEV, size=2500). (NOTE: The function being used here is not sample() but the closely related resample(), with an re in front.)\nCalculate the width of the (simulated) confidence interval on smokersmoker for the sample size of 2500.\n\n\n\n\n\n\nANSWER\n\n\n\n\nlm(FEV ~ smoker, data=resample(FEV, size=2500)) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr .coef  .upr\n  <chr>        <dbl> <dbl> <dbl>\n1 (Intercept)  2.54  2.58  2.61 \n2 smokersmoker 0.561 0.672 0.783\n\n\nThe width of the confidence interval on `smokersmoker is about 0.20.\n\n\n\n\nTask 3\nWere you surprised to see in Task 1 that smoking is associated with a higher FEV than non-smoking? Since a higher FEV is considered healthier, does this mean that smoking is healthy? The answer is “no,” but let’s consider it from the perspective of accuracy versus precision.\nThe confidence interval on smokersmoker in the model FEV ~ smoker was 0.50 to 0.93 liters. This precision is good enough to rightfully claim that the smokersmoker coefficient is not zero or negative.\nBut precision is different from accuracy. One of the major potential determinants of FEV is age.\n\nBuild a model FEV ~ age and construct the confidence interval on age. Explain whether your model is consistent or not with the idea that FEV depends on age.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nlm(FEV ~ age, data=FEV) |> conf_interval()\n\n# A tibble: 2 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept) 0.279 0.432 0.585\n2 age         0.207 0.222 0.237\n\n\nThe age coefficient is about 0.2 liters per year. In other words, FEV increases with age.\n\n\n\nIt also happens that smoking is associated with age. The younger kids don’t smoke. We can demonstrate this with a model of smoker ~ age. Since smoker is a categorical variable, we need to convert it to a zero-one variable before fitting the model. Here’s a chunk to do so, assigning the smokers to have a value of 1:\n\n\nFEV |> \n  mutate(smoke = zero_one(smoker, one=\"smoker\")) |>\n  lm(smoke ~ age, data = _) |>\n  conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) -0.381  -0.308  -0.234 \n2 age          0.0338  0.0410  0.0481\n\n\nInterpret the coefficient as a rate of probability: how the probability that a participant smokes changes per year of age.\n\nIs the age coefficient consistent with the idea that older kids are more likely to smoke?\nNow the point about accuracy and precision being different things. FEV increases with age and so does the probability of being a smoker. That means that smoker is also related to age. In fact, to some extent smoker is a proxy for age. As an exercise, draw on paper a DAG where age influences FEV, and age influences smoking status, and also smoking status influences FEV.\n\nFor the DAG just described, an accurate model to estimate the direct effect of smoking on FEV is FEV ~ age + smoker. Fit this model and use the confidence interval on smoker to make an accurate statement about smoking and FEV.\n\n\n\n\n\n\nANSWER\n\n\n\n\nlm(FEV ~ age + smoker, data = FEV) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr  .coef    .upr\n  <chr>         <dbl>  <dbl>   <dbl>\n1 (Intercept)   0.207  0.367  0.527 \n2 age           0.215  0.231  0.247 \n3 smokersmoker -0.368 -0.209 -0.0504\n\n\nThe confidence interval on smokersmoker is entirely negative. Negative means that smoking is associated with smaller FEV. This sort of thing would be summarized as, “After adjusting for age, smoking is associated with smaller FEV.”"
  },
  {
    "objectID": "Worksheets/Worksheet-22.html",
    "href": "Worksheets/Worksheet-22.html",
    "title": "Lesson 22: Worksheet",
    "section": "",
    "text": "22.1 Describe the logical origin of sampling variation as the variation between multiple samples from the same source.\n22.2 Recognize the several formats in which we describe sampling variation—sampling variance, standard error, margin of error, confidence interval—and show how they are related.\n22.3 Using repeated sampling trials, observe how sampling variance scales with sample size \\(n\\)."
  },
  {
    "objectID": "Worksheets/Worksheet-22.html#part-1",
    "href": "Worksheets/Worksheet-22.html#part-1",
    "title": "Lesson 22: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nUsing dag02, obtain a sample of size 25 and show the values of y.\n\ndag02sample = sample(dag02, size=25)\n\ndag02sample%>%\n  select(y)\n\n# A tibble: 25 × 1\n        y\n    <dbl>\n 1  2.29 \n 2  8.90 \n 3  6.56 \n 4  1.29 \n 5  6.97 \n 6  8.76 \n 7  8.23 \n 8  0.759\n 9  5.18 \n10 10.2  \n# … with 15 more rows\n\n\nCompute the mean those 25 values of y in two different, but entirely equivalent ways. (1) Use data wrangling. (2) Construct a model y ~ 1 report the intercept coefficient. Show that these give the same answer.\n\n\n\n\n\n\nANSWER\n\n\n\n\ndag02sample%>%\n  summarize(mean(y))\n\n# A tibble: 1 × 1\n  `mean(y)`\n      <dbl>\n1      6.37\n\ndag02sample %>% \n  lm(y~1,data=.)%>%\n  conf_interval()\n\n# A tibble: 1 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept)  5.14  6.37  7.60"
  },
  {
    "objectID": "Worksheets/Worksheet-22.html#part-2",
    "href": "Worksheets/Worksheet-22.html#part-2",
    "title": "Lesson 22: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nCreate a new chunk that repeats the generation of a sample from dag02 the the two methods for calculating the mean of the y values. Run the new chunk and observe that the calculated value of the mean differs somewhat from that you found in Part 1. Repeat running the chunk over and over again; the mean value will differ each time.\nTask 2.1. Each time you run the chunk, you are performing a new sampling trial. Run a dozen or so trials, observing the calculated value of the mean of y in order to get a sense for how much it varies from trial to trial. Then summarizing your observations by giving a rough interval for the range of the mean of y across the trials.\n\n\n\n\n\n\nANSWER\n\n\n\n\n\n\nWe are going to automate the process of performing sampling trials so that we can run hundreds of them.\nUsing the do operator, calculate the sampling variance for a set of trials from dag02. The following code chunk shows how to run 500 trials, in each of which the mean of y is calculated using the y ~ 1 method and reporting the intercept coefficient. These will be collected into a data frame named dag02trials25.\n\ndag02trials25 <- do(500) * {\n  sample(dag02, size=25) |> \n  lm(y ~ 1, data=_) |>\n  conf_interval()\n}\n\nTask 2.2. Run the chunk above to create dag02trials25. Then use data wrangling commands to compute three summaries of the trials: i. The mean of the coefficient across the trials. ii. The variance of the coefficient across the trials. iii. The standard deviation of the coefficient across the trials.\n\n\n\n\n\n\nANSWER\n\n\n\nThe coefficient for each of the 500 trials is stored in the .coef column of dag02trials25. Simple data wrangling provides the summary.\n\ndag02trials25%>%\n  summarize(mean_of_means = mean(.coef),\n            sampling_variance = var(.coef),\n            standard_error = sd(.coef))\n\n# A tibble: 1 × 3\n  mean_of_means sampling_variance standard_error\n          <dbl>             <dbl>          <dbl>\n1          4.98             0.551          0.742\n\n\nNotice that the names—sampling_variance and standard_error—we used for the different summaries correspond to the standard statistical nomenclature for these quantities.\n\n\nTask 2.3. Repeat (2) with four different sample sizes (try 50, 100, 200, and 400). Fill in the table below. What do you notice about the standard error as sample size increases?\n\n\n\nSample size\nSampling variance\nStandard error\n\n\n\n\nn=25\n0.439\n0.663\n\n\nn=50\n0.253\n0.503\n\n\nn=100\n0.129\n0.359\n\n\nn=200\n0.059\n0.243\n\n\nn=400\n0.032\n0.178\n\n\n\n\n\n\n\n\n\nANSWER\n\n\n\nThe sampling variance gets smaller as the sample size increases. Specifically, doubling the sample size tends to halve the sampling variance. The standard error—which is just the square-root of the sampling variance—also gets smaller as \\(n\\) increases. As in the nature of square roots, to halve the standard error, the sample size must be doubled twice."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html",
    "href": "Worksheets/Worksheet-20.html",
    "title": "Lesson 20: Worksheet",
    "section": "",
    "text": "20.1 [Technical] Collect a sample from a DAG simulation.\n20.2 [Technical] Examine the formulas behind a DAG simulation and compare to the results of a regression model trained on a sample from the DAG simulation.\n20.3 [Conceptual] Recognize properties of a DAG. i. Identify exogenous nodes. ii. Identify all pathways between two specified end nodes. iii. On a given pathway, is there causal flow from one end node to another? iv. On a given pathway, is there a causal flow from some node on the pathway to both end nodes?"
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-1-samples-from-dags",
    "href": "Worksheets/Worksheet-20.html#part-1-samples-from-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 1: Samples from DAGs",
    "text": "Part 1: Samples from DAGs\n\nUse dag_draw() to draw a picture of the dag08 directed acyclic graph. From this graph, explain why node c is exogenous and why x and y are not.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\ndag_draw(dag08)\n\n\n\n\nNode C is exogenous because it has no incoming arrows.\n\n\n\nUse print() to view the formulas used by dag08 to simulate data. What about the formula for y indicates that it’s receives inputs from x and c.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nprint(dag08)\n\nc ~ exo()\nx ~ c + exo()\ny ~ x + c + 3 + exo()\n\n\nThe right-hand side of the formula for y says that y will be calculated as the sum of x and c (plus 3 plus some random noise). That is, x and c directly shape the value of y.\n\n\n\nThere are three coefficients in the formula for y: an intercept, an x coefficient, and a c coefficient. (There is also some random input from an exogenous source unrelated to c or x.) What are the numerical values of the three coefficients?\n\n\n\n\n\n\n\nANSWER\n\n\n\nFrom the formula for y in the dag, the coefficients are 1 for x, 1 for c, and 3 for the intercept.\n\n\n\nCollect a sample of size \\(n=100\\) from dag08 and use it to train the model with specification y ~ x. Do the coefficients reported match those you found in part (c)? (If you are not sure, use a bigger sample size, say \\(n=1000\\) or even bigger.)\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag08, size=1000)\nlm(y ~ x, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept)  2.95  3.03  3.10\n2 x            1.46  1.52  1.57\n\n\n\n\nThe model says the x coefficient is about 1.5, not the same as in the DAG formula for y.\n\nSimilar to (4), but use the specification y ~ x + c. How do the coefficients for this model compare to those you found in (3)?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag08, size=1000)\nlm(y ~ x + c, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept) 2.92  2.98   3.04\n2 x           0.984 1.05   1.11\n3 c           0.902 0.984  1.07\n\n\nWhen we include both x and c in the model specification, the coefficients work out to match those of the DAG formula for y."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-2-paths-in-dags",
    "href": "Worksheets/Worksheet-20.html#part-2-paths-in-dags",
    "title": "Lesson 20: Worksheet",
    "section": "Part 2: Paths in DAGs",
    "text": "Part 2: Paths in DAGs\n\nIn dag08 there are two paths connecting x andy. One path is direct, \\(X \\longrightarrow Y\\). The other path is indirect, \\(X \\longleftarrow C \\longrightarrow Y\\).\n\nAlong the indirect path, is there a causal flow from x to y?\nAlong the indirect path, is there a causal flow from any node on the graph that reaches both endpoints, x and y?\n\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nYes, the flow runs directly from x to y.\nYes. The flow runs from c to each of x and y.\n\n\n\n\ndag_school2 is a highly simplistic model of the relationship between expenditures on schools and student outcomes in terms of, say, standardized test scores.\n\n\ndag_draw(dag_school2, vertex.label.cex=1, vertex.size=40)\n\n\n\n\nThere is a direct pathway from expenditure to outcome as well as another, indirect pathway.\n\nAre there any exogenous nodes in the graph?\nOn the indirect pathway, is there a causal flow from expenditure to outcome?\nIs there a causal flow from any node on the indirect pathway to both expenditure and outcome? Which one?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nCulture is an exogenous node; there are no incoming arrows to culture.\nYes.\nFrom Culture there is a flow to outcome through expenditure. There is also a flow from Culture to Outcome via Participation."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "href": "Worksheets/Worksheet-20.html#part-3-are-expenditures-good-for-school-outcomes",
    "title": "Lesson 20: Worksheet",
    "section": "Part 3: Are expenditures good for school outcomes?",
    "text": "Part 3: Are expenditures good for school outcomes?\n\nLook at the formulas for dag_school2. Is a higher expenditure connected to a higher outcome?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nprint(dag_school2)\n\nculture ~ unif(-1, 1)\nexpenditure ~ 12000 + 4000 * culture + exo(1000)\nparticipation ~ (50 + 30 * culture + exo(15)) %>% pmax(0) %>% \n    pmin(100)\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + exo(50)\n\n\nThe formula for Outcome has a positive coefficient (0.01) on expenditure. So when expenditure goes up, so will outcome. The magnitude of the coefficient is neither here nor there. Remember that there are always units associated with a coefficient. It’s impossible to say whether a magnitude is large or small unless you know the units.\n\n\n\nGenerate a simple of size 1000 from dag_school2 and use it to train the model outcome ~ expenditure. Is the coefficient on expenditure consistent with what you found in (1)? (If you aren’t sure, use a larger sample size, say 10,000.) What about the coefficient on expenditure leads to your conclusion?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nSamp <- sample(dag_school2, size=1000)\nlm(outcome ~ expenditure, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term             .lwr     .coef      .upr\n  <chr>           <dbl>     <dbl>     <dbl>\n1 (Intercept) 1192.     1216.     1241.    \n2 expenditure   -0.0181   -0.0161   -0.0141\n\n\nThe coefficient on Expenditure is negative in contrast to the known positive coefficient in the DAG formula for Outcome.\n\n\n\nSpeculate on what might be the origin of the evident inconsistency between (1) and (2)?\n\n\n\n\n\n\n\nANSWER\n\n\n\nIn the DAG, Outcome is influenced negatively by Participation. And Expenditure is influenced positively by Participation. The two effects of Participation combine to produce an overall negative link between Expenditure and Outcome. By overall, we mean the combination of the direct Expenditure to Outcome link and the indirect path from Expenditure to Outcome via Participation."
  },
  {
    "objectID": "Worksheets/Worksheet-20.html#part-4-constructing-a-dag",
    "href": "Worksheets/Worksheet-20.html#part-4-constructing-a-dag",
    "title": "Lesson 20: Worksheet",
    "section": "Part 4: Constructing a DAG",
    "text": "Part 4: Constructing a DAG\nIn this task, you will construct DAGs using dag_make() and draw them using dag_draw().\nA DAG is defined by a series of tilde expressions, one for each node in the graph. The tilde expression for a node has the node’s name on the left-hand side of the tilde. The right-hand side contains the nodes which serve as inputs to the node named on the left-hand side. If there are no inputs, write exo().\nFor example, consider a DAG with three nodes: one, two, and three. To define a DAG where node two receives input from node one, and node three receives input from nodes one and two, use make_dag() with three tilde expressions:\n\nexample_dag <- dag_make(\n  one ~ exo(),\n  two ~ one,\n  three ~ two + one\n)\ndag_draw(example_dag)\n\n\n\n\nThe right-hand side of a formula can be any arithmetic expression involving the node names, but we will keep it simple: just use + to separated the node names. If a node receives no inputs, the right-hand side should be simply exo() to mark that node as exogenous.\n\nWhat happens if node one, instead of being exogenous, takes as input one of the other two nodes in example_dag?\n\n\n\n\n\n\n\nANSWER\n\n\n\nThe graph would become cyclic, hence not a DAG. Notice that by using a node on the right-hand side of a tilde expression only when it has already been created by a previous tilde expression, you guarantee that the graph will be acyclic.\n\n\n\nCreate and draw a DAG that has the same arrangement of causal connections as “Professor Butts and the Self-Operating Napkin,” illustrated below:\n\n\nProfessor Butts and the Self-Operating Napkin (1931). Soup_spoon (A) is raised to mouth, pulling string (B) and thereby jerking ladle (C), which throws cracker (D) past toucan (E). Toucan jumps after cracker and perch (F) tilts, upsetting seeds (G) into pail (H). Extra weight in pail pulls cord (I), which opens and ignites lighter (J), setting off skyrocket (K), which causes sickle (L) to cut string_m (M), allowing pendulum with attached napkin to swing back and forth, thereby wiping_chin.\nWatch your spelling of node names! Use this command to draw your napkin_dag:\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)\n\n\n\n\n\n\nANSWER\n\n\n\n\nnapkin_dag <- dag_make(\n  soup_spoon ~ exo(),\n  string ~ soup_spoon,\n  ladle ~ string,\n  cracker ~ ladle,\n  toucan ~ cracker,\n  perch ~ toucan,\n  seeds ~ perch,\n  pail ~ seeds,\n  cord ~ pail,\n  lighter ~ cord,\n  skyrocket ~ lighter,\n  sickle ~ skyrocket,\n  string_m ~ sickle,\n  wiping_chin ~ string_m\n)\ndag_draw(napkin_dag, vertex.label.cex=.5, vertex.size=10, edge.arrow.size = 0.2)"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html",
    "href": "Worksheets/Worksheet-21.html",
    "title": "Lesson 21: Worksheet",
    "section": "",
    "text": "Command patterns:"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#basic-regression-patterns",
    "href": "Worksheets/Worksheet-21.html#basic-regression-patterns",
    "title": "Lesson 21: Worksheet",
    "section": "Basic regression patterns",
    "text": "Basic regression patterns\nEvery regression model involves a response variable, which Lessons in Statistical Thinking always plots on the vertical axis. Most of the regression models we will consider in these Lessons have one or two explanatory variables, although sometimes there will be more than two and sometimes none at all.\nIt is worth memorizing the forms of the tilde-expression specifications of the zero-, one-, and two-explanatory models, as well as their shapes. For this purpose, we’ll write the forms using five generic variable names. In practice, you will replace these generic names with specific names from the data frame of interest.\n\ny — a quantitative response variable (which might be the result of a zero-one transformation).\nx and z — quantitative explanatory variables\ng and h — categorical explanatory variables.\n\n\n\n\n\n\n\n\nModel specification\nShape\n\n\n\n\ny ~ 1\nA line with slope zero.\n\n\ny ~ x\nA line with possibly non-zero slope.\n\n\ny ~ g\nA value for each level of g.\n\n\ny ~ x + g\nSeparate lines for each level of g, all with the same slope.\n\n\ny ~ x + z\nParallel, evenly spaced lines.\n\n\ny ~ g + h\nFor each level of g, a set of spaced values, one for each level of h. The h-spacing will be the same for every level of g.\n\n\n\nNote: It doesn’t matter what order the explanatory variables are given in. The name of the response variable is always on the left-hand side of the tilde expression."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-1",
    "href": "Worksheets/Worksheet-21.html#part-1",
    "title": "Lesson 21: Worksheet",
    "section": "Part 1",
    "text": "Part 1\nDo Exercise 21.4."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-2",
    "href": "Worksheets/Worksheet-21.html#part-2",
    "title": "Lesson 21: Worksheet",
    "section": "Part 2",
    "text": "Part 2\nBy fitting a regression model, we divide the response variable into two components: a signal component and a noise component. The model specification tells what sort of signal to look for. For instance, the Clock_auction data frame records the sales price of antique grandfather clocks sold at auction. Presumably, the price reflects some feature of the clock itself as well as the market conditions. We have only the variables age and bidders to represent the the value of the clock and the market conditions.\nUsing lm() with the specification price ~ age directs the computer to look for a signal in the form of a straight-line relationship between age and price. The estimated noise is the difference between the response variable values (price) and the signal.\n\nHow much clock-to-clock variation is there in price? (Use the variance to measure variation.)\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nClock_auction |> summarize(vprice = var(price), sd=sqrt(vprice))\n\n# A tibble: 1 × 2\n   vprice    sd\n    <dbl> <dbl>\n1 134203.  366.\n\n\nSince the price is in dollars, the variance of price has units of “square dollars.” This is a unit that’s hard to get your head around. That’s why many people prefer the “standard deviation”, which is the square root of the variance.\n\n\n\nFit a model price ~ age, then plot with model_plot(). Describe the pattern between price and age you see in the plot.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 <- lm(price ~ age, data=Clock_auction)\nmodel_plot(mod1)\n\n\n\n\nAccording to the model, price goes up with age. A 25 year increase in age corresponds to about a $200 increase in price.\n\n\n\nUse model_eval() to find the model output for each clock for the model you constructed in (2).\n\nWhat’s the variance of the model .output? How does it compare to the variance of the response variable price?\nThe amount of noise can be measured with the variance of .resid. How much noise is there for price ~ age?\nDemonstrate arithmetically the relationship between the variance of the response variable, the variance of the model .output, and the variance of the noise.\n\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nValues <- model_eval(mod1)\n\nUsing training data as input to model_eval().\n\nValues |> summarize(voutput = var(.output))\n\n   voutput\n1 66352.26\n\n\nThe variance of the model output (note the dot used in the name .output) is 66-thousand square dollars. (Why “square dollars?” Because the response variable is price which is in dollars. The model output always has the same units as the response variable. And the variance of a variable always has units that are the square of the variable’s units.) The variance of the price variable is about 134-thousand square dollars, so the model output has about half the variance of the response.\n\nValues |> summarize(vresid = var(.resid))\n\n    vresid\n1 67850.62\n\n\nThe variance of the residuals is about 68-thousand square dollars. (The residuals always have the same units as the response variable.)\nTo show that the sum of the variances of the model output and residuals equals the variance of the response variable, just add them up and compare:\n\n66352.26 + 67850.62\n\n[1] 134202.9\n\n\nThe result exactly matches the variance of the price variable (calculated above).\n\n\n\nUse R2() to summarize the model you constructed in (2). Demonstrate arithmetically the relationship between R2 and variances of the response variable and the model .output.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 |> R2()\n\n   n k  Rsquared       F     adjR2            p df.num df.denom\n1 32 1 0.4944176 29.3375 0.4775648 5.914169e-06      1       30\n\n\nR2 is 0.49. This exactly matches the quotient of the variance of the model output divided by the variance of the response variable:\n\nValues |> summarize(ratio = var(.output) / var(.response))\n\n      ratio\n1 0.4944176\n\n\n\n\n\nThe quantity 1 - R2 describes the amount of noise. Arithmetically, how does 1 - R2 correspond to the variance of the .resid from part (3)?\n\n\n\n\n\n\n\nANSWER\n\n\n\nFirst, the value of 1 - R2, then the ratio of the variance of the residuals to the variance of the response variable:\n\n1 - 0.4944176\n\n[1] 0.5055824\n\nValues |> summarize(ratio = var(.resid) / var(.response))\n\n      ratio\n1 0.5055824"
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-3-r2",
    "href": "Worksheets/Worksheet-21.html#part-3-r2",
    "title": "Lesson 21: Worksheet",
    "section": "Part 3: R2",
    "text": "Part 3: R2\ndag10 has a simple structure, with nodes a through f each contributing to the value of y. Use sample() to generate a sample of size 1000. Using your sample, construct several models and calculate the R2 statistic.\n\ny ~ 1.\ny ~ a\ny ~ b\ny ~ a + b\nand so on.\n\n\nWhich of the models gives the smallest value of R2? Explain why that particular model gives such a small R2.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nOur_sample <- sample(dag10, size=1000) \nlm(y ~ 1, data=Our_sample) |> R2()\n\n     n k Rsquared   F adjR2   p df.num df.denom\n1 1000 0        0 NaN     0 NaN      0      999\n\nlm(y ~ a, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 1 0.1163881 131.4552 0.1155028 0      1      998\n\nlm(y ~ b, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 1 0.2561369 343.6448 0.2553916 0      1      998\n\nlm(y ~ a + b, data=Our_sample) |> R2()\n\n     n k Rsquared        F     adjR2 p df.num df.denom\n1 1000 2 0.391591 320.8501 0.3903705 0      2      997\n\n\nThe model y ~ 1 has R2=0 because the pseudo-variable 1 has zero variance and cannot “explain” any of the variance in the response variable y.\n\n\n\nAs you add more terms to the model specification, does R2 ever go down?\n\n\n\n\n\n\n\nANSWER\n\n\n\nModel specifications like y ~ a are actually shorthand for y ~ 1 + a. So y ~ a has an additional explanatory variable in addition to the pseudo-variable 1. Whenever you add a new explanatory variable to an existing model specification, the R2 will increase (or, more precisely, cannot decrease).\n\n\n\nWhat effect does the order of terms in the model have on R2? (For instance, y ~ a + b + c versus y ~ c + a + b.)\n\n\n\n\n\n\n\nANSWER\n\n\n\nCompare, for instance, y ~ a + b + c to y ~ c + a + b\n\nlm(y ~ a + b + c, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 3 0.4293227 249.7649 0.4276038 0      3      996\n\nlm(y ~ c + a + b, data=Our_sample) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1000 3 0.4293227 249.7649 0.4276038 0      3      996\n\n\nThe order of explanatory variables does not matter at all. It’s the collection that matters."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-4-concept-check",
    "href": "Worksheets/Worksheet-21.html#part-4-concept-check",
    "title": "Lesson 21: Worksheet",
    "section": "Part 4: Concept check",
    "text": "Part 4: Concept check\nWrite a sentence or two explaining what each of the following terms refers to.\n\n“Levels of a categorical variable”\n“Zero-one transformation”\n“Model specification”\n“Tilde expression”\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nLevels of a categorical variable. The values of a categorical variable are named. For instance, a household_pets variable would take on values like “cat,” “dog,” “turtle,” “parakeet,” …. The set of possible values for the categorical variable is called the “levels” of the variable.\nZero-one transformation. A means to translate a categorical variable with two levels into a quantitative variable. One of the levels is translated to the numerical value 1, the other to 0. The advantage of this particular scheme is that means or models where the zero-one variable is used for the response variable will have model outputs that can be interpreted as probabilities.\nModel specification. When constructing a regression model, the modeler has to provide two different kinds of inputs: (1) a data frame for training the model, (2) a statement about which variable from the data frame to use as the response variable and which other variables to use as explanatory variables. This statement (2) is called the “model specification.”\nTilde expression. Tilde expressions are an element of the syntax (or “grammar”) of R. They always involve the tilde character (~) which has no other legitimate use in R. Any valid R expression can be used on the right-side of tilde. The left side (if present, as will be the case when used with lm()) can also be any valid R expression. The most prominant role for tilde expressions in Math 300Z is to hold the model specification for use by lm(). In this use, the response variable’s name always goes on the left side of the tilde. Explanatory variable names go on the right side, usually separated by + as punctuation.\nFor the computer-science oriented …. Tilde expressions are a way to represent symbolic expressions such as fragments of code. In ordinary use, R tries to evaluate every code fragment, replacing names with their values and invoking any functions used. Symbolic expressions are taken literally as a code fragment, without any evaluation. This is valuable as a means to pass code fragments to a function which can then parse or otherwise evaluate the fragment in a particular context."
  },
  {
    "objectID": "Worksheets/Worksheet-21.html#part-5-curvy-models",
    "href": "Worksheets/Worksheet-21.html#part-5-curvy-models",
    "title": "Lesson 21: Worksheet",
    "section": "Part 5: Curvy models",
    "text": "Part 5: Curvy models\nHere’s a model of human height versus age based on the NHANES::NHANES data frame. (The package NHANES has the data frame which itself is called NHANES, so the full name is NHANES::NHANES.)\n\nmod1 <- lm(Height ~ Age, data = NHANES::NHANES)\nmodel_plot(mod1, data_alpha=0.05)\n\nWarning: Removed 353 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nDo you think the model gives a good description of the relationship between Age and Height? Explain using simple biological terms what the problem is with the straight-line model.\n\n\n\n\n\n\n\nANSWER\n\n\n\nHumans and other animals grow in a non-linear manner: rapid growth during gestation, infancy, and youth and comparative stasis later in life. With a model specification like Height ~ Age, lm() will look for a purely linear function, as in the straight line in the above graphic. The linear function doesn’t capture the age-dependent growth rate (fast during youth, slow or nil in adulthood), since a straight line has the same slope at all values of the explanatory variable.\nOne of the signs of the ill-fit of a linear model is that the response values tend to be cluster mostly below or mostly above the line for different regions of the explanatory variable. For instance, in the graph above, for ages younger than 10, the height values are systematically below the straight-line function.\n\n\nThere are several modeling techniques for constructing models that are more flexible than a straight line. We won’t be using them in Math 300, but we want to point out that they exist. Try this one:\n\nmod2 <- lm(Height ~ splines::ns(Age,5) * Gender, data = NHANES::NHANES)\nmodel_plot(mod2, data_alpha=0.05)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\nWarning: Removed 353 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nCalculate R2 for the straight-line model and for the curvy model.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nmod1 |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 9647 1 0.2117647 2591.193 0.2116829 0      1     9645\n\nmod2 |> R2()\n\n     n  k  Rsquared        F    adjR2 p df.num df.denom\n1 9647 11 0.8720171 5968.044 0.871871 0     11     9635\n\n\nThe R2 for the rigid, straight-line model is substantially lower than for the more flexible, curvy model. Another way of saying this is that the curvy model stays closer (on average) to the data than the straight-line model.\nAn important theoretical question in statistical modeling is when to prefer a curvy model to a straight-line model. We haven’t yet encountered the statistical concepts that address this question."
  },
  {
    "objectID": "Worksheets/Basic-questions.html",
    "href": "Worksheets/Basic-questions.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "What are the two main types of content in variables?\nWhat are the two main roles of variables in constructing a regression model?\nIn regression modeling, what is the restriction on the kind of variable that can be used for the response variable?\nDescribe the fundamental framework for modeling.\nIn terms of a model specification, where does a “covariate” fit in?\nWhat does “sample statistic” mean?\nWhat does “sampling variation” mean and how is it related to “sampling variance?”\nWhat is the difference between a confidence interval and a prediction interval?"
  },
  {
    "objectID": "Worksheets/Worksheet-28.html",
    "href": "Worksheets/Worksheet-28.html",
    "title": "Lesson 28: Worksheet",
    "section": "",
    "text": "In this Worksheet, you’ll explore how adding a covariate to a model can change the coefficient on an explanatory variable.\nWe will work with two nested models trained on the Galton data frame:\n\nheight ~ mother\nheight ~ mother + covar where you will replace covar with a variable we will construct.\n\nTask A. Replace covar with father and observe how the coefficient on mother changes between (i) and (ii). Describe the change as large or small, providing also your definition for large and small in this context.\n\n\n\n\n\n\nANSWER\n\n\n\nThe two models are\n\nlm(height ~ mother, data=Galton) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n\nlm(height ~ mother + father, data=Galton) |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 13.9   22.3   30.8  \n2 mother       0.187  0.283  0.380\n3 father       0.290  0.380  0.470\n\n\nI describe the change in the mother coefficient as small, since the confidence intervals on the two models overlap very substantially. The difference between the .coefs is very small compared to the width of the confidence intervals.\n\n\nTask B. An important factor in whether a covariate changes a coefficient is the strength of the relationship between the covariate and the other explanatory variable. Measure the strength of the relationship between mother and father by fitting the model mother ~ father and finding R2. Describe whether the R2 you find in this way is large or small. (You’ll have to give a definition for “large” and “small” in this context. It will be different than for the context in part (A). Also, note that in this task, the response variable is mother, not height.)\n\n\n\n\n\n\nANSWER\n\n\n\nTo look at the relationship between the covariate father and the other explanatory variable mother, find R2\n\nlm(mother ~ father, data=Galton) |> R2()\n\n    n k    Rsquared       F      adjR2          p df.num df.denom\n1 898 1 0.005426475 4.88865 0.00431646 0.02728512      1      896\n\n\nThe R2 statistic can range from zero to one. On that scale, the above R2 is very close to zero, so there is little if any relationship between the mother’s height and the father’s height. Following social convention, usually there is little or no genetic relationship between the mother and the father. But if you think that married couples tend to be similar in height, the Galton data suggests otherwise.\n\n\nTask C. Now you are going to create a new covariate that is going to be closely related to mother. This is a matter of making the new variable very similar to mother, like this:\n\nGalton <- Galton %>% \n  mutate(new_var = 6*mother + 2*father)\n\nThe new_var consists of six parts mother and two parts father.\n\nWhat is the R2 between mother and new_var?\nComparing the mother coefficient from the nested pair of models height ~ mother and height ~ mother + new_var, would you say that new_var changes things substantially?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nlm(mother ~ new_var, data = Galton) |> R2()\n\n    n k  Rsquared        F     adjR2 p df.num df.denom\n1 898 1 0.8926256 7448.632 0.8925057 0      1      896\n\n\nAnd now the change in the mother coefficient\n\nlm(height ~ mother, data=Galton) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n\nlm(height ~ mother + new_var, data=Galton) |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 13.9   22.3   30.8  \n2 mother      -1.15  -0.856 -0.563\n3 new_var      0.145  0.190  0.235\n\n\nNotice that the mother coefficient changed sign between the two models. This is called “Simpson’s Paradox.” But it’s really only a paradox to people who don’t understand that using a covariate that is closely related to an explanatory variable can substantially change the coefficient on the explanatory variable.\n\n\nTask D. Going back to the commands in (C), increase the mixture in new_var to fifty parts mother and one part father. What happens to the width of the confidence interval on mother when this close copy of mother is used as a covariate?\n\n\n\n\n\n\nANSWER\n\n\n\nNote that we have already calculated, above, the mother coefficient from height ~ mother.\n\nGalton <- Galton %>% \n  mutate(new_var = 50*mother + 1*father)\nlm(height ~ mother + new_var, data=Galton) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept)  13.9    22.3    30.8  \n2 mother      -23.2   -18.7   -14.2  \n3 new_var       0.290   0.380   0.470\n\n\nThe confidence interval on mother becomes extremely wide!\n\n\nTask E. Just for interest’s sake … Like Task D, but make new_var 50 parts mother and zero parts father. Something perhaps unexpected happens to bother the mother and the new_var coefficients. Describe what this is.\n\n\n\n\n\n\nANSWER\n\n\n\n\nGalton <- Galton %>% \n  mutate(new_var = 50*mother + 0*father)\nlm(height ~ mother + new_var, data=Galton) |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n3 new_var     NA     NA     NA    \n\n\nSince new_var and mother have R2=1, new_var provides no new information. R is programmed to recognize such cases (which are typically the result of a mistake by the modeler) and disregard the no-new-information variable. (This is indicated with NA.) With new_var no longer in the model, the mother coefficient returns to its value from the smaller of the nested models!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300Z",
    "section": "",
    "text": "Take-aways from the most recent class\n\n\n\nOn Math 300 Blog\n\n\n\n\n\n\n\n\n\nDate\nClass day\nTopic\nActivity\nSoln\nTake away\nClassroom video\n\n\n\n\nMar 3\nLesson 19 Notes:Reading\nVariation & variance\nMeasuring by eye\nSoln\nBlog-19\nvideo\n\n\nMar 7\nLesson 20 Notes:Reading\nDAGs & simulation\nLife savers?\nSoln\nBlog-20\nvideo\n\n\nMar 9\nLesson 21 Notes:Reading\nSignal & noise\nMembrane channels Handout\nSoln\nBlog-21\nvideo\n\n\nMar 13\nLesson 22 Notes:Reading\nSampling variation\nCounts and rates\nSoln\nBlog-22\nvideo\n\n\nMar 15\nLesson 23 Notes:Reading\nConfidence intervals\nGot you covered!\nSoln\nBlog-23\nvideo\n\n\nMar 17\nLesson 24 Notes:Reading\nEffect size\nWith respect to …\nSoln\nBlog-24\nvideo\n\n\nMar 21\nLesson 25 Notes:Reading\nPrediction mechanics\nTBA\nSoln\nBlog-25\nvideo\n\n\nMar 23\nLesson 26 Notes:Reading\nPrediction intervals\nIntervals by eye & Handout\nSoln\nBlog-26\nvideo\n\n\n\nSPRING BREAK\nKeep in mind that Problem Set 5\nwill be due April 6.\n\n\n\n\n\nApr 3\nLesson 28 Notes:Reading\nCovariates\nCovariates change coefficients\nSoln\nBlog-28\nvideo\n\n\nApr 5\nLesson 29 Notes:Reading\nCovariates eat variance Adjustment for covariates\nAdjusting activity\nSoln\nBlog-29\nvideo\n\n\n\nAPRIL 6: Problem Set 5 due.\nSee Z-section Problem Set 5 on posit.com\n\n\n\n\n\n\nApr 7\nLesson 30 Notes:Reading\nConfounding\nSimple causal paths\nSoln\nBlog-30\nvideo\n\n\nApr 11\nLesson 31 Notes:Reading\nSpurious correlation\nspurious correlations\nSoln\nBlog-31\nvideo\n\n\nApr 13\nLesson 32 Notes:Reading\nExperiment & random assignment\nlink\nSoln\nBlog-32\nvideo\n\n\nApr 17\nLesson 33 Notes:Reading\nMeasuring and accumulating risk\nAccumulating risk\nSoln\nBlog-33\nvideo\n\n\n\nAPRIL 19\nGR 3\n\n\n\n\n\n\nApr 24\nLesson 34 Notes:Reading\nConstructing a classifier\nTBA\nSoln\nBlog-34\nvideo\n\n\nApr 27\nLesson 35 Notes:Reading\nAccounting for prevalence\nlink\nSoln\nBlog-35\nvideo\n\n\nMay 1\nLesson 36 Notes:Reading\nHypothesis testing\nTBA\nSoln\nBlog-36\nvideo\n\n\nMay 3\nLesson 37 Notes:Reading\nCalculating a p-value\nTBA\nSoln\nBlog-37\nvideo\n\n\n\nMAY 4\nPS6 due\n\n\n\n\n\n\nMay 5\nLesson 38 Notes:Reading\nFalse discovery with hypothesis testing\nTBA\nSoln\nBlog-38\nvideo\n\n\nMay 9\nLesson 39 Notes:Reading\nREVIEW of lessons 28-38\nTBA\nSoln\nBlog-39\nvideo\n\n\nMay 11\nLesson 40 Notes:Reading\nReview of entire course\nN/A\nSoln\nBlog-40\nvideo\n\n\n\n\nMath 300Z is the prototype for scheduled revisions to Math 300. The revisions apply only to Lessons 19 and up; the first 18 lessons come from Math 300."
  },
  {
    "objectID": "Day-by-day/Lesson-34/Teaching-notes-34.html#in-draft",
    "href": "Day-by-day/Lesson-34/Teaching-notes-34.html#in-draft",
    "title": "Instructor Teaching Notes for Lesson 34",
    "section": "IN DRAFT",
    "text": "IN DRAFT\nTell the story of the Chinese spy balloon. After it was detected, the Air Force (according to news reports) increased the sensitivity of radars. This led to an increase in detection and a week-long rash of high-altitude detections, two of which were shot down. Eventually it was realized that there is a surprising amount of stuff floating around at high altitude. https://www.nytimes.com/live/2023/02/16/us/biden-china-balloon-ufo?smid=nytcore-ios-share&referringSource=articleShare"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Ruler-activity-handout.html",
    "href": "Day-by-day/Lesson-19/Ruler-activity-handout.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "In this activity, you will be given two strips of paper printed with:\n\nAn empty rectangular box\nA ruler\n\n\n\n\nWithout using the ruler at all, subdivide by eye the rectangular box into three equal-sized sections, like this:\n\n\n\n\n\n\n\nNow the ruler comes into play. Measure the lengths of your three subdivisions using the ruler.\nRecord your three measurements in this spreadsheet. Also create an ID for yourself, for example your initials or the initials of your favorite aunt, baseball player, or whatever.\n\n\n\nLink to a spreadsheet for data entry\n\nOnce everyone has entered their measurements, copy the following statement into the console in Posit.cloud and run it to create a data frame named Thirds.\n\n\nThirds <- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT_asFV5LD312bYaGgHK3F91kgLVSiaQpNhggDilfPKAiDBNz9iueOiYWKgAtRRwkFlOz6U9znbiMGK/pub?gid=0&single=true&output=csv\")\n\n\nFollow the instructions given in class. These will have you\n\nCalculate the variance of the three measurements for each student separately. Use mutate() to calculate modulus <- ((left-middle)^2 + (left-right)^2 + (middle-right)^2)/3, storing the result back in Thirds as a variable named modulus.\nAnalyze how good you and your colleagues are at sub-dividing evenly by eye.\nCreate a new dataframe that is Thirds re-arranged into “long” format.\n\n Long_form <- tidyr::pivot_longer(Thirds, !Student_initials, names_to = \"position\")\n\nGroup Long_form by student initials and calculate the variance of value. Compare these values to those stored under modulus in Thirds.\n\n\n\n\n\n\nWhy is it not useful, for the purposes of measuring the quality of the subdivision, to calculate the mean of the left, middle, and right segment lengths?\nWhat did you look for in Step (ii)?"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "",
    "text": "You have been learning some basics of data wrangling and visualization, along with what ModernDive calls “basic regression” and “multiple regression.” These are tools which you will continue to use in the second half of the semester.\nSuch tools are necessary but usually not sufficient. Data only occasionally speak for themselves. Most often, we need to interpret data in the context of what we already know or believe about the system under study.\nExample: As you’ve seen, merely fitting a regression model does not demonstrate that there is a causal relationship between the explanatory variables and the response variable. We will need some new concepts to encode our ideas (and speculations) about causal relationships and to use regression modeling to inform (or contradict) our ideas.\nExample: We’ll see how to avoid seeing patterns and relationships for which the evidence in unpersuasive. Example: We will see how detection thresholds can be set to reflect our opinions about the costs and benefits of different ways of getting it right or wrong and our prior knowledge of the frequency of different kinds of events."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#regression-models",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#regression-models",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Regression models",
    "text": "Regression models\nThe point of regression modeling is to detect and quantify patterns of relationship between variables. Sometimes simple data graphics are enough to display a pattern. For instance, let’s look at some Department of Transportation data on models of cars stored in the MPG data frame. We will start by looking at the link between fuel economy and CO_2_ production.\n\nggplot(MPG, aes(x = fuel_year, y = CO2_year)) +\n  geom_jitter(alpha=.3)\n\n\n\n\nThis is a very strong pattern. fuel_year and CO2_year are practically the same thing.\n\nWhy?\nWhy are there some points off of the straight line describing the large majority of points?\n\nOther times, patterns are hidden by extreme variability in the data. For instance, here are data on the effect of kindergarden class size on student outcomes.\n\nggplot(STAR, aes(x=classtype, y=g4math)) + geom_jitter() + geom_violin(alpha=.5, fill=\"blue\")\n\n\n\n\n\n\n\n\nRegression modeling is a technique for looking for simple forms of patterns in the relationships among variables. It is not the only such technique, but it is by far the most widely used in practice across diverse fields.\nWe will use only regression modeling (and allied methods) in this course. You may have heard about other methods such as “deep learning” or “neural networks,” but regression modeling is the basis for most of the others.\nIt’s critically important that you understand the framework for regression modeling.\n\nIn any one model, there is one variable that is identified as the response variable.\n\nThis identification depends on the purpose behind your work. You’ll learn it mostly by example.\n\nOther variables in the model are cast in the role of explanatory variables. There might be one explanatory variable or there might be other. There’s even a use for the case where there are no explanatory variables, but we don’t need to worry about that now.\nIn fitting a model to data (sometimes called “training a model on data”) the computer does the heavy lifting of finding a relationship between the response and explanatory variables that stays close to the data. Often, the “shape” of the relationship is very simple, e.g. a straight-line relationship or, more generally, a linear combination. That’s where the l comes in the function lm() that you will be using again and again in this course.\nIn using lm(), you specify which variables you want in the explanatory role and which single variable you have selected to be the response variable. The computer language syntax is very simple:\n\nresponse ~ var1 + var2 + ...\nThe name of the response variable is always on the left-hand side of the TILDE.\nThe explanatory variables are listed by name on the right side of the TILDE.\nThe + between the names of explanatory variables is mostly just punctuation. You can read it as “and”.\n\nThe TILDE character is usually just pronounced “tilde,” but English-language equivalents are\n\n“as explained by”\n“as accounted for by”\n“as modeled by”\n“versus”"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#data-graphics",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#data-graphics",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Data graphics",
    "text": "Data graphics\nSince the distinction between the response and the explanatory variables is so central, we are going to enforce a graphical style that reflects the distinction.\n\nThe response variable will always be on the vertical axis.\nOne of the explanatory variables will be on the horizontal axis.\nIf there is a second explanatory variable, we will use color.\nWhen we need a third or fourth explanatory variable, we will use faceting.\n\nIn the ggplot2 graphics system, this policy will appear like this:\nggplot(Dataframe, aes(y=response, x=var1, color=var2)) + geom_jitter() or geom_point() and so on."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#models",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#models",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Models",
    "text": "Models\nWe have been working a lot with data frames. Now we are going to add a new type of R object, which we can call a “model”. A model is NOT a data frame, it is a different kind of thing with different properties and different operations.\nMaking a model:\n\nmod1 <- lm(sat~ expend, data=SAT)\n\nOperations we will perform on models:\n\nGraph the model (and usually the data used for training)\n::: {.cell}\nmodel_plot(mod1)\n::: {.cell-output-display}  ::: :::\n\nBeing able to use multiple explanatory variables allows us to see patterns that may be subtle.\n\nmod2 <- lm(sat ~ expend + frac, data=SAT)\nmodel_plot(mod2)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\n\n\n\n\nModel summaries, especially conf_interval() and R2()\n\n\nmod2 |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef    .upr\n  <chr>        <dbl>  <dbl>   <dbl>\n1 (Intercept) 950.   994.   1038.  \n2 expend        3.79  12.3    20.8 \n3 frac         -3.28  -2.85   -2.42\n\nmod2 |> R2()\n\n   n k  Rsquared        F     adjR2 p df.num df.denom\n1 50 2 0.8194726 106.6741 0.8117906 0      2       47\n\n\n\nEvaluate the model at each of the rows of the training data.\n::: {.cell}\nmodel_eval(mod2) |> head()\n::: {.cell-output .cell-output-stderr} Using training data as input to model_eval(). :::\n::: {.cell-output .cell-output-stdout} .response expend frac   .output     .resid     .lwr      .upr  1      1029  4.405    8 1025.1463   3.853661 958.2677 1092.0250  2       934  8.963   47  969.9621 -35.962052 900.0065 1039.9176  3       944  4.778   27  975.5616 -31.561556 909.1283 1041.9948  4      1005  4.459    6 1031.5117 -26.511670 964.6071 1098.4162  5       902  4.992   45  926.8741 -24.874145 860.0437  993.7046  6       980  5.443   29  978.0302   1.969768 912.0035 1044.0570 ::: :::"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#todays-lesson",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#todays-lesson",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Today’s Lesson",
    "text": "Today’s Lesson\nA definition of “statistical thinking” from the book:\n\nStatistic thinking is the accounting for variation in the context of what remains unaccounted for.\n\nImplicit in this definition is a pathway for learning to think statistically:\n\nLearn how to measure variation;\nLearn how to account for variation;\nLearn how to measure what remains unaccounted for.\n\nToday: How to measure variation.\nConsider some closely related words: variable, variation, vary, various, variety, variant. The root is vari.\nOur preferred way to measure the amount of variation numerically: the variance, a single number, always positive.\n\nVariance always involves a single variable; it is about the variation in that variable.\nCalculate the variance is with var() within summarize() r DF |> summarize(NM = var(VAR))\nA good way to conceptualize the variance is as the average squared pairwise difference between values\nThe units of the variance are the square of the units of the variable.\nWhy square the pairwise differences? It’s a convention. Experience has shown this convention simplifies many operations we do with models. Underlying mathematics: Pythagorean theorem and formula for bell-shaped curve.\nOften people talk about the “standard deviation.” This is merely the square-root of the variance. But the variance is more fundamental mathematically. Using standard deviations introduces square roots in many calculations that don’t need to be there if we use variance."
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#activity",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#activity",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Activity",
    "text": "Activity"
  },
  {
    "objectID": "Day-by-day/Lesson-19/Teaching-notes-19.html#administration",
    "href": "Day-by-day/Lesson-19/Teaching-notes-19.html#administration",
    "title": "Instructor Teaching Notes for Lesson 19",
    "section": "Administration",
    "text": "Administration\n\nUse “300Z Section” under Teams.\nClone the Z-section project on posit.cloud. We’ll use this project for the rest of the semester.\nThere will be a “worksheet” almost every day.\n\nThe worksheet is in the form of an Rmd file. To access it, go into the Z-section project and give a command like this: get_lesson_worksheet(19) or whatever the lesson number is.\nAn effective way to prepare for a class is to look at the worksheet before class. Just read it and take note of what doesn’t make sense to you. That way you can be attentive to those things in class.\nComplete the worksheet after class.\nCome with unresolved questions about the worksheet for the next class or for EI.\n\nMost days there will also be a group activity.\nThere will be a couple of problem sets that will be graded.\nThere will be one GR about half-way through the rest of the semester. And a final GR."
  },
  {
    "objectID": "Day-by-day/Lesson-26/Intervals-by-eye.html#intervals-by-eye",
    "href": "Day-by-day/Lesson-26/Intervals-by-eye.html#intervals-by-eye",
    "title": "Spring 2023 Math 300Z",
    "section": "Intervals by eye",
    "text": "Intervals by eye\n\nIn this activity, you are given some point plots of data: y vs x. Your job is to\n\nsketch in an appropriate model fitted (by eye) to the data.\nadd a prediction band showing for each value of x what is the prediction interval\ntransform the prediction band into a confidence band.\n\nTIPS:\n\nThe fitted model will be a line or curve, or in the case of Model 3, two lines.\nThe bounds of the prediction band will be more-or-less parallel to the fitted model, but should include roughly 95% of the \\(n\\) points in the plot.\nThe confidence band is narrower than the prediction band by a factor of \\(1/\\sqrt{n}\\).\n\n\n\n\n\nModel 1: A straight-line model y ~ x\n\n\n\n\n\n\n\n\nModel 2: A sine-wave model, y ~ sin(x)\n\n\n\n\n\n\n\n\n\nModel 3: A function of two variables, y ~ x + group"
  },
  {
    "objectID": "Day-by-day/Lesson-26/Teaching-notes-26.html",
    "href": "Day-by-day/Lesson-26/Teaching-notes-26.html",
    "title": "Instructor Teaching Notes for Lesson 26",
    "section": "",
    "text": "We’re going to be talking about probability today.\nBasketball feat, solar system trajectories, terminal guidance\nDrone video\n360 hospitals, each with a 1% chance of needing a medicine/blood/etc in the interval before it expires."
  },
  {
    "objectID": "Day-by-day/Lesson-26/Teaching-notes-26.html#review-from-lesson-25",
    "href": "Day-by-day/Lesson-26/Teaching-notes-26.html#review-from-lesson-25",
    "title": "Instructor Teaching Notes for Lesson 26",
    "section": "Review from Lesson 25",
    "text": "Review from Lesson 25\n\nThe proper form for a prediction is a list of possible outcomes, each assigned a probability. Over the whole list, the probabilities must add up to 1. We call this list a probability distribution.\n\nFederal reserve forecasts\n\nThe modeling software produces a prediction interval, which is much wider than a confidence interval. Confidence interval length goes to zero as \\(n\\rightarrow\\infty\\), but prediction interval stays pretty much the same.\nFor confidence intervals, use 95% (the convention)."
  },
  {
    "objectID": "Day-by-day/Lesson-26/Teaching-notes-26.html#reverse-engineering-the-prediction-interval",
    "href": "Day-by-day/Lesson-26/Teaching-notes-26.html#reverse-engineering-the-prediction-interval",
    "title": "Instructor Teaching Notes for Lesson 26",
    "section": "Reverse engineering the prediction interval",
    "text": "Reverse engineering the prediction interval\nA prediction interval has a form like [15, 23] or, equivalently, 19 \\(\\pm\\) 4. This is not a probability distribution.\nHOWEVER, it is a shorthand for a distribution: called variously the normal or gaussian or bell-shaped distribution. - This distribution has two parameters: the mean and the standard deviation. - For the 19 \\(\\pm\\) 4 prediction interval, the mean is 19 and the standard deviation is 4/2.\n\n\n\n\n\nThe prediction interval covers the central 95% of the probability.\nEvents at the center are about 7 times more likely than at the ends of the prediction interval. About 2/3 of the probability is within \\(\\pm 1\\) standard deviation."
  },
  {
    "objectID": "Day-by-day/Lesson-26/Teaching-notes-26.html#why-a-probability-distribution-for-prediction",
    "href": "Day-by-day/Lesson-26/Teaching-notes-26.html#why-a-probability-distribution-for-prediction",
    "title": "Instructor Teaching Notes for Lesson 26",
    "section": "Why a probability distribution for prediction?",
    "text": "Why a probability distribution for prediction?\nTo help in making decisions. Example: Silicon Valley Bank is trying to decide what fraction of its assets to put in long-term government bonds.\nIf the interest rate is 2.5%, then a bond paying $1000 in 10-years time, if it is to be worthwhile should cost less: $780.\n\nIf the interest rate falls to 1.5%, then the bond is worth more: $860.\nIf the interest rate increases to 4.5%, then the bond is worth less: $644\n\nSVB made a bet on interest rates. This is closely related to prediction.\nBanks hire economists and other specialists to make predictions about things like interest rates. They use these predictions to estimate risk. For instance, if the bank’s assets fall in value by 15%, the bank will not have enough money on hand to pay depositors, leading to a run on the bank, ….\nImagine this prediction about interest rates one year after buying the government bonds at $780. (We aren’t going to worry about where such predictions come from. The point for us is to illustrate how the probability form of prediction helps in making decisions.)\n\n\n\n\n\n\n\n\n\n\nAccording to prediction model, the probability of the interest rate leading to a 80% decline (or more) is 1.9%."
  },
  {
    "objectID": "Day-by-day/Lesson-26/Teaching-notes-26.html#the-contest",
    "href": "Day-by-day/Lesson-26/Teaching-notes-26.html#the-contest",
    "title": "Instructor Teaching Notes for Lesson 26",
    "section": "The contest",
    "text": "The contest\n\n\n[1] 0.3396823 0.6603177"
  },
  {
    "objectID": "Day-by-day/Lesson-26/Teaching-notes-26.html#updating-a-probability-distribution",
    "href": "Day-by-day/Lesson-26/Teaching-notes-26.html#updating-a-probability-distribution",
    "title": "Instructor Teaching Notes for Lesson 26",
    "section": "Updating a probability distribution",
    "text": "Updating a probability distribution\nAs interest rates climbed from 2.5%, SVB ought to have revised its prediction of future interest rate and reduced the risk of catastrophic failure by selling off the troubled assets.\nBut how to update?\nThe correct procedure is called Bayesian updating. Here’s an example:\n See this Math 300Z blog entry"
  },
  {
    "objectID": "Day-by-day/Lesson-26/Teaching-notes-26.html#review-of-lesson-25",
    "href": "Day-by-day/Lesson-26/Teaching-notes-26.html#review-of-lesson-25",
    "title": "Instructor Teaching Notes for Lesson 26",
    "section": "Review of Lesson 25",
    "text": "Review of Lesson 25"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Patch-clamping.html#membrane-channels",
    "href": "Day-by-day/Lesson-21/Patch-clamping.html#membrane-channels",
    "title": "Spring 2023 Math 300Z",
    "section": "Membrane channels",
    "text": "Membrane channels\nElectrical signaling is one of the means of cell-to-cell communication in organisms. Familiar examples are nerve cells and muscle cells. The electrical activity is mediated by assemblies of a few proteins—called “membrane channels”—that penetrate the cell membrane and switch minute flows of electrical current on and off depending on conditions in the cell and the influence of neighboring cells.\nAmazingly, even though this activity involves only thousands of atoms, it is possible to record the on-again-off-again activity of a single channel. (The 1991 Nobel Prize in Physiology or Medicine was awarded to E. Neher and B. Sakmann for their invention of the measurement technique, which is now widely used in electrophysiology research.)\nThe figure shows 16 recordings of the activity of different channels. Each of the recordings is a combination of signal and noise.\nTASK: In three recordings of your choice,\n\nIdentify the signal by drawing it over the recording.\nMeasure the typical amplitude of the noise. For this purpose, use the guide at the lower-right corner of the figure: a short vertical line marks the amplitude of 5 pA, that is, 5 pico-Amps.\n\nTo identify the signal, you need to know something about what the signal looks like. In our work in Math 300Z, the “signal” corresponds to a relationship between two variables or more. Here, one of the variables is time, the other is electrical current. Usually in Math 300Z we will be interested in “linear” relationships between the variables and we seek to identify the signal using only that limited piece of information. To define the signal in the single-channel recordings, use this information: the channels open and close to current, occasionally staying open (or closed) for the better part of a second, but also opening (or closing) for a much shorter time, say 0.01 second.\n\n\n\n\n\nSource of image: Kawano, R., Tsuji, Y., Sato, K. et al. (2013) “Automated Parallel Recordings of Topologically Identified Single Ion Channels”. Scienfic Reports **3(#1995) https://doi.org/10.1038/srep01995"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "",
    "text": "We think about data with multiple variables as depicting a system: an interconnected network of components. Some components appear as variables in the data frame; other components may be unmeasured but given a name for reference.\nUse “directed acyclic graphs” (DAGs) to draw a picture of the system. Each system component is a node of the DAG. When one component has a causal connection with another, an arrow is drawn between those nodes.\nBy analysis of the DAG—using techniques we haven’t covered yet—you can figure out which variables to include in your model.\nMany DAGs are provided with the {math300} package, with names like dag01 through dag12. Here’s an example:\n\n\nprint(dag03)\n\ng ~ exo()\nx ~ 1 * g + exo()\ny ~ 1 * g + exo()\n\ndag_draw(dag03)\n\n\n\n\nNotice that there is no direct flow between nodes x and y. Still, there is an indirect connection: node g influences both x and y.\n\nWe can collect a sample from a DAG, for instance:\n\n\nMy_data <- sample(dag03, size=10000)\nMy_data |> head(3)\n\n\n\n \n  \n    g \n    x \n    y \n  \n \n\n  \n    -0.1865535 \n    0.0794728 \n    0.7030961 \n  \n  \n    -0.9885917 \n    -1.8180242 \n    -1.9217289 \n  \n  \n    -1.5255245 \n    -0.9124580 \n    -1.6759256 \n  \n\n\n\n\n\nDepending on the structure of the DAG, different model specifications will reveal different aspects of the DAG. For instance,\n\n\nmod1 <- lm(y ~ x, data=My_data)\nmod1 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0381158 \n    -0.0142664 \n    0.0095830 \n  \n  \n    x \n    0.4945608 \n    0.5113660 \n    0.5281712 \n  \n\n\n\n\nwill show if there is any kind of connection between x and y. But another specification will, in this case, show that the connection from x to y is via the connection provided by node g.\n\nmod2 <- lm(y ~ x + g, data=My_data)\nmod2 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0375176 \n    -0.017935 \n    0.0016477 \n  \n  \n    x \n    -0.0012563 \n    0.018331 \n    0.0379183 \n  \n  \n    g \n    0.9597569 \n    0.987604 \n    1.0154511 \n  \n\n\n\n\nWe can look at the coefficients to see that in mod1 there is a non-zero connection between y and x, but in mod2 there is no non-zero connection of x on y.\n\nmod1 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.0381 \n    -0.0143 \n    0.00958 \n  \n  \n    x \n    0.4950 \n    0.5110 \n    0.52800 \n  \n\n\n\nmod2 |> conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .coef \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.03750 \n    -0.0179 \n    0.00165 \n  \n  \n    x \n    -0.00126 \n    0.0183 \n    0.03790 \n  \n  \n    g \n    0.96000 \n    0.9880 \n    1.02000 \n  \n\n\n\n\nOne way we will use DAGs to help us learn statistics is to compare the coefficients of models to the (known) mechanism of the DAG. We can see, for instance, that the g coefficient on y is 1 and the x coefficient is zero. Only mod2 reveals this.\n\nThe sampling and analysis in points (4) and (5) are an example of a “random trial” or “simulation.” We will use random trials to look at the properties of models fit to samples, especially with an eye to understanding the role of the sample size \\(n\\)."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#mathematical-functions-through-data",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#mathematical-functions-through-data",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Mathematical functions through data",
    "text": "Mathematical functions through data\nLet’s collect a small (\\(n=10\\)) sample from dag01:\n\nset.seed(103)\nSmall <- sample(dag01, size=10)\nhead(Small, 3)\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.7860 \n    1.89 \n  \n  \n    0.0547 \n    4.12 \n  \n  \n    -1.1700 \n    2.36 \n  \n\n\n\n\nWe can easily plot the data points. Less obviously, we can find any number of mathematical functions that are consistent with the data.\n\n\n\n\n\nFigure 1: Three of the infinite number of functions that can be drawn through the data ?@tbl-small-dag01.\n\n\n\n\n\nWhat don’t you like about these functions? Why do they insult your intuition?\n\nThey show details that are in no way suggested by the data.\nIf we were to collect more data, the function shapes could be entirely different. (Go back and change the random seed used for sampling from dag01.)"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#close-but-not-on-the-data",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#close-but-not-on-the-data",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "“Close” but not “on” the data",
    "text": "“Close” but not “on” the data\nIn general, we don’t insist that model functions go exactly through the data points. Instead, we imagine that the response variable involves some random noise that we don’t need to “capture” with our model. Doing things this way lets us fit model functions that are much simpler in shape, like this one:\n\n\n\n\n\nFigure 2: The straight-line function (blue) that goes through the data points as closely as possible. The noise is estimated as the difference (red for negative noise, black for positive noise) between the actual data points and the function.\n\n\n\n\nConstructing such a model divides the “explanation” of the response variable values into two parts:\n\nModel values, that is, the output of the model function (blue) at each of the values of the explanatory variable(s).\nWhat’s left over, the residuals, the vertical deviation of the actual response value from the model value.\n\nThe signal is the model values. The noise is the residuals.\nWhen we “fit” (or, “train”) a model, we take an aggressive stance. We look for the particular function of the shape implied by the model specification that will produce the smallest residuals. As usual, we measure the size of a residual by its square."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#activity-identifying-signal",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#activity-identifying-signal",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Activity: Identifying signal",
    "text": "Activity: Identifying signal"
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#five-six-simple-models",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#five-six-simple-models",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Five Six simple models",
    "text": "Five Six simple models\nModels can have any number of explanatory variables. In Math 300, we will be mainly concerned with models with a single explanatory variable or with two explanatory variable. Since an explanatory variable can be either categorical or quantitative, there are six “shapes” of models:\nSingle explanatory variable\n\nCategorical explanatory variable.\nQuantitative explanatory variable.\n\nTwo explanatory variables\n\nCategorical & Categorical\nCategorical & Quantitative\nQuantitative & Quantitative\n\nSometimes, we will use models with Zero explanatory variables\n\nNo explanatory variables.\n\nGraphs of (i) through (v), with (iv) shown in two different modes."
  },
  {
    "objectID": "Day-by-day/Lesson-21/Teaching-notes-21.html#measuring-signal-and-noise",
    "href": "Day-by-day/Lesson-21/Teaching-notes-21.html#measuring-signal-and-noise",
    "title": "Instructor Teaching Notes for Lesson 21",
    "section": "Measuring signal and noise",
    "text": "Measuring signal and noise\nWe depict the “size” of the signal as the amount of variability in the model values. As always, we measure variability using the variance.\nSimilarly, the “size” of the noise is the amount of variability in the residuals.\nThe model_eval() function is convenient for figuring out the model value and the residual for each row in the training data.\n\n\nThe training data\n\nSmall\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.790 \n    1.90 \n  \n  \n    0.055 \n    4.10 \n  \n  \n    -1.200 \n    2.40 \n  \n  \n    -0.170 \n    6.30 \n  \n  \n    -1.900 \n    0.93 \n  \n  \n    -0.120 \n    2.90 \n  \n  \n    0.830 \n    5.70 \n  \n  \n    1.200 \n    5.90 \n  \n  \n    -1.100 \n    2.10 \n  \n  \n    -0.380 \n    4.20 \n  \n\n\n\n\n\n\n\n\n\nOutput of model_eval()\n\nPts <- model_eval(mod)\n\n\n\n\n\n \n  \n    .response \n    x \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    1.90 \n    -0.790 \n    2.9 \n    -1.0000 \n    0.37 \n    5.4 \n  \n  \n    4.10 \n    0.055 \n    4.4 \n    -0.2400 \n    1.80 \n    6.9 \n  \n  \n    2.40 \n    -1.200 \n    2.2 \n    0.1400 \n    -0.38 \n    4.8 \n  \n  \n    6.30 \n    -0.170 \n    4.0 \n    2.4000 \n    1.50 \n    6.5 \n  \n  \n    0.93 \n    -1.900 \n    1.0 \n    -0.0810 \n    -1.80 \n    3.8 \n  \n  \n    2.90 \n    -0.120 \n    4.1 \n    -1.1000 \n    1.50 \n    6.6 \n  \n  \n    5.70 \n    0.830 \n    5.7 \n    -0.0033 \n    3.00 \n    8.4 \n  \n  \n    5.90 \n    1.200 \n    6.3 \n    -0.4400 \n    3.50 \n    9.2 \n  \n  \n    2.10 \n    -1.100 \n    2.4 \n    -0.2300 \n    -0.22 \n    4.9 \n  \n  \n    4.20 \n    -0.380 \n    3.6 \n    0.6200 \n    1.10 \n    6.1 \n  \n\n\n\n\n\n\nHow big is …\n\nThe response variable.\nThe signal.\nThe noise.\n\n\nPts |> summarize(i. = var(.response),\n                 ii. = var(.output),\n                 iii. = var(.resid)) \n\n\n\n \n  \n    i. \n    ii. \n    iii. \n  \n \n\n  \n    3.55 \n    2.6 \n    0.949 \n  \n\n\n\n\n\n\n\n\n\n\nSomething special about the variance\n\n\n\nFor every lm() model you build, the variance of the response variable is exactly equal to the sum of the variance of the model values and the variance of the residuals.\nIn other words, lm() splits the response variable into two parts: the sum of those parts equals the whole.\nR2 is the variance of the model values divided by the variance of the response variable.\n\nmod |> R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n    p \n    df.num \n    df.denom \n  \n \n\n  \n    10 \n    1 \n    0.733 \n    21.9 \n    0.699 \n    0.000863 \n    1 \n    8 \n  \n\n\n\nPts |> \n  summarize(i. = var(.response),\n            ii. = var(.output)) |>\n  mutate(R2 = ii. / i. )\n\n\n\n \n  \n    i. \n    ii. \n    R2 \n  \n \n\n  \n    3.55 \n    2.6 \n    0.733"
  },
  {
    "objectID": "Day-by-day/Lesson-28/covariate-confusion.html",
    "href": "Day-by-day/Lesson-28/covariate-confusion.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "This activity is to investigate how adding a covariate to a model can change the coefficient on another explanatory variable. The basic logic of the activity is to construct a series of pairs of nested models and examine the confidence interval on the coefficient of an explanatory variable.\nYou will use the Anthro_F data frame, which records measurements made on 184 women of body shape: knee circumference, ankle circumference, and so on. Since people have similar body shapes, regardless of size, these measurements tend to be correlated with one another.\nThe models you will build will have as BFat as the response variable. BFat is the measured proportion of body weight that is fat tissue. As an example of a nested pair of models, consider:\n\nBFat ~ Knee\nBFat ~ Knee + Ankle\n\nModel (i) is “nested” in model (ii) because model (ii) includes all the variables in model (i). (Nested models always have the same response variable.)\nYour analysis of each pair will compare the confidence interval on the explanatory variable in the smaller model to that same variable in the larger model, e.g.\n\nlm(Height ~ Knee, data=Anthro_F) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) 1.31    1.44    1.57   \n2 Knee        0.00275 0.00636 0.00996\n\nlm(Height ~ Knee + Ankle, data=Anthro_F) |> conf_interval()\n\n# A tibble: 3 × 4\n  term              .lwr   .coef    .upr\n  <chr>            <dbl>   <dbl>   <dbl>\n1 (Intercept)  1.25      1.40    1.55   \n2 Knee         0.0000406 0.00465 0.00925\n3 Ankle       -0.00325   0.00479 0.0128 \n\n\nWorking with your group partners, try to find pairs of explanatory variables such that the coefficient on one variable changes greatly when the covariate is added to the model.\nTo help, here is the correlation between many pairs of variables presented in the form of an angle in degrees. (See background section below.) Every variable has a angle 0 with itself. Small angles mean the two variables are closely aligned, large angles mean they are not.\n\n\n\nCode\ncorrs <- Anthro_F |>\n  select(Neck, Chest, Calf, Biceps, Hips, Waist, PThigh, MThigh, DThigh, Forearm, Wrist, Knee, Elbow, Ankle, Age) |>\n  cor() \nround(180*acos(corrs)/pi) \n\n\n        Neck Chest Calf Biceps Hips Waist PThigh MThigh DThigh Forearm Wrist Knee Elbow Ankle Age\nNeck       0    53   63     49   52    47     51     53     58      47    51   60    53    58  95\nChest     53     0   71     55   58    52     55     58     67      55    62   67    56    71  90\nCalf      63    71    0     65   63    64     60     59     60      58    63   62    63    58  89\nBiceps    49    55   65      0   45    41     42     43     52      34    45   54    43    60  93\nHips      52    58   63     45    0    40     24     38     48      44    51   42    46    50  90\nWaist     47    52   64     41   40     0     40     48     57      44    49   52    46    58  95\nPThigh    51    55   60     42   24    40      0     27     43      42    49   40    45    50  92\nMThigh    53    58   59     43   38    48     27      0     43      43    47   48    47    50  92\nDThigh    58    67   60     52   48    57     43     43      0      50    55   49    55    53  92\nForearm   47    55   58     34   44    44     42     43     50       0    34   50    39    49  93\nWrist     51    62   63     45   51    49     49     47     55      34     0   54    45    48  94\nKnee      60    67   62     54   42    52     40     48     49      50    54    0    54    51  97\nElbow     53    56   63     43   46    46     45     47     55      39    45   54     0    52  95\nAnkle     58    71   58     60   50    58     50     50     53      49    48   51    52     0  96\nAge       95    90   89     93   90    95     92     92     92      93    94   97    95    96   0\n\n\n\nTASK: Pick several pairs of variables, some related by small angles and some with large angles. For each pair, compare the first variable’s coefficient between the nested models.\nAs a group, find the biggest change you observed in the coefficient when adding the covariate to the model? (You’ll have to agree on a way to measure change in the coefficient.) Do large or small angles tend to produce bigger changes?"
  },
  {
    "objectID": "Day-by-day/Lesson-28/covariate-confusion.html#background-r-r2-and-the-angle-between-variables",
    "href": "Day-by-day/Lesson-28/covariate-confusion.html#background-r-r2-and-the-angle-between-variables",
    "title": "Spring 2023 Math 300Z",
    "section": "Background: r, R2, and the “Angle” between variables",
    "text": "Background: r, R2, and the “Angle” between variables\nSince Francis Galton’s invention/discovery of the correlation coefficient in 1888, it has been the standard introduction to measuring the relationship between two quantitative variables. It has even entered the vocabulary of everyday English as “a mutual relationship or connection between two or more things.” (Oxford Dictionaries)\nAlso in 1888, the phenomenon of electromagnetic waves was discovered by physicist Heinrich Hertz. These had been theoretically predicted in 1865 by James Clerk Maxwell. The mathematics of Maxwell’s representation of electromagnetism was very difficult. Consequently, physicists and mathematicians worked to create a simpler formalism. This eventually emerged in the university-level curriculum as two courses: vector calculus (usually called Calc III) and linear algebra. Naturally, in 1888, Galton was unaware of these developments. Nonetheless, vectors and linear algebra provide a great simplification of the concept of correlation.\nAny quantitative variable—a series of numbers—is also consequently a “vector,” which you can think of as an arrow pointing in a particular direction. The correlation coefficient between two variables amounts to the cosine of the angle between the two vectors. When the angle is very small, the variables are strongly aligned. When the angle is near 90\\(^\\circ\\), the two variables are not at all aligned.\nIn R, a standard way of calculating the correlation coefficient uses cor() as in this example:\n\nGalton |> summarize(correlation = cor(height, mother))\n\n  correlation\n1   0.2016549\n\n\nThe translation of the correlation coefficient into an angle (in degrees) involves some trigonometry (which is not a topic of Math 300):\n\nacos(0.202)*180/pi\n\n[1] 78.34606\n\n\n78 degrees is pretty close to a right angle, meaning that height and mother are barely aligned.\n\n\n\n\n\n\nAside: R2 and r\n\n\n\nWe have not emphasized the correlation coefficient r in Math 300 because r is descriptive only of the (linear) relationship between two variables. In Math 300, we are often using multiple explanatory variables and r does not apply. Instead, we use R2: a much more general description of the relationship between a response variable and explanatory variables.\nIn the case where a model has only one explanatory variable, e.g., height ~ mother, R2 has a simple relationship to r, namely, r2 = R2. This use of lower-case (r) and upper-case (R) can be confusing, so we are not using r much in Math 300.\nTo demonstrate the relationship between r and R2, consider:\n\nlm(height ~ mother, data=Galton) |> R2()\n\n    n k  Rsquared        F      adjR2            p df.num df.denom\n1 898 1 0.0406647 37.98001 0.03959401 1.078142e-09      1      896\n\n\nSince r between height and mother was 0.2016, you can confirm that R2 from the model is exactly r2."
  },
  {
    "objectID": "Day-by-day/Lesson-28/Teaching-notes-28.html",
    "href": "Day-by-day/Lesson-28/Teaching-notes-28.html",
    "title": "Instructor Teaching Notes for Lesson 28",
    "section": "",
    "text": "Critical thinking\nTwo crucial words will be introduced an elaborated upon this week (Lessons 28-30).\nYou already know what a covariate is, even if not formally.\nPolitical spending\nComparative health in Mexico and US\nComparing hospitals"
  },
  {
    "objectID": "Day-by-day/Lesson-28/Teaching-notes-28.html#background-review",
    "href": "Day-by-day/Lesson-28/Teaching-notes-28.html#background-review",
    "title": "Instructor Teaching Notes for Lesson 28",
    "section": "Background review",
    "text": "Background review\nThe fundamental framework that we use over and over again in this course involves:\n\nA data frame holding variables of interest.\nA model specification which\n\na response variable (always quantitative) which we’ll write generically as y\nzero or more explanatory variables\n\ny ~ 1\ny ~ 1 + x (usually written as the shorthand y ~ x)\ny ~ 1 + x + z (with potentially more explanatory variables)\n\n\nTraining the model (also called fitting) to produce coefficients.\n\nFor the “intercept” (that is, the 1 term) there is one coefficient.\nFor each quantitative explanatory variable there is one coefficient.\nFor any categorical explanatory variable with k levels, there are k-1 coefficients.\nWhen a model includes “interactions” (as signified by using * rather than + in the model specification), there are additional coefficients. But we are not emphasizing such models in Math 300."
  },
  {
    "objectID": "Day-by-day/Lesson-28/Teaching-notes-28.html#example-life-expectancy",
    "href": "Day-by-day/Lesson-28/Teaching-notes-28.html#example-life-expectancy",
    "title": "Instructor Teaching Notes for Lesson 28",
    "section": "Example: Life expectancy",
    "text": "Example: Life expectancy\nUsing the gapminder::gapminder data.\n\n\n\nAre life-expectancy (at birth) and wealth (measured by GDP) related?\n\nggplot(gapminder, aes(x=gdp, y=lifeExp)) + \n  geom_point() \n\n\n\n\nWhat do you like or dislike about the above graph?\n\nggplot(gapminder, aes(x=gdp, y=lifeExp)) + \n  geom_point() +\n  scale_x_log10()\n\n\n\n\nCompare these two models:\n\nlm(lifeExp ~ gdp, data=gapminder) |> R2()\n\n     n k  Rsquared        F      adjR2 p df.num df.denom\n1 1704 1 0.0667536 121.7413 0.06620528 0      1     1702\n\nlm(lifeExp ~ gdp + year, data=gapminder) |> R2()\n\n     n k  Rsquared        F     adjR2 p df.num df.denom\n1 1704 2 0.2279324 251.0875 0.2270246 0      2     1701\n\n\nyear is a covariate. We want to do the comparison holding year constant.\n\nlm(lifeExp ~ log(I(gdp/pop)), data=gapminder |> filter(year == 2007)) |> R2()\n\n    n k Rsquared        F     adjR2 p df.num df.denom\n1 142 1 0.654449 265.1501 0.6519808 0      1      140\n\n\nDiscuss whether gdp is the right variable to look at to measure wealth.\n\nlog(gdp) ?\nAdjusting for population size\n\n\n“Intensive” vs “extensive” variables\n\ntemperature (intensive)\npressure (intensive)\nmass (extensive)\nheat capacity (extensive)\nlife expectancy (intensive)\nGDP (extensive)\nPopulation (extensive)\n\nTake care when mixing together intensive and extensive variables in a model.\n\nggplot(gapminder, aes(x=gdpPercap, y=lifeExp, color=country)) + \n  geom_point() + \n  scale_x_log10() + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Day-by-day/Lesson-28/Teaching-notes-28.html#covariates-can-change-coefficients",
    "href": "Day-by-day/Lesson-28/Teaching-notes-28.html#covariates-can-change-coefficients",
    "title": "Instructor Teaching Notes for Lesson 28",
    "section": "Covariates can change coefficients",
    "text": "Covariates can change coefficients\nPredict when this will happen.\nCorrelation coefficient as angle."
  },
  {
    "objectID": "Day-by-day/Lesson-28/Teaching-notes-28.html#in-class-activity",
    "href": "Day-by-day/Lesson-28/Teaching-notes-28.html#in-class-activity",
    "title": "Instructor Teaching Notes for Lesson 28",
    "section": "In-class activity",
    "text": "In-class activity\nSee when adding a covariate changes the coefficients. 1. Look for maximally and minimally correlated variable pairs in Anthro_F 2. Fit two nested models for BFat, one with a single explanatory variable from the pair and the other with both variables from the pair. 3. Repeat using Height just to show that it’s the explanatory variables that are determining the shift.\n\nlm(BFat ~ Hips, data=Anthro_F) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) -53.4   -44.0   -34.6  \n2 Hips          0.581   0.678   0.776\n\nlm(BFat ~ Hips + PThigh, data=Anthro_F) |> conf_interval() \n\n# A tibble: 3 × 4\n  term            .lwr   .coef    .upr\n  <chr>          <dbl>   <dbl>   <dbl>\n1 (Intercept) -49.8    -40.5   -31.1  \n2 Hips          0.0829   0.311   0.539\n3 PThigh        0.243    0.559   0.874\n\n\nAnthro_F |> summarize(cor(Wrist, Waist))\n# A tibble: 1 × 1\n  `cor(Wrist, Waist)`\n                <dbl>\n1               0.660\n> Anthro_F |> summarize(cor(Wrist, Biceps))\n# A tibble: 1 × 1\n  `cor(Wrist, Biceps)`\n                 <dbl>\n1                0.705\n> Anthro_F |> summarize(cor(Wrist, Age))\n# A tibble: 1 × 1\n  `cor(Wrist, Age)`\n              <dbl>\n1           -0.0748\n> lm(BFat ~ Wrist, data=Anthro_F) |> conf_interval()\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) -38.8  -26.4  -14.1 \n2 Wrist         2.29   3.08   3.87\n> lm(BFat ~ Wrist + Age, data=Anthro_F) |> conf_interval()\n# A tibble: 3 × 4\n  term           .lwr    .coef    .upr\n  <chr>         <dbl>    <dbl>   <dbl>\n1 (Intercept) -39.9   -25.1    -10.4  \n2 Wrist         2.28    3.07     3.86 \n3 Age          -0.408  -0.0575   0.293\n> lm(BFat ~ Wrist + Knee, data=Anthro_F) |> conf_interval()\n# A tibble: 3 × 4\n  term           .lwr  .coef   .upr\n  <chr>         <dbl>  <dbl>  <dbl>\n1 (Intercept) -54.8   -43.1  -31.3 \n2 Wrist         0.339   1.20   2.06\n3 Knee          0.943   1.29   1.64"
  },
  {
    "objectID": "Day-by-day/Lesson-28/Teaching-notes-28.html#simpsons-paradox",
    "href": "Day-by-day/Lesson-28/Teaching-notes-28.html#simpsons-paradox",
    "title": "Instructor Teaching Notes for Lesson 28",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\nlm(zero_one(admit, one=\"admitted\") ~ gender, data = UCB_applicants) |> conf_interval()\n\n# A tibble: 2 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept) 0.281 0.304 0.326\n2 gendermale  0.113 0.142 0.170\n\nlm(zero_one(admit, one=\"admitted\") ~ gender + dept, data = UCB_applicants) |>\n  conf_interval()\n\n# A tibble: 7 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept)  0.621   0.660   0.699 \n2 gendermale  -0.0485 -0.0184  0.0117\n3 deptB       -0.0563 -0.0103  0.0356\n4 deptC       -0.347  -0.303  -0.260 \n5 deptD       -0.354  -0.311  -0.268 \n6 deptE       -0.452  -0.403  -0.354 \n7 deptF       -0.631  -0.586  -0.542"
  },
  {
    "objectID": "Day-by-day/Lesson-28/Teaching-notes-28.html#review-of-lesson-27",
    "href": "Day-by-day/Lesson-28/Teaching-notes-28.html#review-of-lesson-27",
    "title": "Instructor Teaching Notes for Lesson 28",
    "section": "Review of Lesson 27",
    "text": "Review of Lesson 27"
  },
  {
    "objectID": "Day-by-day/Lesson-29/Teaching-notes-29.html",
    "href": "Day-by-day/Lesson-29/Teaching-notes-29.html",
    "title": "Instructor Teaching Notes for Lesson 29",
    "section": "",
    "text": "See Takeaways from Lesson 28 for a review of the last class. There, we introduced the term “covariate” to describe an explanatory variable that isn’t of direct interest but which might be important to how the system being modeled works.\nAdjusting for severity …\nThe effect size w.r.t. an explanatory variable is automatically adjusted for all the covariates."
  },
  {
    "objectID": "Day-by-day/Lesson-29/Teaching-notes-29.html#class-activity",
    "href": "Day-by-day/Lesson-29/Teaching-notes-29.html#class-activity",
    "title": "Instructor Teaching Notes for Lesson 29",
    "section": "Class activity",
    "text": "Class activity"
  },
  {
    "objectID": "Day-by-day/Lesson-29/Teaching-notes-29.html#adjusting-for-age",
    "href": "Day-by-day/Lesson-29/Teaching-notes-29.html#adjusting-for-age",
    "title": "Instructor Teaching Notes for Lesson 29",
    "section": "Adjusting for age",
    "text": "Adjusting for age\n“Life tables” are compiled by governments from death certificates.\n\nLTraw <- readr::read_csv(\"life-table-raw.csv\")\n\nRows: 120 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): age, male, mnum, mlife_exp, female, fnum, flife_exp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(LTraw)\n\n# A tibble: 6 × 7\n    age     male   mnum mlife_exp   female   fnum flife_exp\n  <dbl>    <dbl>  <dbl>     <dbl>    <dbl>  <dbl>     <dbl>\n1     0 0.00584  100000      74.1 0.00491  100000      79.8\n2     1 0.00041   99416      73.6 0.000316  99509      79.2\n3     2 0.000254  99376      72.6 0.000196  99478      78.2\n4     3 0.000207  99350      71.6 0.00016   99458      77.2\n5     4 0.000167  99330      70.6 0.000129  99442      76.2\n6     5 0.000141  99313      69.6 0.000109  99430      75.2\n\n\nWrangling to a more convenient format (for our purposes):\n\nLT <- tidyr::pivot_longer(LTraw |> select(age, male, female), c(\"male\", \"female\"), names_to=\"sex\", values_to=\"mortality\")\nLT\n\n# A tibble: 240 × 3\n     age sex    mortality\n   <dbl> <chr>      <dbl>\n 1     0 male    0.00584 \n 2     0 female  0.00491 \n 3     1 male    0.00041 \n 4     1 female  0.000316\n 5     2 male    0.000254\n 6     2 female  0.000196\n 7     3 male    0.000207\n 8     3 female  0.00016 \n 9     4 male    0.000167\n10     4 female  0.000129\n# … with 230 more rows\n\n\nAge pyramids comparing the US population in 1972 and 2021\n\n\n\n\n\nQuestions:\n\nWhen were people aged 35-39 in 1972 born? Why are there so few of them?\nHow old would you have to be in 1972 to be part of the “baby boom?” Can you see the echo of the baby boom in 2021?\nHow many 85+ year-olds will there be in 2040?\n\nThe raw data:\n\n\nCode\nPop2020 <- readr::read_csv(\"nc-est2021-agesex-res.csv\",\n                           show_col_types=FALSE) |>\n  filter(SEX > 0, AGE<999) |>\n  mutate(sex = ifelse(SEX==1, \"female\", \"male\"), \n         age=AGE, pop=ESTIMATESBASE2020) |> \n  select(age, sex, pop)\n\n\n\nPop2020 |> tail()\n\n# A tibble: 6 × 3\n    age sex      pop\n  <dbl> <chr>  <dbl>\n1    95 male  132299\n2    96 male  105435\n3    97 male   79773\n4    98 male   57655\n5    99 male   43072\n6   100 male   78474\n\n\nUS mortality at actual age distribution involves joining the data from these two data frames.\n\nOverall <- Pop2020 |> left_join(LT)\n\nJoining, by = c(\"age\", \"sex\")\n\nhead(Overall)\n\n# A tibble: 6 × 4\n    age sex        pop mortality\n  <dbl> <chr>    <dbl>     <dbl>\n1     0 female 1907982  0.00491 \n2     1 female 1928926  0.000316\n3     2 female 1980392  0.000196\n4     3 female 2028781  0.00016 \n5     4 female 2068682  0.000129\n6     5 female 2081588  0.000109\n\n\nThe calculation is simple wrangling:\n\nOverall |> group_by(sex) |>\n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop))\n\n# A tibble: 2 × 2\n  sex    mortality\n  <chr>      <dbl>\n1 female      708.\n2 male       1351.\n\n\nThe WHO standard age distribution\n\n\nCode\nRaw <- readr::read_csv(\"who-standard-age-distribution.csv\",\n                            show_col_types=FALSE)\nTmp <- Raw |> mutate(mid = (high+low)/2)\npopfun <- approxfun(Tmp$mid, Tmp$pop, yleft=8.860, yright=0.04)\nStandard <- tibble(\n  age = 0:99,\n  pop = popfun(age)\n)\nggplot(Standard, aes(xmin=-pop, xmax=pop, y=age)) + geom_ribbon(alpha=.3, aes(xmin=0), fill=\"green\") + \n  geom_ribbon(alpha=.3, aes(xmax=0), fill=\"blue\")  \n\n\n\n\n\nUS mortality at WHO standard age distribution:\n\nOverall <- Standard |> left_join(LT)\n\nJoining, by = \"age\"\n\nOverall |> group_by(sex) |>\n  summarize(mortality = 100000*sum(pop*mortality)/sum(pop))\n\n# A tibble: 2 × 2\n  sex    mortality\n  <chr>      <dbl>\n1 female      439.\n2 male        681."
  },
  {
    "objectID": "Day-by-day/Lesson-29/Teaching-notes-29.html#age-adjusted-death-rates-over-time",
    "href": "Day-by-day/Lesson-29/Teaching-notes-29.html#age-adjusted-death-rates-over-time",
    "title": "Instructor Teaching Notes for Lesson 29",
    "section": "Age-adjusted death rates over time",
    "text": "Age-adjusted death rates over time\nFrom the SSA (p. 15)"
  },
  {
    "objectID": "Day-by-day/Lesson-29/adjusting-activity.html#adjusting-visually",
    "href": "Day-by-day/Lesson-29/adjusting-activity.html#adjusting-visually",
    "title": "Spring 2023 Math 300Z",
    "section": "Adjusting, visually",
    "text": "Adjusting, visually\nThese are two graphs of the data from Clock_auction showing the relationship between the winning price and the number of bidders. (I’ve simplified the number of bidders to two categories.) The age of the clock is a covariate. The large dots show the mean age and mean price of the clocks in those auctions with 10 or more bidders versus 9 or fewer bidders.\n\n\nCode\nClock_auction <- Clock_auction |> mutate(nbidders = ifelse(bidders >= 10, \"10 or more\", \"9 or fewer\"))\nStats <- Clock_auction |> group_by(nbidders) |>\n  summarize(mp = mean(price), mage = mean(age))\n\n\n\nmod1 <- lm(price ~ nbidders, data=Clock_auction) \n\nmodel_plot(mod1, x=age, color=nbidders) |>\n  gf_point(mp ~ mage, color=~nbidders, data=Stats, size=3)\n\n\n\n\nPart A. In the model without age as a covariate, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\nPart B. Now the picture when including age as a covariate. Adjusting for age, what is the difference in mean prices for the 10-or-more-bidders group versus the 9-or-fewer-bidders group?\n\nmod2 <- lm(price ~ nbidders + age, data=Clock_auction) \n\nmodel_plot(mod2, x=age, color=nbidders) |>\n  gf_point(mp ~ mage, color=~nbidders, data=Stats, size=3)\n\n\n\n\nPart C. Here are confidence intervals for the two models graphed above. Explain what about these coefficients matches the conclusions you got in Parts (A) and (B)?\n\nmod1 |> conf_interval()\n\n# A tibble: 2 × 4\n  term                .lwr .coef   .upr\n  <chr>              <dbl> <dbl>  <dbl>\n1 (Intercept)        1226. 1420  1614. \n2 nbidders9 or fewer -479. -221.   37.1\n\nmod2 |> conf_interval()\n\n# A tibble: 3 × 4\n  term                  .lwr  .coef   .upr\n  <chr>                <dbl>  <dbl>  <dbl>\n1 (Intercept)        -467.    -56.3  354. \n2 nbidders9 or fewer -490.   -336.  -182. \n3 age                   7.79   10.6   13.5\n\n\n\nThis activity was inspired by schematic diagrams in Milo Schield’s Statistical Literacy: Seeing the story behind the statistics, 2011, pp. 224-5."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Life-savers.html#saving-lives",
    "href": "Day-by-day/Lesson-20/Life-savers.html#saving-lives",
    "title": "Spring 2023 Math 300Z",
    "section": "Saving Lives",
    "text": "Saving Lives\nA tourniquet is a belt-like device used to cut off the blook supply to a damaged and severely bleeding limb. A 2014 study of 1413 US casualities in Afghanistan and Iraq concluded that “those who received tourniquets had survival rates similar to those of comparable, transfused casualties who did not receive tourniquets.” That study was careful to take into account injury severity when comparing the casualties with tourniquets to those without. (JF Kragh et al. (2014) “Transfusion for Shock in US Military War Casualties With and Without Tourniquet Use” Annals of Emergency Medicine 65(3) link)\nThe study authors pointed out a potential bias in the collection of data. Only those soldiers who survived up to arrival at the hospital were included.\nConsider these four factors:\n\nInjury SEVERITY\nTourniquet USE (at the battle location)\nADMISSION, that is, arrival at the hospital\nPost-Admission SURVIVAL\n\nTASK: Construct a directed acyclic graph with a node for each of these factors. Draw directed causal links between each pair of nodes that you think are likely to be connected. For each link that you draw, make sure to show the direction of causation, giviving a few words of explanation. Similarly, when there is a pair of nodes without a direct connection, explain why not.\n \n \n \nNote: The direct paths you draw may create longer, indirect paths. For instance, \\(\\mathbb{A} \\longrightarrow \\mathbb{B} \\longrightarrow \\mathbb{C}\\) has direct paths between \\(\\mathbb{A}\\) and \\(\\mathbb{B}\\) as well as between \\(\\mathbb{B}\\) and \\(\\mathbb{C}\\). However, there is no direct connection between \\(\\mathbb{A}\\) and \\(\\mathbb{C}\\).\n\nReference: J Pearl and D Mackenzie (2018) The Book of Why pp343-7"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "",
    "text": "The entire box is 14.43 “inches” long. This should be the total of the left, right, and middle measurements, but people tended to round.\n\nThirds <- readr::read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT_asFV5LD312bYaGgHK3F91kgLVSiaQpNhggDilfPKAiDBNz9iueOiYWKgAtRRwkFlOz6U9znbiMGK/pub?gid=0&single=true&output=csv\")\n\nRows: 16 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Student_initials\ndbl (3): left, middle, right\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nLong_form <- tidyr::pivot_longer(Thirds, !Student_initials, names_to = \"position\")\nLong_form |> group_by(Student_initials) |> summarize(tot = sum(value), v = var(value))\n\n# A tibble: 16 × 3\n   Student_initials   tot        v\n   <chr>            <dbl>    <dbl>\n 1 AMA               13.8 0.0208  \n 2 BAK               14.5 0.271   \n 3 DQS               14.4 0.416   \n 4 EDS               16   0.646   \n 5 EFSF              14.1 0.00750 \n 6 EJD               13.8 0.271   \n 7 HAM               14.2 0.188   \n 8 HJB               14.3 0.000833\n 9 IMW               14   0.396   \n10 JK                14.2 0       \n11 JTSF              14.8 0.271   \n12 KDL               14.2 0.0625  \n13 KZC               14   0.583   \n14 RHP               14.2 0       \n15 RRP               14.2 0.188   \n16 SJA               14.5 1.65    \n\nLong_form |> summarize(vmeas = var(value))\n\n# A tibble: 1 × 1\n  vmeas\n  <dbl>\n1 0.240\n\nlm(value ~ position, data=Long_form) |> conf_interval()\n\n# A tibble: 3 × 4\n  term             .lwr .coef  .upr\n  <chr>           <dbl> <dbl> <dbl>\n1 (Intercept)     4.29  4.50  4.70 \n2 positionmiddle  0.392 0.678 0.964\n3 positionright  -0.126 0.159 0.445\n\nggplot(Long_form, aes(y=value, x=position)) + geom_jitter(width=0.2, alpha=0.5)"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#variation",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#variation",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Variation",
    "text": "Variation\nRemember that statistics focus on variation in the characteristics of a set of multiple specimens. The characteristics of each individual specimen are recorded in a row of a data frame. The data frame itself, with its multiple rows, represents the set of specimens. Each characteristic is arranged as a column in the data frame. We call such columns “variables,” a name that emphasizes that our particular interest is to understand/explain/account-for the variation of the values stored in the column.\nIn a regression model, we attempt to understand/explain/account-for the variation in a single variable, called the “response variable.” We accomplish this explanation by associating the variation in the response with the simultaneous variation in other variables called “explanatory variables.”\nThe lm() model-building function does the work of quantifying the associations. Your task in model building is to provide data for training and to specify which are the explanatory variables you want to use to account for the variation in the response variable. The specification takes the form of a tilde expression listing the response and explanatory variables. All these variables must be in the data frame used for training. We say such variables are “observed.”\nThere are usually other characteristics that are relevant to the system being studied that are not observed, that is, they are not in the data frame. It’s a bad idea to ignore such things."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#starting-lesson-20",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#starting-lesson-20",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Starting Lesson 20",
    "text": "Starting Lesson 20\nToday is a meta-day. It is about tools for learning about statistical methods and gaining insight into why certain kinds of questions/techniques come up over and over again as you work on genuine statistical problems.\nThe two kinds of tools for learning are:\n\nTools for thinking and communicating about hypotheses about causal connections.\n\nDiagrams called “DAGs” for sketching out causation.\nGenerating random, simulated data consistent with the mechanism described by a DAG.\n\nWays to automate the process of random trials. This is purely a labor-saving measure. You are not responsible to generate the code for this automation, but you should learn to read the code to be able to say what’s going on."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#causation-examples",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#causation-examples",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Causation examples",
    "text": "Causation examples\n\nSystolic blood pressure in the elderly:\n\nExperiment shows that lowering SBP reduces mortality.\nObservation shows that lower SBP is associated with increased mortality.\n\nCongressional elections\n\nAmong incumbents, higher election spending is associated with worse vote outcomes.\n\nVitamin D and disease\n\nLow vitamin linked to adverse outcomes in many diseases\nIll people go outside less often so are less exposed to sunlight AND Vitamin D is an acute phase reactant and declines with the inflammatory cytokine rise in acute and chronic diseases AND No evidence from randomized trials that vitamin D supplementation lessens mortality risks in such conditions.\nBring up article"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#directed-acyclic-graphs-dags",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#directed-acyclic-graphs-dags",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Directed acyclic graphs (DAGs)",
    "text": "Directed acyclic graphs (DAGs)\nA DAG is a format for writing down which characteristics, either observed or unobserved, are important in the operation of a system.\nA good dictionary definition of “system” is:\n\nA set of things working together as parts of a mechanism or an interconnecting network.\n\nGraphs, Directed, Acyclic"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#sampling-from-dags",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#sampling-from-dags",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Sampling from DAGs",
    "text": "Sampling from DAGs\nIn Math 300Z, DAGs have been augmented with a simulation mechanism. This consists of formulas that are invoked to create each variable in the DAG."
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#activity-life-savers",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#activity-life-savers",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Activity: Life Savers",
    "text": "Activity: Life Savers"
  },
  {
    "objectID": "Day-by-day/Lesson-20/Teaching-notes-20.html#repeating-trials",
    "href": "Day-by-day/Lesson-20/Teaching-notes-20.html#repeating-trials",
    "title": "Instructor Teaching Notes for Lesson 20",
    "section": "Repeating trials",
    "text": "Repeating trials\n\nfoo <- do(100000)*sum(runif(10))\nggplot(foo, aes(x=\" \", y = sum)) + geom_violin(alpha=0.5)\n\n\n\n\n\nTrials <- do(100) * {lm(x ~ y, data=sample(dag03, size=5)) |> R2()}"
  },
  {
    "objectID": "Day-by-day/Lesson-30/simple-paths.html#simple-causal-paths",
    "href": "Day-by-day/Lesson-30/simple-paths.html#simple-causal-paths",
    "title": "Spring 2023 Math 300Z",
    "section": "Simple causal paths",
    "text": "Simple causal paths"
  },
  {
    "objectID": "Day-by-day/Lesson-30/simple-paths.html#mediator",
    "href": "Day-by-day/Lesson-30/simple-paths.html#mediator",
    "title": "Spring 2023 Math 300Z",
    "section": "Mediator",
    "text": "Mediator\n\n\n\n\n\n   \n\nA\n\nA   \n\nB\n\nB   \n\nA->B\n\n    \n\nC\n\nC   \n\nB->C\n\n   \n\n\n\n\n\n\nTo see the full path from A to C, should you include B as a covariate?\nTo observe the relationship A \\(\\rightarrow\\) B, should you include C as a covariate?\nTo observe the relationship B \\(\\rightarrow\\) C, should you include A as a covariate?\n\n\nmediator <- dag_make(\n  A ~ exo(),\n  B ~ 2.5*A + exo(),\n  C ~ -1.2*B + exo()\n)\ndag_draw(mediator)\n\n\n\nSamp <- sample(mediator, size=1000)\n\n\n# Question 1\nlm(C ~ A, data=Samp) |> conf_interval()\nlm(C ~ A + B, data=Samp) |> conf_interval()\n\n\n# Question 2\nlm(B ~ A, data=Samp) |> conf_interval()\nlm(B ~ A + C, data=Samp) |> conf_interval()\n\n\n# Question 3\nlm(C ~ B, data=Samp) |> conf_interval()\nlm(C ~ B + A, data=Samp) |> conf_interval()"
  },
  {
    "objectID": "Day-by-day/Lesson-30/simple-paths.html#common-cause",
    "href": "Day-by-day/Lesson-30/simple-paths.html#common-cause",
    "title": "Spring 2023 Math 300Z",
    "section": "Common cause",
    "text": "Common cause\n\n\n\n\n\n   \n\nB\n\nB   \n\nC\n\nC   \n\nB->C\n\n    \n\nA\n\nA   \n\nA->C\n\n   \n\n\n\n\n\n\ncommon_cause <- dag_make(\n  A ~ exo(),\n  B ~ exo(),\n  C ~ 5*B - 2*A + exo()\n)\ndag_draw(common_cause)\n\n\n\nSamp <- sample(common_cause, size=1000)\n\n\nTo see the direct relationship between A and C, should you include B as a covariate?\nTo see the relationship between B and A, should you include C as a covariate?\n\n\n# Question 1\nlm(C ~ A, data=Samp) |> conf_interval()\nlm(C ~ A + B, data=Samp) |> conf_interval()\n\n\n# Question 2\nlm(A ~ B, data=Samp) |> conf_interval()\nlm(A ~ B + C, data=Samp) |> conf_interval()"
  },
  {
    "objectID": "Day-by-day/Lesson-30/simple-paths.html#contributing-causes",
    "href": "Day-by-day/Lesson-30/simple-paths.html#contributing-causes",
    "title": "Spring 2023 Math 300Z",
    "section": "Contributing causes",
    "text": "Contributing causes\n\n\n\n\n\n   \n\nA\n\nA   \n\nC\n\nC   \n\nA->C\n\n    \n\nB\n\nB   \n\nB->C\n\n   \n\n\n\n\n\n\ncontrib_causes <- dag_make(\n  B ~ exo(),\n  A ~ exo(),\n  C ~ -2*B + 3*A + exo()\n)\nSamp <- sample(contrib_causes, size=1000)\n\n\nTo see the effect of B on C, do you need to include A as a covariate?\n\n\n\n# A tibble: 2 × 4\n  term           .lwr  .coef   .upr\n  <chr>         <dbl>  <dbl>  <dbl>\n1 (Intercept) -0.0387  0.158  0.355\n2 B           -2.02   -1.81  -1.60 \n\n\n# A tibble: 3 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) -0.0449  0.0172  0.0793\n2 B           -2.01   -1.94   -1.88  \n3 A            2.95    3.01    3.07  \n\n\n\nWhich model has the better confidence interval on B?"
  },
  {
    "objectID": "Day-by-day/Lesson-30/simple-paths.html#collider",
    "href": "Day-by-day/Lesson-30/simple-paths.html#collider",
    "title": "Spring 2023 Math 300Z",
    "section": "Collider",
    "text": "Collider\nSame network as contributing causes, but focussing on relationship between A & B.\n\nTo see the correct (lack of) relationship between A and B, should you include C as a covariate?\n\n\n\n# A tibble: 2 × 4\n  term           .lwr  .coef  .upr\n  <chr>         <dbl>  <dbl> <dbl>\n1 (Intercept) -0.0152 0.0468 0.109\n2 B           -0.0216 0.0436 0.109\n\n\n# A tibble: 3 × 4\n  term           .lwr     .coef   .upr\n  <chr>         <dbl>     <dbl>  <dbl>\n1 (Intercept) -0.0201 -0.000492 0.0191\n2 B            0.562   0.586    0.609 \n3 C            0.293   0.299    0.306"
  },
  {
    "objectID": "Day-by-day/Lesson-30/simple-paths.html#confounder",
    "href": "Day-by-day/Lesson-30/simple-paths.html#confounder",
    "title": "Spring 2023 Math 300Z",
    "section": "Confounder",
    "text": "Confounder\n\n\n\n\n\n   \n\nA\n\nA   \n\nC\n\nC   \n\nA->C\n\n    \n\nB\n\nB   \n\nA->B\n\n    \n\nB->C\n\n   \n\n\n\n\n\n\nconfounder <- dag_make(\n  A ~ exo(),\n  B ~ 4*A + exo(),\n  C ~ 2*A  + 0.5*B + exo()\n)\nSamp <- sample(confounder, size=1000)\n\n\nTo see the correct direct connection between B and C, should you include A as a confounder?\n\n\nlm(C ~ B, data=Samp) |> conf_interval()\nlm(C ~ B + A, data=Samp) |> conf_interval()"
  },
  {
    "objectID": "Day-by-day/Lesson-30/Teaching-notes-30.html",
    "href": "Day-by-day/Lesson-30/Teaching-notes-30.html",
    "title": "Instructor Teaching Notes for Lesson 30",
    "section": "",
    "text": "Aside: Confidence bands and global warming.\nCherry blossoms from Kyoto over 1200 years."
  },
  {
    "objectID": "Day-by-day/Lesson-30/Teaching-notes-30.html#smoking-and-cancer-a-1950s-controversy",
    "href": "Day-by-day/Lesson-30/Teaching-notes-30.html#smoking-and-cancer-a-1950s-controversy",
    "title": "Instructor Teaching Notes for Lesson 30",
    "section": "Smoking and cancer: a 1950s controversy",
    "text": "Smoking and cancer: a 1950s controversy\nDeniers of a smoking/cancer link claimed there was a common cause for both: a “cancer gene.”\n\n\n\n\n\n   \n\nSmoking\n\nSmoking   \n\nLung cancer\n\nLung cancer   \n\nSmoking->Lung cancer\n\n    \n\nSmoking gene\n\nSmoking gene   \n\nSmoking gene->Smoking\n\n    \n\nSmoking gene->Lung cancer\n\n   \n\n\n\n\n\nThe gene had not been identified, so no data could be collected on it.\nThis is an example of confounding: the effects of the (supposed) gene and of smoking are mixed together."
  },
  {
    "objectID": "Day-by-day/Lesson-30/Teaching-notes-30.html#discovering-the-rules-for-small-dags",
    "href": "Day-by-day/Lesson-30/Teaching-notes-30.html#discovering-the-rules-for-small-dags",
    "title": "Instructor Teaching Notes for Lesson 30",
    "section": "Discovering the rules for small DAGs",
    "text": "Discovering the rules for small DAGs\nClass activity"
  },
  {
    "objectID": "Day-by-day/Lesson-30/Teaching-notes-30.html#more-complex-dags",
    "href": "Day-by-day/Lesson-30/Teaching-notes-30.html#more-complex-dags",
    "title": "Instructor Teaching Notes for Lesson 30",
    "section": "More complex DAGs",
    "text": "More complex DAGs\nIn considering the relationship between two nodes, enumerate each of the paths that connect the two nodes.\nExample: Smoking with a non-genetic mediator: Tar\nThere are two paths from Tar to Lung cancer:\n\n\n\n\n\n   \n\nSmoking gene\n\nSmoking gene   \n\nLung cancer\n\nLung cancer   \n\nSmoking gene->Lung cancer\n\n    \n\nSmoking\n\nSmoking   \n\nSmoking gene->Smoking\n\n  Path 2   \n\nTar\n\nTar   \n\nSmoking->Tar\n\n    \n\nTar->Lung cancer\n\n  Path 1  \n\n\n\n\n\nTwo types of path between two endpoint nodes:\n\nA correlating path: Starting from some node on the path, causal influence can flow (along the arrows) to both endpoints.\n\nBLOCK a correlating path by using some node along it as a covariate. Otherwise, it’s open.\n\nA colliding path: There’s no node on the path from which causal influence can flow (along the arrows) to both endpoints.\n\nOPEN a colliding path by using the collider as a covariate. Otherwise, it’s closed.\n\none <- dag_make(\n  A ~ exo(),\n  B ~ A + exo(),\n  D ~ A + exo(), \n  C ~ B + D +exo()\n)\ndag_draw(one)\n\n\n\nSamp <- sample(one, size=1000)\n\n\nlm(D ~ A, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr  .coef   .upr\n  <chr>         <dbl>  <dbl>  <dbl>\n1 (Intercept) -0.0346 0.0286 0.0919\n2 A            0.962  1.02   1.09  \n\nlm(D ~ A + B, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr    .coef   .upr\n  <chr>         <dbl>    <dbl>  <dbl>\n1 (Intercept) -0.0346  0.0286  0.0919\n2 A            0.941   1.03    1.12  \n3 B           -0.0702 -0.00623 0.0578\n\nlm(D ~ A + C, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr  .coef   .upr\n  <chr>         <dbl>  <dbl>  <dbl>\n1 (Intercept) -0.0321 0.0189 0.0699\n2 A            0.252  0.329  0.406 \n3 C            0.312  0.341  0.370 \n\nlm(D ~ A + B + C, data=Samp) |> conf_interval()\n\n# A tibble: 4 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) -0.0285  0.0149  0.0583\n2 A            0.440   0.508   0.576 \n3 B           -0.589  -0.535  -0.481 \n4 C            0.484   0.514   0.544 \n\n\n\none <- dag_make(\n  A ~ exo(),\n  E ~ 10*C + exo(),\n  B ~ A + exo(),\n  D ~ A + exo(), \n  C ~ B + D +exo()\n)\ndag_draw(one)\n\n\n\nSamp <- sample(one, size=1000)\nlm(D ~ A, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef   .upr\n  <chr>         <dbl>   <dbl>  <dbl>\n1 (Intercept) -0.0539 0.00950 0.0729\n2 A            0.902  0.963   1.02  \n\nlm(D ~ A + C, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr    .coef   .upr\n  <chr>         <dbl>    <dbl>  <dbl>\n1 (Intercept) -0.0559 -0.00416 0.0476\n2 A            0.157   0.238   0.319 \n3 C            0.323   0.354   0.385 \n\nlm(D ~ A + E, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr    .coef   .upr\n  <chr>         <dbl>    <dbl>  <dbl>\n1 (Intercept) -0.0564 -0.00453 0.0473\n2 A            0.162   0.243   0.324 \n3 E            0.0321  0.0352  0.0384\n\n\n\nCan Tar be used to avoid the confounding due to genetics? How do you block the back-door pathway?\n\n\n\n\n\n\n## The Berkeley graduate admissions data from 1973\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- model_train(zero_one(admit, one=\"admitted\") ~ gender,\n                    data=UCB_applicants)\nmodel_plot(mod1, x=dept, color=gender, nlevels=10) +\n  ylab(\"Admitted\")\n\n\n\n:::\n\nmod2 <- model_train(zero_one(admit, one=\"admitted\") ~ gender*dept,\n                    data=UCB_applicants)\nmodel_plot(mod2, x=dept, color=gender, nlevels=10, data_alpha=0.1) +\n  ylab(\"Admitted\")\n\n\n\n\n\nmodel_train(zero_one(admit, one=\"admitted\") ~ gender, \n            data=UCB_applicants) |> conf_interval()\n\nWaiting for profiling to be done...\n\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) -0.931 -0.830 -0.732\n2 gendermale   0.485  0.610  0.736\n\nmodel_train(zero_one(admit, one=\"admitted\") ~ gender + dept, \n            data=UCB_applicants) |> conf_interval()\n\nWaiting for profiling to be done...\n\n\n# A tibble: 7 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept)  0.488  0.682   0.877 \n2 gendermale  -0.259 -0.0999  0.0582\n3 deptB       -0.258 -0.0434  0.172 \n4 deptC       -1.47  -1.26   -1.05  \n5 deptD       -1.50  -1.29   -1.09  \n6 deptE       -1.99  -1.74   -1.49  \n7 deptF       -3.65  -3.31   -2.98"
  },
  {
    "objectID": "Day-by-day/Lesson-30/Teaching-notes-30.html#back-to-berkeley",
    "href": "Day-by-day/Lesson-30/Teaching-notes-30.html#back-to-berkeley",
    "title": "Instructor Teaching Notes for Lesson 30",
    "section": "Back to Berkeley",
    "text": "Back to Berkeley\nShould we adjust for department? Let’s go to a DAG.\n\nUCB_dag1 <- dag_make(sex ~ exo(),\n                     dept ~ sex,\n                     admit ~ sex + dept)\ndag_draw(UCB_dag1, vertex.label.cex=1)\n\n\n\n\nIf we think that the connection sex \\(\\longrightarrow\\) department is just a matter of personal choice (as in the 1975 Science article), then we should block the back-door pathway.\nBut if we think that sex \\(\\longrightarrow\\) department reflects systemic issues such as which departments are considered important and get funding, or which careers women think they can succeed in, then we do not want to block the backdoor pathway.\n\nUCB_dag2 <- dag_make(sex ~ exo(),\n                     success ~ sex,\n                     dept_funding ~ sex,\n                     dept ~ success,\n                     admit ~ sex + dept + dept_funding)\ndag_draw(UCB_dag2, vertex.label.cex=1)"
  },
  {
    "objectID": "Day-by-day/Lesson-30/Teaching-notes-30.html#birthweight-collider",
    "href": "Day-by-day/Lesson-30/Teaching-notes-30.html#birthweight-collider",
    "title": "Instructor Teaching Notes for Lesson 30",
    "section": "Birthweight collider",
    "text": "Birthweight collider\nObservations from the 1960s:\n\nSmoking is associated with lower birthweight\nLower birthweight is associated with increased mortality\n\nQuestion: Does smoking have a direct effect on mortality?\n\n\n\n\n\n\n\nH\n\n  \n\nSmoking\n\nSmoking   \n\nBirth weight\n\nBirth weight   \n\nSmoking->Birth weight\n\n    \n\nMortality of infant\n\nMortality of infant   \n\nSmoking->Mortality of infant\n\n   ?   \n\nBirth weight->Mortality of infant\n\n   \n\n\n\n\n\nHow do you look at the direct effect of smoking on mortality? Block the other pathway by using birth weight as a covariate.\nWhen this was done, by looking only at low-birthweight babies, it was found that smoking reduces mortality.\nMight there be something else going on? Is there another cause for low birthweight?\n\n\n\n\n\n\n\nJ\n\n  \n\nBirth defect\n\nBirth defect   \n\nBirth weight\n\nBirth weight   \n\nBirth defect->Birth weight\n\n    \n\nMortality of infant\n\nMortality of infant   \n\nBirth defect->Mortality of infant\n\n    \n\nSmoking\n\nSmoking   \n\nSmoking->Birth weight\n\n    \n\nSmoking->Mortality of infant\n\n    \n\nBirth weight->Mortality of infant"
  },
  {
    "objectID": "Day-by-day/Lesson-31/spurious-correlations.html",
    "href": "Day-by-day/Lesson-31/spurious-correlations.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "Methods of analyzing data can produce artifacts, that is, relationships between variables that are present only because of the methodological approach. We’re going to look at two ways of producing such artifacts … and how to avoid them by taking care to include or exclude appropriate covariates."
  },
  {
    "objectID": "Day-by-day/Lesson-31/spurious-correlations.html#example-1-the-same-quantity-appearing-on-both-sides-of-the-tilde.",
    "href": "Day-by-day/Lesson-31/spurious-correlations.html#example-1-the-same-quantity-appearing-on-both-sides-of-the-tilde.",
    "title": "Spring 2023 Math 300Z",
    "section": "Example 1: The same quantity appearing on both sides of the tilde.",
    "text": "Example 1: The same quantity appearing on both sides of the tilde.\nSuppose A is number of people involved in a cult and B is the number of deaths in one year.\nWe hypothesize \\(A \\longrightarrow B\\) because the cult claims to foster good health of others by projecting crystaline karma via mental ergone energy waves.\nWe have data from many countries, and we decide to normalize both A and B by the count population P.\n\n\\(x = A/P\\ \\ \\) and \\(\\ \\ y=B/P\\)\n\nHere’s a DAG:\n\nmindwave_dag <- dag_make(\n  P ~ 10000*(unif() + 1),\n  A ~ 10*unif(), # A, B, and P are all exogenous\n  B ~ 0*A + 100*unif(),\n  x ~ A/P,\n  y ~ B/P\n)\nSamp <- sample(mindwave_dag, size=1000)\n\nTask I. Analyze the data in Samp with the model y ~ x (looking, as usual, at the appropriate confidence interval). Is there sign of a link between x and y?\n\n# Put your analysis here\n\nTask II. Study the DAG structure to figure out whether it is possible to block all pathways between x and y except pathway between x and y via A and B.\n\n\n\n\n\nWhich covariate accomplishes the blocking? Explain why.\nTask III. Again analyze Samp but this time include the covariate in (II). Now is there a sign of a link between x and y?"
  },
  {
    "objectID": "Day-by-day/Lesson-31/spurious-correlations.html#example-2-colliding-talents",
    "href": "Day-by-day/Lesson-31/spurious-correlations.html#example-2-colliding-talents",
    "title": "Spring 2023 Math 300Z",
    "section": "Example 2: Colliding talents",
    "text": "Example 2: Colliding talents\nYou have been put in charge of a new project to develop an App to streamline a common administrative task. The App must (1) provide correct output and (2) be usable by minimally trained users.\nYour first task is to select a team of developers. The team must be a mixture of people with two talents: (i) empathy and (ii) technical expertise. To this end, you identify 1000 potential workers and give each of them two tests: an empathy measurement and a technical expertise measurement. Here is a DAG that generates the (simulated) data:\n\nteam_dag <- dag_make(\n  empathy_score ~ 50*(3 + exo()),\n  technical_score ~ 40*(5- exo())\n)\nScores <- sample(team_dag, size=1000)\n\nTask I. Is there any link between the empathy and the technical scores? Fit a model, say empathy_score ~ technical_score or the other way around. Use the confidence interval on one of the coefficients to decide whether there is a link.\nTask II. Make a point plot of empathy_score ~ technical_score. Examine the shape of the “cloud” to decide if the plot points to a link between the two types of scores.\nTask III. You want to assemble a strong team of people. You decide to select the 50 individuals with the largest sum of empathy and technical scores. Wrangle the Scores data frame down to a new data frame, My_team, that has the 50 highest scoring individuals. Uncomment and fill in the ....’s in the following command to accomplish the wrangling.\n\n# My_team <- Scores |>\n#   mutate(total = ...) |>\n#   arrange(desc(......)) |>\n#   head(...)\n\nTask IV. Among the people in My_team, is there any link between the empathy and technical scores? Find this out by fitting the appropriate model.\nTask V. Look back at your plot in Task II. By eye, identify (roughly) the points corresponding to the 50 or so people with the highest sum of scores. Draw a loop around those 50 points. Looking just at the people inside the loop, do you see a link between empathy and technical scores?"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "",
    "text": "On Sunday afternoon I was reading the AARP monthly newsletter. This short item drew my attention:\nThis hat startling relevance because—at the time I was reading—I had no chest pain at all. Should I have been concerned? What is the probability I am having a heart attack if not 31%.\nHeuristics: - 31% per what interval of time? If I’m not having chest pain next week, do I still have a 31% probability of having a heart attack at that time? A probability quantifies a specific outcome from a specific event. Is there any guidance about what the specific event is? My whole future life? This month? Today? - … others from students."
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#review-of-lesson-30",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#review-of-lesson-30",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "Review of Lesson 30",
    "text": "Review of Lesson 30\nChild of response as covariate\n\ndag_four <- dag_make(\n  A ~ exo(),\n  B ~ A + exo(),\n  C ~ 2.5*B + exo()\n)\nSamp <- sample(dag_four, size=1000)\nlm(B ~ A, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef   .upr\n  <chr>         <dbl>   <dbl>  <dbl>\n1 (Intercept) -0.0856 -0.0211 0.0433\n2 A            0.847   0.912  0.977 \n\nlm(B ~ A + C, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr    .coef   .upr\n  <chr>         <dbl>    <dbl>  <dbl>\n1 (Intercept) -0.0290 -0.00586 0.0173\n2 A            0.0837  0.114   0.144 \n3 C            0.338   0.346   0.355 \n\n\nParent of explanatory as covariate\n\ndag_six <- dag_make(\n  C ~ exo(),\n  A ~ C +  exo(),\n  B ~ 2.5* A + exo()\n)\nSamp <- sample(dag_six, size=1000)\nlm(B ~ A, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef   .upr\n  <chr>         <dbl>   <dbl>  <dbl>\n1 (Intercept) -0.0576 0.00339 0.0644\n2 A            2.48   2.52    2.56  \n\nlm(B ~ A + C, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr   .coef   .upr\n  <chr>         <dbl>   <dbl>  <dbl>\n1 (Intercept) -0.0573 0.00379 0.0648\n2 A            2.45   2.51    2.57  \n3 C           -0.0691 0.0198  0.109 \n\n\nChild of explanatory as covariate\n\ndag_five <- dag_make(\n  A ~ exo(),\n  C ~ A +  exo(),\n  B ~ 2.5* A + exo()\n)\nSamp <- sample(dag_five, size=1000)\nlm(B ~ A, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr    .coef   .upr\n  <chr>         <dbl>    <dbl>  <dbl>\n1 (Intercept) -0.0728 -0.00867 0.0554\n2 A            2.45    2.51    2.58  \n\nlm(B ~ A + C, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr    .coef   .upr\n  <chr>         <dbl>    <dbl>  <dbl>\n1 (Intercept) -0.0728 -0.00859 0.0556\n2 A            2.43    2.52    2.60  \n3 C           -0.0695 -0.00400 0.0615\n\n\n**Collider\n\ndag_three <- dag_make(\n  A ~ exo(),\n  B ~ 2.5* A + exo(),\n  C ~ A + 1.25*B +  exo()\n)\nSamp <- sample(dag_three, size=1000)\nlm(B ~ A, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr  .coef   .upr\n  <chr>         <dbl>  <dbl>  <dbl>\n1 (Intercept) -0.0391 0.0230 0.0851\n2 A            2.38   2.44   2.50  \n\nlm(B ~ A + C, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr   .coef   .upr\n  <chr>         <dbl>   <dbl>  <dbl>\n1 (Intercept) -0.0343 0.00374 0.0418\n2 A            0.387  0.488   0.589 \n3 C            0.461  0.484   0.508"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#the-berkeley-graduate-admissions-data-from-1973",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#the-berkeley-graduate-admissions-data-from-1973",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "The Berkeley graduate admissions data from 1973",
    "text": "The Berkeley graduate admissions data from 1973\n\nmod1 <- model_train(zero_one(admit, one=\"admitted\") ~ gender,\n                    data=UCB_applicants)\nmodel_plot(mod1, x=dept, color=gender, nlevels=10) +\n  ylab(\"Admitted\")\n\n\n\n\n\nmod2 <- model_train(zero_one(admit, one=\"admitted\") ~ gender*dept,\n                    data=UCB_applicants)\nmodel_plot(mod2, x=dept, color=gender, nlevels=10, data_alpha=0.1) +\n  ylab(\"Admitted\")\n\n\n\n\n\nmodel_train(zero_one(admit, one=\"admitted\") ~ gender, \n            data=UCB_applicants) |> conf_interval()\n\nWaiting for profiling to be done...\n\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) -0.931 -0.830 -0.732\n2 gendermale   0.485  0.610  0.736\n\nmodel_train(zero_one(admit, one=\"admitted\") ~ gender + dept, \n            data=UCB_applicants) |> conf_interval()\n\nWaiting for profiling to be done...\n\n\n# A tibble: 7 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept)  0.488  0.682   0.877 \n2 gendermale  -0.259 -0.0999  0.0582\n3 deptB       -0.258 -0.0434  0.172 \n4 deptC       -1.47  -1.26   -1.05  \n5 deptD       -1.50  -1.29   -1.09  \n6 deptE       -1.99  -1.74   -1.49  \n7 deptF       -3.65  -3.31   -2.98"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#back-to-berkeley",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#back-to-berkeley",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "Back to Berkeley",
    "text": "Back to Berkeley\nShould we adjust for department? Let’s go to a DAG.\n\nUCB_dag1 <- dag_make(sex ~ exo(),\n                     dept ~ sex,\n                     admit ~ sex + dept)\ndag_draw(UCB_dag1, vertex.label.cex=1)\n\n\n\n\nIf we think that the connection sex \\(\\longrightarrow\\) department is just a matter of personal choice (as in the 1975 Science article), then we should block the back-door pathway.\nBut if we think that sex \\(\\longrightarrow\\) department reflects systemic issues such as which departments are considered important and get funding, or which careers women think they can succeed in, then we do not want to block the backdoor pathway.\n\nUCB_dag2 <- dag_make(sex ~ exo(),\n                     success ~ sex,\n                     dept_funding ~ sex,\n                     dept ~ success,\n                     admit ~ sex + dept + dept_funding)\ndag_draw(UCB_dag2, vertex.label.cex=1)"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#birthweight-collider",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#birthweight-collider",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "Birthweight collider",
    "text": "Birthweight collider\nObservations from the 1960s:\n\nSmoking is associated with lower birthweight\nLower birthweight is associated with increased mortality\n\nQuestion: Does smoking have a direct effect on mortality?\n\n\n\n\n\n\n\nH\n\n  \n\nSmoking\n\nSmoking   \n\nBirth weight\n\nBirth weight   \n\nSmoking->Birth weight\n\n    \n\nMortality of infant\n\nMortality of infant   \n\nSmoking->Mortality of infant\n\n   ?   \n\nBirth weight->Mortality of infant\n\n   \n\n\n\n\n\nHow do you look at the direct effect of smoking on mortality? Block the other pathway by using birth weight as a covariate.\nWhen this was done, by looking only at low-birthweight babies, it was found that smoking reduces mortality.\nMight there be something else going on? Is there another cause for low birthweight?\n\n\n\n\n\n\n\nJ\n\n  \n\nBirth defect\n\nBirth defect   \n\nBirth weight\n\nBirth weight   \n\nBirth defect->Birth weight\n\n    \n\nMortality of infant\n\nMortality of infant   \n\nBirth defect->Mortality of infant\n\n    \n\nSmoking\n\nSmoking   \n\nSmoking->Birth weight\n\n    \n\nSmoking->Mortality of infant\n\n    \n\nBirth weight->Mortality of infant"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#links-by-chance",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#links-by-chance",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "Links by chance",
    "text": "Links by chance\nOur method for detecting a link between variable X and Y is to build the model Y ~ X, with perhaps some covariate(s), then example the coefficient on X. If the *confidence interval** includes zero, there is no link.\nWe can simulate what happens when there is no link, for example\n\nno_link_dag <- dag_make(\n  X ~ exo(),\n  Y ~ exo()\n)\n\nHow do we know this DAG represents a system with no link?\nA simulation:\n\nSamp <- sample(no_link_dag, size=100)\nlm(Y ~ X, data=Samp) |> \n  conf_interval() |>\n  filter(term==\"X\")\n\n# A tibble: 1 × 4\n  term    .lwr   .coef  .upr\n  <chr>  <dbl>   <dbl> <dbl>\n1 X     -0.302 -0.0987 0.104\n\n\nLet’s see how often we get a confidence interval that excludes zero even though there is no link between X and Y in the mechanism generating the data.\n\none_trial <- function(n=100) {\n  sample(no_link_dag, size=n) |>\n    lm(Y ~ X, data=_) |>\n    conf_interval() |>\n    filter(term==\"X\") |> \n    mutate(excludes = .lwr > 0 | .upr < 0) |>\n    select(excludes)\n}\none_trial()\n\n# A tibble: 1 × 1\n  excludes\n  <lgl>   \n1 FALSE   \n\n\nNow run 1000 trials …\n\n{do(1000) * one_trial(n=100)} |> \n  summarize(frac_mistakes = sum(excludes) / n())\n\n  frac_mistakes\n1         0.047\n\n\nHow will the fraction of mistakes change if we increase the sample size?"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#other-sources-of-spurious-correlation",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#other-sources-of-spurious-correlation",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "Other sources of spurious correlation",
    "text": "Other sources of spurious correlation"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#the-berkeley-graduate-admissions-data-from-1973-1",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#the-berkeley-graduate-admissions-data-from-1973-1",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "The Berkeley graduate admissions data from 1973",
    "text": "The Berkeley graduate admissions data from 1973\n\nmod1 <- model_train(zero_one(admit, one=\"admitted\") ~ gender,\n                    data=UCB_applicants)\nmodel_plot(mod1, x=dept, color=gender, nlevels=10) +\n  ylab(\"Admitted\")\n\n\n\n\n\nmod2 <- model_train(zero_one(admit, one=\"admitted\") ~ gender*dept,\n                    data=UCB_applicants)\nmodel_plot(mod2, x=dept, color=gender, nlevels=10, data_alpha=0.1) +\n  ylab(\"Admitted\")\n\n\n\n\n\nmodel_train(zero_one(admit, one=\"admitted\") ~ gender, \n            data=UCB_applicants) |> conf_interval()\n\nWaiting for profiling to be done...\n\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) -0.931 -0.830 -0.732\n2 gendermale   0.485  0.610  0.736\n\nmodel_train(zero_one(admit, one=\"admitted\") ~ gender + dept, \n            data=UCB_applicants) |> conf_interval()\n\nWaiting for profiling to be done...\n\n\n# A tibble: 7 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept)  0.488  0.682   0.877 \n2 gendermale  -0.259 -0.0999  0.0582\n3 deptB       -0.258 -0.0434  0.172 \n4 deptC       -1.47  -1.26   -1.05  \n5 deptD       -1.50  -1.29   -1.09  \n6 deptE       -1.99  -1.74   -1.49  \n7 deptF       -3.65  -3.31   -2.98"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#back-to-berkeley-1",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#back-to-berkeley-1",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "Back to Berkeley",
    "text": "Back to Berkeley\nShould we adjust for department? Let’s go to a DAG.\n\nUCB_dag1 <- dag_make(sex ~ exo(),\n                     dept ~ sex,\n                     admit ~ sex + dept)\ndag_draw(UCB_dag1, vertex.label.cex=1)\n\n\n\n\nIf we think that the connection sex \\(\\longrightarrow\\) department is just a matter of personal choice (as in the 1975 Science article), then we should block the back-door pathway.\nBut if we think that sex \\(\\longrightarrow\\) department reflects systemic issues such as which departments are considered important and get funding, or which careers women think they can succeed in, then we do not want to block the backdoor pathway.\n\nUCB_dag2 <- dag_make(sex ~ exo(),\n                     success ~ sex,\n                     dept_funding ~ sex,\n                     dept ~ success,\n                     admit ~ sex + dept + dept_funding)\ndag_draw(UCB_dag2, vertex.label.cex=1)"
  },
  {
    "objectID": "Day-by-day/Lesson-31/Teaching-notes-31.html#birthweight-collider-1",
    "href": "Day-by-day/Lesson-31/Teaching-notes-31.html#birthweight-collider-1",
    "title": "Instructor Teaching Notes for Lesson 31",
    "section": "Birthweight collider",
    "text": "Birthweight collider\nObservations from the 1960s:\n\nSmoking is associated with lower birthweight\nLower birthweight is associated with increased mortality\n\nQuestion: Does smoking have a direct effect on mortality?\n\n\n\n\n\n\n\nH\n\n  \n\nSmoking\n\nSmoking   \n\nBirth weight\n\nBirth weight   \n\nSmoking->Birth weight\n\n    \n\nMortality of infant\n\nMortality of infant   \n\nSmoking->Mortality of infant\n\n   ?   \n\nBirth weight->Mortality of infant\n\n   \n\n\n\n\n\nHow do you look at the direct effect of smoking on mortality? Block the other pathway by using birth weight as a covariate.\nWhen this was done, by looking only at low-birthweight babies, it was found that smoking reduces mortality.\nMight there be something else going on? Is there another cause for low birthweight?\n\n\n\n\n\n\n\nJ\n\n  \n\nBirth defect\n\nBirth defect   \n\nBirth weight\n\nBirth weight   \n\nBirth defect->Birth weight\n\n    \n\nMortality of infant\n\nMortality of infant   \n\nBirth defect->Mortality of infant\n\n    \n\nSmoking\n\nSmoking   \n\nSmoking->Birth weight\n\n    \n\nSmoking->Mortality of infant\n\n    \n\nBirth weight->Mortality of infant"
  },
  {
    "objectID": "Day-by-day/Lesson-22/counts-and-waiting-times.html",
    "href": "Day-by-day/Lesson-22/counts-and-waiting-times.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "Measurements are a combination of signal and noise. This activity aims to help understand how much data is needed to reduce the noise to an acceptable level.\n\n\nOften, measurements are made of how many random events of a particular type happen in a fixed interval of time or a given area of space. Examples:\n\nHow many inches of snow fall per year in a given location?\nHow many traffic accidents happen per year at a particular intersection?\n\nMaking such measurements can be straightforward. The challenge comes in interpreting them. For instance, how different do the measurements of snowfall in two different areas need to be to support a claim that one area is snowier than the other? Similarly, how many years of observation are required to know if one intersection is more dangerous than another?\nCommon sense tells us that one region can be snowier than another or that one intersection can be more dangerous than another. Yet the individual events—a snow storm of a given magnitude or a pedestrian hit by an automobile—are random. That is, there is both signal and noise in the measurements.\nThis activity is about how much data we should collect in order to reduce the noise sufficiently that signals can be read clearly.\n\n\n\n\nEach of the people in your group will create his or her own signal, that is, a single number specifying the rate at which events happen.\n\n\nDo not look at your signal until directed to.\nGenerate your signal with this command:\n\n\nsignal <- 50*(5 + runif(1)) # Don't peek\n\nOnce all of your group members have their own signal, you are going to start to generate noisy data. The idea is to compare the observations you make to those from the other group members to decide the extent to which the signals differ from one another.\n\nGenerate a single (\\(n=1\\)) measurement this way:\n\n\nrpois(n=1, lambda=signal)\n\nCompare your measurement to those of each of the other members of the group. For each of those members, decide among these choices, making sure not to look at the numerical value of signal.\n\nMy signal is certainly higher.\nMy signal is likely to be higher.\nCan’t tell whether my signal is different than the other person’s.\nMy signal is likely to be lower.\nMy signal is certainly lower.\n\nRecord your conclusion for each of your group partners.\n\nRepeat the process in (2), but generate \\(n=2\\) observations. You will have two numbers, which you will compare to the other person’s two numbers. You’ll have to decide how you want to do this.\n\nMake a note of the method you choose do the comparison and record your conclusion (i)-(v) for each of your group partners.\n\nRepeat (3), but this time with \\(n=4\\).\n\nAre you still happy with the method you chose in (3)? If not, figure out a new method that can handle the comparison of 4 numbers to another set of 4 numbers.\nOnce again, record your conclusion (i)-(v) of your data against each of your group partners.\n\nRepeat the following for each of your partners separately. Try higher values of \\(n\\) until the two of you agree that your signals are certainly different.\n\n\n\n\nStarting in June 1944, the Germans engaged in missile attacks against London. At first, the V-1 cruise missile was used. Later, the V-2 ballistic missile was added to the attack.\nThe London City Council kept records of V-1 impact sites within their jurisdiction. Figure 1 shows the locations.\n\n\n\n\n\nFigure 1: Map showing impact sites of V-1 missiles in the area administered by the London City Council. The Thames river is shown in blue. The map does not show impacts outside the LCC region.\n\n\n\n\nAn analysis published in 1946 looked at 537 V-1 impacts within the area outlined in red: a region of 144 km2. The area was divided into 576 boxes each of area 0.25 km2. Five-hundred thirty-seven V-1 impacts fell within the rectangle; suggesting that there might be about 1 impact per box.\nTo be more precise, the signal is 537 / 576 = 0.93 impact per box.\nThere is a mathematical theory of how the measured number of impacts will vary from one box to another. The theory is called the Poisson distribution. In the activity, when you used rpois(n=1, lambda=signal) the computer was using this theory to generate the measurement, that is, signal plus noise.\nThese are the results of comparing the theory to the actual observations of how many boxes had no impact, 1 impact, 2 impacts, and so on.\n\n\n\n\n\nFigure 2: The number of boxes (out of 576, total) with different numbers of impacts. The Poisson theory result is in the middle column. (Copied from the 1946 analysis paper.\n\n\n\n\nThis article gives a more detailed account."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "",
    "text": "Open your Z-section project in Posit.cloud. Use get_lesson_worksheet(22) to bring in today’s worksheet.\n\nOpen the worksheet and leave it for later in class.\nCreate a new Rmd file, say, \"Scratch22.Rmd\". We’ll do scratch work there."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#motivating-problem-1-dags",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#motivating-problem-1-dags",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Motivating problem 1: DAGs",
    "text": "Motivating problem 1: DAGs\nYou propose a DAG to describe a specific situation and want to see how well it matches the available data.\nCommon sense suggests that two variables y and x may not have any causal connection between them.\nIn such a case, we anticipate that the model y ~ x + ... will have a coefficient of zero on x. (The + ... stands for other possible explanatory variables, which we call “covariates.”)\nThis lesson is about two closely related things:\n\nWhen is a coefficient small enough that we can regard it as zero?\nWhen we have a coefficient generated by fitting a model to data, how do we describe how precisely we know it?"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#motivating-problem-2-sustainable-fisheries",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#motivating-problem-2-sustainable-fisheries",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Motivating problem 2: Sustainable fisheries",
    "text": "Motivating problem 2: Sustainable fisheries\nDesigning an enforcement regime for limits on scallop fisheries.\n\n\n\n\n\nFigure 1: Life cycle of a scallop\n\n\n\n\nFisheries are regulated by states and the Federal government in order to avoid collapse due to over-fishing. Often, the regulations attempt to protect juveniles—animals that have not yet reached reproductive age. If the juveniles are harvested, their potential progeny are annihilated. There are various ways to do this, for instance restricting fishing to months where adults are most prevalent, closing fisheries to provide an opportunity for the reproductive stock to recover, and so on.\nIn the 1990s, one of the ways the Federal government regulated scallop fisheries was by setting a minimum acceptable size for harvested scallops. For practical reasons, rather than monitoring individual scallops, the government monitored the average per-scallop weight of each boat’s catch. For the sake of the example, imagine that the minimum acceptable weight is 1/30 pound.\nA fishing boat might have 10,000 or more bags of scallops, which can be handled individually: weigh the bag, then count the number of scallops to get the average weight per scallop.\nDiscussion questions:\n\nHow many bags should be sampled? Should this depend on the number of bags in the cargo. For instance, should a cargo of 1000 bags be sampled differently than a cargo of 10,000 bags.\nWhat should be the threshold for declaring the whole cargo below minimum size? (The whole catch is confiscated in such a case.)\n\nIn this section of the course, you’ll learn some statistical concepts and methods that allow the above questions to be answered to produce a regulation that is protective and fair to the fishermen.\nOne idea is very simple: sampling variation. This is about how much the average per-scallop weight will vary from one bag to another.\nAnother idea is very subtle: What you can say about the whole cargo based on a sample of \\(n\\) bags."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#vocabulary-sample-vs-sampling",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#vocabulary-sample-vs-sampling",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Vocabulary: “Sample” vs “sampling”",
    "text": "Vocabulary: “Sample” vs “sampling”\nThe vocabulary here can be confusing, because similar sounding terms refer to different things.\nA sample is a collection, just as a data frame is a collection of rows. The individual items in the collection—the individual rows—are “specimens,” or “cases,” or “rows,” or “units of observation,” or “observations,” or even “tuples.”\n“Sampling” is the process of collecting a sample.\nStatisticians use the phrase “sample statistic” to refer to a summary calculated from a sample. For instance, if your summary is the variance of a variable, this could properly be called the “sample variance.”\nThe terms “sampling variation” and “sampling variance.” “Sampling variation” is a theoretical concept: how much a model coefficient or other sample statistic would vary from one randomly collected sample to another. Our measurement of sampling variance is often accomplished with our usual tool for measuring variation: the variance.\nYou can’t directly see sampling variation in a single sample; however, we can use the theory of sampling variation to estimate from a single sample how much other samples might differ from the sample at hand. In this Lesson, we will simulate sampling variation in order to understand its properties, particularly how it depends on the sample size \\(n\\).\nTechnical vocabulary:\n\n“Confidence interval” (or “CI”) a range indicating the sampling variation of a sample statistic. Example: [19.1, 21.5]. Every model coefficient comes with a confidence interval. The conf_interval() model summary calculates the CI. Example:\n\n\nlm(mpg ~ hp, data=mtcars) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) 26.8    30.1    33.4   \n2 hp          -0.0889 -0.0682 -0.0476\n\n\n\n“Standard error”: the square-root of the “sampling variance.” It might make more sense to use “sampling standard deviation” for the square root of the “sampling variance.”\n\n“Margin of error” is the plus-or-minus part \\(20.3 \\pm 1.2\\) of another format for writing the CI.\n“Confidence level” is a number between 0 and 1. Almost always this is set to be 95%, which is what we will use. It is used to calculate the margin of error, which is a multiple of the standard error. For 95%, the multiplier is about 2. (If you took AP statistics, you may remember the number 1.96, which is their way of saying “two.”)"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#essential-take-home-points",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#essential-take-home-points",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Essential take-home points",
    "text": "Essential take-home points\n\nThe sampling variance depends on the sample size. Larger n leads to smaller sampling variance. The dependence is simple: sampling variance goes as \\(1/n\\).\nThe width of the confidence interval depends on the square-root of the sampling variance. Consequently, the width of the confidence interval goes as \\(1/\\sqrt{n}\\).\n\nStated more simply: Under normal conditions, more data means shorter confidence intervals."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#class-activity",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#class-activity",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Class Activity",
    "text": "Class Activity\nPoisson data"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#work-though-worksheet-22",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#work-though-worksheet-22",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Work though worksheet 22",
    "text": "Work though worksheet 22\nThis is a simulation demonstration of the essential take-home points listed above. In Lesson 23 we’ll see how it’s possible to do the calculations in real data."
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#confidence-level",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#confidence-level",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Confidence level",
    "text": "Confidence level\nNot a big deal in our course. Use 0.95, which is the default for conf_interval().\nA lower confidence level produces a shorter CI, a higher confidence level produces a longer CI.\n\nlm(mpg ~ hp, data=mtcars) |> conf_interval(level=0.80) |> filter(term==\"hp\")\n\n# A tibble: 1 × 4\n  term     .lwr   .coef    .upr\n  <chr>   <dbl>   <dbl>   <dbl>\n1 hp    -0.0815 -0.0682 -0.0550\n\nlm(mpg ~ hp, data=mtcars) |> conf_interval(level=0.90) |> filter(term==\"hp\")\n\n# A tibble: 1 × 4\n  term     .lwr   .coef    .upr\n  <chr>   <dbl>   <dbl>   <dbl>\n1 hp    -0.0854 -0.0682 -0.0511\n\nlm(mpg ~ hp, data=mtcars) |> conf_interval(level=1.00) |> filter(term==\"hp\")\n\n# A tibble: 1 × 4\n  term   .lwr   .coef  .upr\n  <chr> <dbl>   <dbl> <dbl>\n1 hp     -Inf -0.0682   Inf"
  },
  {
    "objectID": "Day-by-day/Lesson-22/Teaching-notes-22.html#digression",
    "href": "Day-by-day/Lesson-22/Teaching-notes-22.html#digression",
    "title": "Instructor teaching notes: Lesson 22",
    "section": "Digression",
    "text": "Digression\nIs process of fitting like the process of learning in animals? Here’s a report from The Economist:\n\nNever before … has the whole brain of such a complex organism—spanning some 548,000 connections between 3,016 neurons in the case of the fruit-fly larva—been mapped.\n\n\nThe latest work, published in Science, marks the culmination of over a decade’s worth of effort ….\n\n\nThe connectome of the fruit-fly larva has already provided insights. For example, regions of the creature’s brain associated with learning had more loops in their circuitry, with downstream neurons connecting back to those close behind them, than other regions of the brain. This suggested some repeat processing of signals. One proposed explanation is that such loops encode predictions, and that the creatures learn by comparing these with actual experiences.\n\n\nInformation about the taste of a leaf, for example, might enter a neuron simultaneously with a prediction based on previous meals. If the taste differs from prediction, the neuron may secrete dopamine, a chemical capable of rewiring the circuitry to create a new memory."
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "",
    "text": "Update your version of the {math300} package.\n\n\nremotes::install_github(\"dtkaplan/math300\")\n\nNow you can get_teaching_notes(24) to get the sources for the teaching notes. Here you can see all the code in an executable format. BUT … often the code goes well beyond the programming expected for Math 300 students. An example is below, where I use information published by the College Board to reconstruct/simulate SAT scores for individuals.\n\nI made a mistake in calculating the regression using Excel. It’s easy to make such mistakes. But if I had done things correctly, Excel would have given the right answer.\n\n\n# Reconstructing the math scores from College Board percentile data\n# Math 300Z students are NOT expected to understand this code\nMath_scores <- tibble::tribble(\n  ~ female, ~ male, ~score,\n  99.5, 99, 800,\n  98, 95, 750,\n  94, 90, 700,\n  88, 81, 650,\n  79, 71, 600,\n  65, 56, 550,\n  43, 37, 500,\n  27, 23, 450,\n  13, 12, 400,\n  4, 4, 350,\n  1, 1, 300, \n  0, 0, 250,\n) %>%\n  mutate(fweights = -diff(c(100, female))) %>%\n  mutate(mweights = -diff(c(100, male)))\n\nN = 10000\nFemales <- Math_scores[-nrow(Math_scores),] %>% \n  dplyr::select(-male) %>% \n  sample_n(size = N, weight = fweights, replace = TRUE) %>% dplyr::select(score) %>% mutate(sex = \"female\")\nMales <- Math_scores[-nrow(Math_scores), ] %>% \n  dplyr::select(-female) %>% \n  sample_n(size = N, weight = mweights, replace = TRUE) %>% dplyr::select(score) %>% mutate(sex = \"male\")\nMath_scores <- rbind(Females, Males) %>% \n  mutate(score = ifelse(score == 800, 800, \n                        score - #runif(nrow(.), min = 0, max = 49) +\n                          rnorm(nrow(.), sd = 20)))"
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#prediction",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#prediction",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Prediction",
    "text": "Prediction\nConfidence intervals are impressive because we have poor intuition about the effects of averaging. (For instance, the \\(1/\\sqrt{n}\\) width of confidence intervals is not obvious.)\n\nmod <- lm(height ~ mother*sex, data=Galton)\nmodel_plot(mod, interval=\"confidence\") +\n  facet_wrap(~ sex)\n\n\n\n\nPrediction intervals are unimpressive because it’s easy for us to draw the boundaries of the raw data.\n\nmodel_plot(mod, interval=\"prediction\") +\n  facet_wrap(~ sex)\n\n\n\n\nThe reasons it’s worthwhile to study prediction are:\n\nTo know when to use prediction and when to use estimation of effect size.\nTo know the proper form of a prediction, which is not the same thing as the proper form of a confidence interval.\n\nPrediction: List the possible outcomes, assign a probability (or prob. density, i.e. a relative probability) to each possible outcome.\n\nTo understand when it’s important to predict extremes rather than “central” values.\nAs a case study in updating probabilities as new information comes in.\n\nMarch Madness brackets."
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#case-study-test-scores",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#case-study-test-scores",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Case study: Test scores",
    "text": "Case study: Test scores\nThere are certain tasks where a confidence interval is appropriate and others where it is utterly misleading. It’s important to distinguish between the two.\nHere’s an example: a headline about SAT scores:\n\n“2016 SAT test results confirm pattern that’s persisted for 50 years — high school boys are better at math than girls”— source\n\nThis is the graph supporting the claim:\n\nUsing the simulated data, we can get a confidence interval on the difference between scores for females and males.\nWhat does the above graph show?\n\nWhy do the lines follow the same up-and-down path? Is this random variation?\nWhat covariates might be at work here?\n\nWho takes math in high-school\n\nfewer boys took low-level math at any time, more boys take physics\n\nNumber taking the exam? about 91M/100F\n“Natural sex ratio” is about 105 M / 100 F. in US Ratio of fraction of population taking the SAT 87 M/100 F."
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#estimating-the-confidence-interval-on-the-means",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#estimating-the-confidence-interval-on-the-means",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Estimating the confidence interval on the means",
    "text": "Estimating the confidence interval on the means\n\n\n\n\n\nFigure 1: The distribution of scores on the SAT mathematics test, reconstructed from summary data published by the College Board.\n\n\n\n\n\nFind the center of the distribution.\n\nFind an interval that includes “almost all” of the data: about 95%.\nFind an interval that includes about 2/3 of the data: 66%.\nThe data’s standard deviation, \\(s\\), will be somewhere in the range from\n\none-half of (3)\none-quarter of (2)\n\nThe margin of error will be \\(2 s/\\sqrt{n}\\). Here, \\(n\\) is roughly 1M.\nAs always, the confidence interval will be “center \\(\\pm\\) margin-of-error.”\n\nAs a computation, here it is for sample size \\(n=20,000\\).\n\nlm(score ~ 1, data=Math_scores) |> conf_interval()\n\n# A tibble: 1 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept)  508.  509.  511.\n\n\nThe actual size of the data is about \\(n=1,000,000\\), 50 times greater. So the confidence interval on the mean of the actual data will be about 1/7 of the one in the calculation: 1.5/7 = 0.2.\nWe can also look at the confidence interval on the difference in scores between the sexes:\n\nlm(score ~ sex, data=Math_scores) |> conf_interval()\n\n# A tibble: 2 × 4\n  term         .lwr .coef  .upr\n  <chr>       <dbl> <dbl> <dbl>\n1 (Intercept) 495.  497.  500. \n2 sexmale      20.1  23.2  26.3\n\n\nWith the full data set, the margin of error is 3/7 = 0.4."
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#explaining-the-difference",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#explaining-the-difference",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Explaining the difference?",
    "text": "Explaining the difference?\nLooking at average SAT from state to state as a function of expenditures and fraction.\n\nsat_model <- lm(math ~ frac + expend, data = SAT)\nmodel_plot(sat_model)\n\n\n\nconf_interval(sat_model)\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 493.   518.   543.  \n2 frac         -1.78  -1.53  -1.29\n3 expend        2.71   7.54  12.4 \n\n\nThe standard deviation of SAT math scores across individuals is about 110 points. The numbers taking the test are so large (about 800,000 F, 700,000 M) that the standard error for each group is \\[\\frac{110}{\\sqrt{750,000}} = 0.13\\ \\text{points}\\]\nThe issue I want to focus on, ignoring the covariates, is the extent to which you can figure out a student’s math aptitude by knowing the student’s sex.\nGiven a female, what’s the predicted test score? For a male?\nHow do you describe a prediction.\n\nIdeally: as a probability for each possible outcome: a probability distribution.\nHorribly: as the mean.\nConventionally: as an interval."
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#prediction-interval",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#prediction-interval",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Prediction interval",
    "text": "Prediction interval\nFormally …\n\nlm(score ~ sex, data=Math_scores) |>\n  model_eval(skeleton = TRUE, interval=\"prediction\")\n\n     sex  .output     .lwr     .upr\n1 female 497.4977 277.6429 717.3525\n2   male 520.7302 300.8755 740.5850\n\n\nInformally … Cover almost all (say, 95%) of the data."
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#assigning-a-probability",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#assigning-a-probability",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Assigning a probability",
    "text": "Assigning a probability\nAn interval is a convenient summary of a prediction. In the next class, we’ll see how to approximately translate an interval into a probability distribution.\n\nOrings <- Sleuth3::ex2011\nOrings <- Orings %>% mutate(fail = zero_one(Failure, one=\"Yes\"))\nmod_oring <- glm(fail ~ Temperature, data=Orings, family=\"binomial\")\nmodel_plot(mod_oring)\n\n\n\nmodel_eval(mod_oring, Temperature=28, interval=\"confidence\", level=.95)\n\n  Temperature   .output      .lwr     .upr\n1          28 0.9977133 0.3653965 0.999997"
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#converting-to-an-interval",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#converting-to-an-interval",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Converting to an interval",
    "text": "Converting to an interval"
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#watch-out-for-the-extremes",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#watch-out-for-the-extremes",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Watch out for the extremes",
    "text": "Watch out for the extremes\nIn designing infrastructure, it doesn’t matter so much what is the 97.5% max wind velocity per day. What matters is the 99.995% velocity: the biggest in, say, 50 years."
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#review-of-lesson-24",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#review-of-lesson-24",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Review of Lesson 24",
    "text": "Review of Lesson 24\nTHIS IS VERY DRAFTY.\nIn the last lesson, we considered effect size, a way to summarize a model to indicate the strength and direction of the influence of an explanatory variable with respect to a response variable. (As always, sample statistics such as the effect size are according to the model. A different model may give you different results, so the choice of model specification is important and should be carefully considered.)\nHere’s a possible DAG for the height of a child.\n\nheight_dag <- dag_make(\n  mother_genetics ~ exo(),\n  father_genetics ~ exo(),\n  exo_dad ~ exo(),\n  exo_mom ~ exo(),\n  mother_height ~ mother_genetics + exo_mom + exo(),\n  father_height ~ father_genetics + exo_dad + exo(),\n  health ~ exo(),\n  nutrition ~ exo(),\n  child_sex ~ exo(),\n  child_height ~ mother_genetics + father_genetics + child_sex + nutrition + health + exo()\n)\nset.seed(106); dag_draw(height_dag, vertex.size = 13,\n                        vertex.label.cex=0.75, \n                        edge.arrow.size=.25 )\n\n\n\n\nWe talk casually about the effect size of child’s height with respect to mother’s height, but there is no causal flow between mother’s height and child’s height. What we mean is the effect of a change in the mother’s genetics that leads to a 1 inch change in mother’s height and a corresponding change in child’s height.\nThe concept of an effect size is that all the other nodes are held constant when we change the mother’s genetics, including health, nutrition, sex, and father’s genetics.\nThere’s no way to do this; there aren’t two identical mothers to compare who differ only in their height-related genetics and can produce children who have exactly the same father’s genetics.\nInstead, what we think about is comparing two children from two similar mothers (differing only in height genetics) with similar father genetic contribution, nutrition, health, ….\nThe model specification height ~ mother + father + sex does this comparison of similars in a mathematical way. But the model output is not the only thing that changes when we generate this mathematical child’s height from two different height mothers.\nThe residuals from the model indicate the magnitude of all the other influences that we haven’t been able to hold constant.\n\nheight_mod <- lm(height ~ mother + father + sex, data=Galton)\nValues <- model_eval(height_mod)\n\nUsing training data as input to model_eval().\n\nggplot(Values, aes(x=mother, y=.resid)) + geom_jitter(alpha=0.5)\n\n\n\n\n\nIs there any clear pattern to the residuals?\nIf we were making a prediction of a daughter’s height, should we just look at the model value based on the parents’ heights, or should we take into account the residuals."
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#proper-form-for-a-prediction",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#proper-form-for-a-prediction",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Proper form for a prediction",
    "text": "Proper form for a prediction\nAssign a probability to every possible outcome.\n\nggplot(Values, aes(x=\" \", y=.resid)) + geom_violin(alpha=0.4, fill=\"blue\") +\n  geom_jitter(alpha=0.2, width=.15) + xlab(\"Residual\") + ylab(\"\") +\n  geom_hline(yintercept=c(-4.13, 4.04), color=\"red\", alpha=0.5)\n\n\n\nValues |> summarize(m = mean(.resid), sd=sd(.resid), \n                    q2.5=quantile(.resid, 0.025),\n                    q97.5 = quantile(.resid, 0.975))\n\n              m       sd      q2.5    q97.5\n1 -2.882542e-14 2.150721 -4.129319 4.043707"
  },
  {
    "objectID": "Day-by-day/Lesson-25/Teaching-notes-25.html#realistic-predictions",
    "href": "Day-by-day/Lesson-25/Teaching-notes-25.html#realistic-predictions",
    "title": "Instructor Teaching Notes for Lesson 25",
    "section": "Realistic predictions",
    "text": "Realistic predictions\nPrecipitation runoff: It rains. Some of the water is absorbed, some runs off and ends up in a nearby river. The runoff is measured as the depth of water over the entire catchment basin that ends up in the river. Give a forecast of a storm, we might want to know if a flood is likely.\nData for the Monocacy River in Maryland, close to where I grew up.\nRESULT FOR THE BIGGEST RAINFALL DEPENDS ON how logarithms are used.\n\nggplot(Monocacy_river |> filter(precip > 2), aes(x=log(precip), y=log(runoff))) +\n  geom_point() +\n  geom_lm(interval=\"prediction\") +\n  geom_lm(interval=\"confidence\", fill=\"blue\") # +\n\nWarning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n # geom_abline(slope=1.08, intercept=-1.07)"
  },
  {
    "objectID": "Day-by-day/Lesson-24/with-respect-to.html#with-respect-to",
    "href": "Day-by-day/Lesson-24/with-respect-to.html#with-respect-to",
    "title": "Spring 2023 Math 300Z",
    "section": "With respect to …",
    "text": "With respect to …\nThe palmerpenguins::penguins data frame records body size and shape of penguins of three different species.\n\nBuild a model bill_length_mm ~ bill_depth_mm with a single quantitative explanatory variable.\n\nWhat is the effect size?\nWhat are the units? Is it a rate or a difference?\n\nBuild a model bill_length_mm ~ species with a single categorical explanatory variable.\n\nWhat is the effect size?\nWhat are the units? Is it a rate or a difference?\n\nNow a model bill_length_mm ~ flipper_length + body_mass_g. There are two variables, so there will be two effect sizes.\n\nWhat are the effect sizes? What are their units.\nWhich of the two are rates?"
  },
  {
    "objectID": "Day-by-day/Lesson-24/Teaching-notes-24.html",
    "href": "Day-by-day/Lesson-24/Teaching-notes-24.html",
    "title": "Teaching notes, Lesson 24",
    "section": "",
    "text": "Here are some fundamentals:"
  },
  {
    "objectID": "Day-by-day/Lesson-24/Teaching-notes-24.html#why-so-much-interest-in-coefficients",
    "href": "Day-by-day/Lesson-24/Teaching-notes-24.html#why-so-much-interest-in-coefficients",
    "title": "Teaching notes, Lesson 24",
    "section": "Why so much interest in coefficients?",
    "text": "Why so much interest in coefficients?\n\nAn important statistical task (in some situations) is to detect whether there is some connection between an explanatory variable and the response variable. This often amounts to asking whether a coefficient is zero. More precisely, asking whether the confidence interval includes zero.\nMathematical tradition. There are other ways to present the same information. But people with a good head for mathematics find the coefficients easy to interpret … until the models become complicated.\nHere’s a way to plot out the Galton model:\n\n\ngmodel <- Galton |> \n  model_train(height ~ mother + father + sex)\nmodel_plot(gmodel)\n\n\n\n\n\nWe can also show sampling variability graphically, although the plots can be hard to interpret.\n\n\nmodel_plot(gmodel, interval=\"confidence\")\n\n\n\n\n\nAnother important type of statistical questions involves intervention: If we change the value of an input to the model, how much will the output change?\n\n\nmodel_eval(gmodel, mother=c(60,65), father=65, sex=\"F\")\n\n  mother father sex  .output     .lwr     .upr\n1     60     65   F 61.02304 56.77622 65.26986\n2     65     65   F 62.63052 58.38981 66.87122\n\n\nThis is called an “effect size.” From the above table, we can calculate the effect size with respect to mother."
  },
  {
    "objectID": "Day-by-day/Lesson-24/Teaching-notes-24.html#effect-size",
    "href": "Day-by-day/Lesson-24/Teaching-notes-24.html#effect-size",
    "title": "Teaching notes, Lesson 24",
    "section": "Effect size",
    "text": "Effect size\n\nAn effect size (for numerical variables) is a partial derivative of the model function.\nAn effect size (for categorical variables) is a partial difference.\nWhether a model’s effect size says what would happen in the real world if we changed a model input depends on whether we have captured causal connections properly with our model.\n\nWe can’t change the explanatory variables in a meaningful way in human height, but suppose the experiment were to make kindergarten class sizes smaller and look for the effect on later student achievement.\n\nCalculate an effect size: use model_eval() changing one variable at a time.\nCalculating confidence interval on effect size: Involves a lot of accounting, except in the simplest cases.\nUnits of effect size.\n\nQuantitative variable: [response]/[explanatory] a rate (a.k.a slope)\nCategorical variable: [response]"
  },
  {
    "objectID": "Day-by-day/Lesson-24/Teaching-notes-24.html#why-always-in-model-specifications",
    "href": "Day-by-day/Lesson-24/Teaching-notes-24.html#why-always-in-model-specifications",
    "title": "Teaching notes, Lesson 24",
    "section": "Why always + in model specifications",
    "text": "Why always + in model specifications\nThis is just for us, to make model interpretation easier.\nWith + models, the coefficient is always the same as the effect size.\nBut not generally. Examples with Galton data\n\nmod1 <- lm(height ~ mother*father*sex, data=Galton)\nmodel_plot(mod1)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\n\n\n\nNotice that the lines aren’t parallel. We can also do curvy functions.\n\nlibrary(splines)\nmod2 <- lm(height ~ ns(mother,2)*ns(father,2)*sex, data=Galton)\nmodel_plot(mod2)\n\nWarning: Ignoring unknown aesthetics: fill\n\n\n\n\n\nMight the curviness or non-parallel nature of the lines just be a matter of sampling variation?\n\nmodel_plot(mod1, interval=\"confidence\")\n\n\n\nmodel_plot(mod2, interval=\"confidence\")\n\n\n\n\nAll the information for the confidence bands is contained in the coefficients (and residuals), but good luck figuring it out!\n\nmod2 |> conf_interval()\n\n# A tibble: 18 × 4\n   term                                  .lwr   .coef  .upr\n   <chr>                                <dbl>   <dbl> <dbl>\n 1 (Intercept)                         50.1    55.4   60.7 \n 2 ns(mother, 2)1                       1.01   10.7   20.5 \n 3 ns(mother, 2)2                     -13.0    -4.36   4.31\n 4 ns(father, 2)1                       4.97   15.4   25.9 \n 5 ns(father, 2)2                      -0.486   4.98  10.5 \n 6 sexM                                -1.19    6.00  13.2 \n 7 ns(mother, 2)1:ns(father, 2)1      -32.1   -12.8    6.55\n 8 ns(mother, 2)2:ns(father, 2)1       -1.03   15.1   31.3 \n 9 ns(mother, 2)1:ns(father, 2)2       -8.41    2.27  12.9 \n10 ns(mother, 2)2:ns(father, 2)2       -3.90    4.43  12.8 \n11 ns(mother, 2)1:sexM                -17.3    -3.92   9.43\n12 ns(mother, 2)2:sexM                -19.7    -6.97   5.72\n13 ns(father, 2)1:sexM                -15.0    -0.854 13.3 \n14 ns(father, 2)2:sexM                 -4.29    3.48  11.3 \n15 ns(mother, 2)1:ns(father, 2)1:sexM -20.2     6.10  32.4 \n16 ns(mother, 2)2:ns(father, 2)1:sexM -10.1    13.2   36.5 \n17 ns(mother, 2)1:ns(father, 2)2:sexM -23.4    -8.08   7.25\n18 ns(mother, 2)2:ns(father, 2)2:sexM -15.8    -2.90   9.97\n\n\nStatisticians keep in mind this folk wisdom:\n\nIf you try to capture too much detail in the relationship, you won’t capture anything.\n\nNotice that all but three of the terms have confidence intervals that include zero."
  },
  {
    "objectID": "Day-by-day/Lesson-23/got-you-covered.html#got-you-covered",
    "href": "Day-by-day/Lesson-23/got-you-covered.html#got-you-covered",
    "title": "Spring 2023 Math 300Z",
    "section": "Got you covered",
    "text": "Got you covered\nSimulate from a DAG, use the data to train a model that has all the same explanatory variables as in the formula for y in the DAG, and calculate the confidence interval @ 95%\nDid everybody’s interval include the parameter from the DAG?\nMove to an 80% interval. About 4 students should not cover the parameter.\nMove to a 50% interval. About 10 students should not cover the parameter.\nMove to a 100% interval. What do the results tell you?\nFrederick the Great said, “To defend everything is to defend nothing.” paraphrase as “To try to cover everything is to cover nothing.”"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "",
    "text": "Basic procedure in statistical interpretation of data: collect data then calculate one or more sample statistics from those data. Example: Galton’s data on heights and an analysis of whether a child’s height is related to the parents’.\n\n\nlm(height ~ mother + father + sex, data=Galton) |> coefficients()\n\n(Intercept)      mother      father        sexM \n 15.3447600   0.3214951   0.4059780   5.2259513 \n\n\nQuestion: Are fathers more influential on the children’s height than mothers?\n\nBut statisticians don’t regard the sample statistics in (1) as completely informative because we know, in our hearts, that the particular sample we worked with is just one of many, many samples that we might have collected. Each of those hypothetical samples has its own sample statistic, which likely differs from the sample statistic we calculated in (1). This is what we mean by sampling variability.\nWe can say that sampling variability implies that our sample statistics are noisy, that they are not absolutely precise.\nWe describe the precision of a sample statistic via a confidence interval, which can be in either of two forms, e.g.:\n\n\\([56.3, 57.1]\\) or \\(56.7 \\pm 0.4\\)"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#lesson-23",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#lesson-23",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "Lesson 23",
    "text": "Lesson 23\nThere are mathematical techniques that allow us to get a handle on the precision of a sample statistic using just the sample at hand. Here is the central mathematical fact:\n\nWidth of confidence interval is proportional to \\(1/\\sqrt{n}\\).\n\nLet’s demonstrate this using simulations (via DAGs).\n\nsample(dag06, size=400) |> \n  lm(d ~ c + a, data=_) |>\n  conf_interval() |>\n  mutate(width = .upr - .lwr) \n\n# A tibble: 3 × 5\n  term           .lwr  .coef  .upr width\n  <chr>         <dbl>  <dbl> <dbl> <dbl>\n1 (Intercept) -0.0588 0.0345 0.128 0.187\n2 c            0.919  0.984  1.05  0.130\n3 a            0.868  0.980  1.09  0.223\n\n\nHere, the width of each term is subject, like everything else, to sampling variation.\nWe can average over many trials to reduce the sampling variation.\n\n{do(500) * {\n  sample(dag06, size=100) |> \n  lm(d ~ c + a, data=_) |>\n  conf_interval() |>\n  mutate(width = .upr - .lwr)} \n  } |>\n  group_by(term) %>% \n  summarize(ave_width = mean(width))\n\n# A tibble: 3 × 2\n  term        ave_width\n  <chr>           <dbl>\n1 (Intercept)     0.401\n2 a               0.495\n3 c               0.286\n\n\nTry this for different sample sizes: 25, 100, 400.\n\nWhat will be the width of the confidence interval when the sample size is 1600?"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#how-to-find-the-confidence-interval-when-we-have-only-one-sample",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#how-to-find-the-confidence-interval-when-we-have-only-one-sample",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "How to find the confidence interval when we have only one sample",
    "text": "How to find the confidence interval when we have only one sample\n\nConstruct many trials of subsamples, say 1/50th the size of the data.\n\n\nlm(height ~ mother + father + sex, data=sample(Galton, size=18)) |>\n  conf_interval() |>\n  select(term, .coef)\n\n# A tibble: 4 × 2\n  term         .coef\n  <chr>        <dbl>\n1 (Intercept) 15.6  \n2 mother       0.224\n3 father       0.474\n4 sexM         6.04 \n\n\n\n{do(500) * {\n  lm(height ~ mother + father + sex, data=sample(Galton, size=18)) |>\n  conf_interval() |>\n  select(term, .coef)\n}} |> \n  group_by(term) |>\n  summarize(var_coef = var(.coef))\n\n# A tibble: 4 × 2\n  term        var_coef\n  <chr>          <dbl>\n1 (Intercept) 508.    \n2 father        0.0607\n3 mother        0.0710\n4 sexM          1.21  \n\n\nWe can look at any of the coefficients. Let’s use sexM for the example.\nThe sampling variance of the sexM coefficient for a sample of size \\(n=18\\) is 1.14 inches2.\n\nThe “standard error” corresponding to this is \\(\\sqrt{1.14} = 1.07\\) inches.\nThe width of the confidence interval for a sample of size 18 will be four times the standard error—that’s part of the definition of the confidence interval, that is 4.28.\nFour the whole sample, \\(n=898\\), the width of the confidence interval will be \\(4.28 \\times \\sqrt{\\frac{18}{898}} = 0.606\\).\n\nCheck this against the calculation:\n\nlm(height ~ mother + father + sex, data=Galton) |>\n  conf_interval() %>%\n  mutate(width=.upr - .lwr)\n\n# A tibble: 4 × 5\n  term         .lwr  .coef   .upr  width\n  <chr>       <dbl>  <dbl>  <dbl>  <dbl>\n1 (Intercept) 9.95  15.3   20.7   10.8  \n2 mother      0.260  0.321  0.383  0.123\n3 father      0.349  0.406  0.463  0.115\n4 sexM        4.94   5.23   5.51   0.565\n\n\n\nWhat does this report tell you about whether fathers contribute more to children’s height than mothers? Compare the confidence interval on mother and father."
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#activity",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#activity",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "Activity",
    "text": "Activity\nGot you covered"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#precision-versus-accuracy",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#precision-versus-accuracy",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "Precision versus accuracy",
    "text": "Precision versus accuracy\nIn the case of our model y ~ x + a on data simulated from dag02 we can compare the model coefficients to the formula for y in the DAG.\n\ndag_draw(dag06)\n\n\n\nprint(dag06)\n\na ~ exo()\nb ~ a + exo()\nc ~ b + exo()\nd ~ c + a + exo()\n\nsample(dag06, size=25) |> \n  lm(d ~ c + a, data=_) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr .coef  .upr\n  <chr>        <dbl> <dbl> <dbl>\n1 (Intercept) -0.338 0.132 0.602\n2 c            0.687 0.970 1.25 \n3 a            0.705 1.27  1.84 \n\n\nThe coefficients are a close match to the DAG formula.\nIf I repeat this simulation many times, roughly one time in twenty the actual formula coefficient will be outside the confidence interval. (Do the trials.)\nMake the sample size very large so that we get a very narrow confidence interval to demonstrate the accuracy.\n\nFrom the above, predict what sample size is needed to get a confidence interval that is about 0.01 wide. (The CI on x for example, is has width about 1. To get 0.01 we need 1002 as much data.)\n\n\nsample(dag06, size=250000) |> \n  lm(d ~ c + a, data=_) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term            .lwr   .coef    .upr\n  <chr>          <dbl>   <dbl>   <dbl>\n1 (Intercept) -0.00243 0.00150 0.00543\n2 c            0.998   1.00    1.00   \n3 a            0.994   0.998   1.00   \n\n\nMore data buys better precision. But accuracy is a different matter altogether. The above model is both precise and accurate.\nHere is a model that is very precise, but not at all accurate:\n\nsample(dag06, size=250000) |> \n  lm(d ~ c, data=_) |>\n  conf_interval()\n\n# A tibble: 2 × 4\n  term            .lwr     .coef    .upr\n  <chr>          <dbl>     <dbl>   <dbl>\n1 (Intercept) -0.00593 -0.000862 0.00420\n2 c            1.33     1.33     1.34   \n\n\n\nWhat is it that tells you the precision of the coefficients?\n\n\nWhat is it that tells you the coefficient on c is not accurate?"
  },
  {
    "objectID": "Day-by-day/Lesson-23/Teaching-notes-23.html#going-further",
    "href": "Day-by-day/Lesson-23/Teaching-notes-23.html#going-further",
    "title": "Instructor Teaching Notes for Lesson 23",
    "section": "Going further",
    "text": "Going further\nStatistics texts tend to feature simple models without covariates.\nObviously, the choice of covariates or other model terms is not an issue. Sample statistics are designed mathematically so that they are accurate. This is called being unbiased.\nBut in cases of actual interest, where we are interested in causal connections, we generally cannot know from our data whether the sample statistic is accurate.\nCollecting more data will not help us determine the accuracy; more data improves precision but not accuracy.\nLater, in Lesson 30, we can return to accuracy. There, we’ll look at accuracy with respect to a DAG, choosing covariates so that the model would be accurate if the data were indeed generated by a mechanism well represented by our DAG."
  },
  {
    "objectID": "textbooks.html",
    "href": "textbooks.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "For Lessons 1-17, the textbook is Statistical Inference via Data Science (“ModernDive”)"
  },
  {
    "objectID": "textbooks.html#lessons-19-onward",
    "href": "textbooks.html#lessons-19-onward",
    "title": "Spring 2023 Math 300Z",
    "section": "Lessons 19 onward …",
    "text": "Lessons 19 onward …\n\n\nFor Lessons 19-39, the textbook is Lessons in Statistical Thinking"
  },
  {
    "objectID": "math300-software.html",
    "href": "math300-software.html",
    "title": "R package for Math 300Z",
    "section": "",
    "text": "Most students will want to use POSIT.cloud. Use the “Z-Section” project."
  },
  {
    "objectID": "math300-software.html#accessing-the-daily-worksheet",
    "href": "math300-software.html#accessing-the-daily-worksheet",
    "title": "R package for Math 300Z",
    "section": "Accessing the daily worksheet",
    "text": "Accessing the daily worksheet\nAlmost all class days there will be a worksheet in the form of an Rmd file.\nFrom within your POSIT.cloud project, download the Rmd file using this command, substituting the number of the relevant Lesson:\nmath300::get_lesson_worksheet(19)\n\nMake a habit of reading each day’s Rmd file before class. This way you can note what doesn’t yet make sense so that you can be receptive to the topic in class.\nComplete the worksheet after class."
  },
  {
    "objectID": "math300-software.html#math-300z-r-commands",
    "href": "math300-software.html#math-300z-r-commands",
    "title": "R package for Math 300Z",
    "section": "Math 300Z R commands",
    "text": "Math 300Z R commands\nThere will be only a dozen commands that you will be using in the second half of Math 300Z. Almost all of them involve constructing or summarizing models.\nAs a reminder, here are some of the commands/syntax that should be familiar to you from the first half of the course.\n\nlm() fits (or “trains”) a “linear model” on data from a data frame.\nggplot() sets things up for a new graphic. Use aes() as an argument.\ngraphics layers to add onto the output of ggplot():\n\ngeom_point(), geom_jitter(alpha=0.5)\n\nfilter(), mutate() and summarize() are basic data-wrangling commands we will use often.\n\nNew commands in the second half of the course:\n\nsummarize a model: conf_intervals(), R2()\nevaluate a model: model_eval()\ngraphic of a model: model_plot() (This replaces the geom_smooth() used in the first half of the course.)\nvariance of a variable in a data frame: DF %>% summarize(NM = var(VAR))\ndraw a DAG: dag_draw()\nsample from a DAG: sample(DAG, size=100)\n\nA few other commands will be used occasionally in examples and demonstrations. You should know what they do, but typically there will be a reminder of the syntax: zero_one(), shuffle(), do() * {}, dag_intervene(), tibble(), regression_summary(), anova_summary()."
  },
  {
    "objectID": "math300-software.html#running-rrstudio-on-your-laptop",
    "href": "math300-software.html#running-rrstudio-on-your-laptop",
    "title": "R package for Math 300Z",
    "section": "Running R/RStudio on your laptop?",
    "text": "Running R/RStudio on your laptop?\nInstall the {math300} and other packages with these two commands:\ninstall.packages(c(\"mosaic\", \"ggplot\", \"dplyr\", \"openintro\", \"moderndive\", \"nycflights13\", \"knitr\"))\n# additional package for Math 300Z\nremotes::install_github(\"dtkaplan/math300\")"
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "",
    "text": "The topic of the last four lessons was using the technology of “multiple regression” in order to make accurate models of a supposed causal connection between variables, for instance A \\(\\longrightarrow\\) B. This involved two phases:\n\nUsing covariates along with B and A in order to adjust for other factors that might make the effect size estimated from B ~ A misleading. For us in Math 300Z, this has been presented as a matter of including the relevant covariates among the explanatory variables. There are more sophisticated techniques (such as “structural equation modeling” and Bayesian estimation of “latent variables”) but the multiple regression technique is powerful and so common in professional work that it might be considered the mother of all other techniques.\nExpressing our “beliefs” about the network of causal relationships that ties the covariates to A and B in the form of a DAG. Analysis of pathways between A and B in the DAG reveals which covariates ought to be included and which excluded from our model. When our beliefs are uncertain, it can be helpful to construct multiple plausible DAGs and examine whether the proper choice of covariates is robust across the range of plausible DAGs. Often, it will not be robust, a situation calling for re-assessment and consideration of whether there are additional variables that might be included to avoid the ambiguity.\n\nMultiple regression with the use of covariates and DAGs is a powerful technique and often produces results that decision-makers can rely on. Every modeler should have it among her tools. It is not, however, all-powerful and there are situations where it should be supplemented by other techniques, especially in situations where those techniques can be easily applied."
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#limits-to-the-multiple-regressioncovariate-approach",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#limits-to-the-multiple-regressioncovariate-approach",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Limits to the multiple-regression/covariate approach",
    "text": "Limits to the multiple-regression/covariate approach\n\nThere may be influential factors that are not or cannot be measured. Without measuring such factors, it’s impossible to use them as covariates.\nThere can be factors that have not been recognized as such. These are sometimes called “lurking variables.”\nDAGs are always hypotheses; they represent our beliefs. Our beliefs may not align well with reality. There are some data-analysis techniques that can help to establish the plausibility of a proposed DAG, but ultimately they should be treated as unconfirmed hypotheses.\nThere can be irreconcilable differences in beliefs about causal connections and data-analysis techniques cannot always resolve these.\nRegression techniques provide no guarantee that they capture the functional form of relationships. We don’t address this issue in Math 300Z so the vocabulary of unhard-to-capture functional forms will be unfamiliar, but as background information we’ll name some: threshold effects, interactions, nonlinearities, non-normality, multi-collinearity, and heteroscedasticity."
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#experiment",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#experiment",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Experiment",
    "text": "Experiment\nAn experiment is an intervention intended to simplify the system under study to reduce ambiguity in conclusions.\nWe will focus on conclusions of the form B ~ A, but in many practical applications there is a set of explanatory variables of interest, as in B ~ A1 * A2 * A3. For instance, an agriculture experiment might evaluate the differing yields of varieties of a crop, looking at the same time at the effects of fertilizers, watering, as well as herbicide and pesticide usage."
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#factors-leading-to-ambiguous-conclusions",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#factors-leading-to-ambiguous-conclusions",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Factors leading to ambiguous conclusions:",
    "text": "Factors leading to ambiguous conclusions:\n\nnoise and unwanted randomness\nconfounding and other forms of backdoor causal pathways\nthe possibility of “lurking variables”\nsheer complexity of systems"
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#simplifying-interventions",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#simplifying-interventions",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Simplifying interventions",
    "text": "Simplifying interventions\n\n[Domain specific] Hold as constant as possible any influences that are not of direct interest. E.g. use pure reagents, keep temperature at a standard level, collect data at similar times of day, week, or year, keep pristine laboratory conditions (no food or drink, avoid dust, vibration, wear white lab coats!).\n[General] If you can’t hold constant a factor that’s important to the system (e.g. weather, time), replicate the experiment across different levels of the factor. Then average or adjust at the data-analysis phase.\n[Domain specific] Measure accurately and sensitively, as with calibrated instruments. Example: Michelson and Morley’s work on the effect of “aether” on the propagation of light. They made an ingenious design using interferometry that was sensitive to small differences in the speed of light in different. Millikan’s measurement of the charge of the electron through the movement of tiny oil drops.\n[Of primary interest to studying causality] Intervene in the system under study to cut the explanatory variable off from any influence other than one the experimenter controls.\n\n\n\n\n\n\n\n\n(a) Natural system\n\n\n\n\n\n\n\n(b) Experimental intervention\n\n\n\n\nFigure 1: Intervening to simplify the network of causal connections.\n\n\n\nImpose variation via the experimenter \\(\\rightarrow\\) explanatory route. Compare and contrast.\nAvoid any input to explanatory other than from experimenter.\n\n\n\n\n\n\nFigure 2: Letting the experimenter’s decision about treatment be influenced by the patient’s condition opens the back-door pathway from treatment to outcome.\n\n\n\n\n\nAvoid any connection between experimenter and nodes other than explanatory.\n\n\n\n\n\n\nFigure 3: A situation calling for double-blinding. There are back-door pathways from vaccination to diagnosis. Blinding eliminates the vaccination \\(\\longrightarrow\\) risk-taking link. Double-blinding eliminates the vaccination \\(\\longrightarrow\\) diagnosis link.\n\n\n\n\n\nSet experimenter to be uncorrelated with any measured, unmeasured, or unknown factor."
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#ways-that-e-gets-violated",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#ways-that-e-gets-violated",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Ways that (e) gets violated",
    "text": "Ways that (e) gets violated\n\nSubjects don’t comply with their assigned treatment, e.g. drug/placebo.\nSubjects react differently based on their treatment. Example: drivers with anti-lock brakes tended to raise the threshold of bad conditions in which they would drive. Partial solution: make the patient blind to the treatment.\nAssignment of treatment is based (in part or in whole) on some assessment of the subject. Example: doctor assigns surgery to those patients who are in the best condition to survive it."
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#ways-to-implement-g",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#ways-to-implement-g",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Ways to implement (g)",
    "text": "Ways to implement (g)\n\nMeasured covariates: Use blocking.\nUnmeasured or unknown: Random assignment.\n\nBoth: Random assignment within blocking."
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#example-agricultural-experiments",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#example-agricultural-experiments",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Example: Agricultural experiments",
    "text": "Example: Agricultural experiments\nA major feature of agricultural experiments is that the fertility of soil varies from place to place, and can vary differently according to the year because of the weather, exhaustion of nutrients, and so on.\nThe Wheat data frame contains measurements of the yield of several varieties of winter wheat. The data shows how they organized the experiment.\n\nThe field was divided into three blocks. Each variety was replicated by planting in each block, so that the effect of different block fertility would average out.\nEach block was divided into many smaller areas, plots, where the fertilities might be different. Each variety was randomly assigned to a plot, separately for each block..\nThe comparative yields of the different varieties might vary according to the weather. We can’t hold the weather constant. Consequently, the experiment replicated across several years, so that the weather-related effects might average out.\n\n\n\n\n\n\n\nwith(Wheat, table(genotype, year, block))\n\n, , block = 1\n\n          year\ngenotype   1996 1997 1998 1999 2000 2001 2002\n  Colosseo    1    1    1    1    1    1    1\n  Creso       1    1    1    1    1    1    1\n  Duilio      1    1    1    1    1    1    1\n  Grazia      1    1    1    1    1    1    1\n  Iride       1    1    1    1    1    1    1\n  Sancarlo    1    1    1    1    1    1    1\n  Simeto      1    1    1    1    1    1    1\n  Solex       1    1    1    1    1    1    1\n\n, , block = 2\n\n          year\ngenotype   1996 1997 1998 1999 2000 2001 2002\n  Colosseo    1    1    1    1    1    1    1\n  Creso       1    1    1    1    1    1    1\n  Duilio      1    1    1    1    1    1    1\n  Grazia      1    1    1    1    1    1    1\n  Iride       1    1    1    1    1    1    1\n  Sancarlo    1    1    1    1    1    1    1\n  Simeto      1    1    1    1    1    1    1\n  Solex       1    1    1    1    1    1    1\n\n, , block = 3\n\n          year\ngenotype   1996 1997 1998 1999 2000 2001 2002\n  Colosseo    1    1    1    1    1    1    1\n  Creso       1    1    1    1    1    1    1\n  Duilio      1    1    1    1    1    1    1\n  Grazia      1    1    1    1    1    1    1\n  Iride       1    1    1    1    1    1    1\n  Sancarlo    1    1    1    1    1    1    1\n  Simeto      1    1    1    1    1    1    1\n  Solex       1    1    1    1    1    1    1"
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#dont-fetishize-experiment",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#dont-fetishize-experiment",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Don’t fetishize experiment",
    "text": "Don’t fetishize experiment\nWhen you need information about a system, work with the available resources and do the best job you can."
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#for-worksheet",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#for-worksheet",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "For worksheet:",
    "text": "For worksheet:\nShow that experiment works.\n\ndag08\n\nc ~ exo()\nx ~ c + exo()\ny ~ x + c + 3 + exo()\n\nexp08 <- dag_intervene(dag08, x ~ binom())\ndag_draw(exp08)\n\n\n\n\nAnalyze the Wheat data"
  },
  {
    "objectID": "Day-by-day/Lesson-32/Teaching-notes-32.html#demonstration-random-assignment",
    "href": "Day-by-day/Lesson-32/Teaching-notes-32.html#demonstration-random-assignment",
    "title": "Instructor Teaching Notes for Lesson 32",
    "section": "Demonstration: Random assignment",
    "text": "Demonstration: Random assignment\nConfounding with some lurking variable. If we could measure it, we could use adjustment to get the correct relationship between group and outcome.\n\nassignment_dag1 <- dag_make(\n  lurking_var ~ exo(),\n  group ~ binom(lurking_var, labels=c(\"A\", \"B\")),\n  outcome ~ lurking_var + (group==\"B\") + exo()\n)\ndag_draw(assignment_dag1, vertex.label.cex=1)\n\n\n\nSamp <- sample(assignment_dag1, size=1000)\nlm(outcome ~ group, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr   .coef   .upr\n  <chr>        <dbl>   <dbl>  <dbl>\n1 (Intercept) -0.175 -0.0549 0.0654\n2 groupB       0.914  1.08   1.26  \n\nlm(outcome ~ group + lurking_var, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr   .coef   .upr\n  <chr>         <dbl>   <dbl>  <dbl>\n1 (Intercept) -0.0767 0.00891 0.0945\n2 groupB       0.868  0.990   1.11  \n3 lurking_var  0.922  0.983   1.04  \n\n\nRandom assignment makes group unconnected with lurking_var, so we don’t need to adjust for it to get the correct result.\n\nassignment_dag2 <- dag_make(\n  lurking_var ~ exo(),\n  group ~ binom(exo(), labels=c(\"A\", \"B\")),\n  outcome ~ lurking_var + (group==\"B\") + exo()\n)\ndag_draw(assignment_dag2, vertex.label.cex=1)\n\n\n\nSamp <- sample(assignment_dag2, size=1000)\nlm(outcome ~ group, data=Samp) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef  .upr\n  <chr>        <dbl>  <dbl> <dbl>\n1 (Intercept) -0.103 0.0225 0.148\n2 groupB       0.750 0.927  1.10 \n\n\nStill, when we can adjust, it may reduce the confidence intervals.\n\nlm(outcome ~ group + lurking_var, data=Samp) |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr   .coef   .upr\n  <chr>        <dbl>   <dbl>  <dbl>\n1 (Intercept) -0.126 -0.0353 0.0551\n2 groupB       0.944  1.07   1.20  \n3 lurking_var  0.915  0.978  1.04"
  },
  {
    "objectID": "Worksheets/Worksheet-32.html",
    "href": "Worksheets/Worksheet-32.html",
    "title": "Lesson 32: Worksheet",
    "section": "",
    "text": "Note: If you installed the {math300} R package before Apr. 12, 2023, update it by giving this command in your console:\nremotes::install_github(\"dtkaplan/math300)\n\nNote 2: You’ll be using dag_make() in this worksheet. You’ll be given the commands you need. The exams, however, will not require you to understand how to use dag_make(); it’s just for demonstration purposes here.\n\nThis worksheet is about random assignment to treatment in an experiment, as well as blocking, and how they make the treatment uncorrelated with a random covariate or, in the case of blocking, the covariate used for blocking.\nThe general setting is that we have a system in which output y is possibly influenced by x and some covariates and we want to use randomization or blocking to assign treatment. For the sake of definiteness, we’ll work with job_dag, which implements a simple confounder. You’ll have to run the following DAG to\n\n\n\n\n\nThe relationship of interest is the direct link between participation in the training program and the person’s outcome in terms of getting a job.\nWith the diagram and formulas for job_dag in hand, we can see that the person’s diligence is a confounder for the relationship participation \\(\\longrightarrow\\) outcome. In a realistic situation, however, we don’t know for certain what is the data-generation process; any DAG that we have is simply a hypothesis. In addition, in a realistic situation we might not have any way to measure diligence: it is a “lurking variable.”\nQUESTION: Is education a confounder of the link between participation and outcome?\n\n\n\n\n\n\nANSWER\n\n\n\neducation is not a confounder. There is no path from participation to outcome via education.\n\n\nTask 1: Carry out an “observational” study by generate a sample of size \\(n=400\\) from job_dag and analyzing it simply to estimate the effect of participation on outcome. The following chunk does the analysis in two different ways: 1) the fractions of the participators and non-participators who have a successful outcome and 2) a regression technique that gives confidence intervals easily:\n\nObservations <- sample(job_dag, size=400)\n# Technique 1\nObservations |> group_by(participation) |> \n  summarize(frac = mean(outcome == \"job\"))\n\n# A tibble: 2 × 2\n  participation  frac\n  <chr>         <dbl>\n1 enrolled      0.4  \n2 not           0.312\n\n# Technique 2\nObservations |> mutate(job = zero_one(outcome, one=\"job\")) |>\n  lm(job ~ participation, data=_) |>\n  conf_interval()\n\n# A tibble: 2 × 4\n  term               .lwr   .coef    .upr\n  <chr>             <dbl>   <dbl>   <dbl>\n1 (Intercept)       0.331  0.400  0.469  \n2 participationnot -0.182 -0.0884 0.00565\n\n\nInterpret the output from the two analysis techniques. On the face of things, do the data indicate that the job training program helps people to get a job? Do the two analysis techniques give different estimates of the effect size?\n\n\n\n\n\n\nAnswer\n\n\n\nYour results will involve sampling variation, but usually the chunk’s output will show that the fraction of the participants who get a job is roughly 10-20% higher than the non-participants. The regression model gives an exactly equivalent result; the coefficient participantnot will exactly conform to the difference in proportions from the first analysis technique. The confidence interval on the participationnot coefficient will have both ends greater than zero, so we are statistically justified to conclude that there is positive effect size. (If your results are inconsistent with this description, that is an accident of sampling variation. Run the data simulation and analysis again two or three times to see that the description above is generally correct.)\n\n\nA reasonable critique of the results from Task 1 is that there may be confounding. Other factors might be influencing the outcome than participation. Since we know the DAG that generated the data, it’s obvious that a “other factor” is the confounder diligence. But in a realistic situation we could only speculate that such a confounders exist. Unfortunately, in a realistic situation, we might have not data on the hypothetical confounders (like diligence), so we could not adjust for them.\nTask 2: education seems like a reasonable candidate for a confounder. Repeat the analysis using education as a covariate. Does adjusting for education make a noticable difference in the conclusions?\n\n\n\n\n\n\nANSWER\n\n\n\n\nObservations |> mutate(job = zero_one(outcome, one=\"job\")) |>\n  lm(job ~ participation + education, data=_) |>\n  conf_interval()\n\n# A tibble: 3 × 4\n  term                .lwr   .coef    .upr\n  <chr>              <dbl>   <dbl>   <dbl>\n1 (Intercept)       0.332   0.401  0.470  \n2 participationnot -0.186  -0.0918 0.00276\n3 education        -0.0317  0.0172 0.0660 \n\n\neducation has no systematic effect on the effect size with respect to participation.\nAdjusting for measured covariates is a good thing to do, but it does not deal with the possibility of unmeasured covariates. We need a better study than\n\n\nTask 3 Turn the study into an experiment. You’ll do this by intervening in the system, changing the DAG so that participation is set randomly. The following chunk will create a new DAG with particpation.\n\njob_experiment_dag <- dag_intervene(\n  job_dag,\n  participation ~ binom(0, labels=c(\"enrolled\",\"not\"))\n)\nExp_data <- sample(job_experiment_dag, size=400)\n\nGenerate a sample of size \\(n=400\\) from job_experiment_dag. Then carry out the modeling analysis on the experimental data as in Tasks 1 & 2.\nQUESTION: With the experimental data, do you get the same kind of result as with the observational data.\n\n\n\n\n\n\nANSWER\n\n\n\n\n\n\nTask 4. You couldn’t do this in a realistic situation, but since in the simulation we have measured diligence, we can incorporate it into the data analysis of the observational data. What does adjusting for diligence do in aligning the observational results from the experimental results?\n\n\n\n\n\n\nANSWER"
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "",
    "text": "What is risk?"
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html#three-points-from-the-cdc-statement",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html#three-points-from-the-cdc-statement",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "Three points from the CDC statement",
    "text": "Three points from the CDC statement\n1, A statistic like “15 to 30 times more likely” is called a risk ratio. I’m going to assume that [15,30] is a confidence interval.\n\n“Cigarette smoking is linked to about 80% to 90% of lung cancer deaths.” This is called a “population attributable fraction.” This is useful for assigning blame.\n“Tobacco smoke is a toxic mix of more than 7,000 chemicals. Many are poisons.” This is neither here nor there. The risk what matters for decision-making.\n\n\n\n\n\n\n\nCIs and decision making\n\n\n\n\nBasic use for a confidence interval … if you would make a different decision at the two ends of the CI, the decision is not a no-brainer.\n\nStatisticians have the luxury of saying, “You need more data to support the decision.”\n\nHow much more data? The amount needed so that you would make the same decision at either end of the CI.\nThis can be calculated using the \\(1/\\sqrt{n}\\) rule for the width of a CI.\n\nDecision-makers have to make decisions with the information currently at hand.\n\n\n\n\nQUESTION: Your 65-year old uncle Fred is a one-pack a day smoker. You want him to stop. (Good for you!) But Fred says that smoking is his favorite activity. Should a risk ratio of 15 for lung cancer be a compelling argument for stopping?\n\nIs “risk ratio” the right measure for making a decision?\nIs lung cancer the outcome of interest or something else?"
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html#all-cause-mortality",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html#all-cause-mortality",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "All-cause mortality",
    "text": "All-cause mortality\nFive-year mortality, death rate per 1000 person-years\n\n\n\nAge\n# deaths\n# at risk\nrate\nRR (unadjusted)\nRR (adjusted)\n\n\n\n\n65-69\n123\n1835\n13.6\n1\n1\n\n\n70-74\n155\n1616\n19.8\n1.46 (1.15-1.85)\n1.15 (0.90-1.47)\n\n\n75-79\n172\n1061\n34.8\n2.59 (2.04-3.26)\n1.45 (1.13-1.87)\n\n\n80-84\n120\n496\n54.5\n4.08 (3.17-5.25)\n1.72 (1.29-2.29)\n\n\n\\(\\geq\\) 85\n76\n193\n96.6\n7.10 (5.52-9.79)\n2.56 (1.82-3.61)\n\n\n\nRisk Factors\n\n\n\nSource\nLevel\nRR (adjusted)\n\n\n\n\nSex\nmale\n(1.84-2.98)\n\n\nAnnual Income\n>50K\n(0.54-0.98)\n\n\nWeight\n< 142M 115F\n1.00\n\n\n142-156M 115-131F\n(0.67-1.12)\n\n\n\n156-172M 131-145F\n(0.48-1.12)\n\n\n\n172-190M 145-168F\n(0.59-0.77)\n\n\n\n> 190M, 168F\n(0.43-0.75)\n\n\n\n\n\n\n\n\n\n\nBaseline\n\n\n\nMortality rate for 45-year old woman in the US: 0.24% or 240 per 100,000"
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html#age-deaths-at-risk-rate-rr-unadjusted-rr-adjusted",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html#age-deaths-at-risk-rate-rr-unadjusted-rr-adjusted",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "Age | # deaths | # at risk | rate |RR (unadjusted) | RR (adjusted)",
    "text": "Age | # deaths | # at risk | rate |RR (unadjusted) | RR (adjusted)\n65-69 | 123 | 1835 | 13.6 | 1 | 1 70-74 | 155 | 1616 | 19.8 | 1.46 (1.15-1.85) | 1.15 (0.90-1.47) 75-79 | 172 | 1061 | 34.8 | 2.59 (2.04-3.26) | 1.45 (1.13-1.87) 80-84 | 120 | 496 | 54.5 | 4.08 (3.17-5.25) | 1.72 (1.29-2.29) \\(\\geq\\) 85| 76 | 193 | 96.6 | 7.36 (5.52-9.79) | 2.56 (1.82-3.61)\n\n\n\n\n\n\nBaseline\n\n\n\nMortality rate for 45-year old woman in the US: 0.24% or 240 per 100,000"
  },
  {
    "objectID": "Day-by-day/Lesson-33/mortality-and-risk.html",
    "href": "Day-by-day/Lesson-33/mortality-and-risk.html",
    "title": "Spring 2023 Math 300Z",
    "section": "",
    "text": "You and your partners are going to put together a hypothetical person and calculate their mortality risk. Decide jointly what levels of risk factors your hypothetical person has. Write these down so you can describe your hypothetical person to the class. Also, note what are the baseline levels for your risk factors. The baseline is identified by a risk ratio of exactly 1.\nThen, divide into two groups to calculate the mortality risk in two different ways.\n\nusing the unadjusted risk ratios (which is technically incorrect)\nusing the adjusted risk ratios\n\nUse the point estimate of the risk ratio from Fried et al. (1998) “Risk factors for five-year mortality in older adults”. (Note: More recent studies might have found different risks. I do not have an expert opinion on the reliability of the data in Fried (1998).)\nIn constructing your hypothetical person, use whatever risk factors you want. But avoid using levels of these that are rare. To get the estimate of overall risk, multiply together the point estimates. For example, only 7 out of 5201 participants have severe aortic stenosis.\nOnce you have your estimate of overall risk ratio, you can convert it to an absolute risk increase. To do so, we need the absolute risk of death of your baseline person. The table in Fried (1998) does not give this. For the purposes of this calculation, we will use as a baseline a 5-year mortality rate of 1.4%. (This corresponds to a 45-year old woman as given by the life-tables for the entire US.)\n\nMultiply 1.4% times the overall risk ratio for your hypothetical person. That gives the absolute risk for the hypothetical person.\nThen calculate the increase in risk for your hypothetical person compared to the baseline\n\nThe estimate of overall risk made from the unadjusted risk ratios will be higher than that from the adjusted risk ratios. Explain why use of the unadjusted risk ratios is unjustifiable."
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html#example-calculation-of-risk",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html#example-calculation-of-risk",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "Example calculation of risk",
    "text": "Example calculation of risk\nUsing the Whickham data, calculate the 20-year mortality rate for smokers and for non-smokers. (Warning: these data contain a historical artifact which makes the result unreliable.)\nConvert the categorical outcome to a zero-one variable\n\nWhickham <- Whickham |>\n  mutate(dead = zero_one(outcome, one=\"Dead\"))\n\nModel the zero-one response by smoker:\n\nlm(dead ~ smoker, data=Whickham) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr   .coef    .upr\n  <chr>        <dbl>   <dbl>   <dbl>\n1 (Intercept)  0.282  0.314   0.347 \n2 smokerYes   -0.124 -0.0754 -0.0265\n\n\n\nThe baseline 20-year mortality risk is 31%.\nThe 20-year mortality risk for smokers is 31% - 7% = 24%.\n\nThe risk ratio for smokers is 24%/31% = 0.77.\nBut for an individual person, the absolute risk change, -7%, is more informative. So why use risk ratios at all?"
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html#risk-ratios-in-all-cause-mortality",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html#risk-ratios-in-all-cause-mortality",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "Risk ratios in all-cause mortality",
    "text": "Risk ratios in all-cause mortality\nFive-year mortality, death rate per 1000 person-years\n\n\n\nAge\n# deaths\n# at risk\nrate\nRR (unadjusted)\nRR (adjusted)\n\n\n\n\n65-69\n123\n1835\n13.6\n1\n1\n\n\n70-74\n155\n1616\n19.8\n1.46 (1.15-1.85)\n1.15 (0.90-1.47)\n\n\n75-79\n172\n1061\n34.8\n2.59 (2.04-3.26)\n1.45 (1.13-1.87)\n\n\n80-84\n120\n496\n54.5\n4.08 (3.17-5.25)\n1.72 (1.29-2.29)\n\n\n\\(\\geq\\) 85\n76\n193\n96.6\n7.10 (5.52-9.79)\n2.56 (1.82-3.61)\n\n\n\nRisk Factors\n\n\n\nSource\nLevel\nRR (unadjusted)\nRR (adjusted)\n\n\n\n\nSex\nmale\n(1.91-2.63)\n(1.84-2.98)\n\n\nAnnual Income\n>50K\n(0.41-0.73)\n(0.54-0.98)\n\n\nWeight\n< 142M 115F lb\n1.00\n\n\n\n< 156M 131F\n(0.48-0.78)\n(0.67-1.12)\n\n\n\n< 172M 145F\n(0.39-0.64)\n(0.48-1.12)\n\n\n\n< 190M 168F\n(0.34-0.55)\n(0.59-0.77)\n\n\n\n> 190M, 168F\n(0.37-0.61)\n(0.43-0.75)\n\n\n\nActivity\n< 67.5 kcal\n1.00\n\n\n\n< 473\n(0.49-0.80)\n(0.60-1.00)\n\n\n\n< 980\n(0.43-0.70)\n(0.63-1.05)\n\n\n\n< 1890\n(0.23-0.38)\n(0.55-0.93)\n\n\n\nSmoking\nNever smoked\n1.00\n\n\n\n<25 pack-years\n(0.85-1.32)\n(0.88-1.38)\n\n\n\n<50 pack-years\n(1.04-1.60)\n(0.90-1.43)\n\n\n\n> 50.\n(1.65-2.58)\n(1.25-2.00)\n\n\n\nSystolic BP\n< 129\n1.00\n\n\n\n< 147\n\n(0.68-1.13)\n\n\n\n< 153.\n\n(0.75-1.27)\n\n\n\n< 169.\n\n(0.87-1.51)\n\n\n\n> 169.\n\n(1.17-2.08)\n\n\n\nCongestive heart failure\nyes\n(1.84-2.52)\n(1.29-2.16)\n\n\nSelf-assessed health.\npoor\n(5.20-10.88)\n(1.27-2.87)"
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html#activity",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html#activity",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "Activity",
    "text": "Activity"
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html#adjusted-risk",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html#adjusted-risk",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "Adjusted risk",
    "text": "Adjusted risk\nThe “adjusted risk ratio” for a risk factor is the effect size, with respect to that risk factor, holding the covariates constant.\nAs with linear regression, to “hold the covariates constant” they have to be included in the model of risk.\nOur standard modeling framework adds together contributions from different variables. For risk, this might look like\nrisk = intercept + a_1_ smoker + a_2_ high_cholesterol + a_3_ high_BP + a_4_ age + …\nLet’s try this with Whickham:\n\nmod1 <- lm(dead ~ smoker + age, data=Whickham) \nmod1 |> conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr   .coef    .upr\n  <chr>         <dbl>   <dbl>   <dbl>\n1 (Intercept) -0.532  -0.473  -0.414 \n2 smokerYes   -0.0489 -0.0105  0.0279\n3 age          0.0151  0.0162  0.0173\n\nmodel_plot(mod1, x=age, color=smoker, data_alpha=0.03)\n\n\n\n\nInterpretation (naive):\n\n(intercept) the 20-year mortality rate for a newborn is 47%\n(age) mortality risk increases by 1.6 percentage points per year\n(smoking) reduces mortality risk by 1 percentage point.\n\nMathematical problem: The mortality rate at any age has to be between 0 and 100%.\nSolution (used universally): Don’t model probability (which has to stay in [0-1]). Instead, model log odds.\nOdds: For probability p the corresponding odds is p/(1-p). It’s exactly the same information, but in a different format. But odds range from 0 to \\(\\infty\\), so we don’t need to worry about them going out of bounds on the high side.\nLogarithm of odds: For probability p the log odds is ln(p) - ln(1-p). This can range from \\(-\\infty\\) to \\(\\infty\\).\nThis model will never go out of bounds.\nlog odds = intercept + a_1_ smoker + a_2_ high_cholesterol + a_3_ high_BP + a_4_ age + …\nTo find the coefficients, we use a mathematical technique called “logistic regression”.\n\nmod2 <- glm(dead ~ smoker + age, data=Whickham, family=\"binomial\")\nmod2 |> conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) -8.50  -7.60  -6.77 \n2 smokerYes   -0.124  0.205  0.537\n3 age          0.110  0.124  0.138\n\nmodel_plot(mod2, x=age, color=smoker, data_alpha=0.03)\n\n\n\n\nSmoking increases the log odds of mortality by 0.35 (although zero is included in the confidence interval). Each additional year of age increases log odds by 0.01.\nFor instance, for a 50-year old smoker, 20-year mortality expressed as log odds is:\n\\[\\ln(\\text{odds}) = -6.5 + 0.35 + 0.1\\times 50 = -1.15\\]\nSomething interesting happens when we convert to odds:\n\\[odds = e^{\\ln(odds)} = e^{-6.5 + 0.35 + 0.1\\times50} =\n\\underbrace{e^{-6.5}}_\\text{baseline} \\times \\underbrace{e^{0.35}}_\\text{odds ratio} \\times \\underbrace{e^{5}}_\\text{odds ratio}\\] \\[ odds = \\underbrace{0.0015}_\\text{baseline} \\times \\underbrace{1.42}_\\text{odds ratio for smoking} \\times \\underbrace{148.4}_\\text{odds ratio for age 50}\\]\nEach factor in logistic regression contributes a multiplicative odds ratio. How is that related to a risk ratio.\n\ncontour_plot(p1/p2 ~ p1 + p2, bounds(p1=0.01:0.10, p2=0.01:0.10)) + labs(title=\"Risk Ratios\")\n\n\n\ncontour_plot(p1*(1-p2)/(p2*(1-p1)) ~ p1 + p2, bounds(p1=0.01:0.10, p2=0.01:0.1)) + labs(title=\"Odds Ratios\")\n\n\n\n\nRisk ratios and odds ratios are practically the same thing, so long as p1 and p2 are small.\nSUMMARY:\n\nOdds don’t add.\nInstead, odds ratios multiply.\nNeed a baseline odds to calculate the overall odds, multiplying baseline by the appropriate odds ratios.\nWe often interpret odds ratios as risk ratios. Not too bad when the odds (both numerator and denominator) are small, say < 0.1."
  },
  {
    "objectID": "Worksheets/Worksheet-33.html",
    "href": "Worksheets/Worksheet-33.html",
    "title": "Lesson 33: Worksheet",
    "section": "",
    "text": ".\n1\n2\n3\n4\n5\n6\n\n\n\n\nunderloaded\n18%\n18%\n18%\n18%\n18%\n10%\n\n\nfair\n16.67%\n16.67%\n16.67%\n16.67%\n16.67%\n16.67%\n\n\nloaded\n15%\n15%\n15%\n15%\n15%\n25%\nWe have mentioned—just mentioned, mind you—a statistical quantity called the “likelihood.” Depending on how you look at things, a likelihood can be seen from four closely related different perspectives:"
  },
  {
    "objectID": "Day-by-day/Lesson-33/Teaching-notes-33.html#example-smoking",
    "href": "Day-by-day/Lesson-33/Teaching-notes-33.html#example-smoking",
    "title": "Instructor Teaching Notes for Lesson 33",
    "section": "Example: Smoking",
    "text": "Example: Smoking\nIn Lesson 29 we looked at “life tables” for the US, the rate of mortality as a function of age for each sex separately. In Lessons 30 and 31 we considered increased mortality due to smoking.\n\n\n\n\n\n\nCDC information on smoking and lung cancer\n\n\n\n\nCigarette smoking is the number one risk factor for lung cancer. In the United States, cigarette smoking is linked to about 80% to 90% of lung cancer deaths. Using other tobacco products such as cigars or pipes also increases the risk for lung cancer. Tobacco smoke is a toxic mix of more than 7,000 chemicals. Many are poisons. At least 70 are known to cause cancer in people or animals.\n\n\nPeople who smoke cigarettes are 15 to 30 times more likely to get lung cancer or die from lung cancer than people who do not smoke. Even smoking a few cigarettes a day or smoking occasionally increases the risk of lung cancer. The more years a person smokes and the more cigarettes smoked each day, the more risk goes up.\n\n\nPeople who quit smoking have a lower risk of lung cancer than if they had continued to smoke, but their risk is higher than the risk for people who never smoked. Quitting smoking at any age can lower the risk of lung cancer.\n\n\n\nTake five minutes to discuss this with your group. What do you like or not like about this statement for the purposes of guiding an individual’s action?"
  },
  {
    "objectID": "Day-by-day/Review-39/Teaching-notes-31.html",
    "href": "Day-by-day/Review-39/Teaching-notes-31.html",
    "title": "Instructor Teaching Notes for Lesson 39",
    "section": "",
    "text": "Have Col. Horton tell the study of the concern about high numbers of NJPs (non-judicial proceedings for minor offenses) in the Global Strike Command. They were higher than in other commands.\nBut, stratifying by job category, it was found that the excess was attributable to the “police” job category, which had a much higher NJP rate than other categories. This is true across the Air Force. But there is a much larger proportion of police in the Global Strike Command than in other commands. Simpson’s paradox!"
  },
  {
    "objectID": "Worksheets/Worksheet-33.html#likelihood-and-model-fitting",
    "href": "Worksheets/Worksheet-33.html#likelihood-and-model-fitting",
    "title": "Lesson 33: Worksheet",
    "section": "Likelihood and model fitting",
    "text": "Likelihood and model fitting\nRecall that fitting (or “training”) a model is done by taking the observed data and finding the values of parameters for the model specification that produce a model output that is “as close as possible” to the observed response variable. Almost always in statistics, “as close as possible” is defined to mean the parameter values that produce the maximum likelihood of the observed data.\nIn this section, you will “fit a model” of dice rolls in the sense of picking the hypothesis that maximizes the likelihood of the observed data.\nThe data are the results from rolling a die 1000 times and are stored under outcome in the data frame Dice_rolls. (To generate the data, you will need to run the first chunk in this document, the one that says “include=FALSE”.)\nTask 1: Use View to look at Dice_rolls and confirm that it has 1000 rows. Then, use data wrangling to count the number of rows for each of the possible outcomes. From this table of counts, comment on which of the three hypotheses is most likely.\n\n\n\n\n\n\nNote\n\n\n\n\nDice_rolls |> \n  group_by(outcome) |>\n  summarize(count = n())\n\n# A tibble: 6 × 2\n  outcome count\n    <int> <int>\n1       1   184\n2       2   180\n3       3   179\n4       4   192\n5       5   156\n6       6   109\n\n\nThe hypotheses differ only in the likelihood of 6. The table of counts shows that 6 comes up only about two-thirds as often as any of the other possibilities. Thus, “underload” seems the most likely.\n\n\nTask 2: Now we are going to calculate the likelihood of the observed data given each of the hypotheses. For this purpose, you have been provided with a data frame named Hypotheses that records the likelihood function.\nSteps for calculating the likelihood for one of the hypotheses:\n\nJoin the observed data to the hypotheses. This can be done with Temp <- Dice_rolls |> left_join(Hypotheses). View Temp and make sure you understand how it is structured.\n\n\n\n\nQUESTION: How many rows are their in Temp? What are the names of the columns in Temp that were not already in Dice_rolls.\n\n\n\n\n\n\nANSWER\n\n\n\nThere are 1000 rows in Temp, one for each of the rows in the observed data. By joining Hypothesis to Dice_rolls, we are adding new columns, one for each of the hypotheses under consideration: “underload”, “fair”, “overload”.\n\n\n\nUsing summarize(), calculate the product (prod()) of all of the entries in each hypothesis column. Why product and not sum? Each of the rows in Temp records one event. Now consider the fair column. The entries in fair are the probabilities of seeing that row’s event outcome (given the “fair” hypothesis). When there are multiple events (as the 1000 rows of Dice_rolls) then the joint probability of seeing all of those particular outcomes is the product of the probability of seeing each row’s outcome.\n\nQUESTION: What are the numerical values of the likelihood of all 1000 recorded events for each of the three hypotheses under consideration: “underloaded,” “fair,” “loaded?” (Advance warning: This calculation is fast, but unreliable when done by computer. So the results may surprise you.)\n\n\n\n\n\n\nANSWER\n\n\n\n\nTemp |> summarize(\n  L_underloaded = prod(underloaded),\n  L_fair = prod(fair),\n  L_loaded = prod(loaded)\n)\n\n# A tibble: 1 × 3\n  L_underloaded L_fair L_loaded\n          <dbl>  <dbl>    <dbl>\n1             0      0        0\n\n\nAll of the likelihoods come out to zero! According to this report of the calculation, there’s no reason to prefer one hypotheses to the others.\n\n\nTask 3: The calculation you were tasked with in Task 2 is mathematically correct, but gives unreliable results on the computer. The reason has to do with the nature of arithmetic on the computer. The hardware chips and software that implement arithmetic do arithmetic in a way that is indistinguishable from idealized arithmetic … most of the time. But there are gotchas which computer experts know about and work to avoid by making slight modifications to the calculations.\nOne of the gotchas has to do with computer arithmetic being done with a finite number of digits: about 22. Usually, we never look past a few digits, but let’s look at all 22 digits so that we can see the effects of round-off:\n\noptions(digits=22) # sets the detail of printing\n1/3\n\n[1] 0.3333333333333333148296\n\n4/3\n\n[1] 1.333333333333333259318\n\n\nIn the calculation of likelihood in Task 2, the problem is that multiplying 1000 small numbers together gives a result that is rounded off to zero. Experts fix the problem by doing such calculations with logarithms.\nRepeat the likelihood calculation from Task 2, but instead of using, say, prod(fair) calculate sum(log(fair)). The result will be the “log-likelihood” but it will be easy to read off which likelihood is the highest.\nQUESTION: Which is the highest log likelihood?\n\n\n\n\n\n\nANSWER\n\n\n\n\nTemp |> summarize(\n  L_underloaded = sum(log(underloaded)),\n  L_fair = sum(log(fair)),\n  L_loaded = sum(log(loaded))\n)\n\n# A tibble: 1 × 3\n  L_underloaded L_fair L_loaded\n          <dbl>  <dbl>    <dbl>\n1        -1779. -1792.   -1841.\n\n\nAll three log likelihoods are negative; that’s to be expected for the logarithm of a probability. The largest log likelihood is the one that is least negative. That’s the “underloaded” hypothesis, just as we suspected from Task 1."
  },
  {
    "objectID": "Worksheets/Worksheet-33.html#going-further-comparing-hypotheses-quantitatively",
    "href": "Worksheets/Worksheet-33.html#going-further-comparing-hypotheses-quantitatively",
    "title": "Lesson 33: Worksheet",
    "section": "Going further: Comparing hypotheses quantitatively",
    "text": "Going further: Comparing hypotheses quantitatively\nBased on the observed data, how much better is the “underloaded” hypothesis that the “fair” hypothesis.” One way that statisticians present the answer is with the “likelihood ratio” test. A likelihood ratio is larger than 10 is considered “strong” evidence for the favored.\nLet’s calculate the likelihood ratio comparing “fair” to “loaded.” To do this, you need to remember a little bit of high-school math about logarithms: log(A/B) = log(A) - log(B). A/B is the ratio while log(A/B) is the logarithm of the ratio.\nThe logarithm of the ratio (“fair”/“loaded”) is \\(-1791 - -1841 = 50\\). So log(A/B) = 50, but what is A/B itself. Another bit of high-school math: to undo the logarithm use the exponential function. So the ratio will be\n\nexp(50)\n\n[1] 5.18471e+21\n\n\nThis is a HUGE number. So big, that it’s reasonable to say that the data completely rule out the possibility of the “loaded” hypothesis.\nQUESTION: What’s the likelihood ratio comparing the “fair” to the “underloaded” hypotheses?\n\n\n\n\n\n\nANSWER\n\n\n\nLet’s put the likelihood for “underloaded” on the top and “fair” on the bottom.\n\nexp(-1779 - -1791)\n\n[1] 162755\n\n\nThis result is obviously much, much bigger than 10, the convention for calling the evidence “strong.”\nOne reason the likelihood ratio points so strongly to the “underloaded” hypothesis is that we have lots and lots of data: 1000 rows.\nJust for demonstration purposes, I drew a sample of size \\(n=100\\) from the DAG generating the data. The log likelihood for “underloaded” is -171.121 and the log likelihood for “fair” is -179.156. (If you repeat the sampling yourself, the result of your calculation will be different. Sampling variation at work!)\n\nexp(-179.121 - -179.156)\n\n[1] 1.03562\n\n\nWith my particular sample of \\(n=100\\) rolls, the data provide absolutely no reason to favor one hypothesis over the other. But if we compared the “underloaded” to the “loaded” hypothesis, the likelihood ratio would be about 50. So even with \\(n=100\\) rolls, we can rule out the “loaded” hypothesis."
  },
  {
    "objectID": "Worksheets/Worksheet-37.html",
    "href": "Worksheets/Worksheet-37.html",
    "title": "Lesson 37: Worksheet",
    "section": "",
    "text": "This worksheet provides some practice with the “analysis of variance” technique and demonstrates the logic of the test."
  },
  {
    "objectID": "Worksheets/Worksheet-37.html#a-demonstration-of-anova",
    "href": "Worksheets/Worksheet-37.html#a-demonstration-of-anova",
    "title": "Lesson 37: Worksheet",
    "section": "A demonstration of ANOVA",
    "text": "A demonstration of ANOVA\nConsider a program to measure the effectiveness of the 26 third-grade teachers in your district. For this purpose, a series of 20 standardized exams will be given to students over the course of a year. The results of the tests will be scaled to that the average score across all students is always approximately 300. The most talented teachers are anticipated to contribute 20 points to their students, the least talented teachers cause their students to lose 20 points. But the majority of teachers are somewhere in between.\nThe DAG defined below, school_dag, simulates such a situation. The average score on each test is about 300, with the teacher contributing or detracting based on the level of talent.\n\nset.seed(101)\ntalent <- runif(length(LETTERS), -20, 20)\nschool_dag <- dag_make(\n  teacher ~ categorical(levels=LETTERS),\n  score ~ 300 + value(teacher, LETTERS, talent) + 100*exo()\n)\n\n\n\n\n\n\n\nHow talented?\n\n\n\nFor your later reference, here are the talent levels for each teacher used in the simulation:\n\n\n    A     B     C     D     E     F     G     H     I     J     K     L     M \n -5.1 -18.0   8.4   6.3 -10.0  -8.0   3.4  -6.7   4.9   1.8  15.0   8.3   9.3 \n    N     O     P     Q     R     S     T     U     V     W     X     Y     Z \n 17.0  -1.8   3.6  13.0 -11.0  -3.5 -18.0   8.0  18.0 -11.0   6.4  17.0  12.0 \n\n\n\n\nIn the next chunk, the data (20 tests for each of the 26 teachers’ classrooms) is simulated. The model score ~ teacher is fitted to the data then summarized in two familiar ways: R2() and conf_interval().\n\nset.seed(211) \nScores <- sample(school_dag, size=26*20)\nmodel <- lm(score ~ teacher, data=Scores)\nmodel |> R2()\n\n    n  k  Rsquared        F      adjR2         p df.num df.denom\n1 520 25 0.0581967 1.221027 0.01053459 0.2127687     25      494\n\nmodel |> conf_interval(show_p=TRUE) |> filter(.lwr*.upr > 0)\n\n# A tibble: 4 × 5\n  term          .lwr .coef  .upr  p.value\n  <chr>        <dbl> <dbl> <dbl>    <dbl>\n1 (Intercept) 226.   271.   315. 2.57e-29\n2 teacherI     23.0   85.6  148. 7.50e- 3\n3 teacherY     10.6   73.2  136. 2.21e- 2\n4 teacherZ      3.90  66.5  129. 3.74e- 2\n\n\nThe R2 report summarizes the whole model in one line, generating a single p-value. In contrast, the conf_interval() report has 26 lines—the 26 coefficients from the model—with a p-value for each line. In reading a conf_interval() report, it’s tempting to focus on just those coefficients whose confidence intervals exclude zero.  The purpose of the statement |> filter(.lwr*.upr > 0) is to disregard any coefficient whose confidence interval spans zero.When both ends of the confidence interval have different signs, the interval necessarily includes zero. The product of two numbers with different signs is always negative.\nThe set.seed() statement at the start of the chunk is a way to specify a particular sample to use as an example.\n\nRun the chunk with set.seed(201). For that sample, the R^2 report shows a p-value of 0.09, which is correctly interpreted as “failing to reject the Null,” that is, that no link between teacher and score has been detected. Correspondingly, the conf_interval() report does not show any teacher coefficients: the confidence intervals on all such coefficients includes zero. Another way to say this is that the two reports are consistent with one another.\nNow run the chunk again, this time using set.seed(211). The p-value for this sample will be 0.21, above the 0.05 threshold for “rejecting the Null.” Yet there are three teachers, I, Y, and Z, who are detected as having a discernable influence on score. According to the estimated coefficients, all of them have large, positive influences on score.\n\nWhich of the two reports for sample (b), R2 or the confidence intervals should be taken at face value?\nBecause of sampling variation, all sample statistics are somewhat random. In particularly, R2 and every confidence interval has some randomness. When there are many sample statistics, as in the confidence interval report, this randomness has many opportunities to inflate the results. Looking at the individual confidence intervals that exclude zero is to some extent cherry picking. It’s difficult to know if we’re seeing real effects or just sampling variation.\nIn the simulation, of course, we can look at the “true” talent of each teacher. The true talent of teacher I, it happens is 10 points higher than the baseline teacher A, completely outside the confidence interval from the sample (b) report. The talents of teachers Y and Z are 22 and 17 points higher than the baseline, toward the lower end of their respective confidence intervals. On the other hand, Teachers N, T, and V have talents of the same or bigger magnitude, but they are not detected as such in sample (b).\nWhen there are many coefficients, the confidence-interval report is likely to generate results that are over-stated and unreliable.\n\nComment out the set.seed() command in the the data generation chunk. Now, each time the chunk is run a brand new sample is created. Run the data generation and analysis many times in succession, taking note of which teachers, if any, are identified as having “significant” talent, either positive or negative. Are the teachers with the most positive or negative talents routinely identified by the confidence-interval report?"
  },
  {
    "objectID": "Worksheets/Worksheet-37.html#under-the-null",
    "href": "Worksheets/Worksheet-37.html#under-the-null",
    "title": "Lesson 37: Worksheet",
    "section": "Under the Null",
    "text": "Under the Null\nAnother way to compare the reliabilities of the R2 report and the confidence-interval report when there are many levels of a categorical variable is to enforce the Null hypotheses—by shuffling—and check how the reports perform. A properly performing report should show a p-value below 0.05 roughly one time in 20.\nRun the following command chunk many times. Keep a record of how often a p-value below 0.05 appears in each kind of report. Which report performs more properly.\n\nNull_scores <- sample(school_dag, size=26*20) |> mutate(score = shuffle(score))\nlm(score ~ teacher, data=Null_scores) |> R2()\nlm(score ~ teacher, data=Null_scores) |> conf_interval(show_p=TRUE) |> filter(.lwr*.upr > 0)"
  },
  {
    "objectID": "Worksheets/Worksheet-37.html#summary",
    "href": "Worksheets/Worksheet-37.html#summary",
    "title": "Lesson 37: Worksheet",
    "section": "Summary",
    "text": "Summary\nWhen a categorical explanatory variable (like teacher) has many levels, confidence intervals on the coefficients for the individual levels can be misleading, especially if only the “best” intervals are considered. In such cases, use R2 and its p-value to evaluate whether the explanatory variable is linked to the response. Such tests, based on R2, are called “analysis of variance,” or ANOVA for short.\n\nBackground: This example is based on a real-world controversy about the use of “Value added models” (VAM) in evaluating individual teachers. Professional statisticians had such strong concerns about the VAM techniques that their professional society, the American Statistical Association, issued a statement of caution."
  },
  {
    "objectID": "Worksheets/Worksheet-36.html",
    "href": "Worksheets/Worksheet-36.html",
    "title": "Lesson 36: Worksheet",
    "section": "",
    "text": "We have been constructing and using confidence intervals since Lesson 23. Recall that each coefficient of a model has its own confidence interval.\nOn many occasions, we have been interested in whether a connection is detectable between the response and a particular explanatory variable (perhaps conditioned on some covariate(s)). In such a situation, the confidence interval on that explanatory variable provides a quick answer: Does the confidence interval include zero? For instance, can we detect in the Galton data an effect of the number of children in a household (nkids) on the adult height of a child? (Speculation on why this might be: Perhaps less food per child is available in large households.) The simplest model is\n\nlm(height ~ nkids, data=Galton) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef    .upr\n  <chr>        <dbl>  <dbl>   <dbl>\n1 (Intercept) 67.2   67.8   68.4   \n2 nkids       -0.256 -0.169 -0.0826\n\n\nThe confidence interval on nkids does not include zero, so we can fairly say to have detected a connection between nkids and height. (The effect size is about one-sixth of an inch height loss per additional child in the household.)\nTASK 1. Might the link between nkids and height be via sex? Is sex related to nkids\n\nglm(zero_one(sex, one=\"F\") ~ nkids, data=Galton, family=\"binomial\") |> conf_interval(show_p=TRUE)\n\nWaiting for profiling to be done...\n\n\n# A tibble: 2 × 5\n  term            .lwr   .coef    .upr p.value\n  <chr>          <dbl>   <dbl>   <dbl>   <dbl>\n1 (Intercept) -0.693   -0.362  -0.0344  0.0309\n2 nkids       -0.00156  0.0473  0.0967  0.0586\n\n\n\nlm(height ~ nkids + sex + mother + father, data=Galton) |> regression_summary()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  16.2       2.79        5.79 9.52e-  9\n2 nkids        -0.0438    0.0272     -1.61 1.07e-  1\n3 sexM          5.21      0.144      36.1  7.58e-177\n4 mother        0.321     0.0313     10.3  1.85e- 23\n5 father        0.398     0.0296     13.5  8.61e- 38\n\n\nFocus on cases where CONFIDENCE INTERVAL JUST BARELY TOUCHES ZERO, p-value is about 0.05"
  },
  {
    "objectID": "Worksheets/Worksheet-38.html",
    "href": "Worksheets/Worksheet-38.html",
    "title": "Lesson 38: Worksheet",
    "section": "",
    "text": "An important recommendation given by these Lessons is this:\n\nWhenever possible, prefer confidence intervals to p-values.\n\nThis recommendation is consistent with the American Statistical Association’s “Statement on statistical significance and p-values” “Statistical significance is not equivalent to scientific, human, or economic significance. Smaller p-values do not necessarily imply the presence of larger or more important effects, and larger p-values do not imply a lack of importance or even lack of effect. Any effect, no matter how tiny, can produce a small p-value if the sample size or measurement precision is high enough, and large effects may produce unimpressive p-values if the sample size is small or measurements are imprecise.” —From the ASA statement.\nConfidence intervals, unlike p-values, convey clearly what is known about the size of an effect. For experts in the field of application, the effect size indicates the importance or utility of a relationship uncovered by statistical methodology. P-values do not do this even though they are widely misinterpreted as doing so.\nConsider one widely used statistical practice, the use of a correlation coefficient as a measure of the strength of association between two variables, is often interpreted in terms of p-values. The correlation coefficient, r, is always between -1 and 1 and does not appear at first glance to be in the form of an effect size.\nThe point of this worksheet is to show that even a correlation coefficient can be translated into an effect size and a confidence interval.\nTask 1. Use data wrangling and cor() to calculate the correlation coefficient between height and mother in the Galton data. Show that it doesn’t matter which order the variables appear in the argument to cor().\n\n\n\n\n\n\nANSWER\n\n\n\n\nGalton |> summarize(r = cor(height, mother))\n\n          r\n1 0.2016549\n\nGalton |> summarize(r = cor(mother, height))\n\n          r\n1 0.2016549\n\n\n\n\nTask 2: Construct two models using Galton: (i) height ~ mother and (ii) mother ~ height. Show that the confidence intervals on the explanatory variable’s coefficient are not the same.\n\n\n\n\n\n\nNote\n\n\n\n\nlm(height ~ mother, data=Galton) |> conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef   .upr\n  <chr>        <dbl>  <dbl>  <dbl>\n1 (Intercept) 40.3   46.7   53.1  \n2 mother       0.213  0.313  0.413\n\nlm(mother ~ height, data=Galton) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr  .coef   .upr\n  <chr>         <dbl>  <dbl>  <dbl>\n1 (Intercept) 52.7    55.4   58.2  \n2 height       0.0885  0.130  0.171\n\n\n\n\nThe symmetry of the correlation coefficient contrasts with the lack of symmetry of the regression confidence intervals, leading many people to believe that a correlation coefficient is different in kind than a coefficient on an explanatory variable. However, this is not true. It is merely that the correlation coefficient involves a transformation of the variables.\nTask 3. Using wrangling, add two new variables to Galton named H and M. These will be transformed versions of height and mother respectively. This type of transformation is called “standardization.”\nH = (height - mean(height))/sd(height)\nM = (mother - mean(mother))/sd(mother)\n\nPlot H vs height and also M vs mother to show that the transformation is in the form of a straight-line function.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nGalton <- Galton |> \n  mutate(H = (height - mean(height))/sd(height),\n         M = (mother - mean(mother))/sd(mother))\nggplot(Galton, aes(x=height, y=H)) + geom_point()\nggplot(Galton, aes(x=mother, y=M)) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind the means of both H and M and compare them to the means of height and mother. What’s special about H and M?\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nGalton |> \n  summarize(mH = mean(H), mM = mean(M), \n            mheight = mean(height), mmother = mean(mother)) \n\n            mH           mM  mheight  mmother\n1 -6.74454e-16 8.554048e-16 66.76069 64.08441\n\n\nThe means of the standardized variables are zero. (A computer number like 6.7e-16 is effectively zero in this context; it differs from mathematical 0 because of computer round-off error.)\n\n\n\nCompute the variance of both H and M and compare them to the variances of height and mother. What’s distinctive about H and M in this regard.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\nGalton |> summarize(vH=var(H), vM=var(M), vheight=var(height), vmother=var(mother))\n\n  vH vM vheight  vmother\n1  1  1 12.8373 5.322365\n\n\nThe variance of a standardized variable is 1.\n\n\n\nDescribe in words what the standardization transformation accomplishes?\n\n\n\n\n\n\n\nANSWER\n\n\n\nA standardized variable has mean zero and variance one. If the original variable has units (e.g. height is in inches), the standardized variable is unitless.\n\n\nTask 3: Fit the models H ~ M and M ~ H and find the confidence interval on the explanatory variable’s coefficient. The coefficients and confidence intervals are remarkable. Explain how.\n\n\n\n\n\n\nANSWER\n\n\n\n\nlm(H ~ M, data=Galton) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr     .coef   .upr\n  <chr>         <dbl>     <dbl>  <dbl>\n1 (Intercept) -0.0642 -6.58e-16 0.0642\n2 M            0.137   2.02e- 1 0.266 \n\nlm(M ~ H, data=Galton) |> conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr     .coef   .upr\n  <chr>         <dbl>     <dbl>  <dbl>\n1 (Intercept) -0.0642 -1.00e-15 0.0642\n2 H            0.137   2.02e- 1 0.266 \n\n\nThe M coefficient is, in both models, exactly the same as the correlation coefficient. The intercept coefficient is zero (to within computer round-off error). The confidence intervals for the two models are exactly the same.\n\n\nTask 4. The confidence interval on the standardized explanatory variable is describes the effect size of the standardized response variable with respect to the standard explanatory variable. Here is a graph showing the situation in terms of H vs M and the linear regression model:\n\nmod1 <- lm(H ~ M, data=Galton)\nmodel_plot(mod1) + coord_fixed() + labs(title=\"H vs M\")\nmod2 <- lm(M ~ H, data=Galton)\nmodel_plot(mod2) + coord_fixed() + labs(title=\"M vs H\")\n\n\n\n\n\n\n\n\n\n\n\nThe quantities on the axes are denominated as “standard deviations.”\nQuestion: From the graphs, estimate the effect size with respect to the explanatory variable.\n\n\n\n\n\n\nANSWER:\n\n\n\nThe slope of the line is the same in both graphs. A “run” of 2 units corresponds to a “rise” of about 0.4 units, so the slope is 0.4/2 = 0.2. This is exactly the meaning of the correlation coefficient: the effect size of one variable with respect to the other, both variables being given in units of standard deviations.\n\n\nTask 5. The effect size is the expected change in the response variable for a 1 unit change in the explanatory variable. Since the effect size for H with respect to M (and the other way around, as well ) is 0.2016 we expect a 0.2016 standard deviation increase in the response for a 1.0 standard deviation increase in the explanatory variable. Stated another way, the slope of the regression line is\n\\(\\text{slope} = 0.2016 \\frac{\\text{sd(response)}}{\\text{sd(explanatory)}}\\)\nCalculate the standard deviation of both mother and height. Plug them in to the above formula for slope for each style of model, H ~ M and M ~ H. Where have you seen the resulting slopes before?\n\n\n\n\n\n\nANSWER\n\n\n\n\nGalton |> summarize(sdmother = sd(mother), sdheight = sd(height))\n\n  sdmother sdheight\n1 2.307025 3.582918\n\n0.2016*3.583 / 2.307 # for height ~ mother\n\n[1] 0.3131048\n\n0.2016*2.307 / 3.583 # for mother ~ height\n\n[1] 0.129805\n\n\nThe slopes calculated here for the two different arrangements of the model are the same as the coefficients from the corresponding regression models, e.g. lm(height ~ mother, data=Galton)"
  }
]
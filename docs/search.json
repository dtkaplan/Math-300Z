[
  {
    "objectID": "LC/LC-lesson20.html",
    "href": "LC/LC-lesson20.html",
    "title": "Learning Checks Lesson 20",
    "section": "",
    "text": "Only for use in drafting questions\n\n\n\n20.1. Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\n20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples."
  },
  {
    "objectID": "LC/LC-lesson20.html#a",
    "href": "LC/LC-lesson20.html#a",
    "title": "Learning Checks Lesson 20",
    "section": "20.A",
    "text": "20.A\nYou’re having a conversation about your statistics course with your engineer aunt at Thanksgiving. You tell her about how the course uses gaming (e.g. simulated data from DAGs) to develop an understanding of statistical methodology. She says (sensibly), “What? Simulated data? Isn’t statistics supposed to be about real data? It’s not a game.”\n\nWrite a paragraph-long response to your aunt explaining the point of gaming in learning statistics. Your paragraph should make a compelling case for gaming.\nWrite another paragraph that expresses any concerns you have about using games in a course that’s supposed to be about methods for extracting information about the world from real data."
  },
  {
    "objectID": "LC/LC-lesson20.html#b",
    "href": "LC/LC-lesson20.html#b",
    "title": "Learning Checks Lesson 20",
    "section": "20.B",
    "text": "20.B\nWrite statements using the computer commands covered in the first half of the course to calculate the size of the variability of these quantities:\n\nThe age variable from the Cherry_race_longitudinal data frame.\nThe Height variable from the NHANES data frame.\nCompute the body-mass index from the formula \\(\\text{BMI} = \\frac{w^2}/h\\) where \\(w\\) is the person’s weight and \\(h\\) is the person’s height. Then calculate the size of the variability of the BMI you calculate from the NHANES data."
  },
  {
    "objectID": "LC/LC-lesson20.html#c",
    "href": "LC/LC-lesson20.html#c",
    "title": "Learning Checks Lesson 20",
    "section": "20.C",
    "text": "20.C\n\n\n\n\n\n\nIn draft\n\n\n\nShow pictures of data together with violins. Ask students to estimate the “size” of the variation and to mark on the graph an annotation reflecting the “size.”\nIs “size” a single number or a pair of numbers?\nContrast “size” with the coverage interval: why is one of them two numbers and the other just a single number."
  },
  {
    "objectID": "LC/LC-lesson20.html#section",
    "href": "LC/LC-lesson20.html#section",
    "title": "Learning Checks Lesson 20",
    "section": "20.1",
    "text": "20.1\n?@sec-size-of-variable (in the reading for this variable) describes two very closely related summary quantities used to measures of the “size” of a variable: a) the variance and b) the “standard deviation” (which is the square root of the variance).\n\nUsing software, what is the variance of the XXX variable in the YYY data frame? Make sure to include the units.\nWhat is the “standard deviation” of the XXX variable? Calculate this in two different ways: i. “by-hand” taking of the square root of the variance; ii. using the sd() software directly.\n\n[Repeat for a number of variables from different data frames.]\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson20.html#d",
    "href": "LC/LC-lesson20.html#d",
    "title": "Learning Checks Lesson 20",
    "section": "20.D",
    "text": "20.D\n\n\n\n\n\n\nIn draft\n\n\n\nIn arithmetic notation, parentheses are used to group operations. So, \\(4*(3+2)\\) is different from \\(4*3 + 2\\). With do(), a similar logic applies, but we use curly braces—not parentheses—for the grouping. Another difference is that with do() and summarization, some configurations may not work at all because using braces or not will produce different data frames with different names.\n\nTry the following two commands, which differ in how curly braces are used.\n\ndo(5) * {sample(Galton, size=50)} %>% summarize(m = mean(height))\ndo(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))}\nOne command produces an error message and the other doesn’t. Explain what’s wrong with the erroneous command.\n\nTry these two statements. Again, one will work and the other won’t. Diagnose the\n\n\n{ do(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))} } %>% summarize(sz=sd(m))\n\n         sz\n1 0.7345861\n\ndo(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))} %>% summarize(sz = sd(m))\n\n  sz\n1 NA\n2 NA\n3 NA\n4 NA\n5 NA"
  },
  {
    "objectID": "LC/LC-lesson20.html#e",
    "href": "LC/LC-lesson20.html#e",
    "title": "Learning Checks Lesson 20",
    "section": "20.E",
    "text": "20.E\nWrite single-line computer statements that will do the following (related) tasks:\n\nDraw a sample of size 5 from the mosaicData::TenMileRace data\nDraw a sample (but now of size 100) and compute the “size” of variation of the net variable (which gives the net time to complete the race, start line to finish line).\nCarry out 300 trials of (2) using the do() operator. (The result should be a data frame with entries that vary.) Note: Surround the statement for a single trial with curly braces: { and }. Also, arrange for the name of the column in the overall result to be sz. This will appear in the summarize() command.\nAdd on to (3) the computations needed to calculate the mean and the standard deviation of the trials? Note: Surround the statement from (3) with a pair of curly braces so that the summarize() command will look at all 300 trials as a single data frame.\n\nIs the mean thus calculated a single number comprising all trials or a number for each trial? Briefly justify your answer in terms of what the mean of the trials should be about.\nEexplain what the standard deviation of the trials captures and why it’s different from the standard deviation on one trial (as from (2)).\n\nRepeat (4), but now with a sample size of 400 instead of 100. With the larger sample size, how do the mean and the standard deviation compare to what you got with a sample size of 100.\nRepeat (4) but this time use 1200 trials instead of 300. With more trials, how do the mean and standard deviation compare to what you got with 300 trials?\n\n::: {.callout-note} ## Solution\n\n# sample of size 100 from TenMileRace\nsample(TenMileRace, size=5)\n\n     state time  net age sex orig.id\n5524    VA 6884 6403  27   F    5524\n6142    DC 6263 6040  30   F    6142\n8067    VA 5260 4985  46   F    8067\n5228    DC 6101 5905  26   F    5228\n3996    VA 6310 6075  57   M    3996\n\nsample(TenMileRace, size=100) %>% summarize(sz = sd(net))\n\n        sz\n1 975.1356\n\ndo(300)* {sample(TenMileRace, size=100) %>% summarize(sz = sd(net))}\n\n           sz\n1    959.2438\n2    934.4931\n3    902.7186\n4   1004.3998\n5    945.9244\n6    833.2083\n7   1084.5855\n8    954.6535\n9    912.3901\n10   995.7918\n11  1042.0322\n12   982.1383\n13  1013.8613\n14   973.3382\n15   868.8083\n16   882.1596\n17   930.6375\n18   969.4653\n19   861.0170\n20   863.1551\n21   917.3276\n22   976.3435\n23   967.9830\n24   979.1732\n25  1093.7655\n26   872.7911\n27   985.4766\n28  1097.9533\n29  1000.4953\n30   930.4201\n31   891.1693\n32   918.4250\n33   890.3469\n34   998.7142\n35   956.7019\n36   922.2497\n37   978.6689\n38  1085.5178\n39   967.6515\n40   884.9708\n41   988.9340\n42   902.8769\n43   964.9145\n44  1018.4643\n45   970.9328\n46   901.6683\n47   864.7810\n48   965.4061\n49   946.0747\n50   882.1236\n51   984.0817\n52   984.4679\n53   899.1849\n54   903.5176\n55  1150.7143\n56   912.9922\n57  1027.8526\n58   982.6329\n59   929.1650\n60   902.0101\n61   988.0909\n62   952.8978\n63  1100.1091\n64   926.4893\n65   914.0777\n66   900.8727\n67  1109.5132\n68  1119.7061\n69   992.5568\n70  1021.6209\n71  1076.7835\n72   966.1313\n73  1004.1590\n74  1156.9103\n75   840.6673\n76   926.8184\n77   939.6269\n78  1016.1805\n79   995.6977\n80  1007.7892\n81  1066.5578\n82   981.6095\n83   854.4394\n84   886.8302\n85   935.1963\n86  1216.0214\n87   906.2919\n88   889.4195\n89   963.2870\n90   949.8067\n91  1036.6185\n92  1073.3345\n93   954.2095\n94  1141.1939\n95   952.1924\n96  1021.8267\n97   809.7284\n98  1048.6305\n99  1042.6614\n100 1131.7820\n101  943.6481\n102 1169.6773\n103  953.3547\n104  848.8096\n105  849.2869\n106  985.8172\n107  825.2414\n108  937.4161\n109  891.9053\n110 1037.3724\n111  877.5936\n112 1014.7396\n113  947.2007\n114  954.8917\n115  922.6524\n116 1046.5536\n117  966.0246\n118  938.6390\n119  960.6977\n120  879.0129\n121 1036.2257\n122  910.9719\n123 1010.0250\n124 1078.8867\n125  888.4951\n126  858.5824\n127  972.0682\n128 1109.6574\n129  849.3544\n130 1004.0963\n131  961.8095\n132  784.8440\n133 1115.5438\n134  998.9299\n135 1115.8560\n136  931.0851\n137  935.7776\n138  969.5563\n139  920.2575\n140  923.0128\n141  971.1214\n142  997.0698\n143  972.1763\n144  775.7842\n145 1063.2023\n146  975.4015\n147  918.7700\n148 1064.1902\n149  872.8863\n150 1039.3651\n151 1018.6946\n152  955.8719\n153  982.9815\n154  917.1532\n155  953.6226\n156  982.3155\n157 1030.6795\n158 1002.3261\n159  920.1648\n160  838.6909\n161  878.9505\n162  829.6094\n163  802.1561\n164  878.2401\n165 1079.2028\n166  936.0717\n167  950.5361\n168  890.8605\n169  799.3358\n170  897.8961\n171  849.0803\n172  969.3993\n173 1029.8031\n174  990.6682\n175  925.5050\n176  937.0910\n177  971.1083\n178 1027.6437\n179  965.4732\n180  947.4097\n181  862.0483\n182  978.1615\n183 1011.1505\n184  916.7871\n185  870.0635\n186  948.9761\n187  738.6006\n188  880.2537\n189 1132.9984\n190  936.7129\n191  974.7874\n192  830.0050\n193 1028.4885\n194  925.4232\n195  944.6962\n196 1010.6496\n197  895.0447\n198  999.6950\n199  943.9454\n200  998.0013\n201  954.2335\n202  885.1100\n203 1075.9155\n204  893.7337\n205  912.3288\n206 1071.6367\n207  995.9273\n208 1010.3681\n209  961.2334\n210  900.9868\n211 1072.2501\n212  994.5600\n213  893.5359\n214  971.3190\n215  956.0959\n216 1042.6597\n217  900.4032\n218  968.4890\n219 1003.1355\n220  899.7613\n221 1053.0005\n222  863.0198\n223  909.3720\n224 1007.0371\n225  972.8755\n226  903.5665\n227  830.5423\n228 1007.5047\n229  995.8490\n230 1001.6037\n231 1144.1433\n232  964.1477\n233  979.6971\n234  961.2329\n235  874.1971\n236 1070.5725\n237 1191.2779\n238  892.8464\n239 1011.2156\n240  960.1700\n241  840.5881\n242  932.5718\n243  989.4362\n244  893.0370\n245  959.4814\n246  958.7799\n247  924.1331\n248 1010.0642\n249  946.5351\n250  863.7615\n251 1000.0322\n252 1015.9743\n253 1011.4132\n254 1010.4164\n255 1043.1718\n256  881.4145\n257  948.0952\n258  893.7342\n259  953.5396\n260 1069.0680\n261  988.2130\n262  970.3683\n263  874.7507\n264  938.8513\n265  935.1249\n266  880.9166\n267  851.8317\n268 1104.5321\n269  915.6692\n270  990.3238\n271  975.4803\n272  953.9365\n273 1114.3836\n274  895.4929\n275 1092.3845\n276 1172.3690\n277  940.7486\n278  920.0572\n279  907.1867\n280 1035.7034\n281  862.5798\n282  962.2298\n283 1044.7607\n284  896.3378\n285  868.4049\n286 1116.9921\n287  993.4000\n288 1024.9986\n289 1003.5813\n290  982.7132\n291 1096.3792\n292  929.5512\n293  990.4498\n294 1005.9827\n295 1034.0473\n296  924.9723\n297 1000.3796\n298  846.3115\n299 1015.2422\n300  988.0119\n\n{do(300)* {sample(TenMileRace, size=100) %>% summarize(sz = sd(net))}} %>% summarize(m = mean(sz), s=sd(sz))\n\n         m        s\n1 969.3981 85.39019\n\n{do(300)* {sample(TenMileRace, size=400) %>% summarize(sz = sd(net))}} %>% summarize(m = mean(sz), s=sd(sz))\n\n         m        s\n1 971.7669 44.20284\n\n\n\nUsing a sample size that’s four times larger doesn’t affect the mean, but it reduces the standard deviation by a factor of two.\nIncreasing the number of trials does have any noticeable effect on either the mean or standard deviation."
  },
  {
    "objectID": "LC/LC-lesson20.html#e-1",
    "href": "LC/LC-lesson20.html#e-1",
    "title": "Learning Checks Lesson 20",
    "section": "20.E",
    "text": "20.E\nThroughout this course, you’re going to be using lm() to build models. Often, to demonstrate “sampling variation.” you will use sample() on a dataset or a DAG to generate a random sample and then send the result as the data= argument to lm(),\nHere are three computer commands that use the data= argument in different ways. One of them doesn’t work at all. Which one?\nsample(mtcars, size=10) %>% lm(mpg ~ wt + hp, data=.)\nsample(mtcars, size=10) %>% lm(mpg ~ wt + hp)\nlm(mpg ~ wt + hp, data=sample(mtcars, size=10))\n::: {.callout-warning} ## In draft\nThis problem would be come irrelevant if the fitmodel() command described in the [Lesson 19 NTI]{../NTI/NTI-Lesson19.html} is being used."
  },
  {
    "objectID": "lessons.html",
    "href": "lessons.html",
    "title": "Math 300R day-by-day Lessons",
    "section": "",
    "text": "See Fall 2022 repository.\nData, graphics, wrangling\n\nData with R\nScatterplots\nLinegraphs, histograms, facets\nBoxplots and barcharts\nfilter and summarize\ngroup_by, mutate, arrange\njoin, select, rename, & top n\nImporting data\nCase study/review\nGR1 (chapters 1-4)\n\nRegression\n\nSLR: Continuous x\nSLR: Discrete x\nSLR: Related topics\nMultiple regression: Numerical & discrete\nMultiple regression: Two numerical\nMultiple regression: Related topics\nMultiple regression: Conclusion/review\nGR 2 (chapters 5-6)\n\nThere are already learning checks associated with these first 18 chapters. Some additional ideas for learning checks are in this document."
  },
  {
    "objectID": "lessons.html#new-lessons",
    "href": "lessons.html#new-lessons",
    "title": "Math 300R day-by-day Lessons",
    "section": "New lessons",
    "text": "New lessons\n\nA note on computing summaries of data\n\nVariation\n\nStatistical thinking NTI : Objectives : LC : Reading : Student notes\nMeasuring and simulating variation NTI : Objectives : LC : Reading : Student notes\nSignal and noise NTI : Objectives : LC : Reading : Student notes\nSampling variation NTI : Objectives : LC : Reading : Student notes\nEstimate sampling variation from a single sample NTI : Objectives : LC : Reading : Student notes\nEffect size NTI : Objectives : LC : Reading : Student notes\nMechanics of prediction NTI : Objectives : LC : Reading : Student notes\nConstructing a prediction interval NTI : Objectives : LC : Reading : Student notes\nGR 3 (Lessons 19-26)\n\nInference\n\nCovariates NTI : Objectives : LC : Reading : Student notes\nCovariates eat variance NTI : Objectives : LC : Reading : Student notes\nConfounding NTI : Objectives : LC : Reading : Student notes\nSpurious correlation NTI : Objectives : LC : Reading : Student notes\nExperiment and random assignment NTI : Objectives : LC : Reading : Student notes\nMeasuring and accumulating risk NTI : Objectives : LC : Reading : Student notes\nConstructing a classifier NTI : Objectives : LC : Reading : Student notes\nAccounting for prevalence NTI : Objectives : LC : Reading : Student notes\nHypothesis testing NTI : Objectives : LC : Reading : Student notes\nCalculating a p-value NTI : Objectives : LC : Reading : Student notes\nFalse discovery with hypothesis testing NTI : Objectives : LC : Reading : Student notes\nGR 4 (lessons 28-38)\nReview"
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-28.html",
    "href": "Student-notes/Student-notes-lesson-28.html",
    "title": "Spring 2023 Math 300 Revisions",
    "section": "",
    "text": "Practice with a DAG\nTo demonstrate that the apparent relationship between an explanatory variable and a response variable – for instance, school expenditures and education outcomes – depends on the connections of the explanatory variable with covariates, let’s move away from the controversies of political issues and study DAGs, systems where everyone can agree exactly how the variables are connected.\nA simulation implements a hypothesis: a statement about that might or might not be true about the real world. As a starting point for our simulation, let’s imagine that education outcomes increase with school expenditures in a very simple way: each $1000 increase in school expenditures per pupil results in an average increase of 10 points in the SAT score: an effect size of 0.01 points per dollar. Thus, the imagined relationship is:\n\\[\\mbox{sat} = 1100 + 0.01 * \\mbox{dollar expenditure}\\]\nImagine that the fraction of students taking the SAT test also influences the average test score with an effect size of -4 sat points per percentage point. Adding this effect into the simulation leads to an imagined relationship of\n\\[\\mbox{sat} = 1100 + 0.01 * \\mbox{dollar expenditure} - 4 * \\mbox{participation percentage} .\\]\nAnd, of course, there are other factors, but we’ll treat their effect as random with a typical size of \\(\\pm\\) 50 points.\nTo complete the simulation, we’ll need to set values for dollar expenditures and participation percentage. We’ll let the dollar expenditures vary randomly from $7000 to $18,000 from one state to another and the participation percentage vary randomly from 1 to 100 percentage points.\nNotice that in this simulation, both participation percentage and expenditures affect education outcomes, but there is no connection at all between the two explanatory variables. That is, the graphical causal network is that shown in Figure @ref(fig:school-sim-1).\n\ndag_school1\n\nexpenditure ~ unif(7000, 18000)\nparticipation ~ unif(1, 100)\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + exo(50)\n\ndag_draw(dag_school1)\n\n\n\n\nFigure 1: A graphical causal network relating expenditures, participation percentage, and education outcome, where there is no connection between expenditures and participation.\n\n\n\n\nWe can generate simulated data and use the data to train models. ?@fig-school-data-1 shows the data and two different models.\n\nDat1 <- sample(dag_school1, size=500)\nmod1_1 <- lm(outcome ~ ns(expenditure,2), data = Dat1)\nmod1_2 <- lm(outcome ~ ns(expenditure,2) * participation, data = Dat1)\nmod_plot(mod1_1, interval=\"prediction\") %>%\n  gf_point(outcome ~ expenditure, data = Dat1)\nmod_plot(mod1_2, interval=\"prediction\") %>%\n  gf_point(outcome ~ expenditure, alpha=~participation, data = Dat1, inherit=FALSE)\n\n\n\n\nFigure 2: Data and models of the relationship between expenditures and education outcomes from a simulation in which expenditures and participation rate are unconnected as in Figure 1. - (a) The model outcome ~ expenditure - (b) The model with participation as a covariate: outcome ~ expenditure + participation Both models (a) and (b) show the same effect size for outcome with respect to expenditure.\n\n\n\n\n\n\n\nFigure 3: Data and models of the relationship between expenditures and education outcomes from a simulation in which expenditures and participation rate are unconnected as in Figure 1. - (a) The model outcome ~ expenditure - (b) The model with participation as a covariate: outcome ~ expenditure + participation Both models (a) and (b) show the same effect size for outcome with respect to expenditure.\n\n\n\n\nThe relationship between outcome and expenditure can be quantified by the effect size, which appears as the slope of the function. You can see that when the explanatory variables are unconnected, as in Figure 1, the functions have the same slope.\nNow consider a somewhat different simulation. Rather than expenditures and participation being unconnected (as in the causal diagram shown in Figure 1), in this new situation, we will posit a connection between the two explanatory variables. We’ll imagine that there is some broad factor, labeled “culture” in ?@fig-school-sim-2, that influences both the amount of expenditure and the participation in the tests used to measure education outcome. For instance, “culture” might be the importance that the community places on education or the wealth of the community.\n\ndag_school2\n\nculture ~ unif(-1, 1)\nexpenditure ~ 12000 + 4000 * culture + exo(1000)\nparticipation ~ (50 + 30 * culture + exo(15)) %>% pmax(0) %>% \n    pmin(100)\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + exo(50)\n\ndag_draw(dag_school2)\n\n\n\n\nFigure 4: A DAG for school outcomes that links participation and expenditure as a function of culture.\n\n\n\n\nAgain, using data from this simulation, we can train models:\n\n\noutcome ~ expenditures, which has no covariates.\n\n\noutcome ~ expenditures + participation, which includes participation as a covariate.\n\n\n?@fig-school-data-2 shows the data from the new simulation (which is the same in both subplots) and the form of the function trained on the data. Now model (a) shows a very different relationship between expenditures and outcome than model (b).\n\n\n\n\n\nFigure 5: Similar to ?@fig-school-data-1 but using the simulation in which the explanatory variables – expenditure and participation – are connected by a common cause. The two models show very different relationships between outcomes and expenditures. Model (b) matches the mechanism used in the simulation, while that mechanism is obscured in model (a).\n\n\n\n\n\n\n\nFigure 6: Similar to ?@fig-school-data-1 but using the simulation in which the explanatory variables – expenditure and participation – are connected by a common cause. The two models show very different relationships between outcomes and expenditures. Model (b) matches the mechanism used in the simulation, while that mechanism is obscured in model (a).\n\n\n\n\nSince we know the exact mechanism in the simulation—outcome increases with expenditure—we know that model (b) matches the workings of the simulation while model (a) does not.\nFor the simulation where expenditure and participation share a common cause, failing to stratify on participation – that is, looking at the points in @fig:-school-data-2 (a) but ignoring color – gives an utterly different result than if the stratification includes participation. :::"
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-19.html",
    "href": "Student-notes/Student-notes-lesson-19.html",
    "title": "Math 300 Lesson 19 Notes",
    "section": "",
    "text": "Here’s a graph of the data in our standard response-vs-explanatory graphic frame:\nThe graph suggests that non-smokers were more likely than smokers to be dead at the follow-up interview. But it’s hard to calculate proportions from such a graph. It’s reasonable to argue that for the purpose of showing the fraction of smokers and of non-smokers who died, a bar chart would be better.\nThe left barplot, showing counts, suggests that a higher proportion of non-smokers died than of smokers. But its easy to instruct the geom_bar() to graph proportions rather than counts, as done in the left plot. This makes it easy to conclude at a glance that a higher proportion of non-smokers have died.\nThe important question here, “Does smoking affect mortality?” translates well into the response/explanatory paradigm: outcome is the response variable while smoker is the explanatory variable. In the jitter-plot presentation of the data, these assignments are clearly indicated in the computer commands, which set x=smoker, y=outcome. In the barplot, a different notation is used: x=smoker, fill=outcome.\nUnfortunately, neither of the graphic styles—jitter or boxplot—answers the important question. At best they provide a description of the nurses in the Whickham data frame.\nTo answer the important question, we need to invoke statistical thinking. In particular, we need an interval summary of the proportion who died, not the point summary produced by the barplot.\nThis doesn’t mean that we can’t easily calculate the proportions from the categorical response variable: we just have to use the right commands. for instance:\nThe point summary—the prop_Alive column—suggests an obvious difference between the smokers and non-smokers. The interval summary—columns lower and upper—tempers this conclusion a little: the intervals almost touch.\nAlthough regression is our go-to technique for modeling relationships between variables, we can’t use it directly on a categorical response variable.\nTo use regression with a two-level categorical response variable, transform it into a zero-one encoding. In the following, we’ll use 1 to represent \"Alive\" and 0 to represent \"Dead\", although we can equally well do things the other way around.\nYou don’t yet know enough to interpret this interval summary. That will have to wait until Lesson 24. The significant1 feature of the interval on smokerYes is that it does not include zero. In everyday terms, the interval means, “Smokers are 3 to 12 percentage points more likely to survive for 20 years than are non-smokers.”\nUsing interval summaries instead of point summaries is an important aspect of statistical thinking, but there are other aspects that need to be taken into account. A simple, but important, question is whether the nurses recorded in the Whickham data frame are good representatives of all smokers. (It turns out that the nurses in Whickham are all women interviewed in the 1970s. At that moment of history, women were very different than men when it comes to smoking, and the Whickham smokers were also very different from today’s female smokers. We’ll say more about this in the demonstration below.)\nStatistical thinking also leads one to ask another sort of question: What else might be going on other than smoking? In technical language, the other-goings-on are called “covariates,” the topic of Lessons 28 & 29.\nFor instance, you might wonder about the overall result from our brief examination of the Whickham data. Is it really the case that the smokers were more likely to survive than the non-smokers? The answer is “yes,” as we have demonstrated from the previous analysis. But this answer is completely misleading. Tobacco companies worked hard to mislead people into thinking that smoking was not dangerous. They knew full well the negative health consequence of smoking, but they used statistical-sounding claims to hide this knowledge from the public.\nIn the following demonstration, we’ll look at the Whickham data again using the power of regression models to incorporate covariates."
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-19.html#key-ideas",
    "href": "Student-notes/Student-notes-lesson-19.html#key-ideas",
    "title": "Math 300 Lesson 19 Notes",
    "section": "Key ideas",
    "text": "Key ideas\n\nStarting with this lesson, the course will be about ways to extract actionable information from data. “Actionable information” is in a form to guide decision making. Core techniques for extracting information from data are graphics and models.\nIn this lesson …\n\nYou will start to construct data graphics in a standard format suited for modeling: a response variable on the vertical axis and an explanatory variable on the horizontal axis. (Other explanatory variables can be mapped to color or facets. We’ll get to that in later lessons.)\nYou will be introduced to statistical annotations that display an interval over the response variable. (In later lessons, we’ll introduce specific types of intervals.)\nYou will extend regression models to be able to handle a two-level categorical response variable.\n\n\n\n\n\n\n\n\nIn draft\n\n\n\nMake a jitter plot of gestation period versus smoking status. Then find the mean for each smoking status show this table. Then the ci.mean() as a table and graphed as an interval.\n\nvalues <- lm(gestation ~ smoke, data = Gestation) %>% \n  model_eval(interval=\"confidence\", skeleton=TRUE)\nGestation %>% ggplot(aes(x = smoke, y = gestation)) +\n  geom_jitter(width=0.2, alpha=0.1) +\n  geom_errorbar(data = values, aes(x=smoke, ymin=.lwr, ymax=.upr), y=NA)\n\nWarning: Removed 13 rows containing missing values (geom_point)."
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-19.html#objectives",
    "href": "Student-notes/Student-notes-lesson-19.html#objectives",
    "title": "Math 300 Lesson 19 Notes",
    "section": "Objectives",
    "text": "Objectives\nWhile in draft, see Objectives/Obj-lesson-19.qmd. Those will be copied over here."
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-19.html#a-standard-format-for-data-graphics",
    "href": "Student-notes/Student-notes-lesson-19.html#a-standard-format-for-data-graphics",
    "title": "Math 300 Lesson 19 Notes",
    "section": "A standard format for data graphics",
    "text": "A standard format for data graphics\nA “data graphic” is one that displays each of the rows of a data frame. Graphics are fundamentally two-dimensional, so the data graphic has a frame that maps one variable to the vertical axis (“the y aesthetic”) and another to the horizontal axis (“the x aesthetic”).\nModels are important to extracting information from data. Models always have a response variable and one or more explanatory variables. So our standard format for data graphics will put the response variable on the vertical axis and the one of the explanatory variables on the horizontal axis.\n\nExample: Scottish hill racing\nA popular competitive sport in Scotland is hill racing. This is a running race that involves ascending a hill rather than running on the flat. The math300::Hill_racing data frames records about 2000 winning performances in hill races. (See ?Hill_racing for the documentation.)\n\nA simple model of a hill race performance is time ~ distance.\n\nWhich is the response variable and which is the explanatory variable in the model time ~ distance?\n\nANSWER:\n\nMake a data graphic from Hill_racing that is consistent with this choice of explanatory and response variables.\n\n\n\n# complete the code\n# Hill_racing %>%\n#  ggplot(aes(x=_____, y=______)) +\n#  geom_point()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n# complete the code\nHill_racing %>%\n ggplot(aes(x=distance, y=time)) +\n geom_point() \n\nWarning: Removed 10 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\nThe “Null” model\nThe basis of the techniques you’ll learn in this second half of the course is building a model that uses explanatory variables to account for the response variable.\nWe’ll extract various quantities from the models we build and, in particular, use those quantities to compare one model to another, the point being to see SAY WHAT.\nSurprisingly, an important model for starting the chain of comparisons has no explanatory variable. OR RATHER, WE MAKE UP AN EXPLANATORY variable. SHOW MEAN is a coefficient from such a model. Model formula: y ~ 1\nLet’s use the standard data-graphic format to display to display y ~ 1,\n\nWhickham %>% mutate(group = \"all\") %>% ggplot(aes(y=age, x=group)) + geom_jitter()\n\n\n\n\n\nA somewhat more complex model is time ~ distance + climb. In this model there are two explanatory variables. Only one of them can be mapped to the horizontal axis, the other will need to be mapped to some other aesthetic or to faceting.\n\n\n# complete the code\n# Hill_racing %>%\n#  ggplot(aes(x=_____, y=______)) +\n#  geom_point()"
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-21.html",
    "href": "Student-notes/Student-notes-lesson-21.html",
    "title": "Math 300 Lesson 21 Notes",
    "section": "",
    "text": "Note in draft\n\n\n\nOne activity can be to determine whether the parameter to exo() is in terms of standard deviation or of variance."
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-21.html#key-ideas",
    "href": "Student-notes/Student-notes-lesson-21.html#key-ideas",
    "title": "Math 300 Lesson 21 Notes",
    "section": "Key ideas",
    "text": "Key ideas"
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-23.html",
    "href": "Student-notes/Student-notes-lesson-23.html",
    "title": "Student notes lesson 23",
    "section": "",
    "text": "## Margin of error\n```{r}\none_trial <- function(n=2) {\nvals <- rnorm(n)\ntibble(m = mean(vals), s = sd(vals))\n}\n```\nThe confidence interval from each trial will be $m \\pm \\beta s$, where $\\beta$ is a number yet to be determined. How to do so, we want to select $\\beta$ so that, across all trials, 95% will include the mean of the distribution from which the data values were drawn.\n```{r}\n# vary beta until 95% of the trials have a left value smaller than zero.\nn <- 10000\nbeta <- 0.02\nTrials <- do(1000) * one_trial(n=n) %>%\nmutate(left = m - beta*s, right = m + beta*s)\nTrials %>%\nsummarize(coverage = sum(sign(left*right) < 0)/n())\n```\nFor sample size $n=10$, $\\beta$ needs to be 0.72, while for $n=100$, $\\beta$ needs to be 0.20. For $n=1000$, the multiplier needs to be 0.062, and so on. For $n=10000$, the multiplier needs to be 0.02\nn | $\\beta$ | $t = \\beta / \\sqrt{\\strut n}$\n---|---------|------------------\n10 | 0.72 | 2.26\n15 | 0.55 | 2.14\n20 | 0.47 | 2.09\n50 | 0.28 | 2.01\n100 | 0.20 | 1.98\n500 | 0.088 | 1.96\n1000 | 0.062 | 1.96\n10000 | 0.20 | 1.96\nNotice that as $n$ gets bigger, the size of $\\beta$ to cover 95% of the trials gets smaller. More than a century ago, it was known that the multiplier for any sample size $n$ is effectively $2/\\sqrt{n}$. Consequently, the confidence interval for the mean of $n$ values is approximately\n$$\\mathtt{CI} = \\mathtt{mean(x)}\\pm \\underbrace{\\frac{2}{\\sqrt{n}} \\mathtt{sd(x)}}_\\text{margin of error}$$\nThe quantity following the $\\pm$ is called the “**margin of error**.” Because of the $\\pm$, the overall length of the confidence interval is twice the margin of error.\nIt is much easier to remember $2/\\sqrt{n}$ than a list of $\\beta$ values that change from one $n$ to the next. Another ubiquitous memory aid involves another technical term, the **standard error**. This involves a simple re-arrangement of the equation for the confidence interval:\n$$\\mathtt{CI} = \\mathtt{mean(x)}\\pm 2\\underbrace{\\frac{\\mathtt{sd(x)}}{\\sqrt{n}}} _\\text{standard error}$$\nIt’s standard in statistical software to report the standard error of a coefficient. Usually abbreviated `se` or `std.error` or something similar. The software is doing the divide-by-$\\sqrt{n}$ for you, so all you need to construct the margin of error is multiply the standard error by 2. That’s convenient, but it comes at the cost of yet another use of the words “standard” and “error,” which can be confusing.\nHere’s an example of a typical software output summarizing a model in the format called a “**regression report**.” Here’s an example, looking at the fuel economy of cars (`mpg`) as a function of the car’s weight (`wt`) and horsepower (`hp`).\n```{r}\nlm(mpg ~ wt + hp, data = mtcars) %>%\nregression_summary()\n```\nAccording to this report, each additional 1000 lbs of weight decreases fuel economy by an estimated 3.9 miles per gallon. But since the model is based on a sample of data, it’s important to report the *precision* of that number in the face of sampling variation. The confidence interval is the standard format for that precision. It will be the estimate plus-or-minus two times the standard error, that is: $-3.88 \\pm 2\\times0.633$, that is, -5.15 to -2.61 mpg per 1000 lbs. Similarly, each addition horsepower (`hp`) lowers fuel economy by $-0.032 \\pm 2 \\times 0.009$, that is, -0.05 to 0.013 mpg per horsepower.\nEven more convenient is to calculate the confidence interval with `confint()` which handles all the computations, including the ones for tiny $n$ described in @sec-tiny-n."
  },
  {
    "objectID": "LC/LC-lesson37.html",
    "href": "LC/LC-lesson37.html",
    "title": "Learning Checks Lesson 37",
    "section": "",
    "text": "Ideas:"
  },
  {
    "objectID": "LC/LC-lesson37.html#section",
    "href": "LC/LC-lesson37.html#section",
    "title": "Learning Checks Lesson 37",
    "section": "37.1",
    "text": "37.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson23.html",
    "href": "LC/LC-lesson23.html",
    "title": "Learning Checks Lesson 23",
    "section": "",
    "text": "Vocabulary: Sampling distribution, standard error, sampling variability, sample size\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson23.html#section-1",
    "href": "LC/LC-lesson23.html#section-1",
    "title": "Learning Checks Lesson 23",
    "section": "23.2",
    "text": "23.2\nIn LC 22.2, using do(100), you displayed the sampling distribution on the x coefficient of the model y ~ x applied to data simulated from dag01. Among other things, you calculated the standard deviation of the sampling distribution. Copy over the proc1() you wrote for LC 22.2 into your Rmd document for this lesson.\nCalculate the standard deviation of the sampling distribution in each of these situations.\n\n\n\nNumber of trials\nSample size\n\n\n\n\ndo(100)\nsize=25\n\n\ndo(100)\nsize=100\n\n\ndo(100)\nsize=400\n\n\ndo(500)\nsize=25\n\n\ndo(500)\nsize=100\n\n\ndo(500)\nsize=400\n\n\n\nIn each case, the standard deviation is somewhat random, since new simulated data is collected from dag01 each time. Nonetheless, there is a systematic pattern to how the standard deviation varies with the number of trials and with the sample size.\n\nDescribe how the standard deviation of the sampling distribution of the x coefficient varies with sample size. The general trend should be easy to see.\nDoes the standard deviation of the sampling distribution depend on the number of trials?\nGoing back to your results from (1), try to find a simple quantitative relationship that describes how the standard deviation depends on sample size. State that relationship in words."
  },
  {
    "objectID": "LC/LC-lesson23.html#section-2",
    "href": "LC/LC-lesson23.html#section-2",
    "title": "Learning Checks Lesson 23",
    "section": "23.3",
    "text": "23.3\nWe’re going to build models of prices of books based on the moderndive::amazon_books data frame. For each model, you will calculate the confidence interval of one or more coefficients in two ways:\n\nDirectly, using confint().\nIndirectly, using regression_summary()\n\nModel 1.\n\nModel list_price versus amazon_price. Calculate the confidence intervals on the intercept and on the amazon_price coefficient.\nInterpret the amazon_price coefficient in everyday words.\n\n\n\n\n\n\n\nSolution to part 1.\n\n\n\n\nlm(list_price ~ amazon_price, data = amazon_books) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept)  3.716532 5.122737\namazon_price 1.049308 1.127471\n\nlm(list_price ~ amazon_price, data = amazon_books) %>% regression_summary()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      4.42    0.357       12.4 5.22e- 29\n2 amazon_price     1.09    0.0199      54.8 2.82e-165\n\n\n\nYou can read the confidence interval directly from the confint() report. For the regression report, calculate the confidence interval as the estimate \\(\\pm 2\\) times the “standard error.”\nJust to look at the amazon_price() coefficient, the list price is about 8% higher than the Amazon price. Here, “about” means 5% to 13%. But don’t forget the intercept. The list price is, on average, about $4.50 higher than the 1.08 multiplier on the Amazon price.\n\n\n\nModel 2.\n\nModel list_price versus amazon_price, including hard_paper as a covariate.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept)  3.028400 4.466996\namazon_price 1.042536 1.117826\nhard_paperH  1.786977 3.882884\n\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% regression_summary()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      3.75    0.366      10.3  1.62e- 21\n2 amazon_price     1.08    0.0191     56.5  9.65e-169\n3 hard_paperH      2.83    0.533       5.32 1.93e-  7\n\n\nA hardcover costs about $2 to $4 more than a paperback.\n\n\n\n\n\n\n\n\nNote in draft\n\n\n\nReturn to this example in the prediction lesson, to show how the confidence interval and the prediction interval are different.\nMaybe also use it in one of the side-exercises on interaction terms. (The plan is not to strongly emphasize interaction terms but to refer to them in the occasional exercise.)"
  },
  {
    "objectID": "LC/LC-lesson23.html#objective-23.1",
    "href": "LC/LC-lesson23.html#objective-23.1",
    "title": "Learning Checks Lesson 23",
    "section": "23.4 (Objective 23.1)",
    "text": "23.4 (Objective 23.1)\nWe’re going to work with a very short dataset so that you can see directly what resampling a data frame does. (Ordinarily, you use resampling on an entire dataset, but here we are trying to make a point about the mechanism of resampling.)\n\nCreate a data frame Five that consists of the first five rows of moderndive::mythbusters_yawn. (Hint: Use head().) Put the code for doing this into your Rmd homework paper. Note that Five contains the data from subjects 1 through 5.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nFive <- mythbusters_yawn %>% head(5)\n\n\n\n\nUse resample() to generate a new data frame from Five. At this point, you are just going to look at the result, processing it “by eye.” How many distinct human subjects are reported in the resampled data? (Your answer will likely differ from your classmates’, since resampling is done at random.)\nRepeat (2) ten times. Each time, count the number of distinct human subjects.\n\nReport those ten numbers on your write-up.\nThere will usually be one or more subjects repeated in the output. Look at these repeats carefully to check whether the variables have the same value for all the repeats or whether sometimes a repeated subject has different values for group or yawn.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nMost of the time there will be 2, 3, or 4 distinct subject. The balance of the five rows will be repeats of other subjects. When a subject is repeated, the entire row is identical for all instances of that subject.\n\n\nGoing further (optional). It’s pretty easy to automate the process of generating the resample and counting the number of distinct human subjects. Like this:\n\n{resample(Five) %>% unique() %>% nrow()}\n\n[1] 4\n\n\nUsing do(1000), carry out 1000 trials of this process, saving the overall results in a data frame named Trials. What is the mean number of unique human subjects across the 1000 trials? What fraction is this of the five subjects.\nDo the same again, but instead of using Five, use the whole mythbusters_yawn data frame (which has 50 rows). What fraction of the 50 human subjects, on average, shows up in the resamples?\n\n\n\n\n\n\nSolution\n\n\n\n\nTrials <- do(1000) * {resample(mythbusters_yawn) %>% unique() %>% nrow()}\nTrials %>% summarize(mn = mean(result)/nrow(mythbusters_yawn))\n\n       mn\n1 0.63666"
  },
  {
    "objectID": "LC/LC-lesson23.html#objective-23.1-1",
    "href": "LC/LC-lesson23.html#objective-23.1-1",
    "title": "Learning Checks Lesson 23",
    "section": "23.5 (Objective 23.1)",
    "text": "23.5 (Objective 23.1)\nReturn to the amazon_books data frame and the model list_price ~ amazon_price. In Exercise 23.3 you used the regression report to calculate the confidence intervals on the intercept and on the amazon_price coefficient. Now you are going to repeat the calculation in a different way, using randomization, a process called “bootstrapping”.\nThe basic process is to train a model using resampled data, like this:\n\nlm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()\n\n (Intercept) amazon_price \n    4.390164     1.070354 \n\n\nThen, using do(500), carry out 500 trials, saving the result in a data frame named Trials.\nProcess Trials to calculate both the mean and the standard deviation of the intercept and amazon_price columns. How do those results compare to the “standard error” results from the same model (without resampling) as you found in LC 23.3?\n\n\n\n\n\n\nSolution\n\n\n\nSOMETHING IS WRONG HERE. The means are about the same as from the regression report (as they should be) but the standard deviations are 3-4 times larger. WHAT GIVES?\nTHE PROBLEM IS a handful of books where the Amazon price is very different from the list price, because the book itself is very expensive (e.g. $100). Remedy\n\nSwitch to another data example, maybe doing both the regression report and the bootstrapping in one exercise.\nThis is an object lesson in outliers. Since the dollar discount is presumably proportional to the price, we should have used log transforms.\n\n\nTrials <- do(500) * {lm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()}\nTrials %>% summarize(m1 = mean(Intercept), \n                     m2 = mean(amazon_price), \n                     sintercept = sd(Intercept), \n                     samazon_price = sd(amazon_price))\n\n        m1       m2 sintercept samazon_price\n1 4.197692 1.108021  0.9120866    0.07984698"
  },
  {
    "objectID": "LC/LC-lesson22.html",
    "href": "LC/LC-lesson22.html",
    "title": "Learning Checks Lesson 22",
    "section": "",
    "text": "Consider these three data frames:\n\nOne <- sample(dag01, size=25)\nTwo <- do(10) * {\n  lm(y ~ x, data = sample(dag01, size=25)) %>%\n    coefficients()\n  }\nThree <- Two %>% \n  summarize(mx = mean(x), sx = sd(x))\n\n\nBoth One and Two have columns called x, but they stand for different things. Explain what the unit of observation is and what the values in x represent..\nThree does not have a column named x, but it is a summary of the x column from Two. What kind of summary.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nIn One, the x column contains the simulated of the x variable from dag01. The unit of observation is a single case, for instance a person for whom observations were made of x and y. The simulation involves generating 25 rows of data: one row for each of 25 people.\nIn Two, the x column is the regression coefficient on x from the simulation. Each row of Two corresponds to one trial in which regression is being performed on a sample of size 25 of simulated data from dag01.\nThree is a summary of the 10 trials in Two. The columns, named mx and sx, tell about the distribution of x across all the trials."
  },
  {
    "objectID": "LC/LC-lesson22.html#obj-21.3",
    "href": "LC/LC-lesson22.html#obj-21.3",
    "title": "Learning Checks Lesson 22",
    "section": "22.2 (Obj 21.3)",
    "text": "22.2 (Obj 21.3)\nPart 1\nYou are going to write a procedure that automates the following process:\n\nsampling from a DAG, specifically dag01, using sample() with a size of 25.\nfitting a model y ~ x using lm()\nreporting the coefficient on x using coefficients().\n\nCall the procedure proc1().\nTo do this fill in the following template in your Rmd document:\nproc1 <- function() {\n  # your statements go here\n}\nOnce you have proc1() ready, you can carry out the procedure by giving a simple command:\nproc1()\nPart 2\nNow that you have proc1() ready and have tried it out, you are going to run the procedure 100 times repeatedly and look at the distribution in the x coefficient.\nOf course, you could laboriously give the command proc1() 5 times, and write down the x coefficient each time. Far better, though, to automate the process of repeating and collecting the x coefficient.\nYou can do this easily by using do(5) in conjunction with proc1().\n\nWhat’s the form in which the coefficients are collected when using do()?\nIs the x coefficient the same from trial to trial? Explain why or why not.\nChange your statement to run 100 trials rather than just 5, and to store the collected results in a data frame called Trials. Use appropriate graphics to display the distribution of the x coefficient. Summarize the distribution in a sentence or two.\nCreate a consise summary of the x column of Trials using summarize() with sd(x) to calculate the standard deviation. Compare the size of the standard deviation to the graphical display in (3).\n\n\nSolution\nPart 1\n\nproc1 <- function() {\n  Dat <- sample(dag01, size=25)\n  Mod <- lm(y ~ x, data = Dat)\n  coefficients(Mod)\n}\n\nor, more concisely\n\nproc1 <- function() {\n  sample(dag01, size=25) %>%\n    lm(y ~ x, data = .) %>%\n    coefficients()\n}\n\nPart 2\n\ndo(5) * proc1()\n\n  Intercept        x\n1  3.993889 1.823723\n2  3.781726 1.192576\n3  4.152893 1.328287\n4  3.663743 1.329055\n5  3.718211 1.620334\n\n\n\nThe results of the five trials are collected into a data frame.\nThe x coefficients varies from trial to trial.\n\nCollect 100 trials\n\nTrials <- do(100) * proc1()\n\n\nAn appropriate graphical display of the trials:\n\n\nggplot(Trials, aes(x)) + geom_density(fill=\"blue\", alpha=0.3)\n\n\n\n\nThe x coefficient varies from near 0.5 to near 2.5 in a bell-shaped form.\n\nSummarize the trials by the standard deviation.\n\n\nTrials %>% summarize(s = sd(x))\n\n          s\n1 0.2005022\n\n\nThe standard deviation is about 1/4 the width of the distribution."
  },
  {
    "objectID": "LC/LC-lesson36.html",
    "href": "LC/LC-lesson36.html",
    "title": "Learning Checks Lesson 36",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson34.html",
    "href": "LC/LC-lesson34.html",
    "title": "Learning Checks Lesson 34",
    "section": "",
    "text": "Stat2Data::FaithfulFaces. Come back to it concerning prevalence."
  },
  {
    "objectID": "LC/LC-lesson34.html#section",
    "href": "LC/LC-lesson34.html#section",
    "title": "Learning Checks Lesson 34",
    "section": "34.1",
    "text": "34.1\nTo illustrate how stratification is used to build a classifier, consider this very simple, unrealistically small, made-up data frame listing observations of animals:\n\n\n\n\n \n  \n    species \n    size \n    color \n  \n \n\n  \n    A \n    large \n    reddish \n  \n  \n    B \n    large \n    brownish \n  \n  \n    B \n    small \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n\n\n\n\n\nYou are going to build classifiers using the data. The output of the classifier will be the probability that the species is A. The classifier itself will be a simple table: each row lists the different levels of the explanatory variable(s) the the classifier output (as a probability that the species is A).\n\nUse just size as an explanatory variable. Since there are two levels for size, the classifier can take the form of a simple table, giving the proportion of rows for each of the two sizes. Fill in the table to reflect the data.\n\n\n\n\n\n \n  \n    size \n    prop_of_A \n  \n \n\n  \n    large \n     \n  \n  \n    small \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are three rows where the size is large, of which one is species A. The classifier output is thus 2/3 for large.\nSimilarly, there is only one row where the size is small, none of which are species A. The classifier output is 0/1 for small.\n\n\n\nRepeat (1), but instead of “size”, use just “color” as an explanatory variable.\n\n\n\n\n\n \n  \n    color \n    prop_of_A \n  \n \n\n  \n    reddish \n     \n  \n  \n    brownish \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are three rows where the color is brownish, of which two are species A. The classifier output is thus 1/3 for brownish.\nThere is only one row where the color is reddish, and it is species A. The classifier output is 1/1 for reddish.\n\n\n\nAgain build a classifier, but use both color and size as explanatory variables.\n\n\n\n\n\n \n  \n    color \n    size \n    prop_of_A \n  \n \n\n  \n    reddish \n    large \n     \n  \n  \n    reddish \n    small \n     \n  \n  \n    brownish \n    large \n     \n  \n  \n    brownish \n    small \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is just one row in which color is reddish and size is large, and it is species A. The classifier output is thus 1/1.\nThere are two rows in which color is brownish and size is large, one of which is species A. The classifier output is thus 1/2.\nThere is one row in which color is brownish and size is small. It is species B. The classifier output is 1/1.\nThere are no rows in which color is reddish and size is small. A classifier output of 0/0 is meaningless. So our classifier has nothing to say for these inputs.\n\n\n\nFinally, build the “null model”, a no-input classifier. This means there is just one group, which has all four rows. -A- Of the four rows, two are species A, so the classifier output is 2/4."
  },
  {
    "objectID": "LC/LC-lesson34.html#section-1",
    "href": "LC/LC-lesson34.html#section-1",
    "title": "Learning Checks Lesson 34",
    "section": "34.2",
    "text": "34.2\nThe graph below shows data on marital status versus age from National Health and Nutrition Evaluation Survey data. You can see that the probability of the various possibilities are a function of age.\n\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:mosaic':\n\n    cross\n\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n\n\n\n\nA very simple classifier can be constructed just by indicating at each age which marital status is the most likely, as seen in the figure below.\n\n\nmaximum number of iterations reached -0.002303255 -0.002335247\n\n\n\n\n\nA classifier output should be a probability, not a categorical level. On the blank graph below, sketch out a plausible form for probability vs age for each of three categorical levels shown in the above plot. (Hint: At an age where, say, “NeverMarried” is the categorical output, the probability for “NeverMarried” will be higher than the other categories.)\n\n\n\n\n\n\nPresumably the probability output for each category varies somewhat smoothly. There are two constraints:\n\nAt any age/sex, one probability will be the highest of the three. That one should correspond to the category shown in the first graph.\nThe probabilities should add up to 1.\n\nHere’s one possibility. Note that for females, the highest probability around age 80 is “widowed”.\n\n\n\n\n\n\n\nFrom CPS §32.6: openintro::possum. Let’s investigate the possum data set again. This time we want to model a binary outcome variable. As a reminder, the common brushtail possum of the Australia region is a bit cuter than its distant cousin, the American opossum. We consider 104 brushtail possums from two regions in Australia, where the possums may be considered a random sample from the population. The first region is Victoria, which is in the eastern half of Australia and traverses the southern coast. The second region consists of New South Wales and Queensland, which make up eastern and northeastern Australia.\nWe use logistic regression to differentiate between possums in these two regions. The outcome variable, called pop, takes value Vic when a possum is from Victoria and other when it is from New South Wales or Queensland. We consider five predictors: sex, head_l, skull_w, total_l, and tail_l.\nExplore the data by making histograms or boxplots of the quantitative variables, and bar charts of the discrete variables.\n\nAre there any outliers that are likely to have a very large influence on the logistic regression model?\nBuild a logistic regression model with all the variables. Report a summary of the model.\nUsing the p-values decide if you want to remove a variable(s) and if so build that model.\nFor any variable you decide to remove, build a 95% confidence interval for the parameter.\nExplain why the remaining parameter estimates change between the two models.\nWrite out the form of the model. Also identify which of the following variables are positively associated (when controlling for other variables) with a possum being from Victoria: head_l, skull_w, total_l, and tail_l.\nSuppose we see a brushtail possum at a zoo in the US, and a sign says the possum had been captured in the wild in Australia, but it doesn’t say which part of Australia. However, the sign does indicate that the possum is male, its skull is about 63 mm wide, its tail is 37 cm long, and its total length is 83 cm. What is the reduced model’s computed probability that this possum is from Victoria? How confident are you in the model’s accuracy of this probability calculation?"
  },
  {
    "objectID": "LC/LC-lesson34.html#section-2",
    "href": "LC/LC-lesson34.html#section-2",
    "title": "Learning Checks Lesson 34",
    "section": "34.3",
    "text": "34.3\nThe HELPrct date frame (in the mosaicData package) is about a clinical trial (that is, an experiment) conducted with adult inpatients recruited from a detoxification unit. The response variable of interest reflects the success or failure of the detox treatment, namely, did the patient continue use of the substance abused after the treatment.\nFigure @ref(fig:giraffe-fall-door-1) shows the output of a simple classifier (maybe too simple!) of the response given these inputs: the average number of alcoholic drinks consumed per day in the past 30 day (before treatment); and the patient’s self-perceived level of social support from friends. (The scale for social support is zero to fourteen, with a higher number meaning more support.)\n\n\n\n\n\nClassifier based on data from a clinical trial\n\n\n\n\n\nWhat’s the probability of treatment failure for a patient who has 25 alcoholic drinks per day? Does the probability depend on the level of social support? -A- Probability of failure is 75%, and doesn’t depend on the level of social support.\nFor a patient at 0 to 10 alcoholic drinks per day, what’s the probability of treatment failure? Does the probability depend on the level of social support? -A- The probability of failure ranges from about 72% for those with no social support to 82% for those with high social support?\nYou are thinking about a friend who has roughly five alcoholic drinks per day. You are concerned that he will go on to substance abuse. Do the data from the clinical trial give good reason for your concern? Explain why or why not.\n\n\n\n\n\n\n\nSolution\n\n\n\nIt’s always a good idea to be concerned for your friend, but the data reported here are not a basis for that concern. These data are from a population consisting of inpatients from a detoxification unit. These are people who have already shown strong substance abuse. The classifier is not generalizable to your friend, unless he is an inpatient from a detox unit.\n\n\n\nExplain what’s potentially misleading about the y-axis scale selected for the plot.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe selected scale doesn’t include zero and so tends to over-emphasize what amount to small differences in the probability of failure."
  },
  {
    "objectID": "LC/LC-lesson35.html",
    "href": "LC/LC-lesson35.html",
    "title": "Learning Checks Lesson 35",
    "section": "",
    "text": "Given some classifier summaries, calculate the false-positive and false-negative rates as well as the sensitivity and specificity\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson35.html#q",
    "href": "LC/LC-lesson35.html#q",
    "title": "Learning Checks Lesson 35",
    "section": "35.Q",
    "text": "35.Q\nTITLE GOES HERE: Customize the classifiers in (ref:ant-take-room) for a population in which species A is three times as common as species B."
  },
  {
    "objectID": "LC/LC-lesson35.html#solution-1",
    "href": "LC/LC-lesson35.html#solution-1",
    "title": "Learning Checks Lesson 35",
    "section": "Solution",
    "text": "Solution\nFollow the same procedure as in (ref:ant-take-room), but duplicate each of the A rows two times, so that the data show a world in which A is three times as common as B. That is,\n\n\n\n\n \n  \n    species \n    size \n    color \n  \n \n\n  \n    A \n    large \n    reddish \n  \n  \n    A \n    large \n    reddish \n  \n  \n    A \n    large \n    reddish \n  \n  \n    B \n    large \n    brownish \n  \n  \n    B \n    small \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n\n\n\n\n\nFor the classifier using just size as an explanatory variable … There are 7 rows for which size is large. Of these six are species A so the classifier output is 6/7. For size small, there is just one row, which is B, so the classifier output is 0/1.\nFor the classifier using just color as an explanatory variable … There are five rows for which color is brownish. Of these, 2 are species A. So the classifier output is 2/5 for brownish. For reddish, the classifier output is 3/3."
  },
  {
    "objectID": "LC/LC-lesson35.html#r",
    "href": "LC/LC-lesson35.html#r",
    "title": "Learning Checks Lesson 35",
    "section": "35.R",
    "text": "35.R\nThe two tables below are different summaries of the Univ. of California Berkeley graduate admissions data from the 197e fall quarter. (Data frame: UCB_applicants) Each of the tables is about conditional probabilities about admission and sex.\n\n\n\n\nTable A\n \n  \n      \n    admitted \n    rejected \n  \n \n\n  \n    female \n    31.7 \n    46.1 \n  \n  \n    male \n    68.3 \n    53.9 \n  \n\n\n\n\n\n\n\nTable B\n \n  \n      \n    female \n    male \n  \n \n\n  \n    admitted \n    30.4 \n    44.5 \n  \n  \n    rejected \n    69.6 \n    55.5 \n  \n\n\n\n\n\n\nWhich table displays p(sex given admit)?\n\n\n\n\n\n\n\nSolution Table\n\n\n\nA\n\n\n\nWhich table displays p(admit given sex)?\n\n\n\n\n\n\n\nSolution\n\n\n\nTable B\n\n\n\nWhich of these statements is true?\n\nTable A shows that admitted students are more likely to be male.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nTrue\n\n\nb. Table A shows that rejected students are more likely to be male. \n\n\n\n\n\n\nSolution\n\n\n\nTrue\n\n\nc. Table A shows that females are less likely to be admitted than rejected. \n\n\n\n\n\n\nSolution False.\n\n\n\nThere’s nothing in Table A to tell us what fraction of applicants were admitted. Because table A stratifies by admitted/rejected, we don’t know how large those two groups are with respect to one another.\n\n\nd. Table B shows that admitted students are more likely to be male. \n\n\n\n\n\n\nSolution False.\n\n\n\nTable B is stratified on female/male. As a result, there’s no information in Table B about what fraction of applicants is male.\n\n\ne. Table B shows that females are less likely to be admitted than males. \n\nSuppose you are interested in the possibility of discrimination against women in graduate admissions (in Berkeley in 1973). Which of these questions is the right one to ask, and which table gives you the information needed to answer the question?\n\nWhat is the probability of an admitted student being a female compared to the probability of a rejected student being a female?\n\n\n\n\n\n\n\n\nSolution False.\n\n\n\nThe question is about the relative admissions probability of females and males.\n\n\nb. What is the probability of a female applicant being admitted compared to the probability that an admitted student is male? \n\n\n\n\n\n\nSolution False. You want to compare like with like. The group involved in “the probability of a female student being admitted” is females, whereas the group involved in “the probability that an admitted student is male” is admitted students. There’s little if any meaning in comparing a probability among the group of females to a probability among group of admitted students.\n\n\n\n\n\n\nc. What is the probability of  a female applicant being admitted compared to the probability of a male applicant being admitted? \n\n\n\n\n\n\nSolution True.\n\n\n\nWe want to compare two groups: females and males. Here, we’re comparing the probability of being admitted in each of the comparison groups. Table B is formatted to enable this comparison."
  },
  {
    "objectID": "LC/LC-lesson21.html",
    "href": "LC/LC-lesson21.html",
    "title": "Learning Checks Lesson 21",
    "section": "",
    "text": "The following command will generate a data frame with 1000 rows from dag00 and calculate the variance of the x and y variables:\n\nsample(dag00, size=1000) %>%\n  summarize(vx = var(x), vy = var(y))\n\n# A tibble: 1 × 2\n     vx    vy\n  <dbl> <dbl>\n1  4.13  1.06\n\n\nCompare this result to the DAG tilde expressions\n\ndag00\n\nx ~ exo(2) + 5\ny ~ exo(1) - 7\n\n\nIn the tilde expressions, exo(2) means to generate noise of magnitude 2.0.\n\nIs the argument to exo() specified in terms of the variance or the standard deviation?\nThe tilde expression for x specifies that the constant 5 is to be added to exo(2). Similarly, the constant -7 is added to y. How do these constants relate to the calculated magnitudes of x and y?\n\n\n\n\nThe standard deviation. For instance, x has noise of magnitude 2. The variance of x is 4, the square of 2.\nThe standard deviation (and therefore the variance) ignore such added constants."
  },
  {
    "objectID": "LC/LC-lesson21.html#section-1",
    "href": "LC/LC-lesson21.html#section-1",
    "title": "Learning Checks Lesson 21",
    "section": "21.2",
    "text": "21.2\n?@sec-signal-and-noise introduces the idea that variables consist of components. A simple breakdown is into two components: i. the part of the variable that is determined by other variables in the system (“signal”) and ii. the random part of the variable (“noise”). The section uses dag01 as an illustration of how a variable can be partly determined and partly random noise.\n\nWrite and execute a command that will generate 500 rows of simulated data from dag01 and will calculate the standard deviation of x and of y.\nWhat’s the magnitude of x in the simulated data? What’s the magnitude of y?\nDoes this change if you use data with 1000 or 20000 rows?\n\n\nSolution\n\nsample(dag01, size=500) %>% summarize(sx = sd(x), sy=sd(y))\nThe standard deviation of x is about 1, the standard deviation of y is about 1.8.\nNo, the values are roughly the same regardless of the size of the sample."
  },
  {
    "objectID": "LC/LC-lesson21.html#section-2",
    "href": "LC/LC-lesson21.html#section-2",
    "title": "Learning Checks Lesson 21",
    "section": "21.3",
    "text": "21.3\n[DRAW several DAG-like graphs, one of which should be undirected in all edges, one should be undirected on one or two edges (but not all), and one should be cyclic and another acyclic.]\nReferring to the graphs in the figure, say which ones are DAGs. If a graph is not a DAG, say whether that’s because it’s not directed or because it’s not cyclic.\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson21.html#a",
    "href": "LC/LC-lesson21.html#a",
    "title": "Learning Checks Lesson 21",
    "section": "21.A",
    "text": "21.A\nA DAG (directed acyclic graph) is a mathematical object used to state hypothetical causal relationships between variables. Explain briefly (e.g. a few sentences overall) what each of the words “directed,” “acyclic,” and “graph” mean in the context of a DAG.\n\n\n\n\n\n\nSolution\n\n\n\n\nA graph is a relationship between discrete elements (called “nodes” abstraction) each of which for us represents a hypothetical quantity, that is, a variable. In addition to the nodes, the graph contains edges which represent the connections between variables.\nA directed graph is one whose arrows have a direction. For instance \\(A \\rightarrow B \\leftarrow C\\) means that \\(A\\) and \\(C\\) together cause \\(B\\), but \\(B\\) has no influence on \\(A\\) and \\(C\\).\nAn acyclic graph is one where it is impossible to start on any given node and, by following the directed edges, return back to that node."
  },
  {
    "objectID": "LC/LC-lesson21.html#b",
    "href": "LC/LC-lesson21.html#b",
    "title": "Learning Checks Lesson 21",
    "section": "21.B",
    "text": "21.B\nDraw these DAGs:\n\n“April showers bring May flowers.”\n“Price of a rug is determined by the size and the quality of materials.”\n“The weight of an automobile is reflected in the MPG fuel economy, as is the speed of the car, and inflation level of the tires.”\n“Plants tend to grow in the direction of the sunlight.”\n“An ice-cream shop owner needs to plan staffing based on the season, day of the week, and holidays.”"
  },
  {
    "objectID": "LC/LC-lesson21.html#c",
    "href": "LC/LC-lesson21.html#c",
    "title": "Learning Checks Lesson 21",
    "section": "21.C",
    "text": "21.C"
  },
  {
    "objectID": "LC/LC-lesson21.html#section-3",
    "href": "LC/LC-lesson21.html#section-3",
    "title": "Learning Checks Lesson 21",
    "section": "21.4",
    "text": "21.4\nGenerate simulated data from dag01 with 1000 rows. Fit the regression model y ~ x to the data and examine the coefficients.\n\nHow do the coefficients relate to the tilde expressions that define dag01?\nInstead of using the regression model y ~ x, where y is the response variable, try the regression model x ~ y. Do the coefficients from x ~ y correspond in any simple way to the tilde expressions that define dag01?\n\n\nSolution\n\nsample(dag01, size=1000) %>%\n  lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      3.988        1.542  \n\n\nThe intercept corresponds to the additive constant (4) in the y tilde expression. The x coefficient corresponds to the multiplier on x in the tilde expression.\nThe formula for x isn’t reflected by the coefficients.\nUsing x as the response variable:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .)\n\n\nCall:\nlm(formula = x ~ y, data = .)\n\nCoefficients:\n(Intercept)            y  \n    -1.8684       0.4646  \n\n\nThese coefficients do not appear in the dag01 tilde expressions."
  },
  {
    "objectID": "LC/LC-lesson21.html#objective-21.2",
    "href": "LC/LC-lesson21.html#objective-21.2",
    "title": "Learning Checks Lesson 21",
    "section": "21.5 (Objective 21.2)",
    "text": "21.5 (Objective 21.2)\nYou are trying to understand why automobile fuel economy varies from model to model. Using the mtcars data frame (documentation at help(\"mtcars\")) …\n\nWhat’s an appropriate choice of a response variable?\nPick two explanatory variables of interest to you. Build an appropriate model from the data, extracting the coefficients() of the model. Explain what the coefficients mean in everyday terms that your cylinder-head uncle would approve of.\nWhich of the other variables are covariates? iv.Pick a covariate that your intuition suggests would be important. Include that covariate in the model from (ii) and say whether the covariate shows up as important in the model coefficients.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nAll of the other variables are covariates. A covariate is merely a potential explanatory variable that you are not directly interested in. Of course, not all covariates play an important role in the system."
  },
  {
    "objectID": "LC/LC-lesson21.html#obj-21.3",
    "href": "LC/LC-lesson21.html#obj-21.3",
    "title": "Learning Checks Lesson 21",
    "section": "21.6 (Obj 21.3)",
    "text": "21.6 (Obj 21.3)\nGenerate a sample of size \\(n=100\\) from dag03. Use the data to construct a model of y versus x. But instead of using coefficients() to look at the model coefficients, use confint(). While coefficients() reports a single value for each coefficient, confint() reports a plausible interval for coefficients that is consistent with the data.\n\nFor \\(n=100\\), how wide is the interval reported by confint() on the x coefficient.\nRepeat the process of sampling and modeling, but this time use \\(n=400\\). How wide is the interval reported by confint() on the x coefficient.\nAgain repeat the process of sampling and modeling, this time using \\(n=1600\\). iv. Does the interval reported depend systematically on the size \\(n\\) of the sample? Describe what pattern you see."
  },
  {
    "objectID": "LC/LC-lesson21.html#section-4",
    "href": "LC/LC-lesson21.html#section-4",
    "title": "Learning Checks Lesson 21",
    "section": "21.7",
    "text": "21.7\nA short report from the British Broadcasting Company (BBC) was headlined “Millennials’ pay ‘scarred’ by the 2008 banking crisis.”\n\nPay for workers in their 30s is still 7% below the level at which it peaked before the 2008 banking crisis, research has suggested. The Resolution Foundation think tank said people who were in their 20s at the height of the recession a decade ago were worst hit by the pay squeeze. It suggested the crisis had a lasting “scarring” effect on their earnings.\n\n\nThe foundation said people in their 30s who wanted to earn more should move to a different employer. The research found those who stayed in the same job in 2018 had real wage growth of 0.5%, whereas those who found a different employer saw an average increase of 4.5%.\n\nThe phrase “should move” in the second paragraph of the report suggests causation. A corresponding DAG would look like this:\n\nmove_to_new_employer \\(\\rightarrow\\) higher_wage\n\nFor the sake of discussion, let’s add two more variables to the DAG:\n\neffectiveness—standing for how productive the employee is.\nqualifications—standing for whether the employee is a good candidate attractive to potential new employers.\n\nConstruct a DAG with all four variables that represent plausible causal connections between them. The DAG should not contain a direct causal connection between move_to_new_employer and higher_wage.\nIf the world worked this way, would it necessarily be good advice to switch to a new employer with the aim of earning a higher wage?"
  },
  {
    "objectID": "LC/LC-lesson25.html",
    "href": "LC/LC-lesson25.html",
    "title": "Learning Checks Lesson 25",
    "section": "",
    "text": "Just while in draft\n\n\n\n25.1 Given a data frame, construct a predictor function for a specified response variable.\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error. Relate this to a predition interval.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "LC/LC-lesson25.html#section",
    "href": "LC/LC-lesson25.html#section",
    "title": "Learning Checks Lesson 25",
    "section": "25.2",
    "text": "25.2\nThe data frame moderndive::house_prices lists the sales prices of 21,613 houses in King County, Washington (which includes Seattle) sold from May 2014 and May 2015. Often, with price or income data, economists work with the logarithm of the price or income or income-related quantity such as house living area. We are going to do here, but this problem is not about logarithms, so once you create the “logged” data frame, you’ll just be modeling the data using the usual methods.\nTo create the “logged” data, add these two new variables to the data frame, which we will call Seattle.\n\nSeattle <- moderndive::house_prices %>% \n  mutate(log_price = log10(price),\n         log_area = log10(sqft_living))\n\n\nBuild a model of log_price ~ log_area using the Seattle data. Store the model under the name pmod.\n\n\n\n\n\n\n\nSolution\n\n\n\n\npmod <- lm(log_price ~ log_area, data = Seattle)\n\n\n\n\nImagine that you are moving to Seattle in August 2014. Housing is expensive in the Seattle area, so you might decide to live in a small house, say 750 square feet. The log_area of such a house is 2.87. Using mod_eval(), predict the log_price of such a house. (Note that mod_eval() puts the model output in a column named model_output, not log_price.) In your Rmd file, give the command and show the output. If you’re curious about what the predicted price is in dollars (rather than log-dollars), simply raise 10 to the log-dollar amount. For instance, if the model_output were 5, the dollar amount will be \\(10^5 = \\$100,000\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 2.87)\n\n  log_area model_output\n1     2.87     5.324298\n\n\nIn dollar terms, the predicted price is 105.324 = 2.1086281^{5}.\n\n\n\nRepeat (2), but for a house with lots of space: 1500 square feet. The log_area of such a house is 3.18. As in (2), give the command and show the output in your Rmd file.\nYour budget is $200,000. In log dollars this budget is log10(200000) = 5.3. The predicted price of a 750 square-foot house is somewhat beyond your budget. But you figure that some 750-square foot house will be within your budget. To see if this is likely, look at the prediction interval of the house price. You can do this by adding the interval=\"prediction\" to the mod_eval() command. Is your budget (5.3 log dollars) within the prediction interval? Show your command and the result in your Rmd file and also give a sentence stating your conclusion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 2.87, interval=\"prediction\")\n\n  log_area model_output    lower   upper\n1     2.87     5.324298 4.993416 5.65518\n\n\nYeah! Your budget of 5.3 log dollars is near the center of the prediction interval.\n\n\n\nOn a hunch, you decide to see whether you might find a 1500 square foot (log_area = 3.18) might also fall within the prediction interval. Will it? Show your command, the result, and a sentence interpreting the result.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 3.18, interval=\"prediction\")\n\n  log_area model_output    lower    upper\n1     3.18     5.583697 5.252851 5.914543\n\n\nStrictly speaking, your budget (5.3 log dollars) is within the prediction interval. But it is very close to the lower bound of 5.25 log dollars. So there will likely be few houses of 1500 square feet within your budget. So plan that the house you will end up purchasing will be somewhere in the range 750-1500 square feet."
  },
  {
    "objectID": "LC/LC-lesson25.html#xx",
    "href": "LC/LC-lesson25.html#xx",
    "title": "Learning Checks Lesson 25",
    "section": "25.XX",
    "text": "25.XX\nYou are a bus dispatcher in New York City. The Department of Education bus logistics office has called to say that a school bus has broken down and the students need to be offloaded onto a functioning bus to take them to school. Unfortunately, the DOE officer didn’t tell you how many students are on the bus. You need to make a quick prediction in order to decide what kind and how many busses you will need for the pickup.\nYou go to the NYC OpenData site bus breakdown page to get the historical data on how many students are on the bus. There are more than 200,000 bus events listed, each one of them including the number of students. You make a jitter/violin plot of the number of students on each of the 200,000 busses.\n\n\n\n\n\n\nThe violin plot looks like an upside-down T. Explain what’s going on. (Hint: How many students fit on a school bus?) -A- As very often happens, the data file contains data-entry or other mistakes producing outliers. Almost all of the 200,000 bus incidents fall into the horizontal line near zero. There are only 164 with a number above 100 students. In the US, the legal maximum capacity for a school bus is 72 students.\n\nOne of the ways of handling outliers is to delete them from the data. A softer way is to trim the outliers, giving them a value that is distinct but not so far from the mass of values. The figure below shows a violin plot where any record where the number of students is greater than 20 is trimmed to 21.\n\n\n\n\n\n\nIf you sent a small school bus (capacity 14), what fraction of the time would you be able to handle all the students on the school bus? -A- Only about 5% of the area of the violin plot is above 14.\nIf you sent one 14-passenger school bus with another on stand-by (just in case the first bus doesn’t have sufficient capacity), what fraction of the time could you handle all the students?\n\n-A- It’s tempting to say that the 2 x 14 = 28 passenger capacity could handle all the cases, but remember, the cases at 21 stand for “21 or more passengers.” We can’t tell from the violin plot how many of those have more than 28 students on board.\n\nNotice that the violin plot is jagged. Explain why. -A- The number of passengers is an integer, e.g. 1, 2, 3, …. It can’t be a number like 4.5."
  },
  {
    "objectID": "LC/LC-lesson25.html#yyy",
    "href": "LC/LC-lesson25.html#yyy",
    "title": "Learning Checks Lesson 25",
    "section": "25.YYY",
    "text": "25.YYY\nAt a very large ballroom dance class, you are to be teamed up with a randomly selected partner. There are 200 potential partners. The figure below shows their heights.\nFrom the data plotted, calculate a 95% prediction interval on the height of your eventual partner. (Hint: You can do this by counting.)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n59 to 74 inches.\nSince there are 200 points, a 95% interval should exclude the top five cases and the bottom five cases. So draw the bottom boundary of the interval just above the bottom five points, and the top boundary just below the top five points."
  },
  {
    "objectID": "LC/LC-lesson31.html",
    "href": "LC/LC-lesson31.html",
    "title": "Learning Checks Lesson 31",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson19.html",
    "href": "LC/LC-lesson19.html",
    "title": "Learning Checks Lesson 19",
    "section": "",
    "text": "The math300 package will be needed for lessons 20 through 39.\n\nlibrary(math300)\nlibrary(moderndive)\nlibrary(NHANES)\n\n\nOne of these pipeline commands will work and the other won’t. Which one will work? Explain why the other one doesn’t work.\n\nlm(net ~ age, data = TenMileRace)\nTenMileRace %>% lm(net ~ age)\n\nAn example from the OpenIntro book uses data on promotions. Some data wrangling commands that might be relevant are these:\n\npromotions %>% tally()\n\n# A tibble: 1 × 1\n      n\n  <int>\n1    48\n\npromotions %>% group_by(decision) %>% tally()\n\n# A tibble: 2 × 2\n  decision     n\n  <fct>    <int>\n1 not         13\n2 promoted    35\n\npromotions %>% group_by(gender) %>% tally()\n\n# A tibble: 2 × 2\n  gender     n\n  <fct>  <int>\n1 male      24\n2 female    24\n\npromotions %>% group_by(gender, decision) %>% tally()\n\n# A tibble: 4 × 3\n# Groups:   gender [2]\n  gender decision     n\n  <fct>  <fct>    <int>\n1 male   not          3\n2 male   promoted    21\n3 female not         10\n4 female promoted    14\n\n\n\nYou could use such wrangling to compare groups. For instance, you can use the results of the last command to calculate separately the proportion of males who were promoted and, similarly, the proportion of females.\na. What are those proportions?\nThe following wrangling command will calculate the proportions for you, but it is a bit complicated:\n\npromotions %>%\n  group_by(gender) %>%\n  summarize(prop_promoted = sum(decision==\"promoted\") / n())\n\nb. Use the above command to check your calculations in (a).\nc. In the regression paradigm, the comparison of proportions between the two groups is done directly in lm(), like this:\n\npromotions %>%\n  mutate(promoted = zero_one(decision, one=\"promoted\")) %>%\n  lm(promoted ~ gender, data = .) %>%\n  coefficients()\n\n (Intercept) genderfemale \n   0.8750000   -0.2916667 \n\n\nWe’ll explain the purpose of zero_one() in Lesson 19, but putting that matter aside for a moment, compare the two coefficients in the regression model to the proportion results you got from wrangling.\n\nWhat does the value of the intercept coefficient correspond to in the wrangling results?\nWhat does the genderfemale coefficient correspond to in the wrangling results? (Hint: you will have to do a bit of arithmetic on the wrangling results.)"
  },
  {
    "objectID": "LC/LC-lesson19.html#section",
    "href": "LC/LC-lesson19.html#section",
    "title": "Learning Checks Lesson 19",
    "section": "19.1",
    "text": "19.1\nConsider the moderndive::evals data that records students’ evaluations (score, on a 1-5 scale) of the professors in each of several courses (the course ID), as well as the age, “average beauty rating” (bty_avg) of the professor, enrollment in the course (cls_students) and the level o the course (cls_level). Each row in the data frame is an individual course section.\n\n\n\n\n\nID\nscore\nage\nbty_avg\ncls_students\ncls_level\n\n\n\n\n329\n2.7\n64\n2.333\n22\nupper\n\n\n313\n4.2\n42\n2.667\n86\nupper\n\n\n430\n4.5\n33\n5.833\n120\nlower\n\n\n95\n4.2\n48\n4.333\n33\nupper\n\n\n209\n4.8\n60\n3.667\n34\nupper\n\n\n442\n3.6\n61\n3.333\n39\nlower\n\n\n351\n4.6\n50\n3.333\n26\nlower\n\n\n317\n3.7\n52\n6.500\n44\nupper\n\n\n444\n4.1\n52\n4.500\n111\nlower\n\n\n315\n3.8\n52\n6.000\n88\nupper\n\n\n\n\n\nThe following commands model score versus age and plots the data as a point plot.\n\nlm(score ~ age, data = moderndive::evals) %>% coefficients()\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\nopenintro::evals %>% gf_point(score ~ age, alpha=0.2 )\n\n\n\n\n\nExplain why some of the dots are darker than others?\n\n\n\n\n\n\n\nSolution\n\n\n\nAll the ages have integer values—e.g., 43, 44, 45—so the dots line up in vertical lines.\nSimilarly, the scores have values only to one decimal place—e.g., 3.1, 3.2, 3.3—so the dots line up in horizontal lines. If there are two or more rows in evals that have the same age and score, the dots will be plotted over one another. Since transparency (alpha = 0.2) is being used, points where there is a lot of overplotting will appear darker.\n\n\n\nRemake the plot, but using gf_jitter() instead of gf_point(). Explain what’s different about the jittered plot. (Hint: Almost all of the dots are the same lightness.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ age, alpha=0.2 )\n\n\n\n\n“Jittering” means to shift each dot by a small random amount. This reduces the number of instances where dots are overplotted.\n\n\n\nNow make a jitter plot of score versus class level (cls_level).\n\nWhat do the tick-mark labels on the horizontal axis describe? Are they numerical?\nTo judge from the plot, are their more lower-level than upper-level courses? Explain briefly what graphical feature lets you answer this question at a glance.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ cls_level)\n\n\n\n\n\nThe tick-mark labels are the levels of the categorical variable cls_level. The are words, not numbers.\nThere are many more dots in the right column than in the left. Since lower level class are shown in the left column, there are fewer lower-level courses than upper-level courses.\n\n\n\n\nThe two columns of points in the plot you made in (3) are not separated by very much empty space. You can fix this by giving gf_jitter() an argument width=0.2. Try different numerical values for width and report which one you find most effective at making the two columns clearly separated while avoiding overplotting.\nAre the scores, on average, different for the lower- vs upper-level classes? It’s hard to get more than a rough idea of the distribution of scores by looking at the “density” of points. The reason is that the number of points differs in the two columns. But there is an easy fix: add a layer to the graphic that shows the distribution (more or less like a histogram displays a distribution of values). You can do this by piping the jitter plot layer into a geom called a “violin,” like this:\n\n\nopenintro::evals %>% \n  gf_jitter(score ~ cls_level) %>%\n  gf_violin(fill=\"blue\", alpha=0.2, color=NA)\n\n\n\n\nExplain how to read the violins."
  },
  {
    "objectID": "LC/LC-lesson19.html#section-1",
    "href": "LC/LC-lesson19.html#section-1",
    "title": "Learning Checks Lesson 19",
    "section": "19.2",
    "text": "19.2\nThe openintro::promotions data comes the the 1970s and records the gender of 38 people along with the result of a decision to promote (or not) the person. =\nChapter 2 of ModernDive suggests graphically depicting decision versus gender by using a bar plot. There are two ways to make the bar plot, depending on which variable you assign to the horizontal axis and which to the fill color.\n\npromotions %>% gf_bar(~ decision, fill=~ gender)\npromotions %>% gf_bar(~ gender, fill=~decision)\n\n\n\n\nFigure 1: Two different ways to plot promotion outcome and gender\n\n\n\n\n\n\n\nFigure 2: Two different ways to plot promotion outcome and gender\n\n\n\n\nPlots like those in ?@fig-promotion-bars might be attractive or not, depending on your taste. What they don’t accomplish is to make sure which is the response variable and which the explanatory variable.\nThe choice of response and explanatory variables depends, of course, on what you are trying to display. But everyday English gives a big hint. For instance, you might describe the question at hand as, “Does gender affect promotion decisions.” Here, the variable doing the affecting is gender, and the outcome is the decision.\nModeling decision as a function of gender is easy once you convert the response variable to a zero-one variable. Like this:\n\nmod <- lm(zero_one(decision, one=\"promoted\") ~ gender, data = promotions)\ncoefficients(mod)\n\n (Intercept) genderfemale \n   0.8750000   -0.2916667 \n\nmosaicModel::mod_eval(mod)\n\n  gender model_output\n1   male    0.8750000\n2 female    0.5833333\n\n\n\nExplain what is the relationship between the model coefficients and the model outputs.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe coefficients tell how to calculate the model output. These coefficients say that the model output will be 0.875, but subtract 0.292 if the person is female.\nThe model outputs give the probability of being promoted for each of the two genders.\n\n\n\nMake this plot and explain what the red lines show. (We don’t expect you to be able to write the command to generate such plots on your own, but we do expect you to be able to interpret them.)\n\n\npromotions %>% \n  gf_jitter(zero_one(decision) ~ gender, height=0.2, width=0.2) %>%\n  gf_errorbar(model_output + model_output ~ gender, data=mod_eval(mod), \n              color=\"red\", inherit=FALSE) %>%\n  label_zero_one()\n\n\n\n\n\n\n\nSolution\n\n\n\nThe red lines show the proportion of the people in each gender group who were promoted. The y-axis scale on the left refers to the zero-one encoding of decision, while the y-axis labels on the right make it easier to read off the numerical value of the proportion."
  },
  {
    "objectID": "LC/LC-lesson19.html#section-2",
    "href": "LC/LC-lesson19.html#section-2",
    "title": "Learning Checks Lesson 19",
    "section": "19.3",
    "text": "19.3\nThe mosaicData::Whickham data from comes from a survey of a thousand or so nurses in the UK in the 1970s. The data record the age of each nurse along with whether the nurse was still alive in a follow-up survey 20 years later (outcome).\nMake this graph from the Whickham data:\n\ngf_jitter(zero_one(outcome) ~ age, data = Whickham, alpha=0.3, height=0.1) %>% \n  label_zero_one() \n\n\n\n\n\nExplain in everyday language what the graph shows about the lives of humans.\nMake the graph again, but leave out the %>% label_zero_one(). Then explain what label_zero_one() does.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nThe graph shows that young nurses tended to be alive at the 20-year follow-up, older nurses not so much.\n%>% label_zero_one() adds an axis on the left of the graph showing that in the zero-one tranform of outcome, “Alive” is assigned the value 1 and “Dead” the value 0.\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson19.html#section-3",
    "href": "LC/LC-lesson19.html#section-3",
    "title": "Learning Checks Lesson 19",
    "section": "19.4",
    "text": "19.4\nAbout the summarization of models. Pipe the model fit into any of four functions:\n\n%>% coefficients()\n%>% regression_summary()\n%>% rsquared()\n%>% confint()\n\nREDO confint() so that the columns are named lower, middle, upper\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson19.html#obj.-19.3",
    "href": "LC/LC-lesson19.html#obj.-19.3",
    "title": "Learning Checks Lesson 19",
    "section": "19.5 (Obj. 19.3)",
    "text": "19.5 (Obj. 19.3)\nCalculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We’ll use the CO2city variable in the math300::MPG data frame. The basic calculation using the mosaic package is:\n\ndf_stats( ~ CO2city, data = math300::MPG, coverage(0.95))\n\n  response   lower   upper\n1  CO2city 276.475 684.525\n\n\nThe following figure shows a violin plot of CO2city which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.\n\n50% coverage interval -A- (c)\n75% coverage interval -A- (e)\n90% coverage interval -A- (g)\n100% coverage interval -A- (i). This extends from the min to the max, so you could have figured this out just from the figure."
  },
  {
    "objectID": "LC/LC-lesson19.html#obj-19.3",
    "href": "LC/LC-lesson19.html#obj-19.3",
    "title": "Learning Checks Lesson 19",
    "section": "19.6 (Obj 19.3)",
    "text": "19.6 (Obj 19.3)\nThe two jitter + violin graphs below show the distribution of two different variables, X and Y. Which variable has more variability?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is about the same level of variability in variable A and variable B. This surprises some people. Remember, the amount of variability has to do with the spread of values of the variable. In variable B, those values are have a 95% prediction interval of about 30 to 65, about the same as for variable A. There are two things about plot (b) that suggest to many people that there is more variability in variable B.\n\nThe larger horizontal spread of the dots. Note that variable B is shown along the vertical axis. The horizontal spread imposed by jittering is completely arbitrary: the only values that count are on the y axis.\n\nThe scalloped, irregular edges of the violin plot.\n\nOn the other hand, some people look at the clustering of the data points in graph (b) into several discrete values, creating empty spaces in between. To them, this clustering implies less variability. And, in a way, it does. But the statistical meaning of variability has to do with the overall spread of the points, not whether they are restricted to discrete values."
  },
  {
    "objectID": "LC/LC-lesson19.html#objs.-19.3-19.4",
    "href": "LC/LC-lesson19.html#objs.-19.3-19.4",
    "title": "Learning Checks Lesson 19",
    "section": "19.7 (Objs. 19.3 & 19.4)",
    "text": "19.7 (Objs. 19.3 & 19.4)\nThe graphs below show a violin plot of body mass index (BMI) for adults and children. One of the graphs shows a correct 95% coverage interval on BMI, the other does not.\nIdentify the incorrect graph and say what feature of the graph led to your answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nGraph (b) is correct. In graph (a), you can see that the interval fails to include a lot of the low BMI children and extends too high. For adults, the graph (a) interval extends too far low and doesn’t go high enough."
  },
  {
    "objectID": "LC/LC-lesson19.html#e",
    "href": "LC/LC-lesson19.html#e",
    "title": "Learning Checks Lesson 19",
    "section": "19.E",
    "text": "19.E\nThere are two equivalent formats describing an interval numerically that are widely used:\n\nSpecify the lower and upper endpoints of the interval, e.g. 7 to 13.\nSpecify the center and half-width of the interval, e.g. 10 ± 3, which is just the same as 7 to 13.\n\nComplete the following table to show the equivalences between the two notations.\n\n\n\n\n\nInterval\nbottom-to-top\nplus-or-minus\n\n\n\n\n(a)\n3 to 11\n\n\n\n(b)\n\n108 ± 10\n\n\n(c)\n\n30 ± 1\n\n\n(d)\n97 to 100\n\n\n\n(e)\n-4 to 16\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n7 ± 4\n98 to 118\n29 to 31\n98.5 ± 1.5\n6 ± 10\n\nIt’s a matter of judgement which format to use. The bottom-to-top notation highlights the range of the interval while the plus-or-minus notation emphasizes the center of the interval. As a rule of thumb, I suggest this:\n\nIf the first two digits are different between the top and bottom of the interval, use the bottom-to-top notation. So, write 387 to 393. If the first two digits are the same, use plus-or-minus. For instancer, the ratio of the mass of the Earth to that of the Moon is 81.3005678 ± 0.0000027. This is easier to take in at a glance than the equivalent 81.3005651 - 81.3005708"
  },
  {
    "objectID": "LC/LC-lesson19.html#f",
    "href": "LC/LC-lesson19.html#f",
    "title": "Learning Checks Lesson 19",
    "section": "19.F",
    "text": "19.F\n\n\n\n\n\n\nStill in draft\n\n\n\nSuppose there are other explanatory variables to be displayed. In that case, we will use color and faceting. If there are no explanatory variables, as in y ~ 1, we will jitter the data horizontally to avoid overplotting.]"
  },
  {
    "objectID": "LC/LC-lesson30.html",
    "href": "LC/LC-lesson30.html",
    "title": "Learning Checks Lesson 30",
    "section": "",
    "text": "Is horsepower the cause?\n\n\n\nIt might seem from the negative sign on the effect size of engine horsepower on fuel economy that a more powerful engine is not as efficient than a less powerful engine at moving the car a given number of miles. That’s a reasonable conclusion. But the statistical thinker always keeps in mind other possibilities. For instance, another factor in fuel economy is the overall weight of the vehicle. A van designed to haul many passengers weighs more than a 2-passenger sporty vehicle. The van needs more horsepower because it is accelerating more weight.\n\n\n\nFigure 1 shows four DAGs, each of which describe a plausible scenario.\n\n\n\n\n\n\n\n\n(a) Dag A\n\n\n\n\n\n\n\n(b) Dag B\n\n\n\n\n\n\n\n(c) Dag C\n\n\n\n\n\n\n\n(d) Dag D\n\n\n\n\nFigure 1: Scenarios for the relationship between automobile fuel economy, weight, and horsepower\n\n\n\nIn DAG A, the vehicle’s design weight determines that an engine with high horsepower will be part of the design. The weight is also responsible for the lower fuel economy.\nThe other DAGs describe other scenarios. In DAG C, for instance, the car designers decided to build a muscle car and put in a big engine. The engine itself adds to the vehicle’s weight, and the higher weight determines lower miles per gallon. DAG D expresses a slightly different belief: again the choice to build a muscle car (high hp) influences the weight. But in DAG B, the big engine also directly influences the fuel economy, perhaps because the fuel-to-air ratio of the car, in normal use, is not optimal.\nAs we will see in Lesson 28, to reveal the direct causal link between engine power and fuel economy requires different choices for the model formula depending on which DAG you think might be relevant.\nTo illustrate, let’s consider prices of houses as recorded in the mosaicData::SaratogaHouses data frame, based on house sales in Saratoga County, NY, USA in 2006. We’ll follow a question asked by then-student Candice Corvetti in her Stat 101 class at Williams College: “How much is a fireplace worth?” Response variable: price. Explanatory variable: fireplaces. Since a handful of the houses has multiple fireplaces, we will simplify by filtering out those houses to retain only the ones with a single fireplace or none.\nFrom the graphic, you can see that houses with a fireplace tend to have higher prices. From the report of the evaluated model, you can calculate the effect size: $235K for a house with a fireplace, $175K for a house without one. This suggests the value of a fireplace is $60K.\nThere are, of course, many other things that determine the price of a house. Real-estate agents famously list the three most important factors as “location, location, and location.” Common sense brings in other explanatory variables: how big the house is, how luxurious, how many bathrooms, and so on. The statistical thinker knows to put any one explanatory variable into the context of other plausable factors.\nFor simplicity, let’s collect all the factors other than fireplaces into a hypothetical variable which we will call “fancy.” Here are three plausible DAGs that plausibly describe an affect of fireplace on price in the context of fancy.\nIn DAG A, fancy and fireplace both contribute to price, but independently. In DAG B, fireplace directly contributes to price, but whether or not a house has a fireplace depends on the level of fancy. In DAG D, fireplace has no direct affect on price, which is set entirely by fancy. The fireplace variable is just an indicator of fancy.\nWe can’t say from the data alone which of these three DAGs is the closest description of the situation. In Lessons 28, 30, and 31 we will consider how the choice of explanatory variables in a model leads to a faithful or misleading picture of the connections. There you will find out that DAGS A & B both imply that fancy should be an explanatory variable if we want the effect size from the model to represent the direct effect of a fireplace on price. Easy enough to fit that model, … except that we don’t have an actual variable fancy in the SaratogaHouses data frame. To keep things simple for the moment, we will use livingArea—the size of the house—as a rough approximation to the hypothetical fancy.\nThe effect size of fireplaces on price is found by comparing the model output for houses with and without a fireplace, holding the values of all the other explanatory variables constant.\nFor a house with living area 2000 feet2, the model output is $235K with no fireplace and $240K with a fireplace, putting the effect size of fireplace on price at $5K. That’s much smaller than the previous model, price ~ fireplace, gave for the effect size. The reason for the difference in results from the two models is that houses with fireplaces tend to be larger in area.\nFor models that are constructed by adding together different terms, like the price ~ fireplaces + livingArea model of the previous example, the estimated effect size for a given term is the corresponding model coefficient. The confidence interval on that effect size is simply the confidence interval on the coefficient. For example, for fireplaces:\nThus, the confidence interval for the effect of a fireplace ranges from negative $1500 to positive $13,000. Broad though this may seem at first, it does carry genuine information. You can be confident that a fireplace alone will not add as much as $50,000 to the price of the house, nor will it cause the house’s value to fall by $10,000.\nThe confidence interval on the livingArea is pretty narrow $103 to $115 per square foot. If you’re looking to save a bit of money by shopping for a slighly smaller house, say 200 square-feet smaller, you can adjust your budget downwards by something in the range of $206,000 to $230,000. The units here come from multiplying the area units (square feet) by the effect size units (dollars per square feet), producing a quantity denominated in dollars.\nIt’s important always to keep in mind that an estimate of an effect size will likely be misleading if your choice of model seriously misrepresents reality. For instance, a salesperson hawking add-on fireplaces might show you results from the “obvious” model price ~ fireplace, leading to an effect size of $52,000 to $69,000, calculated this way.\nIt would be unfair to say that the $52,000 to $69,000 claim is a lie; it’s entirely consistent with the data. But it relies on a grossly implausible description of the factors that determine house price."
  },
  {
    "objectID": "LC/LC-lesson30.html#section",
    "href": "LC/LC-lesson30.html#section",
    "title": "Learning Checks Lesson 30",
    "section": "30.1",
    "text": "30.1\nDags with longer confounding pathways. Is there mixing when leaving out an element in the pathway. Mix up the directions of the arrows and show that the mixing occurs when the covariate is included in the model.\nRegression to the mean example.\nCollider?\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson24.html",
    "href": "LC/LC-lesson24.html",
    "title": "Learning Checks Lesson 24",
    "section": "",
    "text": "Just to help when writing problems\n\n\n\n24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation."
  },
  {
    "objectID": "LC/LC-lesson24.html#objective-24.1",
    "href": "LC/LC-lesson24.html#objective-24.1",
    "title": "Learning Checks Lesson 24",
    "section": "24.1 (Objective 24.1)",
    "text": "24.1 (Objective 24.1)\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\nSolution\n\nPrediction and (2) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price."
  },
  {
    "objectID": "LC/LC-lesson24.html#objective-24.1-1",
    "href": "LC/LC-lesson24.html#objective-24.1-1",
    "title": "Learning Checks Lesson 24",
    "section": "24.2 (Objective 24.1)",
    "text": "24.2 (Objective 24.1)\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\nSolution\n\nPrediction\nRelationship\nRelationship\n\n\nSOME IDEAS FOR EXERCISE MODES\n\nUse mod_plot() and look at the slope of lines and offsets. Compare to the model coefficients.\nGenerate data from a DAG and look at the confidence interval on the effect size. Then make new samples and see if the effect size in those samples is consistent with the confidence interval.\nIn text, maybe look at the confidence intervals across new samples and show that they tend to overlap. Only a few of them don’t touch a common line. This is basically just a review of confidence intervals, but why not?\nInteraction. Show that when there is an interaction term, the effect size (as calculated by mod_effect()) is not constant, as it is for models with purely linear terms."
  },
  {
    "objectID": "LC/LC-lesson24.html#lc-24.1",
    "href": "LC/LC-lesson24.html#lc-24.1",
    "title": "Learning Checks Lesson 24",
    "section": "LC 24.1",
    "text": "LC 24.1\nThe Computational Probability and Statistics text describes an early study on human-to-human heart transplantation:\n\n“The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. [[Not in data set: Another variable called [MISSING] was used to indicate whether or not the patient was alive at the end of the study.]]”\n\nThe data frame is called Transplants. [NEED TO MOVE TO PACKAGE]\n\n\n\nYou’re going to build a model of outcome vs group based on the data in Transplants. The outcome variable has levels \"Dead\" and \"Alive\", that is, it is a two-level categorical variable. Consequently, the model output will be the probability that the transplant candidate was alive at the end of the study.\n\nBuild a model outcome == \"Alive\" ~ group from the Transplants data. Pay close attention to the left-hand side of the tilde expression: it is a calculation that produces a 1 if outcome is \"Alive\" and zero otherwise. Notice the double equal signs and the quotes around \"Alive\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod <- lm(outcome == \"Alive\" ~ group, data = Transplants)\n\n\n\n\nThe sole explanatory variable here, group also is categorical. It has levels \"Control\" and \"Treatment\".\n\nUsing eval_mod(), find the probability of being alive at the end of the study for the Control group and for the treatment group.\nThe two probabilities in (ii) do not add up to zero. Explain why.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(mod, group=\"Treatment\")\n\n      group model_output\n1 Treatment    0.3478261\n\nmod_eval(mod, group=\"Control\")\n\n    group model_output\n1 Control    0.1176471\n\n\n\n\n\nFind the effect size of the treatment. All you need is your results from (2)?\nUse mod_effect(modelname, ~ group) to calculate the effect size.\n\nIs the result consistent with what you found in (3).\nExplain in everyday language what this effect size means.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_effect(mod, ~ group)\n\n     change     group to_group\n1 -0.230179 Treatment  Control"
  },
  {
    "objectID": "LC/LC-lesson24.html#section",
    "href": "LC/LC-lesson24.html#section",
    "title": "Learning Checks Lesson 24",
    "section": "24.2",
    "text": "24.2\nEffect sizes generally come with units and you have to take into account the units in order to know if the effect is important or not.\n\n\n\n\n\n\nIn draft\n\n\n\nMove the Loans data into the math300 package. But for now …\n\nLoans <- readr::read_csv(\"data/loans.csv\")\n\n\n\nA case in point is provided by the Loans data frame, which records dozens of variables on each of 10,000 loans made through the Lending Club. The interest rate at which the loans are made varies substantially from loan to loan. Presumably, higher interest rates reflect a higher perception of risk of default (which would lead to the lender losing his or her money).\nHere’s a model of the interest rate. (This is for the borrowers who have a low debt-to-income percent; we won’t worry about the few very high debt-to-income cases.) We’re only interested in this problem with the effect size, which is the same as the coefficients on the model.\n\nmod <- lm(interest_rate ~ homeownership + debt_to_income + \n            account_never_delinq_percent + verified_income, \n          data = Loans %>% filter(debt_to_income<50))\nmod %>% confint()\n\n                                     2.5 %      97.5 %\n(Intercept)                    15.31467948 17.26502804\nhomeownershipOWN                0.16056412  0.73009641\nhomeownershipRENT               1.01571903  1.41695574\ndebt_to_income                  0.09565833  0.11523598\naccount_never_delinq_percent   -0.09149666 -0.07122369\nverified_incomeSource Verified  1.37798985  1.79909554\nverified_incomeVerified         2.88724295  3.39001806\n\n\nThere are two quantitative explanatory variables—debt_to_income and account_never_delinq_percent—both of which are measured in percent.\nThere are two categorical explanatory variables: homeownership and verified_income. The levels for homeownership are “MORTGAGE” (meaning money is still owed on the house), “OWN” (without a mortgage), and “RENT” (meaning the borrower rents rather than owning a home). The levels for verified_income are “Not Verified”, “Verified”, “Source Verified”.\nFor the categorical explanatory variables (and the intercept) the effect-size units are “percent interest.” For the quantitative explanatory variables, the effect-size units are “percent interest per percent,” so that when multiplied by the debt_to_income percent or the account_never_delinq_percent the result will be in “percent interest.”\n\nAccording to the model, who pays the higher interest rate (on average): people who OWN their home, people who RENT, or people who have a mortgage on their home? How much higher than the lowest-interest rate category.\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople who rent pay the highest interest rate, a little more than 1 percentage point higher than people who have mortgages. It’s interesting that people who own their homes outright pay (on average) pay about 0.45 percentage points more than people who own outright. This might be because having a mortgage means you also have a credit history.\n\n\n\nAccording to the model, who pays the higher interest rate (on average): people whose income is “not verified,” people whose income is “verified,” or people who have the source of income verified (level: “source verified”)?\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople whose income is verified pay about 3 percentage points higher interest than people whose income is “not verified.” This seems surprising, but it may be that people who have higher perceived default risk are also the people who are asked to verify their income. Things get complicated when explanatory variables are linked to each other.\n\n\n\nThe coefficients on debt_to_income and account_never_delinq_percent are the smallest numerically. Does this mean that the effects of debt_to_income and account_never_delinq_percent are smaller than the other two explanatory variables in the model? Explain why or why not. (Hint: Look at the distribution of debt_to_income and account_never_delinq_percent to get an idea for the range of values these variables take on.)"
  },
  {
    "objectID": "LC/LC-lesson24.html#solution-7",
    "href": "LC/LC-lesson24.html#solution-7",
    "title": "Learning Checks Lesson 24",
    "section": "Solution",
    "text": "Solution\ndebt_to_income varies over about 25 percentage points. The variation in account_never_delinq_percent is about the same, varying from about 80 to 100 percentage points. The effect of the variables (in percent interest) is determined by multiplying the coefficients by the amount of variation in the variables. So, from one extreme to the other, the effect of debt_to_income is about 2 perentage points of interest, and roughly the same for debt_to_income."
  },
  {
    "objectID": "LC/LC-lesson24.html#section-1",
    "href": "LC/LC-lesson24.html#section-1",
    "title": "Learning Checks Lesson 24",
    "section": "24.4",
    "text": "24.4\nThe logic of effect size is to investigate the change in output of a model when one input variable is changed, holding all other things constant. This problem is about the extent to which we mean “all”.\nThe figure shows yearly CO2 production of individual gasoline-fueled passenger vehicles stratified by the number of engine 4 or 6 cylinders.\n\n\n\n\n\n\nThe effect size is the difference between the output variable when a change is made to an input. Consider the effect size of changing from a six-cylinder engine to a four-cylinder engine. Here’s a subtly incorrect way of calculating something like an effect size: Pick a dot from the six-cylinder group and another dot from the four-cylinder group. Subtract the 4-cylinder point’s CO2 value from the six-cylinder point’s CO2 value, and divide by the change in the input, that is, -2 cylinders.\n\nFollow this procedure for the top-most dot in each cloud and calculate the effect size. -A- (5100 kg - 4100 kg) / (-2 cylinders) = -500 kg/cylinder.\nFollow the procedure for the bottom-most dot in each cloud and calculate the effect size. -A- (3400 kg - 2500 kg) / (-2 cylinders) = -450 kg/cylinder.\nFollow the procedure for the bottom-most dot in the four-cylinder cloud and the top-most dot in the six-cylinder cloud. -A- (5100 kg - 3400 kg) / (-2 cylinders) = -850 kg/cylinder.\nDo the same as in (c) but use the bottom-most six-cylinder dot and the top-most four-cylinder dot. -A- (3400 kg - 4100 kg) / (-2 cylinders) = +350 kg/cylinder. In other words, switching from the six\n\nYou can imagine the (tedious) process of repeating the calculation for every possible pair of dots and getting a distribution of effect sizes. What would be the range of this distribution: from the smallest effect size to the biggest? (Hint: You can figure it out from your answers to (1).) -A- -850 kg/cylinder to +350 kg/cylinder\n\nFrom your result in (2), you might be tempted to conclude that the effect size is highly uncertain: it might be negative or it might be positive. But there is a problem, the procedure used in (1) and (2) fails to incorporate the notion of all other things being equal. This notion applies not just to known variables, but to all the other unknown factors that shape the data.\nFor the purpose of envisioning the concept, imagine that we actually had a measurement of all the factors that shaped CO2 emissions from a vehicle. We’ll call this imaginary measurement “all other things”. The figure below shows a conceptualization of what CO2 emissions as a function of the number of cylinders and “all other things” would look like.\n\n\n\n\n\n\nUsing the graph of CO2_year versus “all other things”, calculate the effect size of a change from six to four cylinders. Remember to hold “all other things” constant in your calculations. So do the effect size calculation at each of several values of “all other things”. What is the effect size and about how much does it vary from one value of “all other things” to another? -A- At “all other things being 0.25, the CO2 emissions for the 6- and 4-cylinder cars is 2800 kg and 3800 kg. The effect size is therefore (3800 kg - 2800 kg) / -2 cylinders = -500 kg/cylinder. The value of the effect size for other levels of”all other things” will be about the same, since the position of the six-cylinder dots is more-or-less constant compared to the corresponding four-cylinder dot.\n\nThis exercise is intended to give you a way of thinking about effect size and “all other things”. In reality, of course, we do not have a way to measure “all other things”. Instead, we calculate the effect size not directly from the data but from the model output (indicated by the statistic layer in the first graphic). As we saw in Lesson 22, there is actually some uncertainty about the model output stemming from sampling variability that can be summarized by a “confidence interval”. For the relationship between the number of cylinders and CO2 emissions, the extent of that uncertainty is indicated by the interval layer in the first graphic. A fair depiction for the corresponding uncertainty in the effect size can be had by repeating the process of (2), but only for points falling into the confidence interval.\n\n\n\n\n\n\nDo we need this?\n\n\n\nNote to readers who know something about car engines: As you know, switching out a six-cylinder engine for a four-cylinder engine is likely to change many other things: the weight of the vehicle, the displacement of the engine, the power available, etc. So it’s not useful to insist that everything other than the number of cylinders be held constant. Instead, we’ll be looking at the effect of that whole constellation of changes associated with a change in the number of cylinders."
  },
  {
    "objectID": "LC/LC-lesson24.html#x",
    "href": "LC/LC-lesson24.html#x",
    "title": "Learning Checks Lesson 24",
    "section": "24.X",
    "text": "24.X\nIn very simple settings, you don’t need access to the original data: a simple summary will do.\n\n\n\n\n\n\nIn draft\n\n\n\nUse the data/bloodthinner.csv data from CPS chapter 19. Create the table of counts and calculate the effect size/probabilities from that."
  },
  {
    "objectID": "LC/LC-lesson24.html#z",
    "href": "LC/LC-lesson24.html#z",
    "title": "Learning Checks Lesson 24",
    "section": "24.Z",
    "text": "24.Z\nThe National Cancer Institute publishes an online, interactive “Breast Cancer Risk Assessment Tool”, also called the “Gail model.” The output of the model is a probability that the woman whose characteristics are used as input will develop breast cancer in the next five years. The inputs include:\n\nage\nrace/ethnicity\nage at first menstrual period\nage at birth of first child (if any)\nhow many of the woman’s close relatives (mother, sisters, or children) have had breast cancer\n\nAs a baseline, consider a 55-year-old, African-American woman who has never had a breast biopsy or any history of breast cancer, who doesn’t know her BRCA status, and whose close relatives have no history of breast cancer, whose first menstrual period was at age 13 and first child at age 23. No “subrace” or “place of birth” is specified.\n\nFollow the link above to the cancer risk assessment tool, and enter the baseline values into the assessment tool using the link above. According to the assessment tool, what is the probability (“risk”) of developing breast cancer in the next five years? What is the lifetime risk of developing breast cancer? -A- 1.4% risk in the next five years, 7.8% lifetime risk.\nWhat is the effect size on our baseline subject of finding out that:\n\n\none of her close relatives has developed breast cancer? -A- Using the baseline values for the inputs, but changing the number of close relatives who have developed breast cancer to 1, the new five year risk is 2.2% and the lifetime risk is 12.3%. This is a change of five year risk of 0.8 percentage points and 4.5 percentage points for lifetime risk. (In Lesson 33, you’ll see that expressing effect size of a probability is often done using “log-odds”.)\nmore than one of her close relatives have developed breast cancer? -A- This has a dramatic effect, with five-year risk increasing to 4.8 percentage points and lifetime risk increasing to 29.4%.\n\n\nWhat is the effect size associated with comparing the baseline conditions to a woman with the same conditions but a race of white? -A- Five year risk goes down by 0.3 percentage points, lifetime risk goes down by 0.4 percentage points.\nWhat is the effect size of age? Compare the baseline 55-year old woman to herself when she is 65, without the other inputs changing. Since age is quantitative, Report the effect size as percentage-points-per-year. -A- Five year risk goes up to 1.5%, an increase of 0.1 percentage points per year. Lifetime risk goes down to 5.6%, a rate of -0.67 percentage points per year.\n\n\nPossibly Stat2Data::ArcheryData and ask about effect size."
  },
  {
    "objectID": "LC/LC-lesson24.html#zz",
    "href": "LC/LC-lesson24.html#zz",
    "title": "Learning Checks Lesson 24",
    "section": "24.ZZ",
    "text": "24.ZZ\nThe graphs shows world record times in the 100m and 200m butterfly swim race as a function of year, sex, and race length. (Data frame: math300::Butterfly)\n\n\n\n\n\n\n\n\n\nIn 1980, what is the effect size of race distance? (Make sure to give the units.)\nIn 1980, what is the effect size of sex? (Make sure to give the units.)\nIn 2000, what is the effect size of race distance? (Be sure to give the units.)"
  },
  {
    "objectID": "LC/LC-lesson24.html#ww",
    "href": "LC/LC-lesson24.html#ww",
    "title": "Learning Checks Lesson 24",
    "section": "24WW",
    "text": "24WW\nFigure 1 shows a model of the running time of winners of Scottish hill races as a function of climb, distance, and sex.\n\n\n\n\n\nFigure 1: A model of running time in Scottish hill racing versus climb, distance, and sex of the runner\n\n\n\n\nUsing as a baseline a distance of 10 km, a climb of 500m, and sex female, calculate each of these effect sizes. Remember for quantitative inputs to format the effect size as a rate, for instance seconds-per-meter-of-climb.\n\nThe effect size of climb. Consider an increase of climb by 500m. - For females running a 10 km course, an increase in climb from 500m (the specified baseline for comparison) to 1000m is roughly 2000 seconds. Since climb is quantitative, this should be reported as a rate, the change of output (2000 s) divided by the change in input (500 m). This comes to 4 seconds per meter of climb.\nThe effect size of distance. Consider an increase in distance of 10 km. -A- The model output at baseline is about 3500 seconds. The model output for a distance of 20 km (an additional 10 km above baseline) is about 6000 s. The effect size is a rate: (6000 - 3500) s / 10 km, or 250 s/km. For comparison, the fastest 20 km road run by a woman (at the time this is being written) is 1 hour 1 minute 25 seconds, that is, 3685 seconds. This amounts to an average of about 180 s/km. A walker would cover 1 km in about 720 seconds. So the racers are quite fast.\nIs there any evidence of an interaction between sex and climb? -A- Yes. Note that the lines for different climb amounts are vertically spaced more widely for females than for males. This means that the effect size of climb differs between the sexes. That’s an interaction between climb and sex."
  },
  {
    "objectID": "LC/LC-lesson24.html#r",
    "href": "LC/LC-lesson24.html#r",
    "title": "Learning Checks Lesson 24",
    "section": "24.R",
    "text": "24.R\nOver the past hundred years, attitudes toward sex have changed. Let’s examine one way that this change might show up in data: the age at which people first had sex. The National Health and Nutrition Evaluation Survey includes a question about the age at which a person first had sex. For the moment, let’s focus just on that subset of people who have had sex at least once.\n\n\n\n\n\n\nWhat is the effect size of year born on age at first sex? Use units of years-over-years. -A- The model output is 17 years for those born round about 1940, and slightly more than 16 years for those born around 1990. The effect size is therefore about negative 1 year per 50 years\nIs there evidence for an interaction between gender and year born? -A- No, the slopes of the lines are essentially identical for males and females.\nTranslate the effect size in (1) to the units weeks-over-years. -A- 1 year per 50 years is the same as 52 weeks per 50 years, or about one week per year."
  },
  {
    "objectID": "LC/LC-lesson32.html",
    "href": "LC/LC-lesson32.html",
    "title": "Learning Checks Lesson 32",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson26.html",
    "href": "LC/LC-lesson26.html",
    "title": "Learning Checks Lesson 26",
    "section": "",
    "text": "Ideas\nAt the number of beers you found in (1), what fraction of volunteers will be above the 0.08 level?\nTo have a non-zero guideline, we have to allow that the guideline will put a small fraction of people above the 0.08 BAC level. Suppose that we decide to use the standard 95% level for the prediction interval. Construct the 95% prediction interval on BAC for each of the inputs 1 to 5 beers. Which number of beers will keep the upper limit of the prediction interval below the 0.08 BAC limit?"
  },
  {
    "objectID": "LC/LC-lesson26.html#section",
    "href": "LC/LC-lesson26.html#section",
    "title": "Learning Checks Lesson 26",
    "section": "26.1",
    "text": "26.1\nThe openintro::bac data frame records an experiment with sixteen student volunteers at Ohio State University who each drank a randomly assigned number of cans of beer (beers). These students were evenly divided between men and women, and they differed in weight and drinking habits. Thirty minutes later, a police officer measured their blood alcohol content (bac) in grams of alcohol per deciliter of blood.\nConstruct a model of bac ~ beers using the openintro::bac data.\n\nmod <- lm(bac ~ beers, data = openintro::bac)\n\n\nFederal and state laws typically specify a legal upper limit for blood alcohol content of a driver of 0.08%. According to the model function, how many beers corresponds to this upper limit?"
  },
  {
    "objectID": "LC/LC-lesson26.html#solution",
    "href": "LC/LC-lesson26.html#solution",
    "title": "Learning Checks Lesson 26",
    "section": "Solution",
    "text": "Solution\nOne way to calculate this is to guess at the number of beers, then modify your guess according to whether it’s high or low.\n\nmod_eval(mod, beers=4) # too low\n\n  beers model_output\n1     4   0.05915444\n\nmod_eval(mod, beers=6) # too high\n\n  beers model_output\n1     6   0.09508197\n\n# next guess should be around 5\n\nAnother way, for the avid calculus student, is to turn the model into a function, then use Zeros() to find where the output is 0.08.\n\nf <- makeFun(mod)\nmosaicCalc::Zeros(f(beers) - 0.08 ~ beers, mosaicCalc::bounds(beers=0:10))\n\n# A tibble: 1 × 2\n  beers .output.\n  <dbl>    <dbl>\n1  5.16        0\n\n\nSimplest of all, just graph the model function and read backwards from the vertical axis."
  },
  {
    "objectID": "LC/LC-lesson26.html#gg",
    "href": "LC/LC-lesson26.html#gg",
    "title": "Learning Checks Lesson 26",
    "section": "26.GG",
    "text": "26.GG\nThe town where you live has just gone through a so-called 100-year rain storm, which caused flooding of the town’s sewage treatment plant and consequent general ickiness. The city council is holding a meeting to discuss install flood barriers around the sewage treatment plant. The are trying to decide how urgent it is to undertake this expensive project. When will the next 100-year storm occur.\nTo address the question, the city council has enlisted you, the town’s most famous data scientist, to do some research to find the soonest that a 100-year flood can re-occcur.\nYou look at the historical weather records for towns that had a 100-year flood at least 20 years ago. The records start in 1900 and you found 1243 towns with a 100-year flood that happened 20 or more years ago. The plot shows, for all the towns that had a 100-year flood at least 20 years ago, how long it was until the next flood occurred. Those town for which no second flood occurred are shown in a different color.\nYou explain to the city council what a 95% prediction interval is and that you will put your prediction in the form of a probability of 2.5% that the flood will occur sooner than the date you give. You show them how to count dots on a jitter plot to find the 2.5% level.\n\n\nWarning: Removed 1110 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\nSince the town council is thinking of making the wall-building investment in the next 10 years, you also have provided a zoomed-in plot showing just the floods where the interval to the next flood was less than ten years.\n\nYou have n = 1243 floods in your database. How many is 2.5% of 1243? -A- 31\nUsing the zoomed-in plot, starting at the bottom count the number of floods you calculated in part (a). A line drawn where the counting stops is the location of the bottom of the 95% coverage interval. Where is the bottom of the 95% interval.-A- About 2.5 years.\nA council member proposes that the town act soon enough so that there is a 99% chance that the next 100-year flood will not occur before the work is finished. It will take 1 year to finish the work, once it is started. According to your data, when should the town start work? -A- Find the bottom limit that excludes 1% of the 1243 floods in your data. This will be between the 12th and 13th flood, counting up from the bottom. This will be at about 1.25 years, that is 15 months. So the town has 3 months before work must begin. That answer will be a big surprise to those who think the next 100-year flood won’t come for about 100 years.\nA council member has a question. “Judging from the graph on the left, are you saying that the next 100-year flood must come sometime within the next 120 years?” No, that’s not how the graph shold be read. Explain why. -a- Since the records only start in 1900, the longest possible interval can be 120 years, that is, from about 2020 to 1900. About half of the dots in the plot reflect towns that haven’t yet had a recurrence 100-year flood. Those could happen at any time, and presumably many of them will happen after an interval of, say, 150 years or even longer."
  },
  {
    "objectID": "LC/LC-lesson26.html#ww",
    "href": "LC/LC-lesson26.html#ww",
    "title": "Learning Checks Lesson 26",
    "section": "26.WW",
    "text": "26.WW\n\n\n\n\n\n\nIn draft\n\n\n\nThis exercise will be about the mean square error when modeling BMI versus weight. Weight is not the same thing as BMI, though it is closely related. You can see from the data the amount of variation there is in BMI at any given weight.\nAdd in Height as an explanatory variable. Mean square error gets smaller. (R^2 goes from .85 to .95)\nFOR EXTRA CREDIT? Model log BMI against log Weight + log Height. RMS residual is zero."
  },
  {
    "objectID": "LC/LC-lesson26.html#nn",
    "href": "LC/LC-lesson26.html#nn",
    "title": "Learning Checks Lesson 26",
    "section": "26.NN",
    "text": "26.NN\nThe violin plot shows gasoline consumption (in miles-per-gallon for city driving) stratified by the number of passenger doors for the SDSdata::MPG data frame. (For many vehicles, the values are missing. These are marked “NA”.)\n\n\n\n\n\n\nSketch in 95% prediction intervals for gasoline consumption stratified by the number of passenger doors.\n\n::: {.callout-note} ## Solution\n\n\n\n\n\n:::\n\nWhich category – 2 doors, 4 doors, or NA – has the largest number of vehicles?\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is no way to tell from the violin graph. Each violin shows the distribution of values within its category. There’s no information about how many data rows are comprised by an individual violin.\n\n\nThe unit of observation in the MPG data frame is “a model of car”. There are 1154 different models included in MPG, thus 1154 rows. Imagine that you had another data frame with the same variables as MPG, but where the unit of observation were “a registered vehicle”. There are roughly 290 million vehicles registered in the US, so the data frame would have 290 million rows. Also imagine (probably contrary to reality at present) that car models with relatively high miles per gallon are much more popular than cars with low miles per gallon.\n\nSketch out what the MPG ~ doors violins would look like for the 290 million-row table.\n\n\n\n\n\n\n\nSolution\n\n\n\nCompared to the 1154 data frame, the violins would be fatter at the higher miles-per-gallon and correspondingly thinner at low miles-per-gallon. If they are fatter in one place, they must be thinner in another so that the overall area of each violin stays the same.\nThe graph shows an imagined scenario where cars with a 10 mile-per-gallon increase in fuel efficiency triples the popularity of a car model."
  },
  {
    "objectID": "LC/LC-lesson26.html#r",
    "href": "LC/LC-lesson26.html#r",
    "title": "Learning Checks Lesson 26",
    "section": "26.R",
    "text": "26.R\nYou’ve been told that Jenny is in an elementary school that covers grade K through 6. Predict how old is Jenny.\n\nPut your prediction in the format of assigning a probability to each of the possible outcomes, as listed below. Remember that the sum of your probabilities should be 1. (You don’t have to give too much thought to the details. Anything reasonable will do.)\n\nAge         | 3 or under | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12  | 13 | 14 | 15+\n------------|------------|---|---|---|---|---|---|----|----|-----|----|----|-----\nprobability |            |   |   |   |   |   |   |    |    |     |    |    |\n\n\n\n\n\n\nSolution\n\n\n\nPerhaps something like the following, where the probabilities are given in percentage points.\nAge         | 3 or under  | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15+\n------------|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|------\nprobability |      0      | 2.5 | 12  | 12  | 12  | 12  | 12  | 12  | 12  | 11  | 2   | 0.5 | 0\nAges 5 through 12 are equally likely, with a small possibility of 4-year olds or 14 year olds.\n\n\n\nTranslate your set of probabilities to a 95% prediction interval.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe 95% prediction interval 5 to 12 years old.\nA 95% interval should leave out 2.5% of the total probability on either end. Below age 5 there is 2.5% and above age 12 there is 2.5%.\nIf you wrote your own probabilities so that there’s no cut-off that gives exactly 2.5%, then set the interval to come as close as possible to 2.5%.\n\n\n\n\n\n\n\n\n\nIN DRAFT"
  },
  {
    "objectID": "LC/LC-lesson26.html#move-to-lc",
    "href": "LC/LC-lesson26.html#move-to-lc",
    "title": "Learning Checks Lesson 26",
    "section": "Move to LC",
    "text": "Move to LC\nA prediction function takes values for explanatory variables as inputs and, as output, produces corresponding values for the prediction interval. Typically, the 95% prediction interval is shown. The graphic below shows both the 50% interval and a 95% interval for a model of winning time versus distance trained on the Scottish hill racing data.\n\n\n\n\n\n\n\n\n\nWhich graph, (a) or (b), corresponds to the 50% interval. -A- A 50% interval will be narrower than a 95% interval, so (b) is the 50% interval.\nJudging by eye in (a) and (b), give the top and bottom of the 95% prediction interval and the 50% prediction interval when the distance is 10 km. -A- 50% interval runs from about 670 to 950 seconds. The 90% interval runs from about 500 to 1300 seconds.\n\nDrawing the prediction intervals as bands gives the misleading idea that any value between the top and bottom of the band is equally likely. But values toward the center of the band are much more likely than those toward the edges or outside of the band.\nThe following graphic gives a better idea of the relative probability of each outcome by overlaying several prediction bands: 25%, 50%, 75%, 95%. The darker regions are more likely than the lighter regions.\n\n\n\n\n\n\nAbout how much wider is the 95% band than the 75% band? -A- Almost twice as wide.\nWhat is the probability of an individual outcome being somewhere inside the 95% band, but outside the 75% band? -A- 20%. Events outside the 75% band will be seen only 25% of the time. Events outside the 95% band will be seen only 5% of the time. That leaves 20% for the region between the two bands."
  },
  {
    "objectID": "LC/LC-lesson27.html",
    "href": "LC/LC-lesson27.html",
    "title": "Learning Checks Lesson 27",
    "section": "",
    "text": "This is a QR day.\n\n::: {.callout-warning} ## In draft\nPick out a data frame. Ask\n\nWhat is the sample size?\nWhat’s the amount of variation in the xxx variable, using a standard method for measuring variation?\nHow many coefficients are there in the model xxx ~ y + z + group?"
  },
  {
    "objectID": "LC/LC-lesson33.html",
    "href": "LC/LC-lesson33.html",
    "title": "Learning Checks Lesson 33",
    "section": "",
    "text": "See medGPA from the Stat2Data package. Maybe come back to this in Lesson 34.\nMaybe Data2Stats::FlightResponse\nMaybe space shuttle Challenger o-ring data."
  },
  {
    "objectID": "LC/LC-lesson33.html#section",
    "href": "LC/LC-lesson33.html#section",
    "title": "Learning Checks Lesson 33",
    "section": "33.1",
    "text": "33.1\n\nConvert probability to odds and log odds, and vice versa.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson33.html#xxx",
    "href": "LC/LC-lesson33.html#xxx",
    "title": "Learning Checks Lesson 33",
    "section": "33.XXX",
    "text": "33.XXX\nA major company in big trouble is planning to lay off 20% of its employees. You work in the personnel office and have constructed a multivariate model of the probability of an individual being laid off. The effect size of the different input factors in the model are as follows:\n\nage over 50 years: change in log-odds 1\nsoftware engineer: change in log-odds -0.5\npay level: change in log-odd rate 0.2 per $10,000 above the company average of $40K\n\n\nWhat are the odds of being laid off for an employee for whom you have no information about age, engineering capabilities, or pay level. -A- The risk of being laid off is 20% so the probability of not being laid off is 80%. The odds are therefore 20/80 = 0.25.\nWhat are the log odds of an employee in (1) being laid off? -A- Simply the logarithm of the odds, so \\(\\log(0.25) = -1.4\\).\nConsider a 30-year old software engineer making $50,000 per year.\n\nWhat are the log odds of her being laid off? -A- At baseline, her log odds is -1.4. Being a software engineer, her log odds are lower by 0.5. But she makes more than the average pay by $10,000, which adds 0.2 to her log odds. Altogether, -1.4 - 0.5 + 0.2 = -1.7.\nTranslate the log odds into an absolute risk of her being laid off. -A- Undoing the logarithm on -1.7 gives an odds of 0.18, which corresponds to a probability of 0.18/(1 + 0.18) or 15%.\n\n\n\nIn LC 24.2, we took as baseline inputs for [this online breast-cancer risk assessment model]((https://bcrisktool.cancer.gov/calculator.html), a 55-year-old, African-American woman who has never had a breast biopsy or any history of breast cancer, who doesn’t know her BRCA status, and whose close relatives have no history of breast cancer, whose first menstrual period was at age 13 and first child at age 23.\nFor this baseline case, the output of the model was a probability of 1.4% five-year risk of developing breast cancer. Finding out that one close relative has developed breast cancer elevates this risk to 2.2%.\nThese are absolute risks: probabilities. The effect size is 2.2 - 1.4 = 0.8 percentage points. When risks are specified as probabilities, differences in risks are in “percentage points.”\n\nWhat risk ratio corresponds to the 1.4 to 2.2 change of risk? -A- This is a simple ratio: 2.2% / 1.4% which is 1.57.\nSuppose (hypothetically) that the risks were ten times higher: 14% and 22%.\n\nWhat would be the risk ratio? -A- Still 1.57.\nWhat would be the percentage point change in absolute risk? -A- 8 percentage points.\n\nPut yourself in the place of the baseline woman who has just found out that a close relative developed breast cancer. Which way of reporting a change of risk is more pertinent to your personal life? -A- The change in absolute risk. Even though the risk ratios are the same in (1) and (2), the change in absolute risk is much greater in (2)."
  },
  {
    "objectID": "LC/first-half-LC.html",
    "href": "LC/first-half-LC.html",
    "title": "Additional LC for first half of course",
    "section": "",
    "text": "The US Department of Transportation has a program called the Fatality Analysis Reporting System. FARS has a web site which publishes data. Figure Figure 1 shows partial screen shot of their web page.\n\n\n\n\n\nFigure 1: National statistics from the US on motor0-vehicle accident-related fatalities. Source: https://www-fars.nhtsa.dot.gov/Main/index.aspx.\n\n\n\n\nFor several reasons, the table is not in tidy form.\n\nSome of the rows serve as headers for the next several rows, but don’t contain any data. Identify several of those headers. -A- “Motor vehicle traffic crashes”, “Traffic crash fatalities”, “Vehicle occupants”, “Non-motorists”, “Other national statistics”, “National rates: fatalities”\nIn tidy data, all the entries in a column should describe the same kind of quantity. You can see that all of the columns contain numbers. But the numbers are not all the same kind of quantity. Referring to the 2016 column:\n\nWhat kind of thing is the number 34,439? -A- A number of crashes\nWhat kind of thing is 18,610? -A- A number of drivers\nWhat kind of thing is 1.18? -A- A rate: fatalities per 100-million miles.\n\nIn tidy data, there is a definite unit of observation that is the same kind of thing for every row. Give an example of two rows that are not the same kind of thing. -A- For example, “Registered vehicles” and “Licensed drivers”. The first is a count of cars, the second a count of drivers.\nIdentify a few rows that are summaries of other rows. Such summaries are not themselves a unit of observation. -A- “Sub Total1”, “Sub Total2”, “Total**”"
  },
  {
    "objectID": "LC/first-half-LC.html#graphics",
    "href": "LC/first-half-LC.html#graphics",
    "title": "Additional LC for first half of course",
    "section": "Graphics",
    "text": "Graphics\nThe three graphs below show the distribution of weights broken down by sex. The violin layer is the same in each graph. Each graph has a coverage interval at one of these levels: 25% 50%, 80%, 95%. Which graph has which coverage interval? Which of the listed coverage intervals is not shown in any graph?\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum"
  },
  {
    "objectID": "LC/first-half-LC.html#violin-plots",
    "href": "LC/first-half-LC.html#violin-plots",
    "title": "Additional LC for first half of course",
    "section": "Violin plots",
    "text": "Violin plots\nThe graph below is a violin plot. Using a pencil and your intuition, add a few dozen dots to the graphic as they would appear in a data layer superimposed on the violin layer. The dots should be jittered and be consistent with the shape of the violins.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWhere the violin is wider, there is a greater concentration of dots. In a jittered plot, the exact horizontal position of the dots has no significance."
  },
  {
    "objectID": "LC/first-half-LC.html#untidy-data",
    "href": "LC/first-half-LC.html#untidy-data",
    "title": "Additional LC for first half of course",
    "section": "Untidy data",
    "text": "Untidy data\nList what’s not tidy about this table.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nUnits ought to be in the codebook.\nThe “length of year” variable is in a mixture of units. Some rows are (Earth) days, others are (Earth) years.\nThe numbers have commas, which are intended for human consumption. Data tables are for machine consumption and the commas are a nuisancwe.\nThe \\(\\frac{1}{4}\\) in the “length of year” column is not a standard computer numeral. Write 365.25 instead."
  },
  {
    "objectID": "LC/first-half-LC.html#calculating-new-variables-from-old.",
    "href": "LC/first-half-LC.html#calculating-new-variables-from-old.",
    "title": "Additional LC for first half of course",
    "section": "Calculating new variables from old.",
    "text": "Calculating new variables from old.\nTITLE GOES HERE: Often, a data scientist needs to calculate some new quantity from the quantities already available in a data table. Proper data computing software makes this easy to do in a manner that is clear even to an inexperienced reader. For example, the following statement will take a data frame named Fatality_data with the structure of ?@tbl-pine-hit-pants and augment it with a new variable total_fatalities that gives the total number of fatalities in each year\n\nFARS <-\n  FARS %>%\n  mutate(total_fatalities = drivers + passengers + unknown)\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\ncrashes\ndrivers\npassengers\nunknown\nmiles\nresident_pop\ntotal_fatalities\n\n\n\n\n2016\n34439\n18610\n6407\n79\n3174\n323128\n25096\n\n\n2015\n32539\n17613\n6213\n71\n3095\n320897\n23897\n\n\n2014\n30056\n16470\n5766\n71\n3026\n318563\n22307\n\n\n\n\n\n\nThe mutate() function does the work of creating the new variable. The text between the opening parenthesis and the corresponding closing parenthesis is the argument to mutate(), which gives the specifics of what to do.\nFor each of the following, write the argument for a mutate() statement that will produce the desired new variable. You can assume that total_fatalities is already one of the existing variables. (We created it above!)\n\nFatalities per crash. ::: {.callout-note} ##Solution\n\n\nfatalities_per_crash = total_fatalities / crash\n\n:::\n\nFatalities per million vehicle miles. ::: {.callout-note} ##Solution\n\n\nfatalities_per_distance = total_fatalities / (miles * 1000)\n\nDivide the total number of fatalities by the number of vehicle miles travelled.\nWhy the multiplication by 1000? Recall that miles is in billions of vehicle miles, while we want the units of fatalities_per_distance to be fatalities per million vehicle miles. The 1000 performs the conversion from billions to millions. :::\n\nNumber of crashes per million vehicle miles. ::: {.callout-note} ##Solution\n\n\ncrashes_per_distance = crashes / (miles * 1000)\n\n::: d. Referring back to the original data shown in Figure @ref(fig:bear-ride-pants-1), you can see that the calculation of total fatalities in the introduction to this problem left out the number of motorcyclist and nonmotorist fatalities. Modify the calculation of total fatalities to include these, assuming they are represented by motorcyclist and nonmotorist respectively. ::: {.callout-note} ##Solution\n\ntotal_fatalities = drivers + passengers + unknown +\n  motorcyclist + nonmotorist\n\n:::\n——–="
  },
  {
    "objectID": "LC/first-half-LC.html#unit-of-observation",
    "href": "LC/first-half-LC.html#unit-of-observation",
    "title": "Additional LC for first half of course",
    "section": "Unit of observation",
    "text": "Unit of observation\nThe data table below records activity at a neighborhood car repair shop.\n\n\n\nmechanic\nproduct\nprice\ndate\n\n\n\n\nAnne\nstarter\n170.00\n2019-01-12\n\n\nBeatrice\nshock absorber\n78.42\n2019-01-12\n\n\nAnne\nalternator\n385.95\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nBeatrice\nradiator hose\n17.90\n2019-02-12\n\n\n\nThe codebook for a data table should describe what is the unit of observation. For the purpose of this exercise, your job is to comment on each of the following possibilities and say why or why not this is plausibly the unit of observation.\n\na day. -A- There must be more to it than that, since the same date may be repeated with different values for the other variables.\na mechanic. -A- No. The same mechanic appears multiple times, so the unit of observation is not simply a mechanic.\na car part used in a repair. -A- Could be, for instance if every time a mechanic installs a part a new entry is added to the table describing the part, its price, the date, and the mechanic doing the work."
  },
  {
    "objectID": "LC/first-half-LC.html#graphics-and-data",
    "href": "LC/first-half-LC.html#graphics-and-data",
    "title": "Additional LC for first half of course",
    "section": "Graphics and data",
    "text": "Graphics and data\nThe graphic below contains a single data layer. Four of the data points are annotated with letters in order to identify them specifically.\n\n\n\n\n\n\n\n\n\nPart 1\n\nIs the income level of “a” greater than “b”? -A- no\nIs the income level of “d” greater than “a”? -A- no\nIs the number of rooms greater for “b” than for “a”? -A- no\nIs the number of rooms greater for “c” than for “a”? -A- no\n\nPart 2\nHere is the data plotted in the figure.\n\n\n\n\n \n  \n    row \n    income \n    number_of_rooms \n  \n \n\n  \n    1 \n    0.90 \n    1 \n  \n  \n    2 \n    1.00 \n    3 \n  \n  \n    3 \n    0.31 \n    3 \n  \n  \n    4 \n    0.85 \n    1 \n  \n  \n    5 \n    1.09 \n    3 \n  \n  \n    6 \n    1.19 \n    2 \n  \n  \n    7 \n    1.01 \n    1 \n  \n  \n    8 \n    1.09 \n    3 \n  \n  \n    9 \n    1.16 \n    2 \n  \n  \n    10 \n    2.86 \n    2 \n  \n\n\n ... and so on for 2,765 rows altogether.\n\n\n\n\nThe points a, b, c, and d, are shown in the table. For each of a, b, c, d, say which row corresponds to the point. -A- a is row 8, b is row 7, c is row 2, d is row 1"
  },
  {
    "objectID": "LC/Learning-checks.html",
    "href": "LC/Learning-checks.html",
    "title": "Learning Checks from Modern Dive",
    "section": "",
    "text": "LC 1.1 Block 1 Day 1\n\n\n\nRepeat the earlier installation steps, but for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package for data wrangling, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for generating easy-to-read tables in R. We’ll use these packages in the next section.\n\n\n\n\n\n\n\n\nLC 1.2 Block 1 Day 1\n\n\n\n“Load” the dplyr, nycflights13, and knitr packages as well by repeating the earlier steps.\n\n\nRun View(flights) in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note the uppercase V in View(). R is case-sensitive, so you’ll get an error message if you run view(flights) instead of View(flights)\n\n\n\n\n\n\nLC 1.3 Block 1 Day 1\n\n\n\nWhat does any ONE row in this flights dataset refer to?\n\nA. Data on an airline\nB. Data on a flight\nC. Data on an airport\nD. Data on multiple flights\n\n\n\n\n\n\n\n\n\nLC 1.4 Block 1 Day 1\n\n\n\nWhat are some other examples in this dataset (flights) of categorical variables? What makes them different than quantitative variables?\n\n\n\n\n\n\n\n\nLC 1.5 Block 1 Day 1\n\n\n\nWhat properties of each airport do the variables lat, lon, alt, tz, dst, and tzone describe in the airports data frame? Take your best guess.\n\n\n\n\n\n\n\n\nLC 1.6 Block 1 Day 1\n\n\n\nProvide the names of variables in a data frame with at least three variables where one of them is an identification variable and the other two are not. Further, create your own tidy data frame that matches these conditions.\n\n\n\n\n\n\n\n\nLC 1.7 Block 1 Day 1\n\n\n\nLook at the help file for the airports data frame. Revise your earlier guesses about what the variables lat, lon, alt, tz, dst, and tzone each describe."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-2-visualization",
    "href": "LC/Learning-checks.html#chapter-2-visualization",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 2: Visualization",
    "text": "Chapter 2: Visualization\n\n\n\n\n\n\nLC 2.1 Block 1 Day 2\n\n\n\nTake a look at both the flights and alaska_flights data frames by running View(flights) and View(alaska_flights). In what respect do these data frames differ? For example, think about the number of rows in each dataset.\n\n\n\n\n\n\n\n\nLC 2.2-2.6 Block 1 Day 2\n\n\n\nWhat are some practical reasons why dep_delay and arr_delay have a positive relationship?\nWhat variables in the weather data frame would you expect to have a negative correlation (i.e., a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here. Hint: Explore the weather dataset by using the View() function.\nWhy do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaska Air flights?\nWhat are some other features of the plot that stand out to you?\nCreate a new scatterplot using different variables in the alaska_flights data frame by modifying the example given.\n\n\n\n\n\n\n\n\nLC 2.7-2.8 Block 1 Day 2\n\n\n\nWhy is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot?\nAfter viewing Figure @ref(fig:alpha), give an approximate range of arrival delays and departure delays that occur most frequently. How has that region changed compared to when you observed the same plot without alpha = 0.2 set in Figure @ref(fig:noalpha)?\n\n\n\n\n\n\n\n\nLC 2.9-2.10 Block 1 Day 3\n\n\n\nLC 2.9 Take a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather). In what respect do these data frames differ?\nLC 2.10 View() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement, whereas the hour variable does not?\n\n\n\n\n\n\n\n\nLC 2.11-2.13 Block 1 Day 3\n\n\n\nLC 2.11 Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis?\nLC 2.12 Why are linegraphs frequently used when time is the explanatory variable on the x-axis?\nLC 2.12 Plot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013.\n\n\n\n\n\n\n\n\nLC 2.18-2.21 Block 1 Day 3\n\n\n\nWhat other things do you notice about this faceted plot? How does a faceted plot help us see relationships between two variables?\nWhat do the numbers 1-12 correspond to in the plot? What about 25, 50, 75, 100?\nFor which types of datasets would faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics.\nLC 2.21 Does the temp variable in the weather dataset have a lot of variability? Why do you say that?\n\n\n\n\n\n\n\n\nLC 2.22-2.25 Boxplots Block 1 Day 4\n\n\n\nLC 2.22 What does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point.\nLC 2.23 Which months have the highest variability in temperature? What reasons can you give for this?\nLC 2.24 We looked at the distribution of the numerical variable temp split by the numerical variable month that we converted using the factor() function in order to make a side-by-side boxplot. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative?\nLC 2.25 Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram?\n\n\n\n\n\n\n\n\nLC 2.26-2.29 Histograms Block 1 Day 4\n\n\n\nLC 2.26 Why are histograms inappropriate for categorical variables?\nLC 2.27 What is the difference between histograms and barplots?\nLC 2.28 How many Envoy Air flights departed NYC in 2013?\nLC 2.29 What was the 7th highest airline for departed flights from NYC in 2013? How could we better present the table to get this answer quickly?\n\n\n\n\n\n\n\n\nLC 2.30-2.31 Pie charts Block 1 Day 4\n\n\n\nLC 2.30 Why should pie charts be avoided and replaced by barplots?\nLC 2.31 Why do you think people continue to use pie charts?\n\n\n\n\n\n\n\n\nLC 2.32-2.37 Block 1 Day 4\n\n\n\nLC 2.32 What kinds of questions are not easily answered by looking at Figure @ref(fig:flights-stacked-bar) (2.23)?\nLC 2.33 What can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights?\nLC 2.34 Why might the side-by-side barplot be preferable to a stacked barplot in this case?\nLC 2.35 What are the disadvantages of using a dodged barplot, in general?\nLC 2.36 Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case?\nLC 2.37 What information about the different carriers at different airports is more easily seen in the faceted barplot?"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-3-wrangling",
    "href": "LC/Learning-checks.html#chapter-3-wrangling",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 3: Wrangling",
    "text": "Chapter 3: Wrangling\n\n\n\n\n\n\nLC 3.1 Block 1 Day 5\n\n\n\nWhat’s another way of using the “not” operator ! to filter only the rows that are not going to Burlington, VT nor Seattle, WA in the flights data frame? Test this out using the previous code.\n\n\n\n\n\n\n\n\nLC 3.2 Block 1 Day 5\n\n\n\nSay a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five-year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach?\n\n\n\n\n\n\n\n\nLC 3.3 Block 1 Day 5\n\n\n\nModify the earlier summarize() function code that creates the summary_temp data frame to also use the n() summary function: summarize(... , count = n()). What does the returned value correspond to?\n\n\n\n\n\n\n\n\nLC 3.4 Block 1 Day 5\n\n\n\nWhy doesn’t the following code work? Run the code line-by-line instead of all at once, and then look at the data. In other words, run summary_temp <- weather %>% summarize(mean = mean(temp, na.rm = TRUE)) first.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nsummary_temp <- weather %>%   \n  summarize(mean = mean(temp, na.rm = TRUE)) %>% \n  summarize(std_dev = sd(temp, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\nLC 3.5 Block 1 Day 6\n\n\n\nRecall from Chapter @ref(viz) when we looked at temperatures by months in NYC. What does the standard deviation column in the summary_monthly_temp data frame tell us about temperatures in NYC throughout the year?\n\n\n\n\n\n\n\n\nLC 3.6 Block 1 Day 6\n\n\n\nWhat code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC?\n\n\n\n\n\n\n\n\nLC 3.7 Block 1 Day 6\n\n\n\nRecreate by_monthly_origin, but instead of grouping via group_by(origin, month), group variables in a different order group_by(month, origin). What differs in the resulting dataset?\n\n\n\n\n\n\n\n\nLC 3.8 Block 1 Day 6\n\n\n\nHow could we identify how many flights left each of the three airports for each carrier?\n\n\n\n\n\n\n\n\nLC 3.9 Block 1 Day 6\n\n\n\nHow does the filter() operation differ from a group_by() followed by a summarize()?\n\n\n\n\n\n\n\n\nLC 3.10 Block 1 Day 6\n\n\n\nWhat do positive values of the gain variable in flights correspond to? What about negative values? And what about a zero value?\n\n\n\n\n\n\n\n\nLC 3.11 Block 1 Day 6\n\n\n\nCould we create the dep_delay and arr_delay columns by simply subtracting dep_time from sched_dep_time and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in flights.\n\n\n\n\n\n\n\n\nLC 3.12 Block 1 Day 76\n\n\n\nWhat can we say about the distribution of gain? Describe it in a few sentences using the plot and the gain_summary data frame values.\n\n\n\n\n\n\n\n\nLC 3.13 Block 1 Day 7\n\n\n\nLooking at Figure @ref(fig:reldiagram), when joining flights and weather (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of year, month, day, hour, and origin, and not just hour?\n\n\n\n\n\n\n\n\nLC 3.14 Block 1 Day 7\n\n\n\nWhat surprises you about the top 10 destinations from NYC in 2013?\n\n\n\n\n\n\n\n\nLC 3.15 Block 1 Day 7\n\n\n\nWhat are some advantages of data in normal forms? What are some disadvantages?\n\n\n\n\n\n\n\n\nLC 3.16 Block 1 Day 7\n\n\n\nWhat are some ways to select all three of the dest, air_time, and distance variables from flights? Give the code showing how to do this in at least three different ways.\n\n\n\n\n\n\n\n\nLC 3.17 Block 1 Day 7\n\n\n\nHow could one use starts_with(), ends_with(), and contains() to select columns from the flights data frame? Provide three different examples in total: one for starts_with(), one for ends_with(), and one for contains().\n\n\n\n\n\n\n\n\nLC 3.18 Block 1 Day 7\n\n\n\nWhy might we want to use the select function on a data frame?\n\n\n\n\n\n\n\n\nLC 3.19 Block 1 Day 7\n\n\n\nCreate a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013.\n\n\n::: {.callout-note icon=false} ## LC 3.20 Block 1 Day 7 Let’s now put your newly acquired data wrangling skills to the test!\nAn airline industry measure of a passenger airline’s capacity is the available seat miles, which is equal to the number of seats available multiplied by the number of miles or kilometers flown summed over all flights.\nFor example, let’s consider the scenario in Figure 1. Since the airplane has 4 seats and it travels 200 miles, the available seat miles are \\(4 \\times 200 = 800\\).\n\n\n\n\n\nFigure 1: Example of available seat miles for one flight.\n\n\n\n\nExtending this idea, let’s say an airline had 2 flights using a plane with 10 seats that flew 500 miles and 3 flights using a plane with 20 seats that flew 1000 miles, the available seat miles would be \\(2 \\times 10 \\times 500 + 3 \\times 20 \\times 1000 = 70,000\\) seat miles.\nUsing the datasets included in the nycflights13 package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints:\n\nCrucial: Unless you are very confident in what you are doing, it is worthwhile not starting to code right away. Rather, first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level pseudocode that is informal yet detailed enough to articulate what you are doing. This way you won’t confuse what you are trying to do (the algorithm) with how you are going to do it (writing dplyr code).\nTake a close look at all the datasets using the View() function: flights, weather, planes, airports, and airlines to identify which variables are necessary to compute available seat miles.\nFigure @ref(fig:reldiagram) showing how the various datasets can be joined will also be useful.\nConsider the data wrangling verbs in Table @ref(tab:wrangle-summary-table) as your toolbox! ::"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-4-tidy",
    "href": "LC/Learning-checks.html#chapter-4-tidy",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 4: Tidy",
    "text": "Chapter 4: Tidy\n::: {.callout-note icon=false} ## LC 4.1 Block 1 Day 8 What are common characteristics of “tidy” data frames? ::\n::: {.callout-note icon=false} ## LC 4.2 Block 1 Day 8 What makes “tidy” data frames useful for organizing data? ::\n::: {.callout-note icon=false} ## LC 4.3 Block 1 Day 8 Take a look at the airline_safety data frame included in the fivethirtyeight data package. Run the following:\n\nairline_safety\n\nAfter reading the help file by running ?airline_safety, we see that airline_safety is a data frame containing information on different airline companies’ safety records. This data was originally reported on the data journalism website, FiveThirtyEight.com, in Nate Silver’s article, “Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?”. Let’s only consider the variables airlines and those relating to fatalities for simplicity:\n\nairline_safety_smaller <- airline_safety %>% \n  select(airline, starts_with(\"fatalities\"))\nairline_safety_smaller\n\n# A tibble: 56 × 3\n   airline               fatalities_85_99 fatalities_00_14\n   <chr>                            <int>            <int>\n 1 Aer Lingus                           0                0\n 2 Aeroflot                           128               88\n 3 Aerolineas Argentinas                0                0\n 4 Aeromexico                          64                0\n 5 Air Canada                           0                0\n 6 Air France                          79              337\n 7 Air India                          329              158\n 8 Air New Zealand                      0                7\n 9 Alaska Airlines                      0               88\n10 Alitalia                            50                0\n# … with 46 more rows\n\n\nThis data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, in particular so that it has a variable fatalities_years indicating the incident year and a variable count of the fatality counts? ::\n::: {.callout-note icon=false} ## LC 4.4 Block 1 Day 9 Convert the dem_score data frame into a “tidy” data frame and assign the name of dem_score_tidy to the resulting long-formatted data frame. ::\n::: {.callout-note icon=false} ## LC 4.5 Block 1 Day 9 Read in the life expectancy data stored at https://moderndive.com/data/le_mess.csv and convert it to a “tidy” data frame. ::"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-5-regression",
    "href": "LC/Learning-checks.html#chapter-5-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 5: Regression",
    "text": "Chapter 5: Regression\n\n\n\n\n\n\nLC 5.1 Block 2 Day 1\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) being score but with age as the new explanatory variable \\(x\\). Remember, this involves three things:\n\nLooking at the raw data values.\nComputing summary statistics.\nCreating data visualizations.\n\nWhat can you say about the relationship between age and teaching scores based on this exploration?\n\n\n\n\n\n\n\n\nLC 5.2 Block 2 Day 1\n\n\n\nFit a new simple linear regression using lm(score ~ age, data = evals_ch5) where age is the new explanatory variable \\(x\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your earlier exploratory data analysis?\n\n\n\n\n\n\n\n\nLC 5.3 Block 2 Day 1\n\n\n\nGenerate a data frame of the residuals of the model where you used age as the explanatory \\(x\\) variable.\n\n\n\n\n\n\n\n\nLC 5.4 Block 2 Day 2\n\n\n\nConduct a new exploratory data analysis with the same explanatory variable \\(x\\) being continent but with gdpPercap as the new outcome variable \\(y\\). What can you say about the differences in GDP per capita between continents based on this exploration?\n\n\n\n\n\n\n\n\nLC 5.5 Block 2 Day 2\n\n\n\nFit a new linear regression using lm(gdpPercap ~ continent, data = gapminder2007) where gdpPercap is the new outcome variable \\(y\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis?\n\n\n\n\n\n\n\n\nLC 5.6 Block 2 Day 2\n\n\n\nUsing either the sorting functionality of RStudio’s spreadsheet viewer or using the data wrangling tools you learned in Chapter @ref(wrangling), identify the five countries with the five smallest (most negative) residuals? What do these negative residuals say about their life expectancy relative to their continents’ life expectancy?\n\n\n\n\n\n\n\n\nLC 5.7 Block 2 Day 2\n\n\n\nRepeat this process, but identify the five countries with the five largest (most positive) residuals. What do these positive residuals say about their life expectancy relative to their continents’ life expectancy?\n\n\n\n\n\n\n\n\nLC 5.8 Block 2 Day 3\n\n\n\nNote in Figure @fig:three-lines there are 3 points marked with dots and:\n\nThe “best” fitting solid regression line in blue\nAn arbitrarily chosen dotted red line\nAnother arbitrarily chosen dashed green line\n\n\n\n\n\n\nFigure 2: Regression line and two others.\n\n\n\n\nCompute the sum of squared residuals by hand for each line and show that of these three lines, the regression line in blue has the smallest value."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-6-multiple-regression",
    "href": "LC/Learning-checks.html#chapter-6-multiple-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 6: Multiple regression",
    "text": "Chapter 6: Multiple regression\n\n\n\n\n\n\nLC 6.1 Block 2 Day 4\n\n\n\nCompute the observed values, fitted values, and residuals not for the interaction model as we just did, but rather for the parallel slopes model we saved in score_model_parallel_slopes.\n\n\n\n\n\n\n\n\nLC 6.2\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age?\n\n\n\n\n\n\n\n\nLC 6.3\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age?\n\n\n\n\n\n\n\n\nLC 6.4\n\n\n\nFit a new simple linear regression using lm(debt ~ credit_rating + age, data = credit_ch6) where credit_rating and age are the new numerical explanatory variables \\(x_1\\) and \\(x_2\\). Get information about the “best-fitting” regression plane from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis?"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-7-sampling",
    "href": "LC/Learning-checks.html#chapter-7-sampling",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 7: Sampling",
    "text": "Chapter 7: Sampling\n\n\n\n\n\n\nLC 7.1 Block 3 Day 1\n\n\n\nWhy was it important to mix the bowl before we sampled the balls?\n\n\n\n\n\n\n\n\nLC 7.2 Block 3 Day 1\n\n\n\nWhy is it that our 33 groups of friends did not all have the same numbers of balls that were red out of 50, and hence different proportions red?\n\n\n\n\n\n\n\n\nLC 7.3 Block 3 Day 1\n\n\n\nWhy couldn’t we study the effects of sampling variation when we used the virtual shovel only once? Why did we need to take more than one virtual sample (in our case 33 virtual samples)?\n\n\n\n\n\n\n\n\nLC 7.4 Block 3 Day 1\n\n\n\nWhy did we not take 1000 “tactile” samples of 50 balls by hand?\n\n\n\n\n\n\n\n\nLC 7.5 Block 3 Day 1\n\n\n\nLooking at Figure @ref(fig:samplingdistribution-virtual-1000), would you say that sampling 50 balls where 30% of them were red is likely or not? What about sampling 50 balls where 10% of them were red?\n\n\n\n\n\n\n\n\nLC 7.6 Block 3 Day 1\n\n\n\nIn Figure 7.9, we used shovels to take 1000 samples each, computed the resulting 1000 proportions of the shovel’s balls that were red, and then visualized the distribution of these 1000 proportions in a histogram. We did this for shovels with 25, 50, and 100 slots in them. As the size of the shovels increased, the histograms got narrower. In other words, as the size of the shovels increased from 25 to 50 to 100, did the 1000 proportions\n\nA. vary less,\nB. vary by the same amount, or\nC. vary more?\n\n\n\n\n\n\n\n\n\nLC 7.7 Block 3 Day 1\n\n\n\nWhat summary statistic did we use to quantify how much the 1000 proportions red varied?\n\nA. The interquartile range\nB. The standard deviation\nC. The range: the largest value minus the smallest.\n\n\n\n\n\n\n\n\n\nLC 7.8 Block 3 Day 2\n\n\n\nIn the case of our bowl activity, what is the population parameter? Do we know its value?\n\n\n\n\n\n\n\n\nLC 7.9 Block 3 Day 2\n\n\n\nWhat would performing a census in our bowl activity correspond to? Why did we not perform a census?\n\n\n\n\n\n\n\n\nLC 7.10 Block 3 Day 2\n\n\n\nWhat purpose do point estimates serve in general? What is the name of the point estimate specific to our bowl activity? What is its mathematical notation?\n\n\n\n\n\n\n\n\nLC 7.11 Block 3 Day 2\n\n\n\nHow did we ensure that our tactile samples using the shovel were random?\n\n\n\n\n\n\n\n\nLC 7.12 Block 3 Day 2\n\n\n\nWhy is it important that sampling be done at random?\n\n\n\n\n\n\n\n\nLC 7.13 Block 3 Day 2\n\n\n\nWhat are we inferring about the bowl based on the samples using the shovel?\n\n\n\n\n\n\n\n\nLC 7.14 Block 3 Day 2\n\n\n\nWhat purpose did the sampling distributions serve?\n\n\n\n\n\n\n\n\nLC 7.15 Block 3 Day 2\n\n\n\nWhat does the standard error of the sample proportion \\(\\widehat{p}\\) quantify?\n\n\n\n\n\n\n\n\nLC 7.16 Block 3 Day 2\n\n\n\nThe table that follows is a version of Table @ref(tab:comparing-n-2) matching sample sizes \\(n\\) to different standard errors of the sample proportion \\(\\widehat{p}\\), but with the rows randomly re-ordered and the sample sizes removed. Fill in the table by matching the correct sample sizes to the correct standard errors.\nStandard errors of \\(\\hat{p}\\) based on n = 25, 50, 100\n\n\n\nSample size\nStandard error of \\(\\hat{p}\\)\n\n\n\n\n\\(n=\\)\n0.94\n\n\n\\(n=\\)\n0.45\n\n\n\\(n=\\)\n0.69\n\n\n\nFor the following four Learning checks, let the estimate be the sample proportion \\(\\widehat{p}\\): the proportion of a shovel’s balls that were red. It estimates the population proportion \\(p\\): the proportion of the bowl’s balls that were red.\n\n\n\n\n\n\n\n\nLC 7.17 Block 3 Day 2\n\n\n\nWhat is the difference between an accurate and a precise estimate?\n\n\n\n\n\n\n\n\nLC 7.18 Block 3 Day 2\n\n\n\nHow do we ensure that an estimate is accurate? How do we ensure that an estimate is precise?\n\n\n\n\n\n\n\n\nLC 7.19 Block 3 Day 2\n\n\n\nIn a real-life situation, we would not take 1000 different samples to infer about a population, but rather only one. Then, what was the purpose of our exercises where we took 1000 different samples?\n\n\n\n\n\n\n\n\nLC 7.20 Block 3 Day 2\n\n\n\nFigure @ref(fig:accuracy-vs-precision) with the targets shows four combinations of “accurate versus precise” estimates. Draw four corresponding sampling distributions of the sample proportion \\(\\widehat{p}\\), like the one in the leftmost plot in Figure @ref(fig:comparing-sampling-distributions-3).\n\n\n\n\n\n\n\n\nLC 7.21 Block 3 Day 3\n\n\n\nThe Royal Air Force wants to study how resistant all their airplanes are to bullets. They study the bullet holes on all the airplanes on the tarmac after an air battle against the Luftwaffe (German Air Force).\n\n\n\n\n\n\n\n\nLC 7.22 Block 3 Day 3\n\n\n\nImagine it is 1993, a time when almost all households had landlines. You want to know the average number of people in each household in your city. You randomly pick out 500 phone numbers from the phone book and conduct a phone survey.\n\n\n\n\n\n\n\n\nLC 7.23 Block 3 Day 3\n\n\n\nYou want to know the prevalence of illegal downloading of TV shows among students at a local college. You get the emails of 100 randomly chosen students and ask them, “How many times did you download a pirated TV show last week?”.\n\n\n\n\n\n\n\n\nLC 7.24 Block 3 Day 3\n\n\n\nA local college administrator wants to know the average income of all graduates in the last 10 years. So they get the records of five randomly chosen graduates, contact them, and obtain their answers."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-8-confidence-intervals",
    "href": "LC/Learning-checks.html#chapter-8-confidence-intervals",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 8: Confidence intervals",
    "text": "Chapter 8: Confidence intervals\n\n\n\n\n\n\nLC 8.1 Block 3 Day 5\n\n\n\nWhat is the chief difference between a bootstrap distribution and a sampling distribution?\n\n\n\n\n\n\n\n\nLC 8.2 Block 3 Day 5\n\n\n\nLooking at the bootstrap distribution for the sample mean in Figure @ref(fig:one-thousand-sample-means), between what two values would you say most values lie?\n\n\n\n\n\n\n\n\nLC 8.3 Block 3 Day 6\n\n\n\nWhat condition about the bootstrap distribution must be met for us to be able to construct confidence intervals using the standard error method?\n\n\n\n\n\n\n\n\nLC 8.4 Block 3 Day 6\n\n\n\nSay we wanted to construct a 68% confidence interval instead of a 95% confidence interval for \\(\\mu\\). Describe what changes are needed to make this happen. Hint: we suggest you look at Appendix @ref(appendix-normal-curve) on the normal distribution.\n\n\n\n\n\n\n\n\nLC 8.5 Block 3 Day 8\n\n\n\nConstruct a 95% confidence interval for the median year of minting of all US pennies. Use the percentile method and, if appropriate, then use the standard-error method."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-9-hypothesis-testing",
    "href": "LC/Learning-checks.html#chapter-9-hypothesis-testing",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 9: Hypothesis testing",
    "text": "Chapter 9: Hypothesis testing\n\n\n\n\n\n\nLC 9.1 Block 4 Day 2\n\n\n\nWhy does the following code produce an error? In other words, what about the response and predictor variables make this not a possible computation with the infer package?\n\nlibrary(moderndive)\nlibrary(infer)\nnull_distribution_mean <- promotions %>%\n  specify(formula = decision ~ gender, success = \"promoted\") %>% \n  hypothesize(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\n\n\n\n\n\n\n\n\nLC 9.2 Block 4 Day 2\n\n\n\nWhy are we relatively confident that the distributions of the sample proportions will be good approximations of the population distributions of promotion proportions for the two genders?\n\n\n\n\n\n\n\n\nLC 9.3 Block 4 Day 2\n\n\n\nUsing the definition of p-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the promotion rates for males and females.\n\n\n\n\n\n\n\n\nLC 9.4 Block 4 Day 2\n\n\n\nDescribe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between the promotion rate of males and females using this study.\n\n\n\n\n\n\n\n\nLC 9.5 Block 4 Day 3\n\n\n\nWhat is wrong about saying, “The defendant is innocent.” based on the US system of criminal trials?\n\n\n\n\n\n\n\n\nLC 9.6 Block 4 Day 3\n\n\n\nWhat is the purpose of hypothesis testing?\n\n\n\n\n\n\n\n\nLC 9.7 Block 4 Day 3\n\n\n\nWhat are some flaws with hypothesis testing? How could we alleviate them?\n\n\n\n\n\n\n\n\nLC 9.8 Block 4 Day 3\n\n\n\nConsider two \\(\\alpha\\) significance levels of 0.1 and 0.01. Of the two, which would lead to a more liberal hypothesis testing procedure? In other words, one that will, all things being equal, lead to more rejections of the null hypothesis \\(H_0\\).\n\n\n\n\n\n\n\n\nLC 9.9\n\n\n\nConduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating. What was different and what was the same?\n\n\n\n\n\n\n\n\nLC 9.10\n\n\n\nWhat conclusions can you make from viewing the faceted histogram looking at rating versus genre that you couldn’t see when looking at the boxplot?\n\n\n\n\n\n\n\n\nLC 9.11\n\n\n\nDescribe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies.\n\n\n\n\n\n\n\n\nLC 9.12\n\n\n\nWhy are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres?\n\n\n\n\n\n\n\n\nLC 9.13\n\n\n\nUsing the definition of \\(p\\)-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the mean rating of romance to action movies.\n\n\n\n\n\n\n\n\nLC 9.14\n\n\n\nWhat is the value of the \\(p\\)-value for the hypothesis test comparing the mean rating of romance to action movies?\n\n\n\n\n\n\n\n\nLC 9.15\n\n\n\nTest your data wrangling knowledge and EDA skills:\n\nUse dplyr and tidyr to create the necessary data frame focused on only action and romance movies (but not both) from the movies data frame in the ggplot2movies package.\nMake a boxplot and a faceted histogram of this population data comparing ratings of action and romance movies from IMDb.\nDiscuss how these plots compare to the similar plots produced for the movies_sample data."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-10-inference-for-regression",
    "href": "LC/Learning-checks.html#chapter-10-inference-for-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 10: Inference for regression",
    "text": "Chapter 10: Inference for regression\n\n\n\n\n\n\nLC 10.1 Block 4 Day 7\n\n\n\nContinuing with our regression using age as the explanatory variable and teaching score as the outcome variable.\n\nUse the get_regression_points() function to get the observed values, fitted values, and residuals for all 463 instructors.\nPerform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here.\n\n\n\n\n\n\n\n\n\nLC 10.2 Block 4 Day 8\n\n\n\nRepeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of stat = \"correlation\" in the calculate() function of the infer package."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-11-tell-your-story-with-data",
    "href": "LC/Learning-checks.html#chapter-11-tell-your-story-with-data",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 11: Tell your story with data",
    "text": "Chapter 11: Tell your story with data\n\n\n\n\n\n\n\nLC 11.1 Block 4 Day 2\n\n\n\nRepeat the regression modeling in Subsection 11.2.3 and the prediction making you just did on the house of condition 5 and size 1900 square feet in Subsection 12.2.4, but using the parallel slopes model you visualized in Figure 11.6. Show that it’s $524,807!\n\n\n\n\n\n\n\n\nLC 11.2\n\n\n\nWhat date between 1994 and 2003 has the fewest number of births in the US? What story could you tell about why this is the case?"
  },
  {
    "objectID": "LC/LC-lesson29.html",
    "href": "LC/LC-lesson29.html",
    "title": "Learning Checks Lesson 29",
    "section": "",
    "text": "The `math300::Hill_racing” data frame records 2236 winning times (in seconds) in Scottish hill racing competitions. Consider this model of the winning time as a function of the race distance (km) and the total climb (meters):\n\nmod <- lm(time ~ distance + climb, data=Hill_racing)\n\nThe model_eval() function provides a convenient way to evaluate the model output (.output) for each of the rows in a data frame and, at the same time, calculates row-by-row residuals (.resid) and prediction errors (.lwr and .upr). Make sure to take not of the starting periods on the names.\n\nmodel_eval(mod) %>% head()\n\n  time distance climb  .output     .resid       .lwr     .upr\n1 1630        6   240 1679.215  -49.21475  -29.56279 3387.992\n2 1655        6   240 1679.215  -24.21475  -29.56279 3387.992\n3 2391        6   240 1679.215  711.78525  -29.56279 3387.992\n4 2351        6   240 1679.215  671.78525  -29.56279 3387.992\n5 4151       14   660 4805.779 -654.77947 3097.10184 6514.457\n6 3975       14   660 4805.779 -830.77947 3097.10184 6514.457\n\n\nThe RMS residual from the model can be calculated this way:\n\nmodel_eval(mod) %>%\n  summarize(rms = sqrt(mean(.resid^2)))\n\n       rms\n1 870.4631\n\n\n\nWhat are the units of the RMS residual?\nModify the calculation to compute the sum-of-square residuals. Report the result numerically. Be sure to say what are the units.\nWhat are the units of the effect size on time with respect to climb?\nWhat are the units of the effect size on time with respect to climb?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nRMS residual has the same units as the response variable. In this case, that’s the time to run the race, with units “seconds.”\nSS residual has units that are the square of the respond variable, in this case “square-seconds.”\nRecall that the effect size on the response with respect to an explanatory variable has the units of the response variable divided by the units of the explanatory variable. The climb variable has units of meters, so the effect size has units “seconds/meters.”\nseconds/km\nSHOW that the fitted model values in the model list_price ~ 1 of amazon_books is the same as the mean of list_price.\nCount the coefficients in a models and predict what the R2 will be on a shuffled version."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.b",
    "href": "LC/LC-lesson29.html#lc-29.b",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.B",
    "text": "LC 29.B\nWhich of the following models are not nested within time ~ distance + climb?\n\ntime ~ 1\ntime ~ distance + sex\ntime ~ distance\ntime ~ climb\n\n\n\n\n\n\n\nSolution\n\n\n\nThe model time ~ distance + sex is not nested in time ~ distance + climb.\nNote that time ~ 1 is indeed nested in time ~ distance + climb. The 1 corresponds to the intercept."
  },
  {
    "objectID": "LC/LC-lesson29.html#c",
    "href": "LC/LC-lesson29.html#c",
    "title": "Learning Checks Lesson 29",
    "section": "29.C",
    "text": "29.C\nIn LC -Section 1 you calculated the RMS residuals and the sum-of-square residuals by wrangling the results from mod_eval(). That’s a perfectly good way to do things, but the work becomes tedious when there are multiple models you want to compare.\nFor convenience, there is a compare_model_residuals() command, which can calculate the RMS residual or sum-of-square residual for each of a set of models. All the models must have the same response variable.\n\nHill_racing %>% \n  compare_model_residuals(time ~ 1, \n                          time ~ distance + climb, \n                          time ~ distance + climb + sex,\n                          time ~ distance,\n                          measure = \"RMS\"\n                          )\n\n[1] 1825.557 3507.627 3530.151 3410.390\n\n\nIt happens that all of the models in the command are a nested set. Re-order the models so that each model nests inside the following model, that is, from smaller model to bigger model.\n\nDo the RMS residuals for the nested models increase or decrease when moving from a smaller model to a larger model?\nYou can calculate the sum-of-square residual by using the argument measure=\"SS\". Do the sum-of-square residuals for the nested models increas or decrease when moving from a smaller model to a larger model.\nYou can calculate R2 by using the argument measure=\"R2\". Do the R2 for the nested models increase or decrease when moving from a smaller model to a larger model.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nDecrease\nDecrease\nIncrease. R2 tells you how big the model output is compared to the response variable. 1-R2 tells you how big the residuals are compared to the response variable."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.d",
    "href": "LC/LC-lesson29.html#lc-29.d",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.D",
    "text": "LC 29.D\n::: {.callout-warning} ## Still a draft\nLook at dag07. Notice that d is not connected to any of the other variables.\nGenerate a sample of size \\(n=6\\). Compare the sum of square residual (in sample) from the nested models c ~ 1, c ~ a c ~ a + b, and c ~ a + b + d. (Use the compare_model_residuals() using the argument method=\"SS\".\nWhich, if any, of the variables a, b, or d reduces the in-sample sum-of-squared residuals compared to the previous model.\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag07, c ~ 1, c ~ a, c ~ a + b, c ~ a + b + d, \n                        n=6, measure=\"R2\")\n\n[1] 0.000000 4.600684 4.903270 5.674772\n\n\n\n\nOut of sample, the useless covariate often increases the SS error."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.1",
    "href": "LC/LC-lesson29.html#lc-29.1",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.1",
    "text": "LC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50)\n\n[1] 1.0632998 0.9509876 0.9047081 1.2560676\n\n\nd seems to block effect of a and b on c.\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5)\n\n[1] 0.5530802 0.9951067 0.8465312 0.8289849"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.2",
    "href": "LC/LC-lesson29.html#lc-29.2",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.7309080 0.8497414 0.8085895 0.8019549\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, \n                        testing = \"out-of-sample\")\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 1.376011 1.812284 2.000187 1.653884\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.3",
    "href": "LC/LC-lesson29.html#lc-29.3",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.3",
    "text": "LC 29.3\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::teacher. What’s the base pay difference between a teacher with an MA and a BA degree? What’s a confidence interval on this effect size? How does the confidence interval change if you include years as a covariate."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.4",
    "href": "LC/LC-lesson29.html#lc-29.4",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.4",
    "text": "LC 29.4\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::census Predict log personal income based on other variables. Eat variance using the total_family_income variable.\n\nmod <- lm(log10(total_personal_income) ~ log10(age) + sex + marital_status + log10(total_family_income), data = openintro::census %>% filter(total_personal_income > 0, total_family_income > 0))\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: log10(total_personal_income)\n                            Df Sum Sq Mean Sq  F value    Pr(>F)    \nlog10(age)                   1  5.938  5.9383  35.6102 6.660e-09 ***\nsex                          1  5.976  5.9758  35.8351 6.006e-09 ***\nmarital_status               5  4.302  0.8604   5.1596 0.0001464 ***\nlog10(total_family_income)   1 17.620 17.6198 105.6615 < 2.2e-16 ***\nResiduals                  306 51.028  0.1668                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ngf_jitter(total_personal_income ~ total_family_income | sex, \n         data =openintro::census %>% filter(total_personal_income > 3000),\n         color=~marital_status, alpha=0.3) %>% \n  gf_refine(scale_y_log10(), scale_x_log10())\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning: Removed 20 rows containing missing values (geom_point)."
  },
  {
    "objectID": "LC/LC-lesson29.html#section",
    "href": "LC/LC-lesson29.html#section",
    "title": "Learning Checks Lesson 29",
    "section": "29.5",
    "text": "29.5\n\n\n\n\n\n\nStill in draft\n\n\n\nopenintro::starbucks where do the calories come from? Find effect size of, say, protein on calories. Then see what happens if you use carbohydrates as a covariate.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ fat + carb + fiber + protein, data = openintro::starbucks) %>% confint()\n\n                2.5 %    97.5 %\n(Intercept) -2.938079 13.605279\nfat          8.591250  9.315766\ncarb         3.686593  3.997527\nfiber       -1.418966  1.370022\nprotein      3.631695  4.364091"
  },
  {
    "objectID": "LC/LC-lesson29.html#in-draft-2",
    "href": "LC/LC-lesson29.html#in-draft-2",
    "title": "Learning Checks Lesson 29",
    "section": "In draft",
    "text": "In draft\nMaybe come back to this in confounding lesson. Look for components that tend to go together\n\nwith(openintro::starbucks, cor(fat, protein))\n\n[1] 0.22347\n\nwith(openintro::starbucks, cor(fiber, protein))\n\n[1] 0.488564\n\nlm( calories ~ fiber , data = openintro::starbucks) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 276.119106 343.80739\nfiber         1.923476  24.07453\n\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ protein + fiber, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 247.891487 320.333514\nprotein       1.700777   7.996897\nfiber        -8.099007  15.978382"
  },
  {
    "objectID": "LC/LC-lesson28.html",
    "href": "LC/LC-lesson28.html",
    "title": "Learning Checks Lesson 28",
    "section": "",
    "text": "Remove these hard-coded objectives after drafting problems\n\n\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "LC/LC-lesson28.html#section",
    "href": "LC/LC-lesson28.html#section",
    "title": "Learning Checks Lesson 28",
    "section": "28.1",
    "text": "28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution\n:::"
  },
  {
    "objectID": "LC/LC-lesson28.html#b",
    "href": "LC/LC-lesson28.html#b",
    "title": "Learning Checks Lesson 28",
    "section": "28.B",
    "text": "28.B\nWhenever you seek to study a partial relationship, there must be at least three variables involves: a response variable, an explanatory variable that is of direct interest, and one or more other explanatory variables that will be held constant: the covariates. Unfortunately, it’s hard to graph out models involving three variables on paper: the usual graph of a model just shows one variable as a function of a second.\nOne way to display the relationship between a response variable and two quantitative explanatory variables is to use a contour plot. The two explanatory variables are plotted on the axes and the fitted model values are shown by the contours. Figure 1 shows such a display of the fitted model of used car prices as a function of mileage and age.\n\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\nThe dots are the mileage and age of the individual cars — the model Price is indicated by the contours.\nThe total relationship between Price and mileage involves how the price changes for typical cars of different mileage.\n\nPick a dot that is a typical car with about 10,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price (in dollars)?\n\n18000, 21000, 25000, 30000\n\nPick another dot that is a typical car with about 70,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price?\n\n18000 21000, 25000, 30000\nThe total relationship between Price and mileage is reflected by this ratio: change in model price divided by change in mileage. What is that ratio (roughly)?\n\n\\(\\frac{30000 - 21000}{70000-10000}=0.15\\) dollars/mile\n\\(\\frac{70000-10000}{25000-21000}=15.0\\) dollars/mile\n\\(\\frac{25000 - 18000}{70000-10000}=0.12\\) dollars/mile\n\nIn contrast, the partial relationship between Price and mileage holding age constant is found in a different way, by comparing two points with different mileage but exactly the same age.\n\nMark a point on the graph where age is 3 years and mileage is\nKeep in mind that this point doesn’t need to be an actual car, that is, a data point in the graph typical car. There might be no actual car with an age of 3 years and mileage 10000. But using the contour model, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000 28000, 30000\n\nFind another point, one where the age is exactly the same (3 years) but the mileage is different. Again there might not be an actual car there. Let’s pick mileage as 80000. Using the contours, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000, 28000, 30000\n\nThe partial relationship between price and mileage (holding age constant) is reflected again reflected by the ratio of the change in model price divided by the change in mileage. What is that ratio (roughly)?\n\n\n\\(\\frac{80000-10000}{25000-21000} = 17.50\\) dollars/mile\n\\(\\frac{28000 - 22000}{80000-10000}=0.09\\) dollars/mile\n\\(\\frac{26000 - 24000}{80000-10000}=0.03\\) dollars/mile\n\n\nBoth the total relationship and the partial relationship are indicated by the slope of the model price function given by the contours. The total relationship involves the slope between two points that are typical cars, as indicated by the dots. The partial relationship involves a slope along a different direction. When holding age constant, that direction is the one where mileage changes but age does not (vertical in the graph).\nThere’s also a partial relationship between price and age holding mileage constant. That partial relationship involves the slope along the direction where age changes but mileage is held constant. Estimate that slope by finding the model price at a point where age is 2 years and another point where age is 5 years. You can pick whatever mileage you like, but it’s key that your two points be at exactly the same mileage.\n\nEstimate the slope of the price function along a direction where age changes but mileage is held constant (horizontally on the graph).\n\n\n100 dollars per year\n500 dollars per year\n1000 dollars per year\n2000 dollars per year\n\nThe contour plot in Figure 1 depicts a model in which both mileage and age are explanatory variables. By choosing the direction in which to measure the slope, one determines whether the slope reflects a total relationship (a direction between typical cars), or a partial relationship holding age constant (a direction where age does not change, which might not be typical for cars), or a partial relationship holding mileage constant (a direction where mileage does not change, which also might not be typical for cars).\nIn calculus, the partial derivative of price with respect to mileage refers to an infinitesimal change in a direction where age is held constant. Similarly, the partial derivative of price with respect to age refers to an infinitesimal change in a direction where mileage is held constant.\nOf course, in order for the directional derivatives to make sense, the price function needs to have both age and mileage as explanatory variables. Figure 2 shows a model in which only age has been used as an explanatory variable: there is no dependence of the function on mileage.\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nSuch a model is incapable of distinguishing between a partial relationship and a total relationship. Both the partial and the total relationship involve a ratio of the change in price and change in age between two points. For the total relationship, those two points would be typical cars of different ages. For the partial relationship, those two points would be different ages at exactly the same mileage. But, because the model depend on mileage, the two ratios will be exactly the same."
  },
  {
    "objectID": "LC/LC-lesson28.html#c",
    "href": "LC/LC-lesson28.html#c",
    "title": "Learning Checks Lesson 28",
    "section": "28.C",
    "text": "28.C\nIn each of the following, a situation is described and a question is asked that is to be answered by modeling. Several variables are listed. Imagine an appropriate model and identify each variable as either the response variable, an explanatory variable, a covariate, or a variable to be ignored.\n\nEXAMPLE: Some people have claimed that police foot patrols are more effective at reducing the crime rate than patrols done in automobiles. Data from several different cities is available; each city has its own fraction of patrols done by foot, its own crime rate, etc. The mayor of your town has asked for your advice on whether it would be worthwhile to shift to more foot patrols in order to reduce crime. She asks, “Is there evidence that a larger fraction of foot patrols reduces the crime rate?”\n\nVariables:\n\nCrime rate (e.g., robberies per 100000 population)\nFraction of foot patrols\nNumber of policemen per 1000 population\nDemographics (e.g., poverty rate)\n\nAnswer: The question focuses on how the fraction of foot patrols might influence crime rate, so crime rate is the response variable and fraction of foot patrols is an explanatory variable.\nBut, the crime rate might also depend on the overall level of policing (as indicated by the number of policemen), or on the social conditions that are associated with crime (e.g., demographics). Since the mayor has no power to change the demographics of your town, and probably little power to change the overall level number of policemen, in modeling the data from the different cities, you would want to hold constant number of policemen and the demographics. You can do this by treating number of policemen and demographics as covariates and including them in your model.\n\n\nAlcohol and Road Safety\nFifteen years ago, your state legislature raised the legal drinking age from 18 to 21 years. An important motivation was to reduce the number of car accident deaths due to drunk or impaired drivers. Now, some people are arguing that the 21-year age limit encourages binge drinking among 18 to 20 year olds and that such binge drinking actually increases car accident deaths. But the evidence is that the number of car accident deaths has gone down since the 21-year age restriction was introduced.\nYou are asked to examine the issue: Does the reduction in the number of car-accident deaths per year point to the effectiveness of the 21-year drinking age?\nVariables:\n\nDrinking age limit. Levels: 18 or 21.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car-accident deaths per year.\n\nWhich is it? response explanatory covariate ignore\n\nPrevalence of seat-belt use.\n\nWhich is it? response explanatory covariate ignore\n\nFraction of cars with air bags.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car accidents (with or without death).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nOf direct interest is how the drinking age limit accounts for the number of deaths, so these are, respectively, explanatory and response variables. But a lower death rate might also be explained by increased use of seat belts and of air bags; these can prevent deaths in an accident and they have been increasing over the same period in which the 21-year age limit was introduced.\nIn examining how the drinking age limit might affect the number of deaths, it might be important to hold these other factors constant. So, seat belts and air bags should be covariates included in the model.\nThe number of accidents is different. It seems plausible that the mechanism by which drunk driving causes deaths is by causing accidents. If the number of accidents were included as a covariate, then the model would be examining how the death rate changes with drinking age when {} even though the point is that the higher drinking age might reduce the number of accidents. So, number of accidents ought to be left out of the model.\n\n\n\n\nRating Surgeons\nYour state government wants to guide citizens in choosing physicians. As part of this effort, they are going to rank all the surgeons in your state. You have been asked to build the rating system and you have a set of variables available for your use. These variables have been measured for each of the 342,861 people who underwent surgery in your state last year: one person being treated by one doctor. How should you construct a rating system that will help citizens to choose the most effective surgeon for their own treatment?\nVariables:\n\nOutcome score. (A high score means that the operation did whatit was supposed to. A low score reflects failure, e.g. death. Death is a very bad outcome, post-operative infection a somewhat bad outcome.)\n\nWhich is it? response explanatory covariate ignore\n\nSurgeon. One level for each of the operating surgeons.\n\nWhich is it? response explanatory covariate ignore\n\nExperience of the surgeon.\n\nWhich is it? response explanatory covariate ignore\n\nDifficulty of the case.\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe patient has a choice of doctors and wants to have the best possible outcome. So the model needs to include surgeon as an explanatory variable and outcome score as the response.\nA simple model might be misleading for informing a patient’s choice. The best doctors might take on the most difficult cases and therefore have worse outcomes than doctors who are not as good. But the patient’s condition doesn’t change depending on what doctor is selected. This means that the difficulty of the case ought to be included as a covariate. The model would thus tell what is the typical outcome for each surgeon adjusting for the difficulty of the case, that is, given the patient’s condition.\nAnother variable that might explain the outcome is the experience of the surgeon; possibly more experienced surgeons produce better outcomes. However, experience of the surgeon\nshould not be included in the model used to inform a patient’s choice. The reason is that the patient’s choice of a doctor already reflects the experience of that doctor. From the patient’s point of view, it doesn’t matter whether the doctor’s outcomes reflect a high level of talent, a lot of experience, or superior training.\nThe choice of variables and covariates depends on the purpose of the model. If the purpose of the model were to decide how much experience to require of doctors before they are licensed, then an appropriate model would have outcome as the response, experience as the explanatory variable, and difficulty of the case and surgeon as covariates.\n\n\n\n\nSchool testing\nLast year, your school district hired a new superintendent to ``shake things up.’’ He did so, introducing several controversial new policies. At the end of the year, test scores were higher than last year. A representative of the teachers’ union has asked you to examine the score data and answer this question: Is there reason to think that the higher scores were the result of the superintendent’s new policies?\nVariables:\n\nSuperintendent (levels: New or Former superintendent)\n\nWhich is it? response explanatory covariate ignore\n\nExam difficulty\n\nWhich is it? response explanatory covariate ignore\n\nTest scores\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe issue of direct interest is whether the policies of the new superintendent might have influenced the test scores, so the model should be test scores as the response and superintendent as an explanatory variable. Of course, one possible mechanism that might have improved the scores, outside of the influence of the superintendent’s policies, is the test itself. If it were easier this year than last year, then it wouldn’t be surprising that the test scores improved this year even if the superintendent’s policies had no effect. So exam difficulty should be a covariate to be included in the model.\n\n\n\n\nGravity\nIn a bizarre twist of time, you find yourself as Galileo’s research assistant in Pisa in 1605. Galileo is studying gravity: Does gravity accelerate all materials in the same way, whether they be made of metal, wood, stone, etc.? Galileo hired you as his assistant because you have brought with you, from the 21st century, a stop-watch with which to measure time intervals, a computer, and your skill in statistical modeling. All of these seem miraculous to him.\nHe drops objects off the top of the Leaning Tower of Pisa and you measure the following:\nVariables\n\nThe size of the object (measured by its diameter).\n\nWhich is it? response explanatory covariate ignore\n\nTime of fall of the object.\n\nWhich is it? response explanatory covariate ignore\n\nThe material from which the object is made (brass, lead, wood, stone).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nGalileo wants to know how the material affects the time of fall of the object. These are the explanatory and response variables respectively. But the size of the object also has an influence, due to air resistance. For instance, a tiny ball will fall more slowly than a large ball. So the size of the object should be a covariate.\n\n\n\n##28.D\nPolling organizations often report their results in tabular form, as in Figure 3. The basic question in the poll summarized in Figure 3 asked whether the respondant agrees with the statement, “The US was a better place to live in the 1990s and will continue to decline.”\n\n\n\n\n\nFigure 3: Results from a poll conducted by Time magazine. (Source: Time, July 28, 2008, p. 41)\n\n\n\n\nThe response variable here is “pessimism.” In the report, there are three explanatory variables: race/ethnicity, income, and age. The report’s breakdown is one explanatory variable at a time, meaning that it considers “total change” rather than “change holding other factors constant.” This can be misleading when there are connections among the explanatory variables. For instance, relatively few people in the 18 to 29 age group have high incomes.\nPollsters rarely make available the raw data they collected. This is unfortunate because it prevents others from looking at the data in different ways. For the purpose of this exercise, you’ll use simulated data in the frame math300::Econ_outlook_poll. Of course, the simulation doesn’t necessarily describe people’s attitudes directly, but it does let you see how the conclusions drawn from the poll might have been different if the results for each explanatory variable had been presented in a way that adjusts for the other explanatory variables.\n\nConstruct the model pessimism ~ age - 1. Look at the coefficients and choose the statement that best reflects the results. (In case you’re wondering: The -1 is convenient when the explanatory variable is categorical. It ensures that a coefficient is reported for each level of the age variable. You’ll have to compare coefficients for different age groups to see a trend.)\nMiddle aged people have lower pessimism than young or old people.\nYoung people have the least pessimism.\nThere is no relationship between age and pessimism.\nNow construct the model pessimism ~ income - 1. Look at the coefficients and choose the statement that best reflects the results:\nHigher income people are more pessimistic than low-income people.\nHigher income people are less pessimistic than low-income people.\nThere is no relationship between income and pessimism.\nConstruct a model in which you can look at the relationship between pessimism and age while adjusting for income. That is, include income as a covariate in your model. Look at the coefficients from your model and choose the statement that best reflects the results:\nHolding income constant, older people tend to have higher levels of pessimism than young people.\nHolding income constant, young people tend to have higher levels of pessimism than old people.\nHolding income constant, there is no relationship between age and pessimism.\nYou can also interpret that same model to see the relationship between pessimism and income while adjusting for age. Which of the following statements best reflects the results? (Hint: make sure to pay attention to the sign of the coefficients.)\nHolding age constant, higher income people are more pessimistic than low-income people.\nHolding age constant, higher income people are less pessimistic than low-income people.\nHolding age constant, there is no relationship between income and pessimism."
  },
  {
    "objectID": "LC/LC-lesson28.html#e",
    "href": "LC/LC-lesson28.html#e",
    "title": "Learning Checks Lesson 28",
    "section": "28.E",
    "text": "28.E\nA study1 on drug D indicates that patients who were given the drug were less likely to recover from their condition C. Here is a table showing the overall results:\n\n\n\nDrug\n# recovered\n# died\nRecovery Rate\n\n\n\n\nGiven\n1600\n2400\n40%\n\n\nNot given\n2000\n2000\n50%\n\n\n\nStrangely, when investigators looked at the situation separately for males and females, they found that the drug improves recovery for each group:\nSex | Drug | num recovered | # died | Recovery Rate :—-::———|—–:|—–:|——: Sex | Drug | num recovered | # died | Recovery Rate Females| Given | 900 | 2100 | 30% | Not given | 200 | 800 | 20% e | | | |\nMales | Given | 700 | 300 | 70% | Not given | 1800 | 1200 | 60%\n\nAre the two tables consistent with one another in terms of the numbers reported?\nDoes the drug improve recovery or hinder recovery?\nWhat advice would you give to a physician about whether or not to prescribe the drug to her patients? Give enough of an explanation that the physician can judge whether your advice is reasonable."
  },
  {
    "objectID": "LC/LC-lesson28.html#f",
    "href": "LC/LC-lesson28.html#f",
    "title": "Learning Checks Lesson 28",
    "section": "28.F",
    "text": "28.F\nEconomists measure the inflation rate as a percent change in price per year. Unemployment is measured as the fraction (percentage) of those who want to work who are seeking jobs.\nAccording to economists, in the short run — say, from one year to another — there is a relationship between inflation and unemployment: all other things being equal, as unemployment goes up, inflation should go down. (The relationship is called the “Phillips curve,” but you don’t need to know that or anything technical about economics to answer this question.)\n\n\nIf the Phillips-curve relationship is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative\n\n\n\nBut despite the short term relationship, economists claim that In the long run — over decades — unemployment and inflation should be unrelated.\n\n\nIf the long-run theory is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative}\n\n\n\n\nThe point of this exercise is to figure out how to arrange a model so that you can study the short-term behavior of the relationship, or so that you can study the long term relationship.\nFor your reference, Figure 4 shows inflation and unemployment rates over about 30 years in the US. Each point shows the inflation and unemployment rates during one quarter of a year. The plotting symbol indicates which of three decade-long periods the point falls into.\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\nThe relationship between inflation and unemployment seems to be different from one decade to another — that’s the short term.\n\nWhich decade seems to violate the economists’ Phillips Curve short-term relationship? A, B, C, none, all\n\nUsing the modeling language, express these different possible relationships between the variables Inflation, Unemployment, and Decade, where the variable Decade is a categorical variable with the three different levels shown in the legend for the graph.\n\nInflation depends on Unemployment in a way that doesn’t change over time.\nInflation ~ Decade\n** ~ Inflation ~ Unemployment**\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation changes with the decade, but doesn’t depend on Unemployment.\n** ~ Inflation ~ Decade**\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in the same way every decade, but each decade introduces a new background inflation rate independent of Unemployment.\nInflation ~ Decade\nInflation ~ Unemployment\n** ~ Inflation ~ Unemployment + Decade**\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in a way that differs from decade to decade.\nInflation ~ Decade\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\n** ~ Inflation ~ Unemployment * Decade**\n\n\nWhether a model examines the short-term or the long-term behavior is analogous to whether a partial change or a total change is being considered.\n\nSuppose you wanted to study the long-term relationship between inflation and unemployment. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)\nNow suppose you want to study the short-term relationship. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)"
  },
  {
    "objectID": "LC/LC-lesson28.html#sec-28-G",
    "href": "LC/LC-lesson28.html#sec-28-G",
    "title": "Learning Checks Lesson 28",
    "section": "28.G",
    "text": "28.G\nConsider dag03, involving three variables: x, y, g\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nThe nature of 95% confidence intervals means that even when the true coefficient is zero, 5% of the time the confidence interval will not include zero. In a class with 100 students, around 5 will see this failure to include zero. In order to avoid those students from being fooled by such accidental effects, feel free to analyze 2 or more samples.\n\nRegretably, in the real world, when working with data that have already been collected, you can’t use such multiple samples to check your work. So there is always that 5% chance that a real effect of size zero will produce a confidence interval that doesn’t include zero. This is one reason why replication of results is useful."
  },
  {
    "objectID": "LC/LC-lesson28.html#h",
    "href": "LC/LC-lesson28.html#h",
    "title": "Learning Checks Lesson 28",
    "section": "28.H",
    "text": "28.H\nThis learning challenge is much like LC -@28-G, but uses dag11 instead of dag03.\n\nCompare the graphs of dag03 and dag11. (You can use dag_draw() to generate the graph.) Are the two DAGs equivalent or not? Describe what differences you see between the two graphs and explain whether those differences are sufficient to make the two DAGs causally different.\n\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nNow look at the graph of dag12 and speculate whether the models x ~ y and x ~ y + g will give equivalent results. (Don’t include node h in the models.) Write down your speculation. (No penalty for being wrong, it’s just a speculation!) Then, generate a sample from dag12 and, looking at the 95% confidence intervals on the coefficients, explain whether your speculation was correct or not."
  },
  {
    "objectID": "LC/LC-lesson28.html#i",
    "href": "LC/LC-lesson28.html#i",
    "title": "Learning Checks Lesson 28",
    "section": "28.I",
    "text": "28.I\nIn the hypothetical Mr. Shoebuyer’s data and model width ~ sex, 8 pairs of shoes produced a margin of error of \\(\\pm 0.8\\), estimate how many shoes you would need to reduce the margin of error to \\(\\pm 0.1\\)."
  },
  {
    "objectID": "LC/LC-lesson28.html#more-examples",
    "href": "LC/LC-lesson28.html#more-examples",
    "title": "Learning Checks Lesson 28",
    "section": "More examples",
    "text": "More examples"
  },
  {
    "objectID": "LC/LC-lesson28.html#example-sat-scores-and-school-spending",
    "href": "LC/LC-lesson28.html#example-sat-scores-and-school-spending",
    "title": "Learning Checks Lesson 28",
    "section": "Example: SAT Scores and School Spending",
    "text": "Example: SAT Scores and School Spending\nChapter 7 showed some models relating school expenditures to SAT scores. The model sat ~ 1 + expend produced a negative coefficient on expend, suggesting that higher expenditures are associated with lower test scores. Including another variable, the fraction of students who take the SAT (variable frac) reversed this relationship.\nThe model sat ~ 1 + expend + frac attempts to capture how SAT scores depend both on expend and frac. In interpreting the model, you can look at how the SAT scores would change with expend while holding frac constant. That is, from the model formula, you can study the partial relationship between SAT and expend while holding frac constant.\nThe example also looked at a couple of other fiscally related variables: student-teacher ratio and teachers’ salary. The total relationship between each of the fiscal variables and SAT was negative – for instance, higher salaries were associated with lower SAT scores. But the partial relationship, holding frac constant, was the opposite: Simpson’s Paradox.\nFor a moment, take at face value the idea that higher teacher salaries and smaller class sizes are associated with better SAT scores as indicated by the following models:\nsat = 988 + 2.18 salary - 2.78 frac\nsat = 1119 - 3.73 ratio - 2.55 frac\nIn thinking about the impact of an intervention – changing teachers’ salaries or changing the student-teacher ratio – it’s important to think about what other things will be changing during the intervention. For example, one of the ways to make student-teacher ratios smaller is to hire more teachers. This is easier if salaries are held low. Similarly, salaries can be raised if fewer teachers are hired: increasing class size is one way to do this. So, salaries and student-teacher ratio are in conflict with each other.\nIf you want to anticipate what might be the effect of a change in teacher salary while holding student-teacher ratio constant, then you should include ratio in the model along with salary (and frac, whose dominating influence remains confounded with the other variables if it is left out of the model):\nsat = 1058 + 2.55 salary - 4.64 ratio - 2.91 frac\nComparing this model to the previous ones gives some indication of the trade-off between salaries and student-teacher ratios. When ratio is included along with salary, the salary coefficient is somewhat bigger: 2.55 versus 2.18. This suggests that if salary is increased while holding constant the student-teacher ratio, salary has a stronger relationship with SAT scores than if salary is increased while allowing student-teacher ratio to vary in the way it usually does, accordingly.\nOf course, you still need to have some way to determine whether the precision in the estimate of the coefficients is adequate to judge whether the detected difference in the salary coefficient is real – 2.18 in one model and 2.55 in the other. Such issues are introduced in Chapter 12."
  },
  {
    "objectID": "LC/data/documentation.html",
    "href": "LC/data/documentation.html",
    "title": "Documentation for data sets",
    "section": "",
    "text": "Stanford_heart_study.csv: From Computational Probability and Statistics (CPS): “The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called [MISSING] was used to indicate whether or not the patient was alive at the end of the study.”\n`blood_thinner.csv” : From CPS. “Patients who underwent CPR for a heart attack and were subsequently admitted to a hospital1 were randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. There were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did.”\nloans.csv From CPS §30.5.1: “The Lending Club data set represents thousands of loans made through the Lending Club platform, which is a platform that allows individuals to lend to other individuals. Of course, not all loans are created equal. Someone who is essentially a sure bet to pay back a loan will have an easier time getting a loan with a low interest rate than someone who appears to be riskier. And for people who are very risky? They may not even get a loan offer, or they may not have accepted the loan offer due to a high interest rate. It is important to keep that last part in mind, since this data set only represents loans actually made, i.e. do not mistake this data for loan applications!”\nmedGPA.csv [actually, from Stat2Data package] from CPS §32.6: “The file MedGPA.csv in the data folder has information on medical school admission status and GPA and standardized test scores gathered on 55 medical school applicants from a liberal arts college in the Midwest.”\n\nThe variables are:\n\nAccept Status: A=accepted to medical school or D=denied admission\nAcceptance: Indicator for Accept: 1=accepted or 0=denied\nSex: F=female or M=male\nBCPM: Bio/Chem/Physics/Math grade point average\nGPA: College grade point average\nVR: Verbal reasoning (subscore)\nPS: Physical sciences (subscore)\nWS: Writing sample (subcore)\nBS: Biological sciences (subscore)\nMCAT: Score on the MCAT exam (sum of CR+PS+WS+BS)\nApps: Number of medical schools applied to\n\n\nBuild a logistic regression model to predict if a student where denied admission from GPA and Sex.\nGenerate a 95% confidence interval for the coefficient associated with GPA.\nFit a model with a polynomial of degree 2 in the GPA. Drop Sex from the model. 4. Does a quadratic fit improve the model?\nFit a model with just GPA and interpret the coefficient.\nTry to add different predictors to come up with your best model.\nGenerate a confusion matrix for the best model you have developed.\nFind a 95% confidence interval for the probability a female student with a 3.5 GPA, a BCPM of 3.8, a verbal reasoning score of 10, a physical sciences score of 9, a writing sample score of 8, a biological score of 10, a MCAT score of 40, and who applied to 5 medical schools.”\n\n\n\n\n\nFootnotes\n\n\n”Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎"
  },
  {
    "objectID": "LC/LC-lesson38.html",
    "href": "LC/LC-lesson38.html",
    "title": "Learning Checks Lesson 38",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "Objectives/Obj-lesson-26.html",
    "href": "Objectives/Obj-lesson-26.html",
    "title": "Objectives (Day 26)",
    "section": "",
    "text": "26.1 In evaluating a model function, generate a prediction interval.\n26.2 Interpret prediction bands as a series of intervals, one for each value of the model input.\n26.3 Identify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t."
  },
  {
    "objectID": "Objectives/Obj-lesson-32.html",
    "href": "Objectives/Obj-lesson-32.html",
    "title": "Objectives (Day 32)",
    "section": "",
    "text": "32.1 Properly use nomenclature of experiment.\n32.2 Correctly re-draw DAG for an ideal experimental intervention.\n32.3 Use blocking to set assignment to treatment or control."
  },
  {
    "objectID": "Objectives/Obj-lesson-33.html",
    "href": "Objectives/Obj-lesson-33.html",
    "title": "Objectives (Day 33)",
    "section": "",
    "text": "33.1. Distinguish between absolute and relative risk and identify when a change in risk is being presented as absolute or relative.\n33.2. Calculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\n33.3. Interpret effect size as stated in log odds."
  },
  {
    "objectID": "Objectives/Obj-lesson-27.html",
    "href": "Objectives/Obj-lesson-27.html",
    "title": "Objectives (Day 27)",
    "section": "",
    "text": "This is a QR day."
  },
  {
    "objectID": "Objectives/Obj-lesson-19.html",
    "href": "Objectives/Obj-lesson-19.html",
    "title": "Objectives (Day 19)",
    "section": "",
    "text": "19.1 Convert to the response vs explanatory format for data graphs.\n19.2 Understand the covering of a variable by an interval specified by a coverage level (e.g. 0.95) and make a data graphic annotated by such intervals.\n19.3 Be able to produce point plots overlaid with “violin” plots to display the density of a variable. (This should really go in the first half of the course, but ModernDive doesn’t do it.)\n19.4 Convert categorical variables to a 0-1 encoding. a. Generate graphics appropriate to a categorical encoding. (jitter plots) b. Apply modeling techniques to 0-1 encodings of binomial response variables."
  },
  {
    "objectID": "Objectives/Obj-lesson-31.html",
    "href": "Objectives/Obj-lesson-31.html",
    "title": "Objectives (Day 31)",
    "section": "",
    "text": "31.1 Distinguish “common cause” and “collider” forms of DAG.\n31.2 Construct appropriate DAG to match a narrative hypothesis."
  },
  {
    "objectID": "Objectives/Obj-lesson-25.html",
    "href": "Objectives/Obj-lesson-25.html",
    "title": "Objectives (Day 25)",
    "section": "",
    "text": "25.1 Given a data frame, construct a predictor function for a specified response variable.\n25.2 Use the predictor function to estimate prediction error and summarize with root mean square (RMS) error. Relate this to a prediction interval.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "Objectives/Obj-lesson-24.html",
    "href": "Objectives/Obj-lesson-24.html",
    "title": "Objectives (Day 24)",
    "section": "",
    "text": "24.1 Distinguish between the two settings for decision-making:\na. **Prediction**: predict an outcome for an individual\nb. **Relationship**: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\nGiven a research question, identify whether it corresponds to a prediction setting or a relationship setting.\n24.2 Estimate an effect size from a regression model of one and two variables.\n24.3 Construct a confidence interval on the effect size and evaluate whether a confidence interval indicates that estimated effect size is consistent with a specified value."
  },
  {
    "objectID": "Objectives/Obj-lesson-30.html",
    "href": "Objectives/Obj-lesson-30.html",
    "title": "Objectives (Day 30)",
    "section": "",
    "text": "30.1 Identify confounding in a DAG\n30.2 Choose whether to include covariate depending on form of DAG"
  },
  {
    "objectID": "Objectives/Obj-lesson-34.html",
    "href": "Objectives/Obj-lesson-34.html",
    "title": "Objectives (Day 34)",
    "section": "",
    "text": "34.1. Build a classifier from case-control data.\n34.2. Cross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\n34.3. Calculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome."
  },
  {
    "objectID": "Objectives/Obj-lesson-20.html",
    "href": "Objectives/Obj-lesson-20.html",
    "title": "Objectives (Day 20)",
    "section": "",
    "text": "20.1. Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\n20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples."
  },
  {
    "objectID": "Objectives/Obj-lesson-21.html",
    "href": "Objectives/Obj-lesson-21.html",
    "title": "Objectives (Day 21)",
    "section": "",
    "text": "21.1 Determine whether a proposed graph is directed and acyclic.\n21.2 Having selected a response and one or more explanatory variables, identify other DAG nodes as covariates.\n21.3 Generate data from simulations and use the data to model the relationships."
  },
  {
    "objectID": "Objectives/Obj-lesson-35.html",
    "href": "Objectives/Obj-lesson-35.html",
    "title": "Objectives (Day 35)",
    "section": "",
    "text": "35.1 Explain why case-control data may not give an proper measure of “prevalence.”\n35.2 Understand sensitivity and specificity as conditional probabilities.\n35.3 Calculate false-positive and false-negative rates for a given prevalence."
  },
  {
    "objectID": "Objectives/Obj-lesson-23.html",
    "href": "Objectives/Obj-lesson-23.html",
    "title": "Objectives (Day 23)",
    "section": "",
    "text": "23.1 Use bootstrapping to estimate sampling variation.\n23.2 Infer sampling variation from a regression table: “standard error” of a model coefficient.\n23.3 Construct and interpret confidence intervals on a model coefficient and relate the interval to the sampling distribution.\n23.4. Understand and use scaling of confidence interval length as a function of \\(n\\)."
  },
  {
    "objectID": "Objectives/Obj-lesson-37.html",
    "href": "Objectives/Obj-lesson-37.html",
    "title": "Objectives (Day 37)",
    "section": "",
    "text": "37.1 The permutation test\n37.2 Interpret correctly from regression/ANOVA reports\n37.3 Traditional names for hypothesis tests in different “textbook” settings.\n37.4. Distinguish between p-value and effect size, that is, “significance” and “substance.”"
  },
  {
    "objectID": "Objectives/Obj-lesson-36.html",
    "href": "Objectives/Obj-lesson-36.html",
    "title": "Objectives (Day 36)",
    "section": "",
    "text": "36.1 Understand and use properly hypothesis testing nomenclature: test statistic, sampling distribution under the null, Type-1 and Type-2 error, rejection threshold, p-value\n36.2 Contrast hypothesis testing versus Bayesian framework."
  },
  {
    "objectID": "Objectives/Obj-lesson-22.html",
    "href": "Objectives/Obj-lesson-22.html",
    "title": "Objectives (Day 22)",
    "section": "",
    "text": "22.1 Implement on the computer a procedure to generate a sample, calculate a regression model, and produce a summary.\n22.2 Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\n22.3 Graphically display the distribution of summaries and generate a compact numerical description of the sampling distribution."
  },
  {
    "objectID": "Objectives/Obj-lesson-38.html",
    "href": "Objectives/Obj-lesson-38.html",
    "title": "Objectives (Day 38)",
    "section": "",
    "text": "38.1 Identify signs of false discovery in a research paper.\n38.2 Estimate how overall p-value should change when study is replicated."
  },
  {
    "objectID": "Objectives/Obj-lesson-29.html",
    "href": "Objectives/Obj-lesson-29.html",
    "title": "Objectives (Day 29)",
    "section": "",
    "text": "29.1 Correctly define “covariate”.\n29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n29.4 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "Objectives/Obj-lesson-28.html",
    "href": "Objectives/Obj-lesson-28.html",
    "title": "Objectives (Day 28)",
    "section": "",
    "text": "28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error."
  },
  {
    "objectID": "objective-list.html",
    "href": "objective-list.html",
    "title": "List of objectives",
    "section": "",
    "text": "This list is assembled from the individual-lesson files in the Objectives/ directory. Make any changes in that directory.\n\n19.1 Distinguish between the two settings for decision-making:\n\nPrediction: predict an outcome for an individual\nRelationship: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\n19.2 Given a research question, identify whether it corresponds to a prediction setting or a relationship setting.\n\n20.1. Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\n20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples.\n\n21.1 Determine whether a proposed graph is directed and acyclic.\n21.2 Having selected a response and one or more explanatory variables, identify other DAG nodes as covariates.\n21.3 Generate data from simulations and use the data to model the relationships.\n\n22.1 Implement on the computer a procedure to generate a sample, calculate a regression model, and produce a summary.\n22.2 Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\n22.3 Graphically display the distribution of summaries and generate a compact numerical description of the sampling distribution.\n\n23.1 Use bootstrapping to estimate sampling variation.\n23.2 Infer sampling variation from a regression table: “standard error” of a model coefficient.\n23.3 Construct and interpret confidence intervals on a model coefficient and relate the interval to the sampling distribution.\n23.4. Understand and use scaling of confidence interval length as a function of \\(n\\).\n\n24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation.\n\n25.1 Given a sample from a DAG simulation, construct a predictor function for a specified response variable.\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error.\n\n26.1 In evaluating a model function, generate a prediction interval.\n26.2 Interpret prediction bands as a series of intervals, one for each value of the model input.\n26.3 Identify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t.\n\nThis is a QR day.\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)\n\n29.1 Correctly define “covariate”.\n29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n\n30.1 Identify confounding in a DAG\n30.2 Choose whether to include covariate depending on form of DAG\n\n31.1 Distinguish “common cause” and “collider” forms of DAG.\n31.2 Construct appropriate DAG to match a narrative hypothesis.\n\n32.1 Properly use nomenclature of experiment.\n32.2 Correctly re-draw DAG for an ideal experimental intervention.\n32.3 Use blocking to set assignment to treatment or control.\n\n33.1. Distinguish between absolute and relative risk and identify when a change in risk is being presented as absolute or relative.\n33.2. Calculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\n33.3. Interpret effect size as stated in log odds.\n\n34.1. Build a classifier from case-control data.\n34.2. Cross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\n34.3. Calculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome.\n\n35.1 Explain why case-control data may not give an proper measure of “prevalence.”\n35.2 Understand sensitivity and specificity as conditional probabilities.\n35.3 Calculate false-positive and false-negative rates for a given prevalence.\n\n36.1 Understand and use properly hypothesis testing nomenclature: test statistic, sampling distribution under the null, Type-1 and Type-2 error, rejection threshold, p-value\n36.2 Contrast hypothesis testing versus Bayesian framework.\n\n37.1 The permutation test\n37.2 Interpret correctly from regression/ANOVA reports\n37.3 Traditional names for hypothesis tests in different “textbook” settings.\n37.4. Distinguish between p-value and effect size, that is, “significance” and “substance.”\n\n38.1 Identify signs of false discovery in a research paper.\n38.2 Estimate how overall p-value should change when study is replicated."
  },
  {
    "objectID": "layout.html",
    "href": "layout.html",
    "title": "Layout of this site",
    "section": "",
    "text": "This is the proposal/layout site for the Spring 2023 revisions to Math 300, which we’re calling “Math 300R” to distinguish it from the Fall 2022 version of the course. The site contains notes and other materials for each of the (revised) lessons in Math 300R.\nAs of October 2022, the revisions are for lessons 19-37. Revisions, if any, to earlier lessons may be added. Eventually, the revised lessons and the unrevised lessons should be consolidated into a single site. This might occur in December 2022.\nThere are four main directories. In each directory, the contents are split up on a lesson-by-lesson basis.\n\nObjectives: Student-facing learning goals that make explicit the skills and understandings that students are expected to develop in the course of each lesson. These files are formatted in a specific manner that provides an ID to each objective. Software reads the files so that each objective can be referred to in any document without duplication. That is, the files in this Objectives directory are the source of truth for the objectives for each of the revised lessons. Any edits or additions to the objectives must be performed in the files in this directory.\nNTI: Lesson-by-lesson “Notes To Instructors.” These are intended to guide instructors through each lesson. They usually contain references to the objectives stored in the Objectives directory. In draft form, the NTIs typically contain notes that are to be moved eventually to the “Reading notes.”\nLC: Like Objectives, this directory contains lesson-by-lesson exercises, styled “learning checks” in the style of the Statistical Inference via Data Science textbook. Not all the learning checks need to be assigned. Which ones are assigned for each lesson will be noted in the NTIs.\nReading-notes: Textbook-like readings, also organized lesson-by-lesson (as opposed to the chapter organization typically found in textbooks.) Often, several lessons in sequence refer to the same statistical topic. Nonetheless, the reading notes are divided on a lesson basis. It is anticipated that these will be provided to students in an on-line format.\nStudent-notes: “Student notes” refers to a set of student-facing Rmd files, one for each Lesson. They are usually based on the Learning Challenges Students are expected to work through the notes, answering the questions by completing code chunks and writing short free response answers. A good daily assignment would be complete the notes and compile them to PDF format."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300Z",
    "section": "",
    "text": "Math 300Z is a proposed revision to Math 300 that will be taught for the first time in Spring 2023. The revisions apply only to Lessons 19 and up; the first 18 lessons will come from Math 300.\n\nCourse textbook: Lessons in Statistical Thinking\nR Software package: math300 can be installed with\n\n\n\n\n\n\n\nNeed to test this\n\n\n\nremotes::install_github(\"dtkaplan/Math-300R/_Package\")"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Background",
    "text": "Background\nUp through Spring 2022, Math 300 was organized around the Moore and Notz textbook: Statistics: Concepts and controversies 10/e. This book was designed for a non-technical audience of “consumers of statistics” but is dramatically outdated. For instance, it has no data science content and introduces only primitive statistical methods. A course with such shortcomings seems inappropriate for cadets going on to be officers who will inevitably have to work with modern data and methods.\nIn Fall 2022, Math 300 switched to a very different book, Ismay and Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. The ModernDive book introduces computing on data in an accessible but modern way. It is the only well-known statistics text based on a data-science perspective. Nonetheless, the statistical inference portions of the book regress to the same sort of primitive statistical methods from Concepts and Controversies.\nTo support the Fall 2022 course using ModernDive, a complete set of roughly 35 Notes to Instructors (NTI) was written by Prof. Bradley Warner along with problem sets and other needed materials and deployed for the course.\nThis proposal is for additional improvements to Math 300, building on the Fall 2022 course but replacing the statistical inference portions of the course with more contemporary and general-purpose inference techniques and support for concepts and methods relevant to decision-making.\nIn the following, I refer to three different versions of Math 300:\n\nThe Fall 2022 version of the course, using the ModernDive book, will be called Math 300.\nThe previous version of the course, as taught for several years before Fall 2022, will be called 300CC, which refers to the textbook then used, Concepts & Controversies.\nThe course proposed in this document, a revision of part of Math 300, will be called Math 300R. The R stands for “revised.”"
  },
  {
    "objectID": "index.html#overall-goals-of-math-300r",
    "href": "index.html#overall-goals-of-math-300r",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Overall goals of Math 300R",
    "text": "Overall goals of Math 300R\nThe design of a course revision needs to take into account several factors:\n\nThe target audience’s anticipated technical ability and motivation and, therefore, the appropriate pedagogy and the balance between theory and practice to use in the course.\nInstitutional goals that inform the prioritization of the topics included in the course.\nConstraints of class time and internal coherence of the course, that is, using later topics to reinforce student learning of the earlier topics.\n\nLater, in the rationale section section of this document, I describe how I came to the following conclusions, but for now, a simple statement will suffice.\n\nThe target audience is humanities and social science majors, many of whom will not be confident in the use of calculus but all of whom have had previous exposure to R in the core calculus course.\nInstitutional goals (as revealed by discussions with humanities and social science departments and the wording of the catalog description of Math 300) include a substantial emphasis on data science techniques (data wrangling and visualization) and the use of statistical concepts and methods to support decision-making."
  },
  {
    "objectID": "index.html#statistical-topics-and-framework",
    "href": "index.html#statistical-topics-and-framework",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Statistical topics and framework",
    "text": "Statistical topics and framework\nTransitioning from Math 300CC to Math 300 has already accomplished many data science goals. This proposal centers on the statistical topics/methods to be covered and the path through them.\nThe class-time demands of the new emphasis on data science techniques in Math 300 (and retained in Math 300R) dictate that the statistical concepts and methods be taught more compactly than in Math 300CC. Low-priority, legacy topics from Math 300CC should be dropped. (The GAISE report provides some guidance here.) We can use to advantage that students in Math 300 already see many modeling-related topics in Math 141Z/142Z. Since students already have a background in model-building and computing, we can choose statistical topics that relate well to decision-making.\nA traditional path for statistical methods starts with descriptive statistics (e.g., standard deviation) and then presents “1-sample” statistics (e.g., mean, proportion) and inferential techniques (confidence interval, hypothesis test) in that context. Next comes the inferential techniques for the analogous “2-sample” statistics (difference in mean, difference in proportion), followed by inference techniques for regression.\nThis path is unnecessarily long for our students since regression encompasses all the traditional methods.1 Framing statistical inference in the context of regression avoids the need to teach method-specific calculations or cover the variety of formats for non-regression test results. Regression is part of the data scientist’s standard toolbox and relates well to more advanced data techniques such as machine learning. The ModernDive textbook uses regression as the segue from the first block (about data wrangling and visualization) to the third block (about inference).\nAdditional streamlining comes from motivating statistical inference using a simulation approach. Simulation draws on two conceptually simple data operations: resampling and permutation (shuffling). This approach is well established in the statistics community and is considered by many (including the ModernDive authors) to be a better pedagogy than the traditional formula-and-distribution presentation of statistical methods. Since Math 300 (and 300R) students will already have worked with wrangling and visualization, they will be well prepared to work with the data generated by repeated simulation trials.\nThe focus on decision-making in 300R appears in the addition of new concepts and techniques treated minimally in traditional statistics courses. These include risk, prediction (and its close cousin classification), causality, and confounding. Introductory epidemiology courses provide a model for teaching about risk, causation, and confounding. The pedagogy for these topics in Math 300R comes from the epidemiology course I introduced at Macalester. In addition, Math 300R draws on my decade of experience teaching causality as part of an introductory statistics course. (See the causation chapter of my Statistical Modeling text.)\nThe Statistical Modeling pedagogy for causality uses directed acyclic graphs (DAGs) and causal simulations based on them. Unlike resampling and permutation, which re-arrange existing data, the DAG simulations generate synthetic data with specified properties (such as effect sizes). Simulations allow a concrete demonstration of the extent to which regression techniques can and cannot recover causal information from data.\nThe DAG-simulation approach lends itself naturally to the demonstration of statistical phenomena such as sampling variation and estimation of prediction error. As an example, consider the statistical fallacy of regression to the mean, as with Galton’s finding about comparing children’s heights to their parents’. The natural hypothesis that heights are determined by genetic and other factors is represented by this DAG:\n\\[\\epsilon \\rightarrow parent \\longleftarrow GENES \\longrightarrow child \\leftarrow \\nu\\] In this DAG there is no causal mechanism included for “regression to the mean.” However, Galton’s empirical finding is replicated by data from the DAG-simulation."
  },
  {
    "objectID": "index.html#sec-broad-structure",
    "href": "index.html#sec-broad-structure",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Scope of the proposed changes",
    "text": "Scope of the proposed changes\nMath 300R will retain the first 17 lessons of Math 300. All teaching materials for this part of the course will be used unaltered. (Exception: revisions to Math 300 the Fall 2022 teaching team deems appropriate. Such revisions are not part of this proposal.)\nThe following 19 lessons are entirely refactored and based on new readings, NTIs, exams, and other materials. Objectives for each of these 19 lessons are itemized here.\n\nThe corresponding ModernDive chapters are not used in Math 300R.\nThe software used is the same as that used in the first half of the ModernDive book, specifically the ggplot2 graphics package and the tidyverse data wrangling packages. However, the infer package used in the second half of ModernDive is dropped.\n\nThe theme of the refactored 19 lessons is “informing decisions with data.” Statistical approaches that can inform decision-making include anticipating the impact of interventions, predicting individual outcomes, and the quantification of risk. These are all included in Math 300R.\nTopics to be de-emphasized are the algebra of computing confidence intervals and p-values and the (controversial) role of p-values as a guide to practical “significance.” About half of a traditional course is about the construction of confidence intervals in various settings and, more or less equivalently, the conversion of data into p-values. However, in the contemporary era, when “observational” data are collected en masse, p-values can become very small (“significant”) even when the relationship under study is slight and insubstantial.\nConfounding and methods for dealing with it (statistical adjustment, experiment) are treated substantially in Math 300R. Decision-making about interventions often relies on understanding causal effects. The possibility of confounding is a major source of skepticism about making causal judgments. In a world where much data is observational, the sweeping principles that “correlation is not causation” and “no causation without experimentation” do not support making responsible conclusions about causal connections and the need to make decisions even when data cannot provide a definitive answer. Decision-makers need this support."
  },
  {
    "objectID": "index.html#rationale",
    "href": "index.html#rationale",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Rationale for course revisions",
    "text": "Rationale for course revisions\n\nRelationship to Math 357 and Math 377\nDFMS offers three courses satisfying the statistics component of the Academy’s core requirements: Math 300, Math 357, and Math 377. In designing 300R, attention should be paid to the reasons for supporting three distinct courses. The catalog copy lays out the differences in terms of intended student major, software, mathematical background, and orientation to data science.\nIntended student major: The catalog says, “Math 300 is designed primarily for majors in the Social Sciences and Humanities.” while “Math 356 is primarily designed for cadets in engineering, science, or other technical disciplines. Math majors and Operations Research majors will take Math 377.” Math 377 is also the intended course for prospective Data Science majors, although this is not in the catalog.\nSoftware: The catalog does not describe any software component for either Math 300 or Math 357, but states that, in Math 377, “modern software appropriate for data analysis will be used.” In reality, as of Fall 2022, much the same software is used in all three courses: R with the dplyr package for data wrangling, ggplot2 for data visualization, and “R/Markdown” for creating computationally active documents.\nOne difference between Math 300 and 357/377 relates to computer programming. Both 357 and 377 include content about the underlying structure of the R language, object types, the construction of functions, and arrays and iteration. In contrast, Math 300 is based on a small set of command patterns using data frames. Students see R in Math 300 more or less as an extension of what they learned in 141Z/142Z; what’s added is a few statistical and data-wrangling functions and a handful of new graphics types.\nStudents’ mathematical background: Math 377 explicitly refers to “calculus-based probability.” Math 300 and 357 share identical catalog copy, though in reality Math 357 and Math 377 use the same textbook. Calculus is indeed necessary for the probability topics in Math 357 and 377. My interpretation is that Math 300 should serve as a safe haven for those who lack confidence in their calculus skills. Both the Fall 2022 edition of Math 300 and the proposed Math 300R serve this role as safe haven.\nOrientation to Data Science: Starting with the Fall 2022 edition, Math 300 develops and draws on data-science skills for wrangling and visualization. In this, the new Math 300 is in line with both Math 357 and 377.\nThe above analysis indicates that Math 300 and 300R should diverge from Math 357/377 in these ways:\n\nMath 300R should make little or no use of calculus operations.\nMath 300R should include little consideration of probability distributions or (non-automated) calculations with any but the simplest.\nMath 300R should be computational, but should not draw heavily on computer programming skills such as types of objects, arrays, indexing, and loop-style iteration. Use of R/Markdown documents should be considered as a pedagogical choice, and retained or discontinued based on how it contributes to student success in the other areas of the course.\n\nIn addition, I suggest that …\n\nMath 300R include some work with assembling/curating data using spreadsheets and basic data cleaning with spreadsheets. Awareness of the ubiquity of data errors and a basic understanding of how to deal with such errors is an important component of working with data. (This is not to suggest that data analysis, modeling, and graphical depiction be taught using spreadsheets, which are notoriously unreliable, difficult, and limiting for such purposes. Spreadsheets are, however, appropriate for the phase where non-tabular data is transcribed into a tabular arrangement.)\n\n\n\nInstitutional goals\nIt can be difficult to translate broadly stated institutional goals to apply them to a single course. However, catalog descriptions of programs and individual courses provide some assistance. Here is the catalog copy for Math 300 (which is identical to the catalog description of Math 357).\n\nMath 300. Introduction to Statistics. An introduction in probability and statistics for decision-makers. Topics include basic probability, statistical inference, prediction, data visualization, and data management. This course emphasizes critical thinking among decision-makers, preparing future officers to be critical consumers of data. (Emphasis added.)\n\nI interpret the final sentence as a description of the overall objective of the course:\n\nOverall objective: Prepare officers to use data to inform decisions.\n\nReturning to the idea that the topics listed in the catalog copy ought to be interpreted as serving the overall objective of the course, let’s consider those topics one at a time:\n\ndata management\ndata visualization\nprediction\nstatistical inference\nbasic probability\n\n\nStrictly speaking, as a term of art the phrase “data management” is business jargon describing enterprise-level activities that are unrelated to the other items on the list. It would be unheard of to include it, in this strict sense, in a statistics course. I believe the intent of the phrase to be better served by terms like “data wrangling,” “data cleaning,” “database querying,” and such which make up an important part of “data science.” Data wrangling is a major feature of Math 300 launched and is covered using professional level computing tools well suited to both small and large data. But whatever “data management” might reasonably be taken to mean, it was utterly ignored in Math 300CC.\n“Data visualization” is generally taken to be the process of using graphics to discover and highlight patterns shown in data. Math 300CC included only statistical graphics such as histograms, box-and-whisker plots, and basic “scatter plots.” Math 300 adds to this modern modes of graphics such as transparency, color, and faceting that make it possible to display relationships among multiple variables. The software used in Math 300 is the professional-level ggplot2 which provides the ability to increase the sophistication and generality of data display, using for example density graphics such as violin plots. As such, Math 300 is a big step on the road to rich data visualization. Some of these will be introduced in Math 300R in the second half of the course.\n“Prediction” is a central paradigm used in the important area of “machine learning.” It is also an often used method used to inform decision making and characterize risk, for instance, by indicating the distribution of plausible outcomes. Math 300CC emphasized paradigms such as hypothesis testing and confidence intervals that are not aligned with making and interpreting predictions. Math 300 focuses on these same paradigms. Math 300R will treat prediction as a central statistical path, as well as highlighting its proper use, interpretation, and evaluation.\n“Statistical inference” is traditionally taken to mean the calculation and interpretation of hypothesis tests and confidence intervals in various simple settings. Such settings include the “difference between two means,” the “correlation coefficient,” and the “slope of a regression line.” Math 300CC introduced a handful of such settings, providing distinct formulas for each of them. The “controversies” referred to in the title Concepts and controversies includes the problematic interpretation of “p-values” and the need to use random sampling and/or random assignment in data collection to get “correct” results. Math 300 retains the emphasis on confidence intervals and p-values in the simple settings, but emphasizes a more general and accessible methodology based on bootstrapping and permutation tests.\n\nUnfortunately, appealing to random sampling/assignment is often whistling past the graveyard, since these idealized data collection processes are rarely available. Instead, professionals include “covariates” in their data collection in order to “adjust” for the factors that would have been scrambled into insignificance by random sampling/assignment if it had been available. Math 300R incorporates covariate methods and highlights the importance of identifying appropriate covariates.\n\n“Basic probability” can mean different things to different people. In most introductory statistics courses it refers to the construction, calculation, and study of named distributions such as the binomial, normal, chi-squared, t, etc. Such distributions play an important role in the statistical theory of confidence intervals and hypothesis testing. That is, they are support for statistical inference. Math 300CC followed the traditional pattern of having students memorize which distribution is relevant to which setting and using printed tables for calculation. As described earlier, Math 300 provides a much more natural route to inference through bootstrapping and permutation tests.\n\nWhat’s left out in this conception of basic probability is the support for decision making. Essential to this is the proper use of “conditional probability.” Math 300R emphasizes appropriate use and interpretation of conditional probability, seen most clearly in the “classifiers” part of the course.\n\n\nFaculty opinions\nInsofar as faculty internalize the goals of the institution, their views can point to ways that existing courses do and do not reflect those goals.\nWithin DFMS and other departments, there is a general discontent that Math 300CC was not doing what it ought to. Reasons for this can be seen by examining the textbook used in Math 300CC. The book has clear deficiencies, among which are:\n\nthe material is out of date and does not reflect any of the consensus recommendations (such as GAISE) developed in the last 30 years.\nit does not use data at any level beyond hand calculation.\nit does not deal with decision making at any serious level. (The only decision formally supported is whether or not to reject the Null hypothesis.)\n\nThe opinions of faculty outside DFMS can also be an important guide to institutional priorities. In AY 2021-22 I contacted the departments in the social sciences and humanities. Three of these—history, political science, economics—responded with interest. Discussion with groups of faculty from these departments elucidated a number of points:\n\nThe faculty most highly valued the development of data-science skills such as computing for data wrangling and data visualization.\nThe then-current version of Math 300 did not contribute to the development of such skills.\nMath 357 is not seen as an appropriate alterative to Math 300, both because of perceived difficulty of 357 and because faculty do not value the emphasis on probability distributions seen in 357.\n\nFrom my experience at Macalester and in conducting reviews at many colleges, I am often wary of the motivation of faculty in other departments. These can represent a desire for service courses like Math 300 to cover discipline-specific techniques. However, the faculty I spoke to also had an eye on what their students will need for their post-graduation jobs. Particularly the USAF officers drew on their field experience in areas such as military intelligence.\nBased on these findings, the group of faculty planning for revisions to Math 300 made an easy decision: replace the textbook with one oriented to data science. We selected the ModernDive book, which is unique among introductory statistics textbooks in starting out with data wrangling and visualization. This change of textbook addresses the “use data” part of the course objective stipulated above.\nThe other part of the objective—inform decisions—remains problematic even with the switch in the Math 300 textbook. Discussions I had with the ModernDive authors made clear that their purpose in writing the book was to provide a way to introduce data science into introductory statistics, but that they stuck to the traditional hypothesis-testing/confidence-interval framework in order not to make the change too daunting for instructors thinking of adopting the text. In other words, they were not trying to turn the topic toward decision-making with data, the motivation of the ideas presented in this proposal for Math 300R."
  },
  {
    "objectID": "index.html#plan-of-work",
    "href": "index.html#plan-of-work",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Plan of work",
    "text": "Plan of work\n\nEarly October 2022: Preliminary approval, with appropriate modifications, of the proposed objectives.\nOctober 2022: DTK will draft new day-by-day NTIs for the second half of the course in the same style as the existing NTIs for the first half of the course. In the process of drafting, there will likely be some re-arrangement and modification of the objectives in (1).\nNovember 2022: With the draft NTIs in hand, a faculty team will make a more detailed examination of the proposed objectives. I recommend that this examination be structured as a set of hour-long discussions, one for each of the five divisions described in ?@sec-topics.\nNovember/December 2022: DTK (and others, as interested) will assemble student readings to replace the second half of ModernDive. Much of the content already exists in the form of a draft textbook by DTK. These will be re-arranged to correspond to the day-to-day objectives as determined in (3).\nJanuary/February 2023: The first 18 lessons of 300R will be taught as a repeat of those lessons from Math 300 Fall 2022. DTK will participate mainly as an observer.\nJanuary/February 2023: Revision and refinement will be made of the readings and NTIs in (3) and (4) above.\nMarch/April 2023: Teaching the new lessons. DTK will participate as an instructor for these lessons."
  },
  {
    "objectID": "index.html#need-to-test-this",
    "href": "index.html#need-to-test-this",
    "title": "Math 300Z",
    "section": "Need to test this",
    "text": "Need to test this\nremotes::install_github(\"dtkaplan/Math-300R/_Package\")"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Proposal",
    "section": "",
    "text": "Up through Spring 2022, Math 300 was organized around the Moore and Notz textbook: Statistics: Concepts and controversies 10/e. This book was designed for a non-technical audience of “consumers of statistics” but is dramatically outdated. For instance, it has no data science content and introduces only primitive statistical methods. A course with such shortcomings seems inappropriate for cadets going on to be officers who will inevitably have to work with modern data and methods.\nIn Fall 2022, Math 300 switched to a very different book, Ismay and Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. The ModernDive book introduces computing on data in an accessible but modern way. It is the only well-known statistics text based on a data-science perspective. Nonetheless, the statistical inference portions of the book regress to the same sort of primitive statistical methods from Concepts and Controversies.\nTo support the Fall 2022 course using ModernDive, a complete set of roughly 35 Notes to Instructors (NTI) was written by Prof. Bradley Warner along with problem sets and other needed materials and deployed for the course.\nThis proposal is for additional improvements to Math 300, building on the Fall 2022 course but replacing the statistical inference portions of the course with more contemporary and general-purpose inference techniques and support for concepts and methods relevant to decision-making.\nIn the following, I refer to three different versions of Math 300:\n\nThe Fall 2022 version of the course, using the ModernDive book, will be called Math 300.\nThe previous version of the course, as taught for several years before Fall 2022, will be called 300CC, which refers to the textbook then used, Concepts & Controversies.\nThe course proposed in this document, a revision of part of Math 300, will be called Math 300R. The R stands for “revised.”"
  },
  {
    "objectID": "proposal.html#overall-goals-of-math-300r",
    "href": "proposal.html#overall-goals-of-math-300r",
    "title": "Proposal",
    "section": "Overall goals of Math 300R",
    "text": "Overall goals of Math 300R\nThe design of a course revision needs to take into account several factors:\n\nThe target audience’s anticipated technical ability and motivation and, therefore, the appropriate pedagogy and the balance between theory and practice to use in the course.\nInstitutional goals that inform the prioritization of the topics included in the course.\nConstraints of class time and internal coherence of the course, that is, using later topics to reinforce student learning of the earlier topics.\n\nLater, in the rationale section section of this document, I describe how I came to the following conclusions, but for now, a simple statement will suffice.\n\nThe target audience is humanities and social science majors, many of whom will not be confident in the use of calculus but all of whom have had previous exposure to R in the core calculus course.\nInstitutional goals (as revealed by discussions with humanities and social science departments and the wording of the catalog description of Math 300) include a substantial emphasis on data science techniques (data wrangling and visualization) and the use of statistical concepts and methods to support decision-making."
  },
  {
    "objectID": "proposal.html#statistical-topics-and-framework",
    "href": "proposal.html#statistical-topics-and-framework",
    "title": "Proposal",
    "section": "Statistical topics and framework",
    "text": "Statistical topics and framework\nTransitioning from Math 300CC to Math 300 has already accomplished many data science goals. This proposal centers on the statistical topics/methods to be covered and the path through them.\nThe class-time demands of the new emphasis on data science techniques in Math 300 (and retained in Math 300R) dictate that the statistical concepts and methods be taught more compactly than in Math 300CC. Low-priority, legacy topics from Math 300CC should be dropped. (The GAISE report provides some guidance here.) We can use to advantage that students in Math 300 already see many modeling-related topics in Math 141Z/142Z. Since students already have a background in model-building and computing, we can choose statistical topics that relate well to decision-making.\nA traditional path for statistical methods starts with descriptive statistics (e.g., standard deviation) and then presents “1-sample” statistics (e.g., mean, proportion) and inferential techniques (confidence interval, hypothesis test) in that context. Next comes the inferential techniques for the analogous “2-sample” statistics (difference in mean, difference in proportion), followed by inference techniques for regression.\nThis path is unnecessarily long for our students since regression encompasses all the traditional methods.1 Framing statistical inference in the context of regression avoids the need to teach method-specific calculations or cover the variety of formats for non-regression test results. Regression is part of the data scientist’s standard toolbox and relates well to more advanced data techniques such as machine learning. The ModernDive textbook uses regression as the segue from the first block (about data wrangling and visualization) to the third block (about inference).\nAdditional streamlining comes from motivating statistical inference using a simulation approach. Simulation draws on two conceptually simple data operations: resampling and permutation (shuffling). This approach is well established in the statistics community and is considered by many (including the ModernDive authors) to be a better pedagogy than the traditional formula-and-distribution presentation of statistical methods. Since Math 300 (and 300R) students will already have worked with wrangling and visualization, they will be well prepared to work with the data generated by repeated simulation trials.\nThe focus on decision-making in 300R appears in the addition of new concepts and techniques treated minimally in traditional statistics courses. These include risk, prediction (and its close cousin classification), causality, and confounding. Introductory epidemiology courses provide a model for teaching about risk, causation, and confounding. The pedagogy for these topics in Math 300R comes from the epidemiology course I introduced at Macalester. In addition, Math 300R draws on my decade of experience teaching causality as part of an introductory statistics course. (See the causation chapter of my Statistical Modeling text.)\nThe Statistical Modeling pedagogy for causality uses directed acyclic graphs (DAGs) and causal simulations based on them. Unlike resampling and permutation, which re-arrange existing data, the DAG simulations generate synthetic data with specified properties (such as effect sizes). Simulations allow a concrete demonstration of the extent to which regression techniques can and cannot recover causal information from data.\nThe DAG-simulation approach lends itself naturally to the demonstration of statistical phenomena such as sampling variation and estimation of prediction error. As an example, consider the statistical fallacy of regression to the mean, as with Galton’s finding about comparing children’s heights to their parents’. The natural hypothesis that heights are determined by genetic and other factors is represented by this DAG:\n\\[\\epsilon \\rightarrow parent \\longleftarrow GENES \\longrightarrow child \\leftarrow \\nu\\] In this DAG there is no causal mechanism included for “regression to the mean.” However, Galton’s empirical finding is replicated by data from the DAG-simulation."
  },
  {
    "objectID": "proposal.html#sec-broad-structure",
    "href": "proposal.html#sec-broad-structure",
    "title": "Proposal",
    "section": "Scope of the proposed changes",
    "text": "Scope of the proposed changes\nMath 300R will retain the first 17 lessons of Math 300. All teaching materials for this part of the course will be used unaltered. (Exception: revisions to Math 300 the Fall 2022 teaching team deems appropriate. Such revisions are not part of this proposal.)\nThe following 19 lessons are entirely refactored and based on new readings, NTIs, exams, and other materials. Objectives for each of these 19 lessons are itemized here.\n\nThe corresponding ModernDive chapters are not used in Math 300R.\nThe software used is the same as that used in the first half of the ModernDive book, specifically the ggplot2 graphics package and the tidyverse data wrangling packages. However, the infer package used in the second half of ModernDive is dropped.\n\nThe theme of the refactored 19 lessons is “informing decisions with data.” Statistical approaches that can inform decision-making include anticipating the impact of interventions, predicting individual outcomes, and the quantification of risk. These are all included in Math 300R.\nTopics to be de-emphasized are the algebra of computing confidence intervals and p-values and the (controversial) role of p-values as a guide to practical “significance.” About half of a traditional course is about the construction of confidence intervals in various settings and, more or less equivalently, the conversion of data into p-values. However, in the contemporary era, when “observational” data are collected en masse, p-values can become very small (“significant”) even when the relationship under study is slight and insubstantial.\nConfounding and methods for dealing with it (statistical adjustment, experiment) are treated substantially in Math 300R. Decision-making about interventions often relies on understanding causal effects. The possibility of confounding is a major source of skepticism about making causal judgments. In a world where much data is observational, the sweeping principles that “correlation is not causation” and “no causation without experimentation” do not support making responsible conclusions about causal connections and the need to make decisions even when data cannot provide a definitive answer. Decision-makers need this support."
  },
  {
    "objectID": "proposal.html#rationale",
    "href": "proposal.html#rationale",
    "title": "Proposal",
    "section": "Rationale for course revisions",
    "text": "Rationale for course revisions\n\nRelationship to Math 357 and Math 377\nDFMS offers three courses satisfying the statistics component of the Academy’s core requirements: Math 300, Math 357, and Math 377. In designing 300R, attention should be paid to the reasons for supporting three distinct courses. The catalog copy lays out the differences in terms of intended student major, software, mathematical background, and orientation to data science.\nIntended student major: The catalog says, “Math 300 is designed primarily for majors in the Social Sciences and Humanities.” while “Math 356 is primarily designed for cadets in engineering, science, or other technical disciplines. Math majors and Operations Research majors will take Math 377.” Math 377 is also the intended course for prospective Data Science majors, although this is not in the catalog.\nSoftware: The catalog does not describe any software component for either Math 300 or Math 357, but states that, in Math 377, “modern software appropriate for data analysis will be used.” In reality, as of Fall 2022, much the same software is used in all three courses: R with the dplyr package for data wrangling, ggplot2 for data visualization, and “R/Markdown” for creating computationally active documents.\nOne difference between Math 300 and 357/377 relates to computer programming. Both 357 and 377 include content about the underlying structure of the R language, object types, the construction of functions, and arrays and iteration. In contrast, Math 300 is based on a small set of command patterns using data frames. Students see R in Math 300 more or less as an extension of what they learned in 141Z/142Z; what’s added is a few statistical and data-wrangling functions and a handful of new graphics types.\nStudents’ mathematical background: Math 377 explicitly refers to “calculus-based probability.” Math 300 and 357 share identical catalog copy, though in reality Math 357 and Math 377 use the same textbook. Calculus is indeed necessary for the probability topics in Math 357 and 377. My interpretation is that Math 300 should serve as a safe haven for those who lack confidence in their calculus skills. Both the Fall 2022 edition of Math 300 and the proposed Math 300R serve this role as safe haven.\nOrientation to Data Science: Starting with the Fall 2022 edition, Math 300 develops and draws on data-science skills for wrangling and visualization. In this, the new Math 300 is in line with both Math 357 and 377.\nThe above analysis indicates that Math 300 and 300R should diverge from Math 357/377 in these ways:\n\nMath 300R should make little or no use of calculus operations.\nMath 300R should include little consideration of probability distributions or (non-automated) calculations with any but the simplest.\nMath 300R should be computational, but should not draw heavily on computer programming skills such as types of objects, arrays, indexing, and loop-style iteration. Use of R/Markdown documents should be considered as a pedagogical choice, and retained or discontinued based on how it contributes to student success in the other areas of the course.\n\nIn addition, I suggest that …\n\nMath 300R include some work with assembling/curating data using spreadsheets and basic data cleaning with spreadsheets. Awareness of the ubiquity of data errors and a basic understanding of how to deal with such errors is an important component of working with data. (This is not to suggest that data analysis, modeling, and graphical depiction be taught using spreadsheets, which are notoriously unreliable, difficult, and limiting for such purposes. Spreadsheets are, however, appropriate for the phase where non-tabular data is transcribed into a tabular arrangement.)\n\n\n\nInstitutional goals\nIt can be difficult to translate broadly stated institutional goals to apply them to a single course. However, catalog descriptions of programs and individual courses provide some assistance. Here is the catalog copy for Math 300 (which is identical to the catalog description of Math 357).\n\nMath 300. Introduction to Statistics. An introduction in probability and statistics for decision-makers. Topics include basic probability, statistical inference, prediction, data visualization, and data management. This course emphasizes critical thinking among decision-makers, preparing future officers to be critical consumers of data. (Emphasis added.)\n\nI interpret the final sentence as a description of the overall objective of the course:\n\nOverall objective: Prepare officers to use data to inform decisions.\n\nReturning to the idea that the topics listed in the catalog copy ought to be interpreted as serving the overall objective of the course, let’s consider those topics one at a time:\n\ndata management\ndata visualization\nprediction\nstatistical inference\nbasic probability\n\n\nStrictly speaking, as a term of art the phrase “data management” is business jargon describing enterprise-level activities that are unrelated to the other items on the list. It would be unheard of to include it, in this strict sense, in a statistics course. I believe the intent of the phrase to be better served by terms like “data wrangling,” “data cleaning,” “database querying,” and such which make up an important part of “data science.” Data wrangling is a major feature of Math 300 launched and is covered using professional level computing tools well suited to both small and large data. But whatever “data management” might reasonably be taken to mean, it was utterly ignored in Math 300CC.\n“Data visualization” is generally taken to be the process of using graphics to discover and highlight patterns shown in data. Math 300CC included only statistical graphics such as histograms, box-and-whisker plots, and basic “scatter plots.” Math 300 adds to this modern modes of graphics such as transparency, color, and faceting that make it possible to display relationships among multiple variables. The software used in Math 300 is the professional-level ggplot2 which provides the ability to increase the sophistication and generality of data display, using for example density graphics such as violin plots. As such, Math 300 is a big step on the road to rich data visualization. Some of these will be introduced in Math 300R in the second half of the course.\n“Prediction” is a central paradigm used in the important area of “machine learning.” It is also an often used method used to inform decision making and characterize risk, for instance, by indicating the distribution of plausible outcomes. Math 300CC emphasized paradigms such as hypothesis testing and confidence intervals that are not aligned with making and interpreting predictions. Math 300 focuses on these same paradigms. Math 300R will treat prediction as a central statistical path, as well as highlighting its proper use, interpretation, and evaluation.\n“Statistical inference” is traditionally taken to mean the calculation and interpretation of hypothesis tests and confidence intervals in various simple settings. Such settings include the “difference between two means,” the “correlation coefficient,” and the “slope of a regression line.” Math 300CC introduced a handful of such settings, providing distinct formulas for each of them. The “controversies” referred to in the title Concepts and controversies includes the problematic interpretation of “p-values” and the need to use random sampling and/or random assignment in data collection to get “correct” results. Math 300 retains the emphasis on confidence intervals and p-values in the simple settings, but emphasizes a more general and accessible methodology based on bootstrapping and permutation tests.\n\nUnfortunately, appealing to random sampling/assignment is often whistling past the graveyard, since these idealized data collection processes are rarely available. Instead, professionals include “covariates” in their data collection in order to “adjust” for the factors that would have been scrambled into insignificance by random sampling/assignment if it had been available. Math 300R incorporates covariate methods and highlights the importance of identifying appropriate covariates.\n\n“Basic probability” can mean different things to different people. In most introductory statistics courses it refers to the construction, calculation, and study of named distributions such as the binomial, normal, chi-squared, t, etc. Such distributions play an important role in the statistical theory of confidence intervals and hypothesis testing. That is, they are support for statistical inference. Math 300CC followed the traditional pattern of having students memorize which distribution is relevant to which setting and using printed tables for calculation. As described earlier, Math 300 provides a much more natural route to inference through bootstrapping and permutation tests.\n\nWhat’s left out in this conception of basic probability is the support for decision making. Essential to this is the proper use of “conditional probability.” Math 300R emphasizes appropriate use and interpretation of conditional probability, seen most clearly in the “classifiers” part of the course.\n\n\nFaculty opinions\nInsofar as faculty internalize the goals of the institution, their views can point to ways that existing courses do and do not reflect those goals.\nWithin DFMS and other departments, there is a general discontent that Math 300CC was not doing what it ought to. Reasons for this can be seen by examining the textbook used in Math 300CC. The book has clear deficiencies, among which are:\n\nthe material is out of date and does not reflect any of the consensus recommendations (such as GAISE) developed in the last 30 years.\nit does not use data at any level beyond hand calculation.\nit does not deal with decision making at any serious level. (The only decision formally supported is whether or not to reject the Null hypothesis.)\n\nThe opinions of faculty outside DFMS can also be an important guide to institutional priorities. In AY 2021-22 I contacted the departments in the social sciences and humanities. Three of these—history, political science, economics—responded with interest. Discussion with groups of faculty from these departments elucidated a number of points:\n\nThe faculty most highly valued the development of data-science skills such as computing for data wrangling and data visualization.\nThe then-current version of Math 300 did not contribute to the development of such skills.\nMath 357 is not seen as an appropriate alterative to Math 300, both because of perceived difficulty of 357 and because faculty do not value the emphasis on probability distributions seen in 357.\n\nFrom my experience at Macalester and in conducting reviews at many colleges, I am often wary of the motivation of faculty in other departments. These can represent a desire for service courses like Math 300 to cover discipline-specific techniques. However, the faculty I spoke to also had an eye on what their students will need for their post-graduation jobs. Particularly the USAF officers drew on their field experience in areas such as military intelligence.\nBased on these findings, the group of faculty planning for revisions to Math 300 made an easy decision: replace the textbook with one oriented to data science. We selected the ModernDive book, which is unique among introductory statistics textbooks in starting out with data wrangling and visualization. This change of textbook addresses the “use data” part of the course objective stipulated above.\nThe other part of the objective—inform decisions—remains problematic even with the switch in the Math 300 textbook. Discussions I had with the ModernDive authors made clear that their purpose in writing the book was to provide a way to introduce data science into introductory statistics, but that they stuck to the traditional hypothesis-testing/confidence-interval framework in order not to make the change too daunting for instructors thinking of adopting the text. In other words, they were not trying to turn the topic toward decision-making with data, the motivation of the ideas presented in this proposal for Math 300R."
  },
  {
    "objectID": "proposal.html#plan-of-work",
    "href": "proposal.html#plan-of-work",
    "title": "Proposal",
    "section": "Plan of work",
    "text": "Plan of work\n\nEarly October 2022: Preliminary approval, with appropriate modifications, of the proposed objectives.\nOctober 2022: DTK will draft new day-by-day NTIs for the second half of the course in the same style as the existing NTIs for the first half of the course. In the process of drafting, there will likely be some re-arrangement and modification of the objectives in (1).\nNovember 2022: With the draft NTIs in hand, a faculty team will make a more detailed examination of the proposed objectives. I recommend that this examination be structured as a set of hour-long discussions, one for each of the five divisions described in ?@sec-topics.\nNovember/December 2022: DTK (and others, as interested) will assemble student readings to replace the second half of ModernDive. Much of the content already exists in the form of a draft textbook by DTK. These will be re-arranged to correspond to the day-to-day objectives as determined in (3).\nJanuary/February 2023: The first 18 lessons of 300R will be taught as a repeat of those lessons from Math 300 Fall 2022. DTK will participate mainly as an observer.\nJanuary/February 2023: Revision and refinement will be made of the readings and NTIs in (3) and (4) above.\nMarch/April 2023: Teaching the new lessons. DTK will participate as an instructor for these lessons."
  }
]
[
  {
    "objectID": "Reading-notes/Reading-notes-lesson-37.html",
    "href": "Reading-notes/Reading-notes-lesson-37.html",
    "title": "Math 300R Lesson 37 Reading Notes",
    "section": "",
    "text": "Ask, and it shall be given you; seek, and ye shall find; knock, and it shall be opened unto you: For every one that asketh receiveth; and he that seeketh findeth; and to him that knocketh it shall be opened. – Matthew 7:7-8\nThe modeling techniques we’ve covered are surprisingly powerful at identifying patterns in data. With power comes responsibility. This chapter is about how spurious patterns can arise in data and processes you can use to help ensure that the patterns your models identify are genuine.\nIt’s well known that people are particularly adept at finding patterns. To see this, spend a minute or two with Figure 1, which shows x-y pairs generated by a complex mathematical procedure called the Mersenne-Twister algorithm. How many of the structures created by Mersenne-Twister algorithm can you identify by eye? Take five of the stronger-looking patterns: clusters of points, large empty areas, strings of dots, etc. Write down a list of the patterns you spotted, including the coordinate location of each, a short description (e.g. “arc of dots”), and your subjective sense of how strong or convincing that pattern is.\nWith your list in hand, look at Figure 2 at the end of this section, which displays another n = 1000 x-y pairs generated by the same mathematical procedure. You’re going to check which of the patterns you found in the testing data are confirmed by the training data. Go through your list, looking at each location where you found a pattern in the training data and checking whether a similar pattern appears at that location in the testing data.\nWere any of the patterns you saw in the training data confirmed by the testing data?\nThere’s no denying that the patterns you saw were in the data. But the Mersenne-Twister algorithm is specifically designed not to produce regular patterns. Any that you saw were accidental alignments in the particular sample of data from the algorithm.\nThe “patterns” abstractly referred to in the previous paragraphs appear in data. In data used for modeling, a pattern might be a relationship or correlation between two or more variables, or a cluster of rows in a data frame that have similar values for a response variable and explanatory variables.\nTraining models on data can encode the underlying patterns. For instance, a pattern in the data might result in a model generating detailed predictions or demonstrating a strong effect size of one variable on another.\nA valid pattern is one that steadily appears from one sample of data to another (so long as the sample is big enough). Such consistency suggests that the pattern reflects some genuine aspect of the system generating the data. A false or accidental pattern is one that appears in a sample of data, but is unlikely to show up in another sample. This inconsistency indicates that conclusions based on this pattern are unlikely to be applicable in the future or in new situations.\nThe obvious, direct way to check the validity of a pattern encoded by a model is to see if the same pattern occurs in new data, data that was not used in building the initial model. Lesson 22 took this approach by constructing a sampling distribution of a statistic such as an effect size. To create a sampling distribution, we train many models on different subsets of a data set.\nWhen working with prediction models, the sign of a valid pattern is that the quality of the predictions – perhaps measured with a root-mean-square-error or a sensitivity/specificity – remains consistent when we calculate it on new data. A prediction that shows very small error on the data used to train the model but large error on new data is not a prediction that we can rely on in new settings.\nThe historical rapid growth in data analysis activity and the construction of data sets with large numbers of explanatory variables has made it easier to capture with models both valid patterns and false patterns. This makes it important to recognize that the false detection of patterns is possible whenever you train a model, to be aware of the characteristics of models and data that make false detection more likely, and to adopt procedures to mitigate the risk that the results of your work may not generalize beyond the particular sample of data you have in hand."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-37.html#example-falsely-discovering-purchasing-habits",
    "href": "Reading-notes/Reading-notes-lesson-37.html#example-falsely-discovering-purchasing-habits",
    "title": "Math 300R Lesson 37 Reading Notes",
    "section": "Example: Falsely discovering purchasing habits",
    "text": "Example: Falsely discovering purchasing habits\nYou are a data scientist for an internet retailer, Potomac.com, which has just bought a national grocery chain, Austin Foods. You’re part of the team that is connecting the customer loyalty card data from Austin Foods with Potomac’s own large record of purchases. This is accomplished by offering a 10% Xdiscount for an item on Potomac to people who enter their Austin loyalty card number.\nPotomac’s management wants to create a cross-marketing program in which a customer shopping at Potomac will be offered coupons for Austin products. The hope is that the coupon discount will attract new customers to start shopping at Austin’s. In order for this to work, it’s best if the coupons are for products that the customer finds attractive.\nYour job is to build the coupon assignment system, that is, to figure out how to choose which products a customer is most likely to find attractive. To do this, you’ll create a set of classifiers that indicates the interest of a Potomac customer in an Austin product.\nYou’ve got data on 10,000 Potomac/Austin customers, that is, people whose records from Potomac and from Austin you can bring together. There are ten popular Austin products for which coupons can be offered. Among the 10,000 customers, about 16% have actually bought any given Austin product. You have built ten classifiers, one for each of the ten products. The input to the classifiers is 100 standard measures of a customer’s Potomac activity. The output of each classifier is the probability that the customer actually bought the corresponding Austin product.\nThe no-input classifier gives a probability of about 16% that the customer will buy the product. Management hopes that you will be able to segment the market to identify the products that a given person is much more likely to buy.\nIt’s a lot to ask of a person to sort through 100 potential explanatory variables to identify those that are predictive of buying a product. But it’s straightforward to use a model family that can learn on its own which variables are informative. You train the ten classifiers using a tree family of models.\n\nHeads up! The “data” has been created using random numbers, so that there are no actual relationships between the explanatory variables and the purchase outcomes. That is, no actual relationships aside from the accidental ones, such as the patterns encountered in Figure 1.\n\nTo illustrate how the coupon assignment system works, Table @ref(tab:some-results) shows an intermediate step in the calculation, where a probability for each of the ten products is calculated for each customer.\n\n\n\n\n\n\nTable @ref(tab:some-results) shows the output of the classifiers for just the first fifteen customers out of the 10,000 used to build the coupon selection system. For each person, all ten classifiers have been applied to estimate the probability that the person would buy each of the ten products. Highlighted in green are those products with a purchase probability greater than 40%.\nThe final output of the coupon assignment system is, for each customer, the identification of the specific products for which the probability is large. Reading Table @ref(tab:some-results), you’ll see that for person 1, product 9 merits a coupon. For person 2, products 2 and 10 merit a coupon. A winning product has not been identified for every customer, but you can’t please everyone.\n\n\n\nAttaching package: 'formattable'\n\n\nThe following objects are masked from 'package:scales':\n\n    comma, percent, scientific\n\n\n\n\n(ref:some-results-cap)\n \n\n\nCustomer ID\n\n  \n    product \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n  \n \n\n  \n    1 \n    9 \n    13 \n    10 \n    13 \n    5 \n    13 \n    8 \n    7 \n    13 \n    5 \n    44 \n    15 \n    8 \n    13 \n    13 \n  \n  \n    2 \n    16 \n    14 \n    7 \n    13 \n    11 \n    16 \n    13 \n    10 \n    11 \n    71 \n    8 \n    14 \n    73 \n    0 \n    16 \n  \n  \n    3 \n    89 \n    13 \n    10 \n    13 \n    11 \n    12 \n    100 \n    12 \n    10 \n    11 \n    10 \n    8 \n    14 \n    0 \n    9 \n  \n  \n    4 \n    9 \n    13 \n    22 \n    10 \n    14 \n    10 \n    3 \n    6 \n    12 \n    12 \n    12 \n    11 \n    14 \n    10 \n    8 \n  \n  \n    5 \n    14 \n    3 \n    10 \n    7 \n    11 \n    8 \n    11 \n    11 \n    15 \n    10 \n    11 \n    11 \n    14 \n    0 \n    8 \n  \n  \n    6 \n    16 \n    10 \n    7 \n    11 \n    64 \n    9 \n    25 \n    10 \n    8 \n    7 \n    5 \n    11 \n    11 \n    15 \n    12 \n  \n  \n    7 \n    10 \n    5 \n    9 \n    69 \n    13 \n    20 \n    13 \n    8 \n    8 \n    13 \n    73 \n    20 \n    6 \n    10 \n    13 \n  \n  \n    8 \n    14 \n    12 \n    15 \n    5 \n    4 \n    14 \n    13 \n    12 \n    4 \n    10 \n    14 \n    21 \n    14 \n    14 \n    13 \n  \n  \n    9 \n    12 \n    12 \n    14 \n    12 \n    12 \n    12 \n    14 \n    62 \n    12 \n    12 \n    80 \n    9 \n    0 \n    9 \n    12 \n  \n  \n    10 \n    9 \n    13 \n    6 \n    10 \n    12 \n    6 \n    17 \n    16 \n    6 \n    6 \n    25 \n    9 \n    5 \n    15 \n    7 \n  \n\n\n\n\n\n(ref:some-results-cap) The output of the ten classifiers for the first 15 customers. Green highlighting is used for those products which a given customer is likely to buy.\n\n\nWarning: `group_by_()` was deprecated in dplyr 0.7.0.\nℹ Please use `group_by()` instead.\nℹ See vignette('programming') for more help\nℹ The deprecated feature was likely used in the dplyr package.\n  Please report the issue at <\u001b]8;;https://github.com/tidyverse/dplyr/issues\u0007https://github.com/tidyverse/dplyr/issues\u001b]8;;\u0007>.\n\n\nTo test the performance of the system, we can look at the product/customer combinations for which a coupon was merited, and check how many of them actually corresponded to a purchase: it’s 74%. But for the combinations with no coupon, the purchase rate was only 11%.\nThe results are impressive. For about half of the customers, the coupon assignment system has identified customers/product combinations with a purchase probability of more than 40%. Often, the probability of purchase is considerably higher than 40%. Targeting each customer with a coupon for the right product is likely to generate a lot of new sales!\n\nSince data was generated using random numbers, we know that the “success” of the coupon assignment system is illusory. Later, we’ll see how the process was able to uncover so many accidental patterns from random data and list some things to look out for when modeling. But first, let’s provide a reliable method for you to identify when your results are based in accidental patterns: using testing data.\n\nA true measure of the performance of a model should be based not on the data on which the model was trained, but data which have been held back for use in testing and not used in training. For this example, we’ll use testing data consisting of 10,000 customers for whom we have the same 100 explanatory variables from the Potomac database and for whom we know if each customer purchased any of the ten products from Austin Foods. Only about 1 in 6 customers bought any single product from Austin. We want to see if the classifier assigns a high probability to those customers who did buy the product. If so, it means we can use just the 100 explanatory variables to find winning products for customers for whom we have no Austin purchasing data.\n\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\nℹ The deprecated feature was likely used in the tibble package.\n  Please report the issue at <\u001b]8;;https://github.com/tidyverse/tibble/issues\u0007https://github.com/tidyverse/tibble/issues\u001b]8;;\u0007>.\n\n\n\n\n(ref:purchase-test-cap)\n \n\n\nCustomer ID\n\n  \n    product \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n  \n \n\n  \n    1 \n    13 \n    7 \n    13 \n    17 \n    12 \n    8 \n    58 \n    13 \n    7 \n    7 \n    13 \n    9 \n    9 \n    13 \n    6 \n  \n  \n    2 \n    8 \n    19 \n    86 \n    13 \n    13 \n    9 \n    13 \n    18 \n    9 \n    75 \n    16 \n    13 \n    8 \n    13 \n    71 \n  \n  \n    3 \n    25 \n    12 \n    16 \n    0 \n    8 \n    10 \n    16 \n    10 \n    71 \n    8 \n    7 \n    80 \n    10 \n    10 \n    9 \n  \n  \n    4 \n    15 \n    13 \n    12 \n    11 \n    10 \n    9 \n    17 \n    14 \n    8 \n    3 \n    17 \n    14 \n    12 \n    11 \n    3 \n  \n  \n    5 \n    8 \n    10 \n    10 \n    38 \n    9 \n    13 \n    11 \n    10 \n    7 \n    11 \n    11 \n    9 \n    7 \n    14 \n    7 \n  \n  \n    6 \n    9 \n    10 \n    8 \n    60 \n    6 \n    4 \n    89 \n    9 \n    7 \n    11 \n    8 \n    10 \n    12 \n    8 \n    20 \n  \n  \n    7 \n    9 \n    15 \n    6 \n    20 \n    8 \n    100 \n    5 \n    10 \n    10 \n    9 \n    9 \n    13 \n    72 \n    8 \n    9 \n  \n  \n    8 \n    21 \n    12 \n    14 \n    14 \n    12 \n    12 \n    3 \n    11 \n    14 \n    13 \n    13 \n    14 \n    12 \n    12 \n    14 \n  \n  \n    9 \n    12 \n    9 \n    8 \n    9 \n    21 \n    6 \n    9 \n    71 \n    8 \n    8 \n    9 \n    8 \n    10 \n    9 \n    14 \n  \n  \n    10 \n    12 \n    16 \n    14 \n    6 \n    10 \n    12 \n    15 \n    10 \n    9 \n    17 \n    19 \n    7 \n    12 \n    9 \n    8 \n  \n\n\n\n\n\n(ref:purchase-test-cap) Similar to Table @ref(tab:some-results) but for the testing data.\n\n\n\nA valid evaluation of the performance of the system involves using the testing data rather than the training data. Figure @ref(fig:purchase-test) shows the assignment of coupons for the customers in the test data. Although coupons are assigned to these customers, the purchase rate for these items is only 16%, no different than the probability of purchase for no-coupon items. In other words, the coupon assignment system doesn’t work at all!"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-37.html#sources-of-false-discovery",
    "href": "Reading-notes/Reading-notes-lesson-37.html#sources-of-false-discovery",
    "title": "Math 300R Lesson 37 Reading Notes",
    "section": "Sources of false discovery",
    "text": "Sources of false discovery\nHow did the coupon classifier system identify so many accidental patterns, patterns that existed in the training data but not in the testing data?\nOne source of false discovery stems from having multiple potential response variables. In the Potomac/Austin example, there were ten different classifiers at work, one for each of the ten Austin products. Even if the probability of finding an accidental pattern in one classifier is small, looking in ten different places dramatically increases the odds of finding something.\nSimilarly, having a large number of explanatory variables – we had 100 in the coupon classifier – provides many opportunities for false discovery. The probability of an accidental pattern between one outcome and one explanatory variable is small, but with many explanatory variables each being considered it’s much more likely to find something.\nA third source of false discovery at work in the coupon classifier relates to the family of models selected to implement the classifier. We used a tree model classifier capable of searching through the (many) explanatory variables to find ones that are associated with the response outcome. Unbridled, the tree model is capable of very fine stratification. Each coupon classifiers stratified the customers into about 200 levels. On average, then, there were about 50 customers in each strata. But there is variation, so many of the strata are much smaller, with ten or fewer customers. The small groups were constructed by the tree-building algorithm to have similar outcomes among the members, so it’s not surprising to see a very strong pattern in each group. For each classifier, about 15% of all customers fall into a strata with 20 or fewer customers.\nTo illustrate, Figure 3 shows the shape of the tree model for a typical coupon classifier. Each of the splits reflects an accidental alignment of the response variable with one of the explanatory variables. As more splits are made, the group of customers contained in the split becomes smaller. Many of the leaves on the tree contain just a handful of customers who accidentally had similar values for the several explanatory variables used in the splits.\n\n\n\n\n\nFigure 3: A sketch of one of the classifiers constructed for the coupon selection system. The tree-growing algorithm was allowed to keep going until the customer data was split up into very small strata.\n\n\n\n\nThe tree is too complex to be plausible as a real-world mechanism. None of the details in Figure 3 have any validity beyond the training data itself."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-37.html#identifying-false-discovery",
    "href": "Reading-notes/Reading-notes-lesson-37.html#identifying-false-discovery",
    "title": "Math 300R Lesson 37 Reading Notes",
    "section": "Identifying false discovery",
    "text": "Identifying false discovery\nWe use data to build statistical models and systems such as the coupon-assignment machine. False discovery occurs when a pattern or model performance seen with one set of data does not generalize to other potential data sets.\nThe basic technique to avoid false discovery is called cross validation. One simple approach to cross validation splits the data frame into two randomly selected non-overlapping sets of rows: one for training and the other for testing. Use the training data to build the system. Use the testing data to evaluate the system’s performance.\nMost often, cross validation is used to test model prediction performance such as the root-mean-square error or the sensitivity and specificity of a classifier. This can be accomplished by taking the trained model and providing as input the explanatory variables from the testing data, then comparing the model output to the actual response variable values in the testing data. Note that using testing data in this way does not involve retraining the model on the testing data.\nHow big should the training set be compared to the testing set? For now, we’ll keep things simple and encourage use of a 50:50 split or something very close to that.\nThis is a simple and reliable approach that should always be used."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-37.html#false-discovery-and-multiple-testing",
    "href": "Reading-notes/Reading-notes-lesson-37.html#false-discovery-and-multiple-testing",
    "title": "Math 300R Lesson 37 Reading Notes",
    "section": "False discovery and multiple testing",
    "text": "False discovery and multiple testing\nWhen the main interest is in an effect size, standard procedure calls for calculating a confidence interval on the effect. For example, a 2008 study examined the possible relationship between a woman’s diet before conception and the sex of the conceived child. The popular press was particularly taken by this result from the study:\n\nWomen producing male infants consumed more breakfast cereal than those with female infants. The odds ratio for a male infant was 1.87 (95% CI 1.31, 2.65) for women who consumed at least one bowl of breakfast cereal daily compared with those who ate less than or equal to one bowlful per week. [@fetal-sex-2008]\n\nThe model here is a classifier of the sex of the baby based on the amount of breakfast cereal eaten. The effect size tells the change in the odds of a male when the explanatory variable changes from one bowlful of cereal per week to one bowl per day (or more). This effect size is sensibly reported as a ratio of the two odds. A ratio bigger than one means that boys are more likely outcomes for the one-bowl-a-day potential mother than the one-bowl-a-week potential mother. The 95% confidence interval is given as 1.31 to 2.65. This confidence interval does not contain 1. In a conventional interpretation, this provides compelling evidence that the relationship between cereal consumption and sex is not a false pattern.\nBut the confidence interval is not the complete story. The authors are clear in stating their methodology: “Data of the 133 food items from our food frequency questionnaire were analysed, and we also performed additional analyses using broader food groups.” In other words, the authors had available more than 133 potential explanatory variables. For each of these explanatory variables, the study’s authors constructed a confidence interval on the odds ratio. Most of the confidence intervals included 1, providing no compelling evidence of a relationship between that food item and the sex of the conceived child. As it happens, breakfast cereal produced the confidence interval that was the most distant from an odds ratio of 1.\nLet’s look at the range of confidence intervals that can be found from studying 100 potential random variables that are each unrelated to the response variable. We’ll simulate a response randomly generated “sex” G and B where the odds of G is 1. Similarly, each explanatory variable will be a randomly generated “consumption” high or low where the odds of high is 1. A simple stratification of sex by consumption will generate the odds of G for those cases with consumption Y and also the odds of G for those cases with consumption N. Taking the ratio of these odds gives, naturally enough, the odds ratio. We can also calculate from the stratified data a 95% confidence interval on the odds ratio.\nSo that the results will be somewhat comparable to the results in @fetal-sex-2008, we’ll use a similar sample size, that is, n = 740. Table @ref(tab:sex-consumption-1) shows one trial of the simulation.\n\n\n\n\n\n\n(ref:sex-consumption-1-cap)\n\n\n\n\n\n\nhigh\n\n\nlow\n\n\n\n\n\n\nB\n\n\n165\n\n\n182\n\n\n\n\nG\n\n\n211\n\n\n182\n\n\n\n\n\n(ref:sex-consumption-1-cap) A stratification of sex outcome (B or G) on consumption (high or low) for one trial of the simulation described in the text.\nReferring to Table @ref(tab:sex-consumption-1), you can see that the odds of G when consumption is low is 182 / 182 = 1. The odds of G when consumption is high is 211/165 = 1.28. The 95% confidence interval on the odds ratio can be calculated. It is 0.95 to 1.73. Since that includes 1, the data underlying Table @ref(tab:sex-consumption-1) provide little or no evidence for a relationship between sex and consumption. This is exactly what we expect, since the simulation involves entirely random data.\nFigure 4 shows the 95% confidence interval on the odds ratio for 133 trials like that in Table @ref(tab:sex-consumption-1). The confidence interval from each trial is shown as a horizontal line. The large majority of them include 1. That’s to be expected because the data have been generated so that sex and consumption have no relationship except those arising by chance.\n\n\nWarning: geom_vline(): Ignoring `mapping` because `xintercept` was provided.\n\n\n\n\n\nFigure 4: Confidence intervals on the odds ratio comparing female and male birth rates for many trials of simulated data with no genuine relationship between the explanatory and response variables.\n\n\n\n\nNonetheless, out of 133 simulations there are six where the confidence interval does not include 1. These are shown in red. By necessity, one of the intervals will be the most extreme. If instead of numbering the simulations, we had labelled them with food items – e.g. grapefruit, breakfast cereal, toast – we would have a situation very similar to what seems to have happened in the sex-vs-food study. (For a more detailed analysis of the impact of multiple testing in @fetal-sex-2008, see @young-2009.)\nSuppose now that half of the data used in @fetal-sex-2008 had been held back as testing data. Using the training data, it would be an entirely legitimate practice to generate hypotheses about which specific food items might be related to the sex of the baby. The validity of any one selected hypothesis could then be established using the testing data without the ambiguity introduced by multiple testing. The testing data confidence interval can be taken at face value; the training data confidence interval cannot."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-37.html#example-organic-discovery",
    "href": "Reading-notes/Reading-notes-lesson-37.html#example-organic-discovery",
    "title": "Math 300R Lesson 37 Reading Notes",
    "section": "Example: Organic discovery?",
    "text": "Example: Organic discovery?\nIt’s easy to find organic foods in many large grocery stores. Advocates of an organic diet are attracted by a view that it is sustainable, promotes small farms, and helps avoid contact with pesticides. There are also nay-sayers who make valid points, but that is not our purpose here. Informally, I find that many people and news reports point to the health benefits of an organic diet. Usually they believe that these benefits are an established fact.\nA 2018 New York Times article observed:\n\nPeople who buy organic food are usually convinced it’s better for their health, and they’re willing to pay dearly for it. But until now, evidence of the benefits of eating organic has been lacking. [@NYT-2018-10-23-Rabin]\n\nThe new evidence of health benefits is reported in an article in the Journal of the American Medical Association: Internal Medicine [@baudry-2018]\nDescribing the findings of the research, the Times article continued:\n\nEven after these adjustments [for covariates], the most frequent consumers of organic food had 76 percent fewer lymphomas, with 86 percent fewer non-Hodgkin’s lymphomas, and a 34 percent reduction in breast cancers that develop after menopause.\n\nThe study warrants being taken seriously: it involved about 70,000 French adults among whom 1340 cancers were noted. The summary of organic foot consumption was a scale from 0 to 32 and included 16 labeled products including dairy, meat and fish, eggs, coffee and tea, wine, vegetable oils, and sweets such as chocolate. Adjustment was made for a substantial number of covariates: age, sex, educational level, marital status, income, physical activity, smoking, alcohol intake, family history of cancer, body mass index, hormonal treatment for menopause, and others.\nYet … the reseach displays many of the features that can lead to false discovery. For instance, results were reported for four different types of cancer: breast, prostate, skin, lymphomas. The study reports p-values and hazard ratios1 comparing cancer rates among the four quartiles of the organic consumption index.\nComparing the most organic (average organic index 19.36/32) and the least organic (average index 0.72/32) groups the 95% confidence interval on the relative risk and p-values given in the study’s Table 4 are:\n\nBreast cancer: 0.66 - 1.16 (p = 0.38)\nProstate cancer: 0.61- 1.73 (p = 0.39)\nSkin cancer: 0.49 - 1.28 (p = 0.11)\nLymphomas: 0.07 - 0.69 (p = 0.05)\n\nYou might be surprised to see that the confidence interval on the relative risk for breast cancer includes 1.0, which suggests no evidence for an effect. As clearly stated in the report, the risk reduction for breast cancer is seen only in a subgroup of study participants: those who are postmenopausal. And even then, the confidence intervals continue to include 1.0:\n\nBreast cancer pre-menopausal: 0.67 - 1.52 (p = 0.85)\nBreast cancer post-menopausal: 0.53 - 1.18 (p = 0.18)\n\nSo where is the claimed 34% reduction in breast cancer cited in the New York Times article. It turns out the the study used two different indices of organic food consumption. The 0 to 32 scale which includes many items for which the amount consumed is very small (e.g., coffee, chocolate) and a “simplified, plant derived organic food score.” It’s only when you look at the full 0 to 32 scale that you see the reduction in post-menopausal breast cancer: the confidence interval is 0.45 to 0.96 (p = 0.03).\nWhat about cancer rates overall? For the 0 to 32 scale the risk ratio was 0.58 - 1.01 (p = 0.10). To see the claimed reduction clearly you need to look at the simplified food score which gives 0.63 - 0.89 (p < 0.005). And it’s only in comparing the highest-index quarter of participants with the lowerest quarter participants that any difference at all is seen in any type of cancer: the middle-half of participants show no difference in relative risk from the lowest-organic quarter of participants. (Because of this, had the study compared the highest quarter to the next highest quarter, they would have seen basically the same relative risks reported in the highest-to-lowest quarter comparison. Then the conclusion would have had a different flavor, perhaps to be reported as “Typical organic food consumption levels show no cancer benefits.”)\nA further source of potential false discovery stems from the study’s starting and stop times. It’s not clear that these were pre-defined; the reported results are intermediate to a longer follow up. The choice to report intermediate results is another way that the number of opportunities for false discovery is increased. And the choice is important: for the follow-up time used, about 2% of the participants developed cancer. In an earlier study of more than 600,000 middle-aged UK women (average age 59), the incidence of cancer was four times larger: 8.6%. [@bradbury-2014] That study did not find any relationship between organic food consumption and overall cancer rates, and found no relationship for 15 out of 16 different types of cancer. The exception is extremely interesting: non-Hodgkin lymphoma for which a similar result was found in the French study.\nSo is the study reported in the New York Times a matter of false discovery? It’s emotionally unsatisfying to discount a result about organic food and non-Hodgkin lymphoma simply because it’s part of a larger study that looked at many different combinations of cancer types and organic food indices. What if the researchers had only studied non-Hodgkin lymphoma – they would have gotten the same result and it wouldn’t have the deficiencies of being the strongest result of many possibilities. It would have stood on its own. But it doesn’t and we are left in a state of doubt."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-37.html#p_values",
    "href": "Reading-notes/Reading-notes-lesson-37.html#p_values",
    "title": "Math 300R Lesson 37 Reading Notes",
    "section": "p-values and “significance”",
    "text": "p-values and “significance”\nFalse discovery is not a new problem. The traditional logic can be traced back to 1710, when John Arbuthnot was examining London birth records from 1629 to 1710. Arbuthnot was surprised to find that for each year males were more common than females. In interpreting this finding, Arbuthnot refered to the conventional wisdom that births of males and females are equally likely. If this were the case, in any one year there might, by chance, be more females than males or the other way around. While it’s theoretically possible that chance might produce the string of 82 years with more males, it’s very unlikely. “From whence it follows, that it is Art, not Chance, that governs,” Arburthnot wrote. In more modern language, Arburthnot concluded that the hypothesis of equal rates of male and female births was not consistent with the data. Arbuthnot’s “Chance” corresponds to false discovery, while “Art” is a valid discovery.\nArburthnot’s logic became a standard component of statistical method.\n\nSummarize the data into a single number called a “test statistic”. For Arburthnot the test statistic was the number of years where male births predominated, out of the 82 years being examined. The observed value of the test statistic was 82.\nState a “null hypothesis”, typically something that is the conventional wisdom. For Arburthnot, the null hypothesis was that male and female births are equally likely.\nCalculate a hypothetical quantity based on the null hypothesis: the probability that the test statistic produced in a world in which the null hypothesis holds true would be at least as large as the test statistic.\nIf the probability in (3) is small, one is entitled to “reject the null hypothesis.” Typically, “small” is defined as 0.05 or less.\n\nIn the 1890s, statistical pioneer Karl Pearson invented a test statistic he called \\(\\chi^2\\) (“chi”-squared, with “chi” pronounced “ki” as in “kite”) that can be applied in a variety of settings. In 1900, Pearson published a table [@pearson-1900] that makes it an easy matter to look up the probability in step (3) above. He called this theoretical probability “P”, a name that has stuck but is conventionally written as lower-case “p”.\nData scientists tend to work with “big data”, but for many applications of statistics, data is so scarce that use of separate training and testing data is impractical. For such small data, the calculation of a p-value can be a sensible guard against false discovery. Still, a p-value does not address any of the sources of false discovery outlined in the previous sections of this chapter. When used with small data and simple modeling methods, those sources of false discovery are not so much of a problem. In small data there won’t be multiple explanatory variables that can be searched and there won’t be a choice of response variables. This doesn’t eliminate all problems, since in small data results can depend critically on the inclusion or exclusion of a single row of data. The name “p hacking” has been given to the various ways that researchers can manipulate p-values to get them below 0.05.\nAnother problem with p-values stems from misinterpretation of the admittedly difficult logic that underlies them. The misinterpretations are encouraged by the use of the term “tests of significance” to the p-value method. Particularly galling is the use of the description “statistically significant” to describe a result where p < 0.05. The everyday meaning of “significant” as something of importance is in no way justified by p < 0.05. Instead, the practical importance or not is more clearly signaled by examining an effect size. (It’s extremely disappointing that journalists, who are writing for an audience that for the most part has no understanding of p-value methodology, use “significant” when reporting on the statistics of research findings. It would be more honest to use a neutral term such as “null-validated” or “p-validated” which does not confuse the statistical result with actual practical importance.)\nThe p-value methodology has little or nothing to contribute to data science practice. When data is big there is a much more straightforward method to guard against false discovery: cross validation. And when data is big there is another, more fundamental problem with p-values. They are calculated with reference to a specific null hypothesis of “no effect” or “no relationship.” More realistically, they should be calculated with respect to a hypothesis of “trivial (but potentially non-zero) effect”. There are all sorts of mechanisms in the world (such as common causes) that can create the appearance of some effect or relationship. No matter how trivial in size this is, with sufficient data the p-value will become small. To illustrate, Figure 5 shows the p-value as a function of the sample size n in a system with an R-squared of 0.001, which in most settings would be of no practical signficance.\n\n\nWarning: geom_hline(): Ignoring `mapping` because `yintercept` was provided.\n\n\n\n\n\nFigure 5: The p-value as a function of sample size n when the test statistic R-squared has the trivial value 0.001. The horizontal line shows the usual threshold for “significance” of p < 0.05."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-37.html#notes-in-draft",
    "href": "Reading-notes/Reading-notes-lesson-37.html#notes-in-draft",
    "title": "Math 300R Lesson 37 Reading Notes",
    "section": "NOTES IN DRAFT",
    "text": "NOTES IN DRAFT\n“Statistical crisis” in science\nhttps://www.americanscientist.org/article/the-statistical-crisis-in-science\nGarden of the Forking Paths\nIonedes"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-33.html",
    "href": "Reading-notes/Reading-notes-lesson-33.html",
    "title": "Math 300R Lesson 33 Reading Notes",
    "section": "",
    "text": "We all face many yes/no situations. A patient has a disease or does not. A credit card transaction is genuine or fraudulent. A classifier is a statistical model designed to predict the unknown outcome of a yes/no situation from information that is already available.\nConsider a credit-card company might building a classifier to predict at the time of the transaction whether a purchase of gasoline is fraudulent. The company knows how often and how much gasoline the individual cardholders buys, where the cardholder lives, whether the cardholder travels extensively, typical times of day for a purchase, and so on. Feature engineering is the process of using existing data—including, in our example, whether the purchase turned out to be fraudulent—to develop potential markers or signals of the outcome. For simplicity, imagine the features selected are the number of days since the last gasoline purchase and the distance from the last place of purchase.\nOnce potential features have been proposed, the engineers building the classifier assemble training and testing data sets. Suppose, for the purpose of illustration, that the training data has 2000 fraudulent transactions and 4000 non-fraudulent ones, and the testing set is about the same.\nThe word “assemble” was used intentionally to describe how the testing and training data were collected: a case-control study. Since the objective is to detect fraud, it is reasonable to have a lot of “yes” cases in the data. The “no” cases serve as a kind of control; they were included specifically to have balance in the data. If data had been collected as a simple random sample of credit card transactions, there would have been many, many more “no” cases than “yes.”\nWith such training data it is easy to build a statistical model with Fraud as the response variable. That model can then be evaluated on the testing data to produce a model output for each row:\nIt’s understandable that a classifier may not have perfect performance. After all, it iss trying to make a prediction based on limited data, and randomness may play a role.\nThere are different ways of making a mistake, and these different ways have very different consequences. One kind of mistake, called a “false positive”, involves a classifier output that’s positive (i.e. the classifier indicates fraud) but which is wrong. The consequence of this sort of mistake in the present example is a customer who has to find another way to pay for gasoline.\nThe other kind of mistake is called a “false negative”. Here, the classifer output is that the transaction is not fraudulent, but in actuality it was. The consequence of this kind of mistake is different: a successful theft.\nThe nomenclature signals that a mistake has been made with the word “false.” The kind of mistake is either “positive” or “negative”, corresponding to the output of the classifier.\nWhen the classifier gets things right, that is a “true” result. As with the false results, a true result is possible both for a “positive” and a “negative” classifier output. So the two ways of getting things right are called “true positive” and “true negative”.\nTabulating all 6000 rows of the testing data might produce something like this:"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-33.html#incidence",
    "href": "Reading-notes/Reading-notes-lesson-33.html#incidence",
    "title": "Math 300R Lesson 33 Reading Notes",
    "section": "Incidence",
    "text": "Incidence"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-33.html#sensitivity-and-specificity",
    "href": "Reading-notes/Reading-notes-lesson-33.html#sensitivity-and-specificity",
    "title": "Math 300R Lesson 33 Reading Notes",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\nExample: Accuracy of airport security screening\n\n\n\nAirplane passengers have, for decades, gone through a security screening process involving identity checks, “no fly” lists, metal detection, imaging of baggage, random pat-downs, and such. How accurate is such screening? Almost certainly, the accuracy is not as good as an extremely simple, no-input, alternative process: automatically identify every passenger as “not a security problem.” We can estimate the accuracy of the “not a security problem” classifier by guessing what fraction of airplane passengers are indeed a threat to aircraft. In the US alone, there are about 2.5 million airplane passengers each day and security problems of any sort rarely happen. So the accuracy of the no-input classifier is something like 99.999%.\nThe actual screening system, using metal detectors, baggage x-rays, etc. will have a lower accuracy. We know this since it regularly mis-identifies innocent people as security problems.\nThe problem here is not with airport security screening, but with the flawed use of accuracy as a measure of performance. Indeed, achieving super-high accuracy is not the objective of the security screening process. Instead, the objective is to deter security problems by convincing potential terrorists that they are likely to get caught before they can get on a plane. This has to do with the sensitivity of the system. The specificity of the system, although important to the everyday traveller, is not what deters the terrorist."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-32.html",
    "href": "Reading-notes/Reading-notes-lesson-32.html",
    "title": "Math 300R Lesson 32 Reading Notes",
    "section": "",
    "text": "[From The Model Thinker, p. 52]\nGain Framing: You have two options\nOption A) Win $400 for certain\nOption B) Win $1000 if a fair coin comes up heads and $0 if tails\nLoss Framing: You are given $1000 and have two options:\nOption a) Lose $600 for certain\nOption b) Lose $0 if a fair coin comes up heads and lose $1000 if tails.\nHyperbolic discounting: see pp 52-43\n“Prospect theory”, Kahneman and Tversky (1979) “Prospect theory: an analysis of decisions under risk,” Econometrica 47(2):263-291 link to paper"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-24.html",
    "href": "Reading-notes/Reading-notes-lesson-24.html",
    "title": "Math 300R Lesson 24 Reading Notes",
    "section": "",
    "text": "SHOW makeFun() applied to a model producing a model function.\nEffect size of an input is partial derivative of model function with respect to that input.\nEffect size is a rate: the change in output per unit change in input. It’s a measure of the size of a relationship."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html",
    "href": "Reading-notes/Reading-notes-lesson-25.html",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "",
    "text": "Use mod_eval()"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#a-model-as-function",
    "href": "Reading-notes/Reading-notes-lesson-25.html#a-model-as-function",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "A model as function",
    "text": "A model as function\nInconvenient because functions don’t know how to translate from the individual arguments to values taken from a data frame."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#feeding-inputs-into-a-model",
    "href": "Reading-notes/Reading-notes-lesson-25.html#feeding-inputs-into-a-model",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "Feeding inputs into a model",
    "text": "Feeding inputs into a model"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#drawing-a-model-function",
    "href": "Reading-notes/Reading-notes-lesson-25.html#drawing-a-model-function",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "Drawing a model function",
    "text": "Drawing a model function\nEvaluate the model at many inputs, then plot out model_output versus input.\nmod_plot() automates this."
  },
  {
    "objectID": "layout.html",
    "href": "layout.html",
    "title": "Layout of this site",
    "section": "",
    "text": "As of October 2022, the revisions are for lessons 19-37. Revisions, if any, to earlier lessons may be added. Eventually, the revised lessons and the unrevised lessons should be consolidated into a single site. This might occur in December 2022.\nThere are four main directories. In each directory, the contents are split up on a lesson-by-lesson basis.\n\nObjectives: Student-facing learning goals that make explicit the skills and understandings that students are expected to develop in the course of each lesson. These files are formatted in a specific manner that provides an ID to each objective. Software reads the files so that each objective can be referred to in any document without duplication. That is, the files in this Objectives directory are the source of truth for the objectives for each of the revised lessons. Any edits or additions to the objectives must be performed in the files in this directory.\nNTI: Lesson-by-lesson “Notes To Instructors.” These are intended to guide instructors through each lesson. They usually contain references to the objectives stored in the Objectives directory. In draft form, the NTIs typically contain notes that are to be moved eventually to the “Reading notes.”\nLC: Like Objectives, this directory contains lesson-by-lesson exercises, styled “learning checks” in the style of the Statistical Inference via Data Science textbook. Not all the learning checks need to be assigned. Which ones are assigned for each lesson will be noted in the NTIs.\nReading-notes: Textbook-like readings, also organized lesson-by-lesson (as opposed to the chapter organization typically found in textbooks.) Often, several lessons in sequence refer to the same statistical topic. Nonetheless, the reading notes are divided on a lesson basis. It is anticipated that these will be provided to students in an on-line format."
  },
  {
    "objectID": "NTIs.html",
    "href": "NTIs.html",
    "title": "Notes to Instructors",
    "section": "",
    "text": "Links to NTIs will go here and in the day-to-day objectives."
  },
  {
    "objectID": "schedule-of-objectives.html",
    "href": "schedule-of-objectives.html",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "",
    "text": "As done in Fall 2020. Possible revisions to those lessons is not a topic of this proposal."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-19-decisions-with-data-nti",
    "href": "schedule-of-objectives.html#lesson-19-decisions-with-data-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 19: Decisions with data (nti)",
    "text": "Lesson 19: Decisions with data (nti)\n\nDistinguish between the two settings for decision-making:\n\nPrediction: predict an outcome for an individual\nRelationship: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\nGiven a research question, identify whether it corresponds to a prediction setting or a relationship setting."
  },
  {
    "objectID": "schedule-of-objectives.html#gaming-intro",
    "href": "schedule-of-objectives.html#gaming-intro",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 20: Reality versus gaming (nti)",
    "text": "Lesson 20: Reality versus gaming (nti)\n\nGaming: Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\nDistinguish between a sample, a summary of a sample, and a sample of summaries of samples."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-21-dags-noise-and-simulation-nti",
    "href": "schedule-of-objectives.html#lesson-21-dags-noise-and-simulation-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 21: DAGs, noise, and simulation (nti)",
    "text": "Lesson 21: DAGs, noise, and simulation (nti)\n\nDetermine whether a proposed graph is directed and acyclic.\nRead notation to identify response variable, explanatory variable, covariates, and effect sizes.\nCharacterize the magnitude of random noise.\nGaming: Generate data from simulations and use the data to model the relationships."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-22-sampling-variation-nti",
    "href": "schedule-of-objectives.html#lesson-22-sampling-variation-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 22: Sampling variation (nti)",
    "text": "Lesson 22: Sampling variation (nti)\n\nGaming: Implement on the computer a procedure to generate a sample, calculate a regression model, and produce a summary.\nGaming: Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\nGraphically display the distribution of summaries and generate a compact numerical description (“confidence interval”) of the sampling distribution.\nUnderstand and use scaling of confidence interval length as a function of \\(n\\)."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-23-estimate-sampling-variation-from-a-single-sample-nti",
    "href": "schedule-of-objectives.html#lesson-23-estimate-sampling-variation-from-a-single-sample-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 23: Estimate sampling variation from a single sample (nti)",
    "text": "Lesson 23: Estimate sampling variation from a single sample (nti)\n\nUse bootstrapping to estimate sampling variation.\nInfer sampling variation from a regression table: “standard error” of a model coefficient.\nConstruct and interpret confidence intervals on a model coefficient."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-24-effect-size-nti",
    "href": "schedule-of-objectives.html#lesson-24-effect-size-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 24: Effect size (nti)",
    "text": "Lesson 24: Effect size (nti)\n\nEstimate an effect size from a regression model of the two variables.\nConstruct a confidence interval on the effect size.\nGaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-25-mechanics-of-prediction-nti",
    "href": "schedule-of-objectives.html#lesson-25-mechanics-of-prediction-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 25: Mechanics of prediction (nti)",
    "text": "Lesson 25: Mechanics of prediction (nti)\n\nGiven a sample from a DAG simulation, construct a predictor function for a specified response variable.\nUse the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error.\nDistinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-26-constructing-a-prediction-interval-nti",
    "href": "schedule-of-objectives.html#lesson-26-constructing-a-prediction-interval-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 26: Constructing a prediction interval (nti)",
    "text": "Lesson 26: Constructing a prediction interval (nti)\n\nIn evaluating a model function, generate a prediction interval.\nInterpret prediction bands as a series of intervals, one for each value of the model input.\nIdentify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-27-covariates-nti",
    "href": "schedule-of-objectives.html#lesson-27-covariates-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 27: Covariates (nti)",
    "text": "Lesson 27: Covariates (nti)\n\nShow that including covariates in a prediction model always reduces in-sample mean square residual, but may not reduce residuals out-of-sample.\nGiven regression coefficients, calculate model degrees of freedom and residual degrees of freedom.\nCalculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-28-covariates-eat-variance-nti",
    "href": "schedule-of-objectives.html#lesson-28-covariates-eat-variance-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 28: Covariates eat variance (nti)",
    "text": "Lesson 28: Covariates eat variance (nti)\n\nCorrectly define “covariate”.\nUnderstand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\nRead a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-29-confounding-nti",
    "href": "schedule-of-objectives.html#lesson-29-confounding-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 29: Confounding (nti)",
    "text": "Lesson 29: Confounding (nti)\n\nIdentify confounding in a DAG\nChoose whether to include covariate depending on form of DAG"
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-30-non-causal-correlation-nti",
    "href": "schedule-of-objectives.html#lesson-30-non-causal-correlation-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 30: Non-causal correlation (nti)",
    "text": "Lesson 30: Non-causal correlation (nti)\n\nDistinguish “common cause” and “collider” forms of DAG.\nConstruct appropriate DAG to match a narrative hypothesis."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-31-experiment-and-random-assignment-nti",
    "href": "schedule-of-objectives.html#lesson-31-experiment-and-random-assignment-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 31: Experiment and random assignment (nti)",
    "text": "Lesson 31: Experiment and random assignment (nti)\n\nProperly use nomenclature of experiment.\nCorrectly re-draw DAG for an ideal experimental intervention.\nUse blocking to set assignment to treatment or control."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-32-measuring-and-accumulating-risk-nti",
    "href": "schedule-of-objectives.html#lesson-32-measuring-and-accumulating-risk-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 32: Measuring and accumulating risk (nti)",
    "text": "Lesson 32: Measuring and accumulating risk (nti)\n\nDistinguish between absolute and relative risk and identify when a change in risk is being presented as absolute or relative.\nCalculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\nInterpret effect size as stated in log odds."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-33-constructing-a-classifier-nti",
    "href": "schedule-of-objectives.html#lesson-33-constructing-a-classifier-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 33: Constructing a classifier (nti)",
    "text": "Lesson 33: Constructing a classifier (nti)\n\nBuild a classifier from case-control data.\nCross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\nCalculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-34-accounting-for-prevalence-nti",
    "href": "schedule-of-objectives.html#lesson-34-accounting-for-prevalence-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 34: Accounting for prevalence (nti)",
    "text": "Lesson 34: Accounting for prevalence (nti)\n\nExplain why case-control data may not give an proper measure of “prevalence.”\nUnderstand sensitivity and specificity as conditional probabilities.\nCalculate false-positive and false-negative rates for a given prevalence."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-35-hypothesis-testing-nti",
    "href": "schedule-of-objectives.html#lesson-35-hypothesis-testing-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 35: Hypothesis testing (nti)",
    "text": "Lesson 35: Hypothesis testing (nti)\n\nUnderstand and use properly hypothesis testing nomenclature: test statistic, sampling distribution under the null, Type-1 and Type-2 error, rejection threshold, p-value\nContrast hypothesis testing versus Bayesian framework."
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-36-calculating-a-p-value-nti",
    "href": "schedule-of-objectives.html#lesson-36-calculating-a-p-value-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 36: Calculating a p-value (nti)",
    "text": "Lesson 36: Calculating a p-value (nti)\n\nThe permutation test\nInterpret correctly from regression/ANOVA reports\nTraditional names for hypothesis tests in different “textbook” settings.\nDistinguish between p-value and effect size, that is, “significance” and “substance.”"
  },
  {
    "objectID": "schedule-of-objectives.html#lesson-37-false-discovery-with-hypothesis-testing-nti",
    "href": "schedule-of-objectives.html#lesson-37-false-discovery-with-hypothesis-testing-nti",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Lesson 37: False discovery with hypothesis testing (nti)",
    "text": "Lesson 37: False discovery with hypothesis testing (nti)\n\nIdentify signs of false discovery in a research paper.\nEstimate how overall p-value should change when study is replicated."
  },
  {
    "objectID": "schedule-of-objectives.html#alternative-1",
    "href": "schedule-of-objectives.html#alternative-1",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Alternative 1",
    "text": "Alternative 1\nTheme: ANOVA and the F statistic\n\nCorrectly define “covariate”.\nUnderstand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\nRead a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction."
  },
  {
    "objectID": "schedule-of-objectives.html#alternative-2",
    "href": "schedule-of-objectives.html#alternative-2",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Alternative 2",
    "text": "Alternative 2\nTheme: Classifiers: ROC and loss function"
  },
  {
    "objectID": "schedule-of-objectives.html#alternative-3",
    "href": "schedule-of-objectives.html#alternative-3",
    "title": "Day-by-day Objectives for Math 300R",
    "section": "Alternative 3",
    "text": "Alternative 3\nTheme: Accumulating risk: Logistic regression\nPossible reading https://openintro-ims.netlify.app/model-logistic.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "",
    "text": "This site holds the proposal for the Spring 2023 version of Math 300."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Background",
    "text": "Background\nUp through Spring 2022, Math 300 was organized around the Moore and Notz textbook: Statistics: Concepts and controversies 10/e. This book was designed for a non-technical audience of “consumers of statistics” but is dramatically outdated. For instance, it has no data science content and introduces only primitive statistical methods. A course with such shortcomings seems inappropriate for cadets going on to be officers who will inevitably have to work with modern data and methods.\nIn Fall 2022, Math 300 switched to a very different book, Ismay and Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. The ModernDive book introduces computing on data in an accessible but modern way. It is the only well-known statistics text based on a data-science perspective. Nonetheless, the statistical inference portions of the book regress to the same sort of primitive statistical methods from Concepts and Controversies.\nTo support the Fall 2022 course using ModernDive, a complete set of roughly 35 Notes to Instructors (NTI) was written by Prof. Bradley Warner along with problem sets and other needed materials and deployed for the course.\nThis proposal is for additional improvements to Math 300, building on the Fall 2022 course but replacing the statistical inference portions of the course with more contemporary and general-purpose inference techniques and support for concepts and methods relevant to decision-making.\nIn the following, I refer to three different versions of Math 300:\n\nThe Fall 2022 version of the course, using the ModernDive book, will be called Math 300.\nThe previous version of the course, as taught for several years before Fall 2022, will be called 300CC, which refers to the textbook then used, Concepts & Controversies.\nThe course proposed in this document, a revision of part of Math 300, will be called Math 300R. The R stands for “revised.”"
  },
  {
    "objectID": "index.html#overall-goals-of-math-300r",
    "href": "index.html#overall-goals-of-math-300r",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Overall goals of Math 300R",
    "text": "Overall goals of Math 300R\nThe design of a course revision needs to take into account several factors:\n\nThe target audience’s anticipated technical ability and motivation and, therefore, the appropriate pedagogy and the balance between theory and practice to use in the course.\nInstitutional goals that inform the prioritization of the topics included in the course.\nConstraints of class time and internal coherence of the course, that is, using later topics to reinforce student learning of the earlier topics.\n\nLater, in the rationale section section of this document, I describe how I came to the following conclusions, but for now, a simple statement will suffice.\n\nThe target audience is humanities and social science majors, many of whom will not be confident in the use of calculus but all of whom have had previous exposure to R in the core calculus course.\nInstitutional goals (as revealed by discussions with humanities and social science departments and the wording of the catalog description of Math 300) include a substantial emphasis on data science techniques (data wrangling and visualization) and the use of statistical concepts and methods to support decision-making."
  },
  {
    "objectID": "index.html#statistical-topics-and-framework",
    "href": "index.html#statistical-topics-and-framework",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Statistical topics and framework",
    "text": "Statistical topics and framework\nTransitioning from Math 300CC to Math 300 has already accomplished many data science goals. This proposal centers on the statistical topics/methods to be covered and the path through them.\nThe class-time demands of the new emphasis on data science techniques in Math 300 (and retained in Math 300R) dictate that the statistical concepts and methods be taught more compactly than in Math 300CC. Low-priority, legacy topics from Math 300CC should be dropped. (The GAISE report provides some guidance here.) We can use to advantage that students in Math 300 already see many modeling-related topics in Math 141Z/142Z. Since students already have a background in model-building and computing, we can choose statistical topics that relate well to decision-making.\nA traditional path for statistical methods starts with descriptive statistics (e.g., standard deviation) and then presents “1-sample” statistics (e.g., mean, proportion) and inferential techniques (confidence interval, hypothesis test) in that context. Next comes the inferential techniques for the analogous “2-sample” statistics (difference in mean, difference in proportion), followed by inference techniques for regression.\nThis path is unnecessarily long for our students since regression encompasses all the traditional methods.1 Framing statistical inference in the context of regression avoids the need to teach method-specific calculations or cover the variety of formats for non-regression test results. Regression is part of the data scientist’s standard toolbox and relates well to more advanced data techniques such as machine learning. The ModernDive textbook uses regression as the segue from the first block (about data wrangling and visualization) to the third block (about inference).\nAdditional streamlining comes from motivating statistical inference using a simulation approach. Simulation draws on two conceptually simple data operations: resampling and permutation (shuffling). This approach is well established in the statistics community and is considered by many (including the ModernDive authors) to be a better pedagogy than the traditional formula-and-distribution presentation of statistical methods. Since Math 300 (and 300R) students will already have worked with wrangling and visualization, they will be well prepared to work with the data generated by repeated simulation trials.\nThe focus on decision-making in 300R appears in the addition of new concepts and techniques treated minimally in traditional statistics courses. These include risk, prediction (and its close cousin classification), causality, and confounding. Introductory epidemiology courses provide a model for teaching about risk, causation, and confounding. The pedagogy for these topics in Math 300R comes from the epidemiology course I introduced at Macalester. In addition, Math 300R draws on my decade of experience teaching causality as part of an introductory statistics course. (See the causation chapter of my Statistical Modeling text.)\nThe Statistical Modeling pedagogy for causality uses directed acyclic graphs (DAGs) and causal simulations based on them. Unlike resampling and permutation, which re-arrange existing data, the DAG simulations generate synthetic data with specified properties (such as effect sizes). Simulations allow a concrete demonstration of the extent to which regression techniques can and cannot recover causal information from data.\nThe DAG-simulation approach lends itself naturally to the demonstration of statistical phenomena such as sampling variation and estimation of prediction error. As an example, consider the statistical fallacy of regression to the mean, as with Galton’s finding about comparing children’s heights to their parents’. The natural hypothesis that heights are determined by genetic and other factors is represented by this DAG:\n\\[\\epsilon \\rightarrow parent \\longleftarrow GENES \\longrightarrow child \\leftarrow \\nu\\]\nIn this DAG there is no causal mechanism included for “regression to the mean.” However, Galton’s empirical finding is replicated by data from the DAG-simulation."
  },
  {
    "objectID": "index.html#sec-broad-structure",
    "href": "index.html#sec-broad-structure",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Scope of the proposed changes",
    "text": "Scope of the proposed changes\nMath 300R will retain the first 17 lessons of Math 300. All teaching materials for this part of the course will be used unaltered. (Exception: revisions to Math 300 the Fall 2022 teaching team deems appropriate. Such revisions are not part of this proposal.)\nThe following 19 lessons are entirely refactored and based on new readings, NTIs, exams, and other materials. Objectives for each of these 19 lessons are itemized here.\n\nThe corresponding ModernDive chapters are not used in Math 300R.\nThe software used is the same as that used in the first half of the ModernDive book, specifically the ggplot2 graphics package and the tidyverse data wrangling packages. However, the infer package used in the second half of ModernDive is dropped.\n\nThe theme of the refactored 19 lessons is “informing decisions with data.” Statistical approaches that can inform decision-making include anticipating the impact of interventions, predicting individual outcomes, and the quantification of risk. These are all included in Math 300R.\nTopics to be de-emphasized are the algebra of computing confidence intervals and p-values and the (controversial) role of p-values as a guide to practical “significance.” About half of a traditional course is about the construction of confidence intervals in various settings and, more or less equivalently, the conversion of data into p-values. However, in the contemporary era, when “observational” data are collected en masse, p-values can become very small (“significant”) even when the relationship under study is slight and insubstantial.\nConfounding and methods for dealing with it (statistical adjustment, experiment) are treated substantially in Math 300R. Decision-making about interventions often relies on understanding causal effects. The possibility of confounding is a major source of skepticism about making causal judgments. In a world where much data is observational, the sweeping principles that “correlation is not causation” and “no causation without experimentation” do not support making responsible conclusions about causal connections and the need to make decisions even when data cannot provide a definitive answer. Decision-makers need this support."
  },
  {
    "objectID": "index.html#rationale",
    "href": "index.html#rationale",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Rationale for course revisions",
    "text": "Rationale for course revisions\n\nRelationship to Math 357 and Math 377\nDFMS offers three courses satisfying the statistics component of the Academy’s core requirements: Math 300, Math 357, and Math 377. In designing 300R, attention should be paid to the reasons for supporting three distinct courses. The catalog copy lays out the differences in terms of intended student major, software, mathematical background, and orientation to data science.\nIntended student major: The catalog says, “Math 300 is designed primarily for majors in the Social Sciences and Humanities.” while “Math 356 is primarily designed for cadets in engineering, science, or other technical disciplines. Math majors and Operations Research majors will take Math 377.” Math 377 is also the intended course for prospective Data Science majors, although this is not in the catalog.\nSoftware: The catalog does not describe any software component for either Math 300 or Math 357, but states that, in Math 377, “modern software appropriate for data analysis will be used.” In reality, as of Fall 2022, much the same software is used in all three courses: R with the dplyr package for data wrangling, ggplot2 for data visualization, and “R/Markdown” for creating computationally active documents.\nOne difference between Math 300 and 357/377 relates to computer programming. Both 357 and 377 include content about the underlying structure of the R language, object types, the construction of functions, and arrays and iteration. In contrast, Math 300 is based on a small set of command patterns using data frames. Students see R in Math 300 more or less as an extension of what they learned in 141Z/142Z; what’s added is a few statistical and data-wrangling functions and a handful of new graphics types.\nStudents’ mathematical background: Math 377 explicitly refers to “calculus-based probability.” Math 300 and 357 share identical catalog copy, though in reality Math 357 and Math 377 use the same textbook. Calculus is indeed necessary for the probability topics in Math 357 and 377. My interpretation is that Math 300 should serve as a safe haven for those who lack confidence in their calculus skills. Both the Fall 2022 edition of Math 300 and the proposed Math 300R serve this role as safe haven.\nOrientation to Data Science: Starting with the Fall 2022 edition, Math 300 develops and draws on data-science skills for wrangling and visualization. In this, the new Math 300 is in line with both Math 357 and 377.\nThe above analysis indicates that Math 300 and 300R should diverge from Math 357/377 in these ways:\n\nMath 300R should make little or no use of calculus operations.\nMath 300R should include little consideration of probability distributions or (non-automated) calculations with any but the simplest.\nMath 300R should be computational, but should not draw heavily on computer programming skills such as types of objects, arrays, indexing, and loop-style iteration. Use of R/Markdown documents should be considered as a pedagogical choice, and retained or discontinued based on how it contributes to student success in the other areas of the course.\n\nIn addition, I suggest that …\n\nMath 300R include some work with assembling/curating data using spreadsheets and basic data cleaning with spreadsheets. Awareness of the ubiquity of data errors and a basic understanding of how to deal with such errors is an important component of working with data. (This is not to suggest that data analysis, modeling, and graphical depiction be taught using spreadsheets, which are notoriously unreliable, difficult, and limiting for such purposes. Spreadsheets are, however, appropriate for the phase where non-tabular data is transcribed into a tabular arrangement.)\n\n\n\nInstitutional goals\nIt can be difficult to translate broadly stated institutional goals to apply them to a single course. However, catalog descriptions of programs and individual courses provide some assistance. Here is the catalog copy for Math 300 (which is identical to the catalog description of Math 357).\n\nMath 300. Introduction to Statistics. An introduction in probability and statistics for decision-makers. Topics include basic probability, statistical inference, prediction, data visualization, and data management. This course emphasizes critical thinking among decision-makers, preparing future officers to be critical consumers of data. (Emphasis added.)\n\nI interpret the final sentence as a description of the overall objective of the course:\n\nOverall objective: Prepare officers to use data to inform decisions.\n\nReturning to the idea that the topics listed in the catalog copy ought to be interpreted as serving the overall objective of the course, let’s consider those topics one at a time:\n\ndata management\ndata visualization\nprediction\nstatistical inference\nbasic probability\n\n\nStrictly speaking, as a term of art the phrase “data management” is business jargon describing enterprise-level activities that are unrelated to the other items on the list. It would be unheard of to include it, in this strict sense, in a statistics course. I believe the intent of the phrase to be better served by terms like “data wrangling,” “data cleaning,” “database querying,” and such which make up an important part of “data science.” Data wrangling is a major feature of Math 300 launched and is covered using professional level computing tools well suited to both small and large data. But whatever “data management” might reasonably be taken to mean, it was utterly ignored in Math 300CC.\n“Data visualization” is generally taken to be the process of using graphics to discover and highlight patterns shown in data. Math 300CC included only statistical graphics such as histograms, box-and-whisker plots, and basic “scatter plots.” Math 300 adds to this modern modes of graphics such as transparency, color, and faceting that make it possible to display relationships among multiple variables. The software used in Math 300 is the professional-level ggplot2 which provides the ability to increase the sophistication and generality of data display, using for example density graphics such as violin plots. As such, Math 300 is a big step on the road to rich data visualization. Some of these will be introduced in Math 300R in the second half of the course.\n“Prediction” is a central paradigm used in the important area of “machine learning.” It is also an often used method used to inform decision making and characterize risk, for instance, by indicating the distribution of plausible outcomes. Math 300CC emphasized paradigms such as hypothesis testing and confidence intervals that are not aligned with making and interpreting predictions. Math 300 focuses on these same paradigms. Math 300R will treat prediction as a central statistical path, as well as highlighting its proper use, interpretation, and evaluation.\n“Statistical inference” is traditionally taken to mean the calculation and interpretation of hypothesis tests and confidence intervals in various simple settings. Such settings include the “difference between two means,” the “correlation coefficient,” and the “slope of a regression line.” Math 300CC introduced a handful of such settings, providing distinct formulas for each of them. The “controversies” referred to in the title Concepts and controversies includes the problematic interpretation of “p-values” and the need to use random sampling and/or random assignment in data collection to get “correct” results. Math 300 retains the emphasis on confidence intervals and p-values in the simple settings, but emphasizes a more general and accessible methodology based on bootstrapping and permutation tests.\n\nUnfortunately, appealing to random sampling/assignment is often whistling past the graveyard, since these idealized data collection processes are rarely available. Instead, professionals include “covariates” in their data collection in order to “adjust” for the factors that would have been scrambled into insignificance by random sampling/assignment if it had been available. Math 300R incorporates covariate methods and highlights the importance of identifying appropriate covariates.\n\n“Basic probability” can mean different things to different people. In most introductory statistics courses it refers to the construction, calculation, and study of named distributions such as the binomial, normal, chi-squared, t, etc. Such distributions play an important role in the statistical theory of confidence intervals and hypothesis testing. That is, they are support for statistical inference. Math 300CC followed the traditional pattern of having students memorize which distribution is relevant to which setting and using printed tables for calculation. As described earlier, Math 300 provides a much more natural route to inference through bootstrapping and permutation tests.\n\nWhat’s left out in this conception of basic probability is the support for decision making. Essential to this is the proper use of “conditional probability.” Math 300R emphasizes appropriate use and interpretation of conditional probability, seen most clearly in the “classifiers” part of the course.\n\n\nFaculty opinions\nInsofar as faculty internalize the goals of the institution, their views can point to ways that existing courses do and do not reflect those goals.\nWithin DFMS and other departments, there is a general discontent that Math 300CC was not doing what it ought to. Reasons for this can be seen by examining the textbook used in Math 300CC. The book has clear deficiencies, among which are:\n\nthe material is out of date and does not reflect any of the consensus recommendations (such as GAISE) developed in the last 30 years.\nit does not use data at any level beyond hand calculation.\nit does not deal with decision making at any serious level. (The only decision formally supported is whether or not to reject the Null hypothesis.)\n\nThe opinions of faculty outside DFMS can also be an important guide to institutional priorities. In AY 2021-22 I contacted the departments in the social sciences and humanities. Three of these—history, political science, economics—responded with interest. Discussion with groups of faculty from these departments elucidated a number of points:\n\nThe faculty most highly valued the development of data-science skills such as computing for data wrangling and data visualization.\nThe then-current version of Math 300 did not contribute to the development of such skills.\nMath 357 is not seen as an appropriate alterative to Math 300, both because of perceived difficulty of 357 and because faculty do not value the emphasis on probability distributions seen in 357.\n\nFrom my experience at Macalester and in conducting reviews at many colleges, I am often wary of the motivation of faculty in other departments. These can represent a desire for service courses like Math 300 to cover discipline-specific techniques. However, the faculty I spoke to also had an eye on what their students will need for their post-graduation jobs. Particularly the USAF officers drew on their field experience in areas such as military intelligence.\nBased on these findings, the group of faculty planning for revisions to Math 300 made an easy decision: replace the textbook with one oriented to data science. We selected the ModernDive book, which is unique among introductory statistics textbooks in starting out with data wrangling and visualization. This change of textbook addresses the “use data” part of the course objective stipulated above.\nThe other part of the objective—inform decisions—remains problematic even with the switch in the Math 300 textbook. Discussions I had with the ModernDive authors made clear that their purpose in writing the book was to provide a way to introduce data science into introductory statistics, but that they stuck to the traditional hypothesis-testing/confidence-interval framework in order not to make the change too daunting for instructors thinking of adopting the text. In other words, they were not trying to turn the topic toward decision-making with data, the motivation of the ideas presented in this proposal for Math 300R."
  },
  {
    "objectID": "index.html#plan-of-work",
    "href": "index.html#plan-of-work",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Plan of work",
    "text": "Plan of work\n\nEarly October 2022: Preliminary approval, with appropriate modifications, of the proposed objectives.\nOctober 2022: DTK will draft new day-by-day NTIs for the second half of the course in the same style as the existing NTIs for the first half of the course. In the process of drafting, there will likely be some re-arrangement and modification of the objectives in (1).\nNovember 2022: With the draft NTIs in hand, a faculty team will make a more detailed examination of the proposed objectives. I recommend that this examination be structured as a set of hour-long discussions, one for each of the five divisions described in ?@sec-topics.\nNovember/December 2022: DTK (and others, as interested) will assemble student readings to replace the second half of ModernDive. Much of the content already exists in the form of a draft textbook by DTK. These will be re-arranged to correspond to the day-to-day objectives as determined in (3).\nJanuary/February 2023: The first 18 lessons of 300R will be taught as a repeat of those lessons from Math 300 Fall 2022. DTK will participate mainly as an observer.\nJanuary/February 2023: Revision and refinement will be made of the readings and NTIs in (3) and (4) above.\nMarch/April 2023: Teaching the new lessons. DTK will participate as an instructor for these lessons."
  }
]
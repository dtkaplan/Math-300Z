[
  {
    "objectID": "LC/LC-lesson29.html",
    "href": "LC/LC-lesson29.html",
    "title": "Learning Checks Lesson 29",
    "section": "",
    "text": "The `math300::Hill_racing” data frame records 2236 winning times (in seconds) in Scottish hill racing competitions. Consider this model of the winning time as a function of the race distance (km) and the total climb (meters):\n\nmod <- lm(time ~ distance + climb, data=Hill_racing)\n\nThe model_eval() function provides a convenient way to evaluate the model output (.output) for each of the rows in a data frame and, at the same time, calculates row-by-row residuals (.resid) and prediction errors (.lwr and .upr). Make sure to take not of the starting periods on the names.\n\nmodel_eval(mod) %>% head()\n\n  time distance climb  .output     .resid       .lwr     .upr\n1 1630        6   240 1679.215  -49.21475  -29.56279 3387.992\n2 1655        6   240 1679.215  -24.21475  -29.56279 3387.992\n3 2391        6   240 1679.215  711.78525  -29.56279 3387.992\n4 2351        6   240 1679.215  671.78525  -29.56279 3387.992\n5 4151       14   660 4805.779 -654.77947 3097.10184 6514.457\n6 3975       14   660 4805.779 -830.77947 3097.10184 6514.457\n\n\nThe RMS residual from the model can be calculated this way:\n\nmodel_eval(mod) %>%\n  summarize(rms = sqrt(mean(.resid^2)))\n\n       rms\n1 870.4631\n\n\n\nWhat are the units of the RMS residual?\nModify the calculation to compute the sum-of-square residuals. Report the result numerically. Be sure to say what are the units.\nWhat are the units of the effect size on time with respect to climb?\nWhat are the units of the effect size on time with respect to climb?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nRMS residual has the same units as the response variable. In this case, that’s the time to run the race, with units “seconds.”\nSS residual has units that are the square of the respond variable, in this case “square-seconds.”\nRecall that the effect size on the response with respect to an explanatory variable has the units of the response variable divided by the units of the explanatory variable. The climb variable has units of meters, so the effect size has units “seconds/meters.”\nseconds/km"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.b",
    "href": "LC/LC-lesson29.html#lc-29.b",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.B",
    "text": "LC 29.B\nWhich of the following models are not nested within time ~ distance + climb?\n\ntime ~ 1\ntime ~ distance + sex\ntime ~ distance\ntime ~ climb\n\n\n\n\n\n\n\nSolution\n\n\n\nThe model time ~ distance + sex is not nested in time ~ distance + climb.\nNote that time ~ 1 is indeed nested in time ~ distance + climb. The 1 corresponds to the intercept."
  },
  {
    "objectID": "LC/LC-lesson29.html#c",
    "href": "LC/LC-lesson29.html#c",
    "title": "Learning Checks Lesson 29",
    "section": "29.C",
    "text": "29.C\nIn LC -Section 1 you calculated the RMS residuals and the sum-of-square residuals by wrangling the results from mod_eval(). That’s a perfectly good way to do things, but the work becomes tedious when there are multiple models you want to compare.\nFor convenience, there is a compare_model_residuals() command, which can calculate the RMS residual or sum-of-square residual for each of a set of models. All the models must have the same response variable.\n\nHill_racing %>% \n  compare_model_residuals(time ~ 1, \n                          time ~ distance + climb, \n                          time ~ distance + climb + sex,\n                          time ~ distance,\n                          measure = \"RMS\"\n                          )\n\n[1] 3122.4821  870.4631  775.2962 1189.7148\n\n\nIt happens that all of the models in the command are a nested set. Re-order the models so that each model nests inside the following model, that is, from smaller model to bigger model.\n\nDo the RMS residuals for the nested models increase or decrease when moving from a smaller model to a larger model?\nYou can calculate the sum-of-square residual by using the argument measure=\"SS\". Do the sum-of-square residuals for the nested models increas or decrease when moving from a smaller model to a larger model.\nYou can calculate R2 by using the argument measure=\"R2\". Do the R2 for the nested models increase or decrease when moving from a smaller model to a larger model.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nDecrease\nDecrease\nIncrease. R2 tells you how big the model output is compared to the response variable. 1-R2 tells you how big the residuals are compared to the response variable."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.d",
    "href": "LC/LC-lesson29.html#lc-29.d",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.D",
    "text": "LC 29.D\n::: {.callout-warning} ## Still a draft\nLook at dag07. Notice that d is not connected to any of the other variables.\nGenerate a sample of size \\(n=6\\). Compare the sum of square residual (in sample) from the nested models c ~ 1, c ~ a c ~ a + b, and c ~ a + b + d. (Use the compare_model_residuals() using the argument method=\"SS\".\nWhich, if any, of the variables a, b, or d reduces the in-sample sum-of-squared residuals compared to the previous model.\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag07, c ~ 1, c ~ a, c ~ a + b, c ~ a + b + d, \n                        n=6, measure=\"R2\")\n\n[1] 0.0000000 0.4390922 0.6557035 0.7487311\n\n\n\n\nOut of sample, the useless covariate often increases the SS error."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.1",
    "href": "LC/LC-lesson29.html#lc-29.1",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.1",
    "text": "LC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50)\n\n[1] 0.9916292 0.8672413 0.8119626 0.7180731\n\n\nd seems to block effect of a and b on c.\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5)\n\n[1] 1.136096 1.106351 1.446975 1.423358"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.2",
    "href": "LC/LC-lesson29.html#lc-29.2",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, \n                        testing = \"out-of-sample\")\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.3",
    "href": "LC/LC-lesson29.html#lc-29.3",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.3",
    "text": "LC 29.3\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::teacher. What’s the base pay difference between a teacher with an MA and a BA degree? What’s a confidence interval on this effect size? How does the confidence interval change if you include years as a covariate."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.4",
    "href": "LC/LC-lesson29.html#lc-29.4",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.4",
    "text": "LC 29.4\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::census Predict log personal income based on other variables. Eat variance using the total_family_income variable.\n\nmod <- lm(log10(total_personal_income) ~ log10(age) + sex + marital_status + log10(total_family_income), data = openintro::census %>% filter(total_personal_income > 0, total_family_income > 0))\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: log10(total_personal_income)\n                            Df Sum Sq Mean Sq  F value    Pr(>F)    \nlog10(age)                   1  5.938  5.9383  35.6102 6.660e-09 ***\nsex                          1  5.976  5.9758  35.8351 6.006e-09 ***\nmarital_status               5  4.302  0.8604   5.1596 0.0001464 ***\nlog10(total_family_income)   1 17.620 17.6198 105.6615 < 2.2e-16 ***\nResiduals                  306 51.028  0.1668                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ngf_jitter(total_personal_income ~ total_family_income | sex, \n         data =openintro::census %>% filter(total_personal_income > 3000),\n         color=~marital_status, alpha=0.3) %>% \n  gf_refine(scale_y_log10(), scale_x_log10())\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning: Removed 20 rows containing missing values (geom_point)."
  },
  {
    "objectID": "LC/LC-lesson29.html#section",
    "href": "LC/LC-lesson29.html#section",
    "title": "Learning Checks Lesson 29",
    "section": "29.5",
    "text": "29.5\n\n\n\n\n\n\nStill in draft\n\n\n\nopenintro::starbucks where do the calories come from? Find effect size of, say, protein on calories. Then see what happens if you use carbohydrates as a covariate.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ fat + carb + fiber + protein, data = openintro::starbucks) %>% confint()\n\n                2.5 %    97.5 %\n(Intercept) -2.938079 13.605279\nfat          8.591250  9.315766\ncarb         3.686593  3.997527\nfiber       -1.418966  1.370022\nprotein      3.631695  4.364091"
  },
  {
    "objectID": "LC/LC-lesson29.html#in-draft-2",
    "href": "LC/LC-lesson29.html#in-draft-2",
    "title": "Learning Checks Lesson 29",
    "section": "In draft",
    "text": "In draft\nMaybe come back to this in confounding lesson. Look for components that tend to go together\n\nwith(openintro::starbucks, cor(fat, protein))\n\n[1] 0.22347\n\nwith(openintro::starbucks, cor(fiber, protein))\n\n[1] 0.488564\n\nlm( calories ~ fiber , data = openintro::starbucks) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 276.119106 343.80739\nfiber         1.923476  24.07453\n\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ protein + fiber, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 247.891487 320.333514\nprotein       1.700777   7.996897\nfiber        -8.099007  15.978382"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html",
    "href": "NTI/NTI-Lesson19.html",
    "title": "Math 300R NTI Lesson 19",
    "section": "",
    "text": "In the first half of the course, we examined data wrangling methods—group_by(), summarize() and tally() in particular—for summarizing variables and breaking up the summaries by groups.\nIn this second half of the course, we will be emphasizing the relationships between variables. The conceptual framework we’ve used for describing such relationships involves choosing a response variable and selecting one or more explanatory variables. Within this framework, you’ve already seen regression techniques for describing how a numerical response variable can be related to numerical explanatory variables. Something new you will see in the second half of the course is using regression techniques to handle categorical response variables. This will allow us to extend regression to cover such things as proportions and probabilities.\nGraphics can be a powerful way to perceive relationships between variables. In keeping with the response/explanatory framework, our go-to graphical technique will be to plot data as a “point plot” absolutely sticking to the convention that the response variable will be assigned to the vertical axis. Note that we’ll use the term “point plot” rather than the “scatter plot” that was used in the early graphics chapters. That’s partly because we’re going to use the idea of “scattering” in a different way.\nWe’re also going to adopt a new graphical convention: that statistical summaries should always be graphed as a layer on top of the raw data from which the summaries are derived. This means that we’ll switch away from histograms as a way of displaying distributions. Why? Because the vertical axis on a histogram does not correspond to the values of a response variable. Similarly, barplots won’t be appropriate, because the response variable in a barplot is represented by color rather than position on a vertical axis.\nOverall, both the graphical display of data and summaries and the calculation of numerical summaries like means and proportions will become much simpler to accomplish, since relationship summaries will be produced using using lm() and closely related model types. Data graphics will be made with geom_point() and closely related geoms."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#graphing-data-and-statistical-annotations",
    "href": "NTI/NTI-Lesson19.html#graphing-data-and-statistical-annotations",
    "title": "Math 300R NTI Lesson 19",
    "section": "Graphing data and statistical annotations",
    "text": "Graphing data and statistical annotations"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#categorical-response-variables",
    "href": "NTI/NTI-Lesson19.html#categorical-response-variables",
    "title": "Math 300R NTI Lesson 19",
    "section": "Categorical response variables",
    "text": "Categorical response variables\nTo get started, we’ll work with categorical response variables that are “dicotomous” or, equivalently, “binomial,” that is, with only two methods. This covers a large fraction of the situations where categorical response values are needed. We’ll leave those situations where there are lots of levels for the response variable to courses on “machine learning,” a topic we can only scratch the surface of in this introductory course.\nThe important insight into using categorical variables in regression models is encapsulated in the idea of the 0-1 (“zero-one”) encoding. To illustrate, consider the mosaicData::Whickham data frame that records the age, smoking status, and survival of 1000 or so nurses in the UK. The relationship that motivated the collection of these data is between smoking and survival. Taking a few rows from the data frame let’s us easily see the types of the variables involved:\n\n\n\n\n \n  \n    outcome \n    smoker \n    age \n  \n \n\n  \n    Dead \n    No \n    56 \n  \n  \n    Alive \n    Yes \n    54 \n  \n  \n    Alive \n    Yes \n    21 \n  \n  \n    Dead \n    No \n    72 \n  \n  \n    Alive \n    Yes \n    30 \n  \n  \n    Alive \n    No \n    19 \n  \n  \n    Alive \n    Yes \n    46 \n  \n  \n    Alive \n    No \n    31 \n  \n  \n    Alive \n    No \n    74 \n  \n  \n    Alive \n    Yes \n    32 \n  \n\n\n\n\n\nHOW TO USE math300::zero_one()\nPlotting a zero-one response variable with correctly labelled axes.\n\nP <- Whickham %>% \n  gf_jitter(zero_one(outcome) ~ age, color=~smoker, height=0.1, alpha=0.5) %>% \n  label_zero_one()\nP\n\n\n\n\nWe might equally well plot age as a function of outcome:\n\ngf_jitter(age ~ outcome, data = Whickham, width=0.2) %>% gf_violin(color=NA, fill=\"blue\", alpha=0.5)\n\n\n\n\nPLOT OUT A MODEL\n\nmod <- model_train(zero_one(outcome) ~ age + smoker, data = Whickham)\n# fun <- mod_fun(mod) \n# P %>% mosaicCalc::slice_plot(fun(age, smoker=\"Yes\") ~ age, color=\"red\")"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#intervals",
    "href": "NTI/NTI-Lesson19.html#intervals",
    "title": "Math 300R NTI Lesson 19",
    "section": "Intervals",
    "text": "Intervals\nOnce you have FIXES MOSAICMODEL, Generate a “prediction” interval and use gf_errorbar() to plot it over the model."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#objectives",
    "href": "NTI/NTI-Lesson19.html#objectives",
    "title": "Math 300R NTI Lesson 19",
    "section": "Objectives",
    "text": "Objectives\n19.1 Convert to the response vs explanatory format for data graphs.\n19.1 Understand the covering of a variable by an interval specified by a coverage level (e.g. 0.95).\n19.2 Be able to produce pointUse “violin” plots to display the density of a variable. (This should really go in the first half of the course, but ModernDive doesn’t do it.)\n19.3 Convert categorical variables to a 0-1 encoding. a. Generate graphics appropriate to a categorical encoding. (jitter plots) b. Apply modeling techniques to 0-1 encodings of binomial response variables."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#reading",
    "href": "NTI/NTI-Lesson19.html#reading",
    "title": "Math 300R NTI Lesson 19",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#lesson",
    "href": "NTI/NTI-Lesson19.html#lesson",
    "title": "Math 300R NTI Lesson 19",
    "section": "Lesson",
    "text": "Lesson\nThis lesson marks the beginning of a new phase of the course. Thus far, we’ve worked with techniques* for data wrangling, graphics, and regression modeling. Now we address the question of what a regression model (and other information that we might have) can tell us about the real world.*\n\n\n\n\n\n\nSetup\n\n\n\nlibrary(mosaicData)\n\n\n\n\n\n\n\n\nGuided activity: House prices\n\n\n\nHave students do the calculations for the first model, answering the questions that follow. After this is complete, have students do the calculations for the second model and answer those questions.\nThe mosaicData::SaratogaHouses data frame contains information about the sales price and various attributes of about 1700 houses. (See ?SaratogaHouses for a detailed description.)\nDo a regression of price ~ bedrooms and explain what the regression coefficients mean.\n\nlm(price ~ bedrooms, data=SaratogaHouses) |> coefficients()\n\n(Intercept)    bedrooms \n   59862.96    48217.81 \n\n\n\nWhat are the units of the intercept and of the bedrooms coefficient? *Intercept in dollars, bedrooms in dollars per bedroom.\nInterpret what the coefficients indicate about the price of houses and bedrooms. Each additional bedroom adds about $50000 to the value of a house.\nAccording to the model, predict what would be the sales price (at the time the data was collected, 2006) of a house with two bedrooms? \\(59863 + 2\\times 48218\\)\n\nOf course, bedrooms are not the only important thing about a house. Let’s include livingArea along with bedrooms in the model.\n\nlm(price ~ livingArea + bedrooms, data=SaratogaHouses) |> coefficients()\n\n(Intercept)  livingArea    bedrooms \n  36667.895     125.405  -14196.769 \n\n\n\nWhat are the units of the livingArea coefficient? dollars per square foot\nWhat does this model say about the value of adding a bedroom? It seems to reduce the value of the house by about $15,000.\n\nBased on exactly the same data, the two models seem to give contradictory statements about the value of an additional bedroom.\n\nIs one model right and the other wrong? If so, which one is right? (Explain your reasoning.) Both models are mathematically correct. But they need to be interpreted in different ways. Each is telling us something different about the real world.\nCould both models be right? If so, explain why the bedroom coefficients have opposite signs. We will need to develop some additional tools and concepts before we can take on this question.\n\nLearning how to interpret models in terms of what they say about the world is a major theme of this second half of Math 300.\nAnother, more technical question that we will address has to do with the precision of coefficients like 125.40 dollars per square foot. Might it actually be $200/ft2? How about $500/ft2? And how seriously should we take the sales value that we calculate by setting numbers for bedrooms and livingArea into the model?"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#learning-checks",
    "href": "NTI/NTI-Lesson19.html#learning-checks",
    "title": "Math 300R NTI Lesson 19",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#setup-1",
    "href": "NTI/NTI-Lesson19.html#setup-1",
    "title": "Math 300R NTI Lesson 19",
    "section": "Setup",
    "text": "Setup\nThe math300 package will be needed for lessons 20 through 39.\n\nlibrary(math300)\nlibrary(moderndive)\nlibrary(NHANES)"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section",
    "href": "NTI/NTI-Lesson19.html#section",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.1",
    "text": "19.1\nConsider the moderndive::evals data that records students’ evaluations (score, on a 1-5 scale) of the professors in each of several courses (the course ID), as well as the age, “average beauty rating” (bty_avg) of the professor, enrollment in the course (cls_students) and the level o the course (cls_level). Each row in the data frame is an individual course section.\n\n\n\n\n \n  \n    ID \n    score \n    age \n    bty_avg \n    cls_students \n    cls_level \n  \n \n\n  \n    329 \n    2.7 \n    64 \n    2.333 \n    22 \n    upper \n  \n  \n    313 \n    4.2 \n    42 \n    2.667 \n    86 \n    upper \n  \n  \n    430 \n    4.5 \n    33 \n    5.833 \n    120 \n    lower \n  \n  \n    95 \n    4.2 \n    48 \n    4.333 \n    33 \n    upper \n  \n  \n    209 \n    4.8 \n    60 \n    3.667 \n    34 \n    upper \n  \n  \n    442 \n    3.6 \n    61 \n    3.333 \n    39 \n    lower \n  \n  \n    351 \n    4.6 \n    50 \n    3.333 \n    26 \n    lower \n  \n  \n    317 \n    3.7 \n    52 \n    6.500 \n    44 \n    upper \n  \n  \n    444 \n    4.1 \n    52 \n    4.500 \n    111 \n    lower \n  \n  \n    315 \n    3.8 \n    52 \n    6.000 \n    88 \n    upper \n  \n\n\n\n\n\nThe following commands model score versus age and plots the data as a point plot.\n\nlm(score ~ age, data = moderndive::evals) %>% coefficients()\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\nopenintro::evals %>% gf_point(score ~ age, alpha=0.2 )\n\n\n\n\n\nExplain why some of the dots are darker than others?\n\n\n\n\n\n\n\nSolution\n\n\n\nAll the ages have integer values—e.g., 43, 44, 45—so the dots line up in vertical lines.\nSimilarly, the scores have values only to one decimal place—e.g., 3.1, 3.2, 3.3—so the dots line up in horizontal lines. If there are two or more rows in evals that have the same age and score, the dots will be plotted over one another. Since transparency (alpha = 0.2) is being used, points where there is a lot of overplotting will appear darker.\n\n\n\nRemake the plot, but using gf_jitter() instead of gf_point(). Explain what’s different about the jittered plot. (Hint: Almost all of the dots are the same lightness.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ age, alpha=0.2 )\n\n\n\n\n“Jittering” means to shift each dot by a small random amount. This reduces the number of instances where dots are overplotted.\n\n\n\nNow make a jitter plot of score versus class level (cls_level).\n\nWhat do the tick-mark labels on the horizontal axis describe? Are they numerical?\nTo judge from the plot, are their more lower-level than upper-level courses? Explain briefly what graphical feature lets you answer this question at a glance.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ cls_level)\n\n\n\n\n\nThe tick-mark labels are the levels of the categorical variable cls_level. The are words, not numbers.\nThere are many more dots in the right column than in the left. Since lower level class are shown in the left column, there are fewer lower-level courses than upper-level courses.\n\n\n\n\nThe two columns of points in the plot you made in (3) are not separated by very much empty space. You can fix this by giving gf_jitter() an argument width=0.2. Try different numerical values for width and report which one you find most effective at making the two columns clearly separated while avoiding overplotting.\nAre the scores, on average, different for the lower- vs upper-level classes? It’s hard to get more than a rough idea of the distribution of scores by looking at the “density” of points. The reason is that the number of points differs in the two columns. But there is an easy fix: add a layer to the graphic that shows the distribution (more or less like a histogram displays a distribution of values). You can do this by piping the jitter plot layer into a geom called a “violin,” like this:\n\n\nopenintro::evals %>% \n  gf_jitter(score ~ cls_level) %>%\n  gf_violin(fill=\"blue\", alpha=0.2, color=NA)\n\n\n\n\nExplain how to read the violins."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section-1",
    "href": "NTI/NTI-Lesson19.html#section-1",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.2",
    "text": "19.2\nThe openintro::promotions data comes the the 1970s and records the gender of 38 people along with the result of a decision to promote (or not) the person. =\nChapter 2 of ModernDive suggests graphically depicting decision versus gender by using a bar plot. There are two ways to make the bar plot, depending on which variable you assign to the horizontal axis and which to the fill color.\n\npromotions %>% gf_bar(~ decision, fill=~ gender)\npromotions %>% gf_bar(~ gender, fill=~decision)\n\n\n\n\nFigure 1: Two different ways to plot promotion outcome and gender\n\n\n\n\n\n\n\nFigure 2: Two different ways to plot promotion outcome and gender\n\n\n\n\nPlots like those in ?@fig-promotion-bars might be attractive or not, depending on your taste. What they don’t accomplish is to make sure which is the response variable and which the explanatory variable.\nThe choice of response and explanatory variables depends, of course, on what you are trying to display. But everyday English gives a big hint. For instance, you might describe the question at hand as, “Does gender affect promotion decisions.” Here, the variable doing the affecting is gender, and the outcome is the decision.\nModeling decision as a function of gender is easy once you convert the response variable to a zero-one variable. Like this:\n\nmod <- lm(zero_one(decision, one=\"promoted\") ~ gender, data = promotions)\ncoefficients(mod)\n\n (Intercept) genderfemale \n   0.8750000   -0.2916667 \n\nmosaicModel::mod_eval(mod)\n\n  gender model_output\n1   male    0.8750000\n2 female    0.5833333\n\n\n\nExplain what is the relationship between the model coefficients and the model outputs.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe coefficients tell how to calculate the model output. These coefficients say that the model output will be 0.875, but subtract 0.292 if the person is female.\nThe model outputs give the probability of being promoted for each of the two genders.\n\n\n\nMake this plot and explain what the red lines show. (We don’t expect you to be able to write the command to generate such plots on your own, but we do expect you to be able to interpret them.)\n\n\npromotions %>% \n  gf_jitter(zero_one(decision) ~ gender, height=0.2, width=0.2) %>%\n  gf_errorbar(model_output + model_output ~ gender, data=mod_eval(mod), \n              color=\"red\", inherit=FALSE) %>%\n  label_zero_one()\n\n\n\n\n\n\n\nSolution\n\n\n\nThe red lines show the proportion of the people in each gender group who were promoted. The y-axis scale on the left refers to the zero-one encoding of decision, while the y-axis labels on the right make it easier to read off the numerical value of the proportion."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section-2",
    "href": "NTI/NTI-Lesson19.html#section-2",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.3",
    "text": "19.3\nThe mosaicData::Whickham data from comes from a survey of a thousand or so nurses in the UK in the 1970s. The data record the age of each nurse along with whether the nurse was still alive in a follow-up survey 20 years later (outcome).\nMake this graph from the Whickham data:\n\ngf_jitter(zero_one(outcome) ~ age, data = Whickham, alpha=0.3, height=0.1) %>% \n  label_zero_one() \n\n\n\n\n\nExplain in everyday language what the graph shows about the lives of humans.\nMake the graph again, but leave out the %>% label_zero_one(). Then explain what label_zero_one() does.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nThe graph shows that young nurses tended to be alive at the 20-year follow-up, older nurses not so much.\n%>% label_zero_one() adds an axis on the left of the graph showing that in the zero-one tranform of outcome, “Alive” is assigned the value 1 and “Dead” the value 0.\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section-3",
    "href": "NTI/NTI-Lesson19.html#section-3",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.4",
    "text": "19.4\nAbout the summarization of models. Pipe the model fit into any of four functions:\n\n%>% coefficients()\n%>% broom::tidy()\n%>% rsquared()\n%>% confint()\n\nREDO confint() so that the columns are named lower, middle, upper\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#obj.-19.3",
    "href": "NTI/NTI-Lesson19.html#obj.-19.3",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.5 (Obj. 19.3)",
    "text": "19.5 (Obj. 19.3)\nCalculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We’ll use the CO2city variable in the math300::MPG data frame. The basic calculation using the mosaic package is:\n\ndf_stats( ~ CO2city, data = math300::MPG, coverage(0.95))\n\n  response   lower   upper\n1  CO2city 276.475 684.525\n\n\nThe following figure shows a violin plot of CO2city which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.\n\n50% coverage interval -A- (c)\n75% coverage interval -A- (e)\n90% coverage interval -A- (g)\n100% coverage interval -A- (i). This extends from the min to the max, so you could have figured this out just from the figure."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#obj-19.3",
    "href": "NTI/NTI-Lesson19.html#obj-19.3",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.6 (Obj 19.3)",
    "text": "19.6 (Obj 19.3)\nThe two jitter + violin graphs below show the distribution of two different variables, X and Y. Which variable has more variability?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is about the same level of variability in variable A and variable B. This surprises some people. Remember, the amount of variability has to do with the spread of values of the variable. In variable B, those values are have a 95% prediction interval of about 30 to 65, about the same as for variable A. There are two things about plot (b) that suggest to many people that there is more variability in variable B.\n\nThe larger horizontal spread of the dots. Note that variable B is shown along the vertical axis. The horizontal spread imposed by jittering is completely arbitrary: the only values that count are on the y axis.\n\nThe scalloped, irregular edges of the violin plot.\n\nOn the other hand, some people look at the clustering of the data points in graph (b) into several discrete values, creating empty spaces in between. To them, this clustering implies less variability. And, in a way, it does. But the statistical meaning of variability has to do with the overall spread of the points, not whether they are restricted to discrete values."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#objs.-19.3-19.4",
    "href": "NTI/NTI-Lesson19.html#objs.-19.3-19.4",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.7 (Objs. 19.3 & 19.4)",
    "text": "19.7 (Objs. 19.3 & 19.4)\nThe graphs below show a violin plot of body mass index (BMI) for adults and children. One of the graphs shows a correct 95% coverage interval on BMI, the other does not.\nIdentify the incorrect graph and say what feature of the graph led to your answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nGraph (b) is correct. In graph (a), you can see that the interval fails to include a lot of the low BMI children and extends too high. For adults, the graph (a) interval extends too far low and doesn’t go high enough."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#e",
    "href": "NTI/NTI-Lesson19.html#e",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.E",
    "text": "19.E\nThere are two equivalent formats describing an interval numerically that are widely used:\n\nSpecify the lower and upper endpoints of the interval, e.g. 7 to 13.\nSpecify the center and half-width of the interval, e.g. 10 ± 3, which is just the same as 7 to 13.\n\nComplete the following table to show the equivalences between the two notations.\n\n\n\n\n \n  \n    Interval \n    bottom-to-top \n    plus-or-minus \n  \n \n\n  \n    (a) \n    3 to 11 \n     \n  \n  \n    (b) \n     \n    108 ± 10 \n  \n  \n    (c) \n     \n    30 ± 1 \n  \n  \n    (d) \n    97 to  100 \n     \n  \n  \n    (e) \n    -4 to  16 \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n7 ± 4\n98 to 118\n29 to 31\n98.5 ± 1.5\n6 ± 10\n\nIt’s a matter of judgement which format to use. The bottom-to-top notation highlights the range of the interval while the plus-or-minus notation emphasizes the center of the interval. As a rule of thumb, I suggest this:\n\nIf the first two digits are different between the top and bottom of the interval, use the bottom-to-top notation. So, write 387 to 393. If the first two digits are the same, use plus-or-minus. For instancer, the ratio of the mass of the Earth to that of the Moon is 81.3005678 ± 0.0000027. This is easier to take in at a glance than the equivalent 81.3005651 - 81.3005708"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#f",
    "href": "NTI/NTI-Lesson19.html#f",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.F",
    "text": "19.F"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#documenting-software",
    "href": "NTI/NTI-Lesson19.html#documenting-software",
    "title": "Math 300R NTI Lesson 19",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-11-04\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html",
    "href": "NTI/NTI-Lesson29.html",
    "title": "Math 300R NTI Lesson 29",
    "section": "",
    "text": "29.1 Correctly define “covariate”.\n29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n29.4 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#reading",
    "href": "NTI/NTI-Lesson29.html#reading",
    "title": "Math 300R NTI Lesson 29",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lesson",
    "href": "NTI/NTI-Lesson29.html#lesson",
    "title": "Math 300R NTI Lesson 29",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nSummary\n\n\n\nIncluding covariates in a model can help, or can hurt. These are the conclusions we’re working toward:\n\nIn-sample, including covariates in a model always reduces the (in-sample) prediction error. The pattern is stronger the smaller the training data set.\nOut-of-sample, including covariates may or may not reduce prediction error. It depends on whether the covariates are genuinely connected to the response variable.\nIrrelevant covariates make (out-of-sample) prediction worse. This effect is strongest for small training data sets.\n\nRemember, since some of the conclusions depend on what variables are connected to what, we need to demonstrate the phenomena using a system where we know the structure.\nWe’ll come back to this topic, as a basis for ANOVA, when we do hypothesis testing. There we’ll look at the sum of squares, mean square, F and such.\n\n\nToday, we’ll work mostly with in-sample modeling. This reflects the case in the real world, where you have a data set but not usually an easy way to collect more data for testing.1\nLet’s work with dag04,dag05, and dag07 to illustrate some points about covariates.\n\ndag_draw(dag04)\n\n\n\ndag_draw(dag05)\n\n\n\ndag_draw(dag07)\n\n\n\n\nStart with dag04, where variables a, b, and c all contribute to the formation of d.\n\ncompare_model_residuals(dag04, d ~ c, d~ b + c, d ~ a + b + c, n=50)\n\n[1] 1.628050 1.375794 1.105072\n\n\nPrediction error gets smaller, the more covariates are included.\nThe situation can be different. In dag05, a, b, and c all contribute to d, but not separately. a and b communicate with d only via c. If c is in the model, a and b contribute nothing to reducing prediction error.\n\ncompare_model_residuals(dag05, d ~ c, d~ b + c, d ~ a + b + c, n=50)\n\n[1] 1.138071 1.132395 1.126588\n\n\ndag07 is a case where we have covariates, but they aren’t actually connected to d. Will they reduce prediction error? We’ll use a very small sample size, \\(n=4\\), to make the situation obvious.\n\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 1.387186 2.217092 3.216340 3.231776\n\n\n\n\n\n\n\n\nPattern shown by dag07 model\n\n\n\nConfirm these by running many simulations.\n\nThe prediction error gets smaller the more covariates are included in the model.\nThe last prediction error, with 4 terms in the model (don’t forget 1!) is zero. A perfect model?"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#learning-checks",
    "href": "NTI/NTI-Lesson29.html#learning-checks",
    "title": "Math 300R NTI Lesson 29",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#sec-LC-29-A",
    "href": "NTI/NTI-Lesson29.html#sec-LC-29-A",
    "title": "Math 300R NTI Lesson 29",
    "section": "29.A",
    "text": "29.A\nThe `math300::Hill_racing” data frame records 2236 winning times (in seconds) in Scottish hill racing competitions. Consider this model of the winning time as a function of the race distance (km) and the total climb (meters):\n\nmod <- lm(time ~ distance + climb, data=Hill_racing)\n\nThe model_eval() function provides a convenient way to evaluate the model output (.output) for each of the rows in a data frame and, at the same time, calculates row-by-row residuals (.resid) and prediction errors (.lwr and .upr). Make sure to take not of the starting periods on the names.\n\nmodel_eval(mod) %>% head()\n\n  time distance climb  .output     .resid       .lwr     .upr\n1 1630        6   240 1679.215  -49.21475  -29.56279 3387.992\n2 1655        6   240 1679.215  -24.21475  -29.56279 3387.992\n3 2391        6   240 1679.215  711.78525  -29.56279 3387.992\n4 2351        6   240 1679.215  671.78525  -29.56279 3387.992\n5 4151       14   660 4805.779 -654.77947 3097.10184 6514.457\n6 3975       14   660 4805.779 -830.77947 3097.10184 6514.457\n\n\nThe RMS residual from the model can be calculated this way:\n\nmodel_eval(mod) %>%\n  summarize(rms = sqrt(mean(.resid^2)))\n\n       rms\n1 870.4631\n\n\n\nWhat are the units of the RMS residual?\nModify the calculation to compute the sum-of-square residuals. Report the result numerically. Be sure to say what are the units.\nWhat are the units of the effect size on time with respect to climb?\nWhat are the units of the effect size on time with respect to climb?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nRMS residual has the same units as the response variable. In this case, that’s the time to run the race, with units “seconds.”\nSS residual has units that are the square of the respond variable, in this case “square-seconds.”\nRecall that the effect size on the response with respect to an explanatory variable has the units of the response variable divided by the units of the explanatory variable. The climb variable has units of meters, so the effect size has units “seconds/meters.”\nseconds/km"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.b",
    "href": "NTI/NTI-Lesson29.html#lc-29.b",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.B",
    "text": "LC 29.B\nWhich of the following models are not nested within time ~ distance + climb?\n\ntime ~ 1\ntime ~ distance + sex\ntime ~ distance\ntime ~ climb\n\n\n\n\n\n\n\nSolution\n\n\n\nThe model time ~ distance + sex is not nested in time ~ distance + climb.\nNote that time ~ 1 is indeed nested in time ~ distance + climb. The 1 corresponds to the intercept."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#c",
    "href": "NTI/NTI-Lesson29.html#c",
    "title": "Math 300R NTI Lesson 29",
    "section": "29.C",
    "text": "29.C\nIn LC -Section 5 you calculated the RMS residuals and the sum-of-square residuals by wrangling the results from mod_eval(). That’s a perfectly good way to do things, but the work becomes tedious when there are multiple models you want to compare.\nFor convenience, there is a compare_model_residuals() command, which can calculate the RMS residual or sum-of-square residual for each of a set of models. All the models must have the same response variable.\n\nHill_racing %>% \n  compare_model_residuals(time ~ 1, \n                          time ~ distance + climb, \n                          time ~ distance + climb + sex,\n                          time ~ distance,\n                          measure = \"RMS\"\n                          )\n\n[1] 3122.4821  870.4631  775.2962 1189.7148\n\n\nIt happens that all of the models in the command are a nested set. Re-order the models so that each model nests inside the following model, that is, from smaller model to bigger model.\n\nDo the RMS residuals for the nested models increase or decrease when moving from a smaller model to a larger model?\nYou can calculate the sum-of-square residual by using the argument measure=\"SS\". Do the sum-of-square residuals for the nested models increas or decrease when moving from a smaller model to a larger model.\nYou can calculate R2 by using the argument measure=\"R2\". Do the R2 for the nested models increase or decrease when moving from a smaller model to a larger model.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nDecrease\nDecrease\nIncrease. R2 tells you how big the model output is compared to the response variable. 1-R2 tells you how big the residuals are compared to the response variable."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.d",
    "href": "NTI/NTI-Lesson29.html#lc-29.d",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.D",
    "text": "LC 29.D\n::: {.callout-warning} ## Still a draft\nLook at dag07. Notice that d is not connected to any of the other variables.\nGenerate a sample of size \\(n=6\\). Compare the sum of square residual (in sample) from the nested models c ~ 1, c ~ a c ~ a + b, and c ~ a + b + d. (Use the compare_model_residuals() using the argument method=\"SS\".\nWhich, if any, of the variables a, b, or d reduces the in-sample sum-of-squared residuals compared to the previous model.\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag07, c ~ 1, c ~ a, c ~ a + b, c ~ a + b + d, \n                        n=6, measure=\"R2\")\n\n[1] 0.000000 4.123805 5.041728 3.817478\n\n\n\n\nOut of sample, the useless covariate often increases the SS error."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.1",
    "href": "NTI/NTI-Lesson29.html#lc-29.1",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.1",
    "text": "LC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50)\n\n[1] 0.9584840 0.8647784 0.7687133 0.7156887\n\n\nd seems to block effect of a and b on c.\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5)\n\n[1] 0.5045527 0.9949466 1.2849421 1.3575294"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.2",
    "href": "NTI/NTI-Lesson29.html#lc-29.2",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, \n                        testing = \"out-of-sample\")\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.3",
    "href": "NTI/NTI-Lesson29.html#lc-29.3",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.3",
    "text": "LC 29.3\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::teacher. What’s the base pay difference between a teacher with an MA and a BA degree? What’s a confidence interval on this effect size? How does the confidence interval change if you include years as a covariate."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.4",
    "href": "NTI/NTI-Lesson29.html#lc-29.4",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.4",
    "text": "LC 29.4\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::census Predict log personal income based on other variables. Eat variance using the total_family_income variable.\n\nmod <- lm(log10(total_personal_income) ~ log10(age) + sex + marital_status + log10(total_family_income), data = openintro::census %>% filter(total_personal_income > 0, total_family_income > 0))\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: log10(total_personal_income)\n                            Df Sum Sq Mean Sq  F value    Pr(>F)    \nlog10(age)                   1  5.938  5.9383  35.6102 6.660e-09 ***\nsex                          1  5.976  5.9758  35.8351 6.006e-09 ***\nmarital_status               5  4.302  0.8604   5.1596 0.0001464 ***\nlog10(total_family_income)   1 17.620 17.6198 105.6615 < 2.2e-16 ***\nResiduals                  306 51.028  0.1668                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ngf_jitter(total_personal_income ~ total_family_income | sex, \n         data =openintro::census %>% filter(total_personal_income > 3000),\n         color=~marital_status, alpha=0.3) %>% \n  gf_refine(scale_y_log10(), scale_x_log10())\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning: Removed 20 rows containing missing values (geom_point)."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#section",
    "href": "NTI/NTI-Lesson29.html#section",
    "title": "Math 300R NTI Lesson 29",
    "section": "29.5",
    "text": "29.5\n\n\n\n\n\n\nStill in draft\n\n\n\nopenintro::starbucks where do the calories come from? Find effect size of, say, protein on calories. Then see what happens if you use carbohydrates as a covariate.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ fat + carb + fiber + protein, data = openintro::starbucks) %>% confint()\n\n                2.5 %    97.5 %\n(Intercept) -2.938079 13.605279\nfat          8.591250  9.315766\ncarb         3.686593  3.997527\nfiber       -1.418966  1.370022\nprotein      3.631695  4.364091"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#in-draft-2",
    "href": "NTI/NTI-Lesson29.html#in-draft-2",
    "title": "Math 300R NTI Lesson 29",
    "section": "In draft",
    "text": "In draft\nMaybe come back to this in confounding lesson. Look for components that tend to go together\n\nwith(openintro::starbucks, cor(fat, protein))\n\n[1] 0.22347\n\nwith(openintro::starbucks, cor(fiber, protein))\n\n[1] 0.488564\n\nlm( calories ~ fiber , data = openintro::starbucks) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 276.119106 343.80739\nfiber         1.923476  24.07453\n\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ protein + fiber, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 247.891487 320.333514\nprotein       1.700777   7.996897\nfiber        -8.099007  15.978382"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#documenting-software",
    "href": "NTI/NTI-Lesson29.html#documenting-software",
    "title": "Math 300R NTI Lesson 29",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html",
    "href": "NTI/NTI-Lesson28.html",
    "title": "Math 300R NTI Lesson 28",
    "section": "",
    "text": "28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#reading",
    "href": "NTI/NTI-Lesson28.html#reading",
    "title": "Math 300R NTI Lesson 28",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#lesson",
    "href": "NTI/NTI-Lesson28.html#lesson",
    "title": "Math 300R NTI Lesson 28",
    "section": "Lesson",
    "text": "Lesson\nWe’ve talked about explanatory variables and the response variable. Sometimes, we have one or a few explanatory variables that we care about, but recognize that others may be playing a role in the formation of the outcome. The explanatory variables that we don’t care about are called covariates. A covariate is nothing more than an explanatory variable in which we don’t have a direct interest.\nToday’s lesson is about whether using covariates can change the prediction error, either for better (a smaller prediction error) or for worse (a bigger prediction error).\nTo illustrate, consider dag04 in which multiple variables contribute to an outcome:\n\ndag_draw(dag04)\n\n\n\n\nIt might seem evident that, to predict d, using a, b, and c as explantory variables will produce narrower prediction intervals than using just one or two of the variables. We can confirm this intuition—we’ll do it with out-of-sample RMS error.\n\nTraining <- sample(dag04, size=500)\nmod1 <- lm(d ~ b, data = Training)\nmod2 <- lm(d ~ a + b + c, data = Training)\nTesting <- sample(dag04, size=1000)\nmod_eval(mod1, data = Testing) %>%\n  summarize(rms = sqrt(mean((d - model_output)^2)))\n\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.72\n\nmod_eval(mod2, data = Testing) %>%\n  summarize(rms = sqrt(mean((d - model_output)^2)))\n\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.01\n\n\nUsing the covariates reduces prediction error.\n\n\n\n\n\n\nAutomating model comparison\n\n\n\nHow about in a situation like dag05:\n\ndag_draw(dag05)\n\n\n\n\n\ncompare_model_residuals(dag05, n=500, d ~ c, d ~ b, d ~ a, d ~ a + b + c)\n\n[1] 1.010311 1.508200 1.840362 1.011861\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\ndag_draw(dag06)\n\n\n\n\n\nIn dag06, which are the best explanatory variables for predicting d?\nCan d and b help in predicting a?\n\n\n\nDo these principles hold for in-sample prediction error?\n\ncompare_model_residuals(dag05, n=500, d ~ c, d ~ b, d ~ a, d ~ a + b + c,\n                        testing=\"out-of-sample\")\n\n[1] 1.013161 1.428794 1.730756 1.014568\n\n\n\nLearning Checks\n\n\n\n\n\n\nRemove these hard-coded objectives after drafting problems\n\n\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)\n\n\n\n\n28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#learning-checks",
    "href": "NTI/NTI-Lesson28.html#learning-checks",
    "title": "Math 300R NTI Lesson 28",
    "section": "Learning Checks",
    "text": "Learning Checks\n\n\n\n\n\n\nRemove these hard-coded objectives after drafting problems\n\n\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#section",
    "href": "NTI/NTI-Lesson28.html#section",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.1",
    "text": "28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#b",
    "href": "NTI/NTI-Lesson28.html#b",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.B",
    "text": "28.B\nWhenever you seek to study a partial relationship, there must be at least three variables involves: a response variable, an explanatory variable that is of direct interest, and one or more other explanatory variables that will be held constant: the covariates. Unfortunately, it’s hard to graph out models involving three variables on paper: the usual graph of a model just shows one variable as a function of a second.\nOne way to display the relationship between a response variable and two quantitative explanatory variables is to use a contour plot. The two explanatory variables are plotted on the axes and the fitted model values are shown by the contours. Figure 1 shows such a display of the fitted model of used car prices as a function of mileage and age.\n\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\nThe dots are the mileage and age of the individual cars — the model Price is indicated by the contours.\nThe total relationship between Price and mileage involves how the price changes for typical cars of different mileage.\n\nPick a dot that is a typical car with about 10,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price (in dollars)?\n\n18000, 21000, 25000, 30000\n\nPick another dot that is a typical car with about 70,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price?\n\n18000 21000, 25000, 30000\nThe total relationship between Price and mileage is reflected by this ratio: change in model price divided by change in mileage. What is that ratio (roughly)?\n\n\\(\\frac{30000 - 21000}{70000-10000}=0.15\\) dollars/mile\n\\(\\frac{70000-10000}{25000-21000}=15.0\\) dollars/mile\n\\(\\frac{25000 - 18000}{70000-10000}=0.12\\) dollars/mile\n\nIn contrast, the partial relationship between Price and mileage holding age constant is found in a different way, by comparing two points with different mileage but exactly the same age.\n\nMark a point on the graph where age is 3 years and mileage is\nKeep in mind that this point doesn’t need to be an actual car, that is, a data point in the graph typical car. There might be no actual car with an age of 3 years and mileage 10000. But using the contour model, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000 28000, 30000\n\nFind another point, one where the age is exactly the same (3 years) but the mileage is different. Again there might not be an actual car there. Let’s pick mileage as 80000. Using the contours, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000, 28000, 30000\n\nThe partial relationship between price and mileage (holding age constant) is reflected again reflected by the ratio of the change in model price divided by the change in mileage. What is that ratio (roughly)?\n\n\n\\(\\frac{80000-10000}{25000-21000} = 17.50\\) dollars/mile\n\\(\\frac{28000 - 22000}{80000-10000}=0.09\\) dollars/mile\n\\(\\frac{26000 - 24000}{80000-10000}=0.03\\) dollars/mile\n\n\nBoth the total relationship and the partial relationship are indicated by the slope of the model price function given by the contours. The total relationship involves the slope between two points that are typical cars, as indicated by the dots. The partial relationship involves a slope along a different direction. When holding age constant, that direction is the one where mileage changes but age does not (vertical in the graph).\nThere’s also a partial relationship between price and age holding mileage constant. That partial relationship involves the slope along the direction where age changes but mileage is held constant. Estimate that slope by finding the model price at a point where age is 2 years and another point where age is 5 years. You can pick whatever mileage you like, but it’s key that your two points be at exactly the same mileage.\n\nEstimate the slope of the price function along a direction where age changes but mileage is held constant (horizontally on the graph).\n\n\n100 dollars per year\n500 dollars per year\n1000 dollars per year\n2000 dollars per year\n\nThe contour plot in Figure 1 depicts a model in which both mileage and age are explanatory variables. By choosing the direction in which to measure the slope, one determines whether the slope reflects a total relationship (a direction between typical cars), or a partial relationship holding age constant (a direction where age does not change, which might not be typical for cars), or a partial relationship holding mileage constant (a direction where mileage does not change, which also might not be typical for cars).\nIn calculus, the partial derivative of price with respect to mileage refers to an infinitesimal change in a direction where age is held constant. Similarly, the partial derivative of price with respect to age refers to an infinitesimal change in a direction where mileage is held constant.\nOf course, in order for the directional derivatives to make sense, the price function needs to have both age and mileage as explanatory variables. Figure 2 shows a model in which only age has been used as an explanatory variable: there is no dependence of the function on mileage.\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nSuch a model is incapable of distinguishing between a partial relationship and a total relationship. Both the partial and the total relationship involve a ratio of the change in price and change in age between two points. For the total relationship, those two points would be typical cars of different ages. For the partial relationship, those two points would be different ages at exactly the same mileage. But, because the model depend on mileage, the two ratios will be exactly the same."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#c",
    "href": "NTI/NTI-Lesson28.html#c",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.C",
    "text": "28.C\nIn each of the following, a situation is described and a question is asked that is to be answered by modeling. Several variables are listed. Imagine an appropriate model and identify each variable as either the response variable, an explanatory variable, a covariate, or a variable to be ignored.\n\nEXAMPLE: Some people have claimed that police foot patrols are more effective at reducing the crime rate than patrols done in automobiles. Data from several different cities is available; each city has its own fraction of patrols done by foot, its own crime rate, etc. The mayor of your town has asked for your advice on whether it would be worthwhile to shift to more foot patrols in order to reduce crime. She asks, “Is there evidence that a larger fraction of foot patrols reduces the crime rate?”\n\nVariables:\n\nCrime rate (e.g., robberies per 100000 population)\nFraction of foot patrols\nNumber of policemen per 1000 population\nDemographics (e.g., poverty rate)\n\nAnswer: The question focuses on how the fraction of foot patrols might influence crime rate, so crime rate is the response variable and fraction of foot patrols is an explanatory variable.\nBut, the crime rate might also depend on the overall level of policing (as indicated by the number of policemen), or on the social conditions that are associated with crime (e.g., demographics). Since the mayor has no power to change the demographics of your town, and probably little power to change the overall level number of policemen, in modeling the data from the different cities, you would want to hold constant number of policemen and the demographics. You can do this by treating number of policemen and demographics as covariates and including them in your model.\n\n\nAlcohol and Road Safety\nFifteen years ago, your state legislature raised the legal drinking age from 18 to 21 years. An important motivation was to reduce the number of car accident deaths due to drunk or impaired drivers. Now, some people are arguing that the 21-year age limit encourages binge drinking among 18 to 20 year olds and that such binge drinking actually increases car accident deaths. But the evidence is that the number of car accident deaths has gone down since the 21-year age restriction was introduced.\nYou are asked to examine the issue: Does the reduction in the number of car-accident deaths per year point to the effectiveness of the 21-year drinking age?\nVariables:\n\nDrinking age limit. Levels: 18 or 21.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car-accident deaths per year.\n\nWhich is it? response explanatory covariate ignore\n\nPrevalence of seat-belt use.\n\nWhich is it? response explanatory covariate ignore\n\nFraction of cars with air bags.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car accidents (with or without death).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nOf direct interest is how the drinking age limit accounts for the number of deaths, so these are, respectively, explanatory and response variables. But a lower death rate might also be explained by increased use of seat belts and of air bags; these can prevent deaths in an accident and they have been increasing over the same period in which the 21-year age limit was introduced.\nIn examining how the drinking age limit might affect the number of deaths, it might be important to hold these other factors constant. So, seat belts and air bags should be covariates included in the model.\nThe number of accidents is different. It seems plausible that the mechanism by which drunk driving causes deaths is by causing accidents. If the number of accidents were included as a covariate, then the model would be examining how the death rate changes with drinking age when {} even though the point is that the higher drinking age might reduce the number of accidents. So, number of accidents ought to be left out of the model.\n\n\n\n\nRating Surgeons\nYour state government wants to guide citizens in choosing physicians. As part of this effort, they are going to rank all the surgeons in your state. You have been asked to build the rating system and you have a set of variables available for your use. These variables have been measured for each of the 342,861 people who underwent surgery in your state last year: one person being treated by one doctor. How should you construct a rating system that will help citizens to choose the most effective surgeon for their own treatment?\nVariables:\n\nOutcome score. (A high score means that the operation did whatit was supposed to. A low score reflects failure, e.g. death. Death is a very bad outcome, post-operative infection a somewhat bad outcome.)\n\nWhich is it? response explanatory covariate ignore\n\nSurgeon. One level for each of the operating surgeons.\n\nWhich is it? response explanatory covariate ignore\n\nExperience of the surgeon.\n\nWhich is it? response explanatory covariate ignore\n\nDifficulty of the case.\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe patient has a choice of doctors and wants to have the best possible outcome. So the model needs to include surgeon as an explanatory variable and outcome score as the response.\nA simple model might be misleading for informing a patient’s choice. The best doctors might take on the most difficult cases and therefore have worse outcomes than doctors who are not as good. But the patient’s condition doesn’t change depending on what doctor is selected. This means that the difficulty of the case ought to be included as a covariate. The model would thus tell what is the typical outcome for each surgeon adjusting for the difficulty of the case, that is, given the patient’s condition.\nAnother variable that might explain the outcome is the experience of the surgeon; possibly more experienced surgeons produce better outcomes. However, experience of the surgeon\nshould not be included in the model used to inform a patient’s choice. The reason is that the patient’s choice of a doctor already reflects the experience of that doctor. From the patient’s point of view, it doesn’t matter whether the doctor’s outcomes reflect a high level of talent, a lot of experience, or superior training.\nThe choice of variables and covariates depends on the purpose of the model. If the purpose of the model were to decide how much experience to require of doctors before they are licensed, then an appropriate model would have outcome as the response, experience as the explanatory variable, and difficulty of the case and surgeon as covariates.\n\n\n\n\nSchool testing\nLast year, your school district hired a new superintendent to ``shake things up.’’ He did so, introducing several controversial new policies. At the end of the year, test scores were higher than last year. A representative of the teachers’ union has asked you to examine the score data and answer this question: Is there reason to think that the higher scores were the result of the superintendent’s new policies?\nVariables:\n\nSuperintendent (levels: New or Former superintendent)\n\nWhich is it? response explanatory covariate ignore\n\nExam difficulty\n\nWhich is it? response explanatory covariate ignore\n\nTest scores\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe issue of direct interest is whether the policies of the new superintendent might have influenced the test scores, so the model should be test scores as the response and superintendent as an explanatory variable. Of course, one possible mechanism that might have improved the scores, outside of the influence of the superintendent’s policies, is the test itself. If it were easier this year than last year, then it wouldn’t be surprising that the test scores improved this year even if the superintendent’s policies had no effect. So exam difficulty should be a covariate to be included in the model.\n\n\n\n\nGravity\nIn a bizarre twist of time, you find yourself as Galileo’s research assistant in Pisa in 1605. Galileo is studying gravity: Does gravity accelerate all materials in the same way, whether they be made of metal, wood, stone, etc.? Galileo hired you as his assistant because you have brought with you, from the 21st century, a stop-watch with which to measure time intervals, a computer, and your skill in statistical modeling. All of these seem miraculous to him.\nHe drops objects off the top of the Leaning Tower of Pisa and you measure the following:\nVariables\n\nThe size of the object (measured by its diameter).\n\nWhich is it? response explanatory covariate ignore\n\nTime of fall of the object.\n\nWhich is it? response explanatory covariate ignore\n\nThe material from which the object is made (brass, lead, wood, stone).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nGalileo wants to know how the material affects the time of fall of the object. These are the explanatory and response variables respectively. But the size of the object also has an influence, due to air resistance. For instance, a tiny ball will fall more slowly than a large ball. So the size of the object should be a covariate.\n\n\n\n##28.D\nPolling organizations often report their results in tabular form, as in Figure 3. The basic question in the poll summarized in Figure 3 asked whether the respondant agrees with the statement, “The US was a better place to live in the 1990s and will continue to decline.”\n\n\n\n\n\nFigure 3: Results from a poll conducted by Time magazine. (Source: Time, July 28, 2008, p. 41)\n\n\n\n\nThe response variable here is “pessimism.” In the report, there are three explanatory variables: race/ethnicity, income, and age. The report’s breakdown is one explanatory variable at a time, meaning that it considers “total change” rather than “change holding other factors constant.” This can be misleading when there are connections among the explanatory variables. For instance, relatively few people in the 18 to 29 age group have high incomes.\nPollsters rarely make available the raw data they collected. This is unfortunate because it prevents others from looking at the data in different ways. For the purpose of this exercise, you’ll use simulated data in the frame math300::Econ_outlook_poll. Of course, the simulation doesn’t necessarily describe people’s attitudes directly, but it does let you see how the conclusions drawn from the poll might have been different if the results for each explanatory variable had been presented in a way that adjusts for the other explanatory variables.\n\nConstruct the model pessimism ~ age - 1. Look at the coefficients and choose the statement that best reflects the results. (In case you’re wondering: The -1 is convenient when the explanatory variable is categorical. It ensures that a coefficient is reported for each level of the age variable. You’ll have to compare coefficients for different age groups to see a trend.)\nMiddle aged people have lower pessimism than young or old people.\nYoung people have the least pessimism.\nThere is no relationship between age and pessimism.\nNow construct the model pessimism ~ income - 1. Look at the coefficients and choose the statement that best reflects the results:\nHigher income people are more pessimistic than low-income people.\nHigher income people are less pessimistic than low-income people.\nThere is no relationship between income and pessimism.\nConstruct a model in which you can look at the relationship between pessimism and age while adjusting for income. That is, include income as a covariate in your model. Look at the coefficients from your model and choose the statement that best reflects the results:\nHolding income constant, older people tend to have higher levels of pessimism than young people.\nHolding income constant, young people tend to have higher levels of pessimism than old people.\nHolding income constant, there is no relationship between age and pessimism.\nYou can also interpret that same model to see the relationship between pessimism and income while adjusting for age. Which of the following statements best reflects the results? (Hint: make sure to pay attention to the sign of the coefficients.)\nHolding age constant, higher income people are more pessimistic than low-income people.\nHolding age constant, higher income people are less pessimistic than low-income people.\nHolding age constant, there is no relationship between income and pessimism."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#e",
    "href": "NTI/NTI-Lesson28.html#e",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.E",
    "text": "28.E\nA study1 on drug D indicates that patients who were given the drug were less likely to recover from their condition C. Here is a table showing the overall results:\n\n\n\nDrug\n# recovered\n# died\nRecovery Rate\n\n\n\n\nGiven\n1600\n2400\n40%\n\n\nNot given\n2000\n2000\n50%\n\n\n\nStrangely, when investigators looked at the situation separately for males and females, they found that the drug improves recovery for each group:\nSex | Drug | num recovered | # died | Recovery Rate :—-::———|—–:|—–:|——: Sex | Drug | num recovered | # died | Recovery Rate Females| Given | 900 | 2100 | 30% | Not given | 200 | 800 | 20% e | | | |\nMales | Given | 700 | 300 | 70% | Not given | 1800 | 1200 | 60%\n\nAre the two tables consistent with one another in terms of the numbers reported?\nDoes the drug improve recovery or hinder recovery?\nWhat advice would you give to a physician about whether or not to prescribe the drug to her patients? Give enough of an explanation that the physician can judge whether your advice is reasonable."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#f",
    "href": "NTI/NTI-Lesson28.html#f",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.F",
    "text": "28.F\nEconomists measure the inflation rate as a percent change in price per year. Unemployment is measured as the fraction (percentage) of those who want to work who are seeking jobs.\nAccording to economists, in the short run — say, from one year to another — there is a relationship between inflation and unemployment: all other things being equal, as unemployment goes up, inflation should go down. (The relationship is called the “Phillips curve,” but you don’t need to know that or anything technical about economics to answer this question.)\n\n\nIf the Phillips-curve relationship is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative\n\n\n\nBut despite the short term relationship, economists claim that In the long run — over decades — unemployment and inflation should be unrelated.\n\n\nIf the long-run theory is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative}\n\n\n\n\nThe point of this exercise is to figure out how to arrange a model so that you can study the short-term behavior of the relationship, or so that you can study the long term relationship.\nFor your reference, Figure 4 shows inflation and unemployment rates over about 30 years in the US. Each point shows the inflation and unemployment rates during one quarter of a year. The plotting symbol indicates which of three decade-long periods the point falls into.\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\nThe relationship between inflation and unemployment seems to be different from one decade to another — that’s the short term.\n\nWhich decade seems to violate the economists’ Phillips Curve short-term relationship? A, B, C, none, all\n\nUsing the modeling language, express these different possible relationships between the variables Inflation, Unemployment, and Decade, where the variable Decade is a categorical variable with the three different levels shown in the legend for the graph.\n\nInflation depends on Unemployment in a way that doesn’t change over time.\nInflation ~ Decade\n** ~ Inflation ~ Unemployment**\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation changes with the decade, but doesn’t depend on Unemployment.\n** ~ Inflation ~ Decade**\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in the same way every decade, but each decade introduces a new background inflation rate independent of Unemployment.\nInflation ~ Decade\nInflation ~ Unemployment\n** ~ Inflation ~ Unemployment + Decade**\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in a way that differs from decade to decade.\nInflation ~ Decade\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\n** ~ Inflation ~ Unemployment * Decade**\n\n\nWhether a model examines the short-term or the long-term behavior is analogous to whether a partial change or a total change is being considered.\n\nSuppose you wanted to study the long-term relationship between inflation and unemployment. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)\nNow suppose you want to study the short-term relationship. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#sec-28-G",
    "href": "NTI/NTI-Lesson28.html#sec-28-G",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.G",
    "text": "28.G\nConsider dag03, involving three variables: x, y, g\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nThe nature of 95% confidence intervals means that even when the true coefficient is zero, 5% of the time the confidence interval will not include zero. In a class with 100 students, around 5 will see this failure to include zero. In order to avoid those students from being fooled by such accidental effects, feel free to analyze 2 or more samples.\n\nRegretably, in the real world, when working with data that have already been collected, you can’t use such multiple samples to check your work. So there is always that 5% chance that a real effect of size zero will produce a confidence interval that doesn’t include zero. This is one reason why replication of results is useful."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#h",
    "href": "NTI/NTI-Lesson28.html#h",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.H",
    "text": "28.H\nThis learning challenge is much like LC -@28-G, but uses dag11 instead of dag03.\n\nCompare the graphs of dag03 and dag11. (You can use dag_draw() to generate the graph.) Are the two DAGs equivalent or not? Describe what differences you see between the two graphs and explain whether those differences are sufficient to make the two DAGs causally different.\n\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nNow look at the graph of dag12 and speculate whether the models x ~ y and x ~ y + g will give equivalent results. (Don’t include node h in the models.) Write down your speculation. (No penalty for being wrong, it’s just a speculation!) Then, generate a sample from dag12 and, looking at the 95% confidence intervals on the coefficients, explain whether your speculation was correct or not."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#i",
    "href": "NTI/NTI-Lesson28.html#i",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.I",
    "text": "28.I"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#documenting-software",
    "href": "NTI/NTI-Lesson28.html#documenting-software",
    "title": "Math 300R NTI Lesson 28",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html",
    "href": "Reading-notes/Reading-notes-lesson-23.html",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "",
    "text": "Case : a row in a data frame\nSample: a data frame.\nSummarized sample: lm(model, data=dataframe) %>% summary()\n\nThis is as far as we can go with real data. DAG simulation (gaming) let us go further:\n\nSample: a draw of \\(n\\) cases from the DAG: sample(DAG, size=50)\nTrial: a summarized sample: r trial <- function(n=50) {   lm(tilde, data=sample(DAG, size=n) %>% summary() }\nRepeated trials to see sampling:distribution: do(100) * trial()\nSummarize the repeated trials: e.g. standard deviation of trial-by-trial coefficients r     do(100) * trial() %>% summarize(sd = sd(coef_on_x)) We found that the standard deviation of trial-by-trial coefficient\n\n\nIs smaller if \\(n\\) is bigger.\nSpecifically, is proportional to \\(\\frac{1}{\\sqrt{n}}\\).\n\n\n\n\n\n\n\nFormulas for sampling distributions\n\n\n\nStatistics textbooks often give formulas for the standard deviation of the sampling distribution. The formulas have been constructed for many of the standard situations. To give you an idea of how this is done, let’s go over the very simplest situation.\nSystem: \\(\\epsilon \\longrightarrow y\\) with tilde xy~ 1.\nInterpretation: The coefficient on 1, that is, the “intercept” is an estimate of the mean of \\(y\\).\nTHIS IS JUST A SKETCH.\nWhen \\(n=1\\), that is, the mean of a sample of size \\(n=1\\), the standard deviation of the sampling distribution is just \\(sd(y)\\). Of course, we can’t estimate this from a single sample of size \\(n=1\\) because we need at least \\(n=2\\) to calculate a standard deviation. If we knew the DAG behind the data, we could read \\(sd(y)\\) from the DAG. Let’s imagine that we do and use the name \\(\\sigmal\\) for the standard deviation from many trials on the DAG. But if we have \\(n > 1\\), we could calculate the SD from the sample. Let’s call that \\(s\\), our estimate of sigma.\nWe also know that the SD of the sampling distribution scales as \\(\\frac{1}{n}\\).\n\n\n\nSample size\nSD of Intercept coefficient\n\n\n\n\n\\(n=1\\)\n\\(\\sigma\\) as stipulated\n\n\n\\(n=2\\)\n\\(\\sigma/\\sqrt{2}\\) estimated by \\(s/\\sqrt{2}\\)\n\n\n\\(n=3\\)\n\\(\\sigma/\\sqrt{3}\\) estimated by \\(s/\\sqrt{3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(\\sigma/\\sqrt{n}\\) estimated by \\(s/\\sqrt{n}\\)\n\n\n\nGIVE FORMULAS for y ~ 1, y ~ yesno, y ~ x\nThe challenge faced by the traditional statistics student is to look up the correct formula for the situation at hand. But the computer can figure out the right formula directly from the tilde model."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#bootstrapping",
    "href": "Reading-notes/Reading-notes-lesson-23.html#bootstrapping",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#regression-table",
    "href": "Reading-notes/Reading-notes-lesson-23.html#regression-table",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Regression table",
    "text": "Regression table"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#is-sampling-variation-the-issue",
    "href": "Reading-notes/Reading-notes-lesson-23.html#is-sampling-variation-the-issue",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Is sampling variation the issue?",
    "text": "Is sampling variation the issue?\n\nFrom the 2018 StatPREP newsletter\n\nIn 1996 my department chair handed me the first statistics textbook I had ever seen. That single gesture constituted my college’s faculty development program for teaching statistics. One of the earliest examples in the book was about the importance of random sampling. It included a picture of President Truman holding up the Chicago Tribune’s infamous “Dewey Defeats Truman” headline. It’s a good story, but hardly timely, having taken place 48 years earlier. Few of my students knew who Truman was and none of them knew anything about Dewey.\nOur students have grown up in an era of “scientific” polling. Being scientific, the results are reported with a margin of error, often ±3 percentage points, to help us know when conclusions are warranted and when not. Many of our statistics courses feature units on constructing a margin of error on a sample proportion, often with explicit reference to political polls. But, like Dewey defeating Truman, the story is no longer timely. The “error” in the “margin of error” is now only a small part of the unreliability of polls. Why?\nIn an unprecedented opening up of the process of polling, The New York Times is letting us observe, live, their polling for the 2018 mid-term elections. You’ll find a description of the project in a September 2018 column and the live action here. It’s worth watching.\nFor those of you reading this after the polling ends, I’ll describe the action. As I write this, 2,070,469 telephone calls have been made. In each Congressional district, the results from the past calls are laid out in a long line of circles, filled red or blue depending on the the recipient’s response. But only 1 or 2% of the dots are filled. The large majority are empty: no response. Each new call generates a wiggling box at the head of the line of dots. It wiggles until the end of the call. Almost always, the box turns into an unfilled circle.\nThe poll I’m watching now, New Jersey 3rd district, is in its early stage. 4250 calls producing 62 responses. The margin of error? There’s a simple but meaningful statement laid right on top of the grayed-out tally so far: “Don’t take this poll seriously until we reach at least 250 people. We’re at 62.”\nThe calls are made based on a random selection from the phone numbers known to be in the district. But the random selection hardly generates a random sample when the response rate is 2%. To get something that resembles the population, pollsters weight their results. The New York Times is weighting “by age, party registration, gender, likelihood of voting, race, education and region, mainly using data from voting records files compiled by L2, a nonpartisan voter file vendor.” And then there’s the “likely voter” model, an informed guess about what fraction of people in each weighting strata will actually vote. There’s a detailed explanation in this article on the site, where the faulty results from the 2016 presidential election are attributed to a failure to weight by education level.\nSeeing the polling process in such detail reveals our misconceptions about what’s important in statistics. The so-called “margin of error” is not an adequate indicator of the reliability of the poll. Instead, we need to be thinking about the factors used in weighting and the extent to which they capture the current configuration of political schisms. Polls are now about big, multivariable data (the “voting records compiled by L2”) and building models of turnout based on previous elections."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#margin-of-error",
    "href": "Reading-notes/Reading-notes-lesson-23.html#margin-of-error",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Margin of error",
    "text": "Margin of error\n\none_trial <- function(n=2) {\n  vals <- rnorm(n)\n  tibble(m = mean(vals), s = sd(vals))\n}\n\nThe confidence interval from each trial will be \\(m \\pm \\beta s\\), where \\(\\beta\\) is a number yet to be determined. How to do so, we want to select \\(\\beta\\) so that, across all trials, 95% will include the mean of the distribution from which the data values were drawn.\n\n# vary beta until 95% of the trials have a left value smaller than zero.\nn <- 10000\nbeta <- 0.02\nTrials <- do(1000) * one_trial(n=n) %>% \n  mutate(left = m - beta*s, right = m + beta*s) \nTrials %>% \n  summarize(coverage = sum(sign(left*right) < 0)/n())\n\n# A tibble: 1 × 1\n  coverage\n     <dbl>\n1    0.956\n\n\nFor sample size \\(n=10\\), \\(\\beta\\) needs to be 0.72, while for a sample size \\(n=100\\), \\(\\beta\\) needs to be 0.20. For \\(n=1000\\), the multiplier needs to be 0.062, and so on. For \\(n=10000\\), the multiplier needs to be 0.02\n\n\n\nn\n\\(\\beta\\)\n\\(t = \\beta / \\sqrt{\\strut n}\\)\n\n\n\n\n10\n0.72\n2.26\n\n\n15\n0.55\n2.14\n\n\n20\n0.47\n2.09\n\n\n50\n0.28\n2.01\n\n\n100\n0.20\n1.98\n\n\n500\n0.088\n1.96\n\n\n1000\n0.062\n1.96\n\n\n10000\n0.20\n1.96\n\n\n\nNotice that as \\(n\\) gets bigger, the size of \\(\\beta\\) to cover 95% of the trials gets smaller. More than a century ago, it was known that the multiplier for any sample size \\(n\\) is effectively \\(2/\\sqrt{n}\\). Consequently, the confidence interval for the mean of \\(n\\) values is approximately\n\\[\\mathtt{CI} = \\mathtt{mean(x)}\\pm \\underbrace{\\frac{2}{\\sqrt{n}} \\mathtt{sd(x)}}_\\text{margin of error}\\]\nThe quantity following the \\(\\pm\\) is called the “margin of error.” Because of the \\(\\pm\\), he overall length of the confidence interval is twice the margin of error.\nIt’s much easier to remember \\(2/\\sqrt{n}\\) than a list of \\(\\beta\\) values that change from one \\(n\\) to the next. Another ubiquitous memory aid involves another technical term, the standard error. This involves a simple re-arrangement of the equation for the confidence interval:\n\\[\\mathtt{CI} = \\mathtt{mean(x)}\\pm 2\\underbrace{\\frac{\\mathtt{sd(x)}}{\\sqrt{n}}} _\\text{standard error}\\]\nIt’s standard in statistical software to report the standard error of a coefficient. Usually abbreviated se or std.error or something similar. The software is doing the divide-by-\\(\\sqrt{n}\\) for you, so all you need to construct the margin of error is multiply the standard error by 2. That’s convenient, but it comes at the cost of yet another use of the words “standard” and “error,” which can be confusing.\nHere’s an example of a typical software output summarizing a model in the format called a “regression report.” Here’s an example, looking at the fuel economy of cars (mpg) as a function the car’s weight (wt) and horsepower (hp).\n\nlm(mpg ~ wt + hp, data = mtcars) %>% \n  broom::tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  37.2      1.60        23.3  2.57e-20\n2 wt           -3.88     0.633       -6.13 1.12e- 6\n3 hp           -0.0318   0.00903     -3.52 1.45e- 3\n\n\nAccording to this report, each additional 1000 lbs of weight decreases fuel economy by an estimated 3.9 miles per gallon. But since the model is based on a sample of data, it’s important to report the precision of that number in the face of sampling variation. The confidence interval is the standard format for that precision. It will be the estimate plus-or-minus two times the standard error, that is: \\(-3.88 \\pm 2\\times0.633\\), that is, -5.15 to -2.61 mpg per 1000 lbs. Similarly, each addition horsepower (hp) lowers fuel economy by \\(-0.032 \\pm 2 \\times 0.009\\), that is, -0.05 to 0.013 mpg per horsepower.\nEven more convenient is to calculate the confidence interval with confint() which handles all the computations, including the ones for tiny \\(n\\) described in ?@sec-tiny-n.\n\n\n\n\n\n\nHow many digits?\n\n\n\nNotice that the estimate of the wt coefficient in the above regression report is -3.87783074. That seems like an awful lot of digits to report when the confidence interval is -5.15 to -2.61. Or, rather, an awful lot of digits for the human reader.\nIt is of course easy for the human to ignore the last several digits of the number. This makes reading more reliable; there are not as many digits to confuse. Even worse, the many digits suggest a level of precision that is belied by the width of the confidence interval. (When the number is going to be part of a continuing computation, that is, the “reader” is a computer, mis-interpretion or faulty reading is not an issue, which is why the software calculates so many digits .)\nSo how many digits ought to be reported for a human reader? There is an easy procedure to determine this.\n\nLook at the standard error in the regression report and multiply by 2 to get the margin of error. For example, for the hp coefficient, the margin of error is \\(2 \\times 0.63273349 = 1.265467\\).\nIt is always the case that no more than two digits of the margin of error have any meaning. (Even the second digit would suffer sampling variation.) So round the margin of error to two digits, that is 1.3 for the hp standard error.\nNotice the location of the second digit of the rounded standard error. For hp, the second digit is 3 and it’s located in the one-tenths place. Round the coefficient to this place. So, the hp coefficient -3.87783074 will round to -3.9.\nThe confidence interval, formatted for the human reader, will be the rounded coefficient plus-or-minus the rounded standard error. For hp, the confidence interval will be \\(-3.9 \\pm 1.3\\) or -5.2 to -2.6."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#tiny-n-optional",
    "href": "Reading-notes/Reading-notes-lesson-23.html#tiny-n-optional",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Tiny \\(n\\) (optional)",
    "text": "Tiny \\(n\\) (optional)\nWhen you have a very small sample size—say, \\(n=2\\)—the values may coincidentally be very close together. Around 1907, William Gosset, a scientist at Guinness, discovered that such coincidences force \\(\\beta\\) to be much larger than \\(2/\\sqrt{n}\\) in order to produce confidence intervals that cover the mean of the data-generating process. Gosset’s particular interest was in making sense of Guinness’s standard testing protocols, which involve averaging the results from three small batches of beer ingredients. Contacting the leading statisticians of the day, Gosset was told that such small \\(n\\) is “brewing, not statistics.” Nonetheless, Gosset had to work within Guinness’s testing protocols, which were indeed brewing but still needed statistical interpretation.\nGosset carried out something very much like the trials we used above, but—amazingly—by hand, since this was the age before electronic computers. To see the problem he observed, let’s look at the confidence intervals calculated in 1000 trials with \\(n=3\\).\nSHOW THE FOLLOWING FOR n=3, n=2 (the worst case), and n=10\n\nn=10\nbeta <- 2 / sqrt(n)\nTrials <- do(100) * one_trial(n=n) %>% \n  mutate(left = m - beta*s, right = m + beta*s) \ngf_errorbarh(.index ~ left + right, data = Trials, alpha=0.5) %>%\n  gf_errorbarh(.index ~ left + right, \n               data = Trials %>% filter(left > 0 | right < 0)) %>%\n  gf_vline(xintercept = ~ 0, color=\"blue\", inherit=FALSE)\n\n\n\n\nGosset effectively tabulated the \\(\\beta\\) multipliers\n\n\n\nn\n\\(\\beta\\)\n\\(t = \\beta / \\sqrt{\\strut n}\\)\n\n\n\n\n2\n8.98\n12.7\n\n\n3\n2.48\n4.30\n\n\n4\n1.59\n3.18\n\n\n5\n1.24\n2.78\n\n\n6\n1.04\n2.57\n\n\n7\n0.92\n2.44\n\n\n\\(\\vdots\\)\n\n\n\n\n10\n0.72\n2.26\n\n\n15\n0.55\n2.14\n\n\n20\n0.47\n2.09\n\n\n50\n0.28\n2.01\n\n\n100\n0.20\n1.98\n\n\n500\n0.088\n1.96\n\n\n1000\n0.062\n1.96\n\n\n\nYou can see that for \\(n\\) bigger than 10 or 20, the \\(t\\) multiplier is 2. But for very small \\(n\\), the t-multiplier can be considerably larger.\nYou can see the wisdom of brewers here. They made tests by averaging measurements from three small batches of beer. If they had used only two batches, the confidence interval would be almost three times larger than for \\(n=3\\), making it very hard to conclude anything about whether the tests show the ingredients to be within the quality-control standards.\nGosset’s work was published under the pseudonym “Student,” since Guinness forbade employees to publish under their own names. Statisticians, recognizing the value of the work (and knowing the name behind the pseudonym), came to use the name \\(t\\), perhaps because tea was considered more refined than “beer.” In many statistics texts, you will see the phrase “Student t” to refer to how Gosset’s work is used."
  }
]
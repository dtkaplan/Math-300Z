[
  {
    "objectID": "NTI/NTI-Lesson31.html",
    "href": "NTI/NTI-Lesson31.html",
    "title": "Math 300R NTI Lesson 31",
    "section": "",
    "text": "By which I mean correlations that are not just statistical noise but do not represent a causal path between two variables."
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#objectives",
    "href": "NTI/NTI-Lesson31.html#objectives",
    "title": "Math 300R NTI Lesson 31",
    "section": "Objectives",
    "text": "Objectives\n31.1 Distinguish “common cause” and “collider” forms of DAG.\n31.2 Construct appropriate DAG to match a narrative hypothesis."
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#reading",
    "href": "NTI/NTI-Lesson31.html#reading",
    "title": "Math 300R NTI Lesson 31",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#lesson",
    "href": "NTI/NTI-Lesson31.html#lesson",
    "title": "Math 300R NTI Lesson 31",
    "section": "Lesson",
    "text": "Lesson\nStatisticians are often careful in their use of language about relationships. They will say two variables are “correlated” or “associated” rather than casually using words like “connected” or “related.” There is even a proverb, “Correlation is not causation.”\n\n\n\n\n\nFigure 1: XKCD’s take on correlation and causation.\n\n\n\n\nTraditionally, statistics courses have emphasized disputing any causal interpretation of correlations found in data. The response “Well, maybe,” in the last panel of the cartoon shows a student correctly having assimilated this lesson.\nAnother formulation might be more enlightening for statistics students: “Correlation is the sign in data of causal connections in mechanisms.” If two variables a and b are correlated, then either a causes b (perhaps indirectly), b causes a (also perhaps indirectly), or both a and b are caused by other factors. There’s another possibility as well, most easily seen with a corresponding DAG in hand. It’s important to consider all of these possibilities, rather than jumping to a conclusion of what causes what.\nIn this lesson, we’re going to look at only those correlations that would be supported by out-of-sample testing. Later, we’ll also need to deal with correlations that are an illusion of sampling fluctuation or due to a systematic hunt.\nWe’ll use the term “spurious correlation” to refer to those correlations created by mechanisms other than a causal path between two variables in a DAG.\n\n\n\n\n\n\nPaths in DAGS\n\n\n\nExamples and exercises classifying causal and correlating paths in DAGs.\nSelection on a collider or using it as a covariate.\n\ndag_draw(dag09)\n\n\n\nSample <- sample(dag09, size=500)\nlm(a ~ b, data = Sample)\n\n\nCall:\nlm(formula = a ~ b, data = Sample)\n\nCoefficients:\n(Intercept)            b  \n  -0.013249     0.007196  \n\nlm(a ~ b, data = Sample %>% filter(c==1))\n\n\nCall:\nlm(formula = a ~ b, data = Sample %>% filter(c == 1))\n\nCoefficients:\n(Intercept)            b  \n      0.652       -0.400  \n\nlm(a ~ b, data = Sample %>% filter(c==0))\n\n\nCall:\nlm(formula = a ~ b, data = Sample %>% filter(c == 0))\n\nCoefficients:\n(Intercept)            b  \n    -0.6600      -0.3592  \n\nlm(a ~ b, data = Sample) %>% summary()\n\n\nCall:\nlm(formula = a ~ b, data = Sample)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9370 -0.7473  0.1003  0.6630  2.5546 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.013249   0.043811  -0.302    0.762\nb            0.007196   0.044263   0.163    0.871\n\nResidual standard error: 0.9796 on 498 degrees of freedom\nMultiple R-squared:  5.307e-05, Adjusted R-squared:  -0.001955 \nF-statistic: 0.02643 on 1 and 498 DF,  p-value: 0.8709\n\n\n\n\n\n\n\n\n\n\nRegression to mediocrity\n\n\n\nAn unusually large deviation is likely to be followed by one not so unusually large."
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#learning-checks",
    "href": "NTI/NTI-Lesson31.html#learning-checks",
    "title": "Math 300R NTI Lesson 31",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#section",
    "href": "NTI/NTI-Lesson31.html#section",
    "title": "Math 300R NTI Lesson 31",
    "section": "31.1",
    "text": "31.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#documenting-software",
    "href": "NTI/NTI-Lesson31.html#documenting-software",
    "title": "Math 300R NTI Lesson 31",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html",
    "href": "NTI/NTI-Lesson19.html",
    "title": "Math 300R NTI Lesson 19",
    "section": "",
    "text": "In the first half of the course, we examined data wrangling methods—group_by(), summarize() and tally() in particular—for summarizing variables and breaking up the summaries by groups.\nIn this second half of the course, we will be emphasizing the relationships between variables. The conceptual framework we’ve used for describing such relationships involves choosing a response variable and selecting one or more explanatory variables. Within this framework, you’ve already seen regression techniques for describing how a numerical response variable can be related to numerical explanatory variables. Something new you will see in the second half of the course is using regression techniques to handle categorical response variables. This will allow us to extend regression to cover such things as proportions and probabilities.\nGraphics can be a powerful way to perceive relationships between variables. In keeping with the response/explanatory framework, our go-to graphical technique will be to plot data as a “point plot” absolutely sticking to the convention that the response variable will be assigned to the vertical axis. Note that we’ll use the term “point plot” rather than the “scatter plot” that was used in the early graphics chapters. That’s partly because we’re going to use the idea of “scattering” in a different way.\nWe’re also going to adopt a new graphical convention: that statistical summaries should always be graphed as a layer on top of the raw data from which the summaries are derived. This means that we’ll switch away from histograms as a way of displaying distributions. Why? Because the vertical axis on a histogram does not correspond to the values of a response variable. Similarly, barplots won’t be appropriate, because the response variable in a barplot is represented by color rather than position on a vertical axis.\nOverall, both the graphical display of data and summaries and the calculation of numerical summaries like means and proportions will become much simpler to accomplish, since relationship summaries will be produced using using lm() and closely related model types. Data graphics will be made with geom_point() and closely related geoms."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#graphing-data-and-statistical-annotations",
    "href": "NTI/NTI-Lesson19.html#graphing-data-and-statistical-annotations",
    "title": "Math 300R NTI Lesson 19",
    "section": "Graphing data and statistical annotations",
    "text": "Graphing data and statistical annotations"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#categorical-response-variables",
    "href": "NTI/NTI-Lesson19.html#categorical-response-variables",
    "title": "Math 300R NTI Lesson 19",
    "section": "Categorical response variables",
    "text": "Categorical response variables\nTo get started, we’ll work with categorical response variables that are “dicotomous” or, equivalently, “binomial,” that is, with only two methods. This covers a large fraction of the situations where categorical response values are needed. We’ll leave those situations where there are lots of levels for the response variable to courses on “machine learning,” a topic we can only scratch the surface of in this introductory course.\nThe important insight into using categorical variables in regression models is encapsulated in the idea of the 0-1 (“zero-one”) encoding. To illustrate, consider the mosaicData::Whickham data frame that records the age, smoking status, and survival of 1000 or so nurses in the UK. The relationship that motivated the collection of these data is between smoking and survival. Taking a few rows from the data frame let’s us easily see the types of the variables involved:\n\n\n\n\n \n  \n    outcome \n    smoker \n    age \n  \n \n\n  \n    Dead \n    No \n    56 \n  \n  \n    Alive \n    Yes \n    54 \n  \n  \n    Alive \n    Yes \n    21 \n  \n  \n    Dead \n    No \n    72 \n  \n  \n    Alive \n    Yes \n    30 \n  \n  \n    Alive \n    No \n    19 \n  \n  \n    Alive \n    Yes \n    46 \n  \n  \n    Alive \n    No \n    31 \n  \n  \n    Alive \n    No \n    74 \n  \n  \n    Alive \n    Yes \n    32 \n  \n\n\n\n\n\nHOW TO USE math300::zero_one()\nPlotting a zero-one response variable with correctly labelled axes.\n\nP <- Whickham %>% \n  gf_jitter(zero_one(outcome) ~ age, color=~smoker, height=0.1, alpha=0.5) %>% \n  label_zero_one()\nP\n\n\n\n\nWe might equally well plot age as a function of outcome:\n\ngf_jitter(age ~ outcome, data = Whickham, width=0.2) %>% gf_violin(color=NA, fill=\"blue\", alpha=0.5)\n\n\n\n\nPLOT OUT A MODEL\n\nmod <- model_train(zero_one(outcome) ~ age + smoker, data = Whickham)\n# fun <- mod_fun(mod) \n# P %>% mosaicCalc::slice_plot(fun(age, smoker=\"Yes\") ~ age, color=\"red\")"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#intervals",
    "href": "NTI/NTI-Lesson19.html#intervals",
    "title": "Math 300R NTI Lesson 19",
    "section": "Intervals",
    "text": "Intervals\nOnce you have FIXES MOSAICMODEL, Generate a “prediction” interval and use gf_errorbar() to plot it over the model."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#objectives",
    "href": "NTI/NTI-Lesson19.html#objectives",
    "title": "Math 300R NTI Lesson 19",
    "section": "Objectives",
    "text": "Objectives\n19.1 Convert to the response vs explanatory format for data graphs.\n19.1 Understand the covering of a variable by an interval specified by a coverage level (e.g. 0.95).\n19.2 Be able to produce pointUse “violin” plots to display the density of a variable. (This should really go in the first half of the course, but ModernDive doesn’t do it.)\n19.3 Convert categorical variables to a 0-1 encoding. a. Generate graphics appropriate to a categorical encoding. (jitter plots) b. Apply modeling techniques to 0-1 encodings of binomial response variables."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#reading",
    "href": "NTI/NTI-Lesson19.html#reading",
    "title": "Math 300R NTI Lesson 19",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#lesson",
    "href": "NTI/NTI-Lesson19.html#lesson",
    "title": "Math 300R NTI Lesson 19",
    "section": "Lesson",
    "text": "Lesson\nThis lesson marks the beginning of a new phase of the course. Thus far, we’ve worked with techniques* for data wrangling, graphics, and regression modeling. Now we address the question of what a regression model (and other information that we might have) can tell us about the real world.*\n\n\n\n\n\n\nSetup\n\n\n\nlibrary(mosaicData)\n\n\n\n\n\n\n\n\nGuided activity: House prices\n\n\n\nHave students do the calculations for the first model, answering the questions that follow. After this is complete, have students do the calculations for the second model and answer those questions.\nThe mosaicData::SaratogaHouses data frame contains information about the sales price and various attributes of about 1700 houses. (See ?SaratogaHouses for a detailed description.)\nDo a regression of price ~ bedrooms and explain what the regression coefficients mean.\n\nlm(price ~ bedrooms, data=SaratogaHouses) |> coefficients()\n\n(Intercept)    bedrooms \n   59862.96    48217.81 \n\n\n\nWhat are the units of the intercept and of the bedrooms coefficient? *Intercept in dollars, bedrooms in dollars per bedroom.\nInterpret what the coefficients indicate about the price of houses and bedrooms. Each additional bedroom adds about $50000 to the value of a house.\nAccording to the model, predict what would be the sales price (at the time the data was collected, 2006) of a house with two bedrooms? \\(59863 + 2\\times 48218\\)\n\nOf course, bedrooms are not the only important thing about a house. Let’s include livingArea along with bedrooms in the model.\n\nlm(price ~ livingArea + bedrooms, data=SaratogaHouses) |> coefficients()\n\n(Intercept)  livingArea    bedrooms \n  36667.895     125.405  -14196.769 \n\n\n\nWhat are the units of the livingArea coefficient? dollars per square foot\nWhat does this model say about the value of adding a bedroom? It seems to reduce the value of the house by about $15,000.\n\nBased on exactly the same data, the two models seem to give contradictory statements about the value of an additional bedroom.\n\nIs one model right and the other wrong? If so, which one is right? (Explain your reasoning.) Both models are mathematically correct. But they need to be interpreted in different ways. Each is telling us something different about the real world.\nCould both models be right? If so, explain why the bedroom coefficients have opposite signs. We will need to develop some additional tools and concepts before we can take on this question.\n\nLearning how to interpret models in terms of what they say about the world is a major theme of this second half of Math 300.\nAnother, more technical question that we will address has to do with the precision of coefficients like 125.40 dollars per square foot. Might it actually be $200/ft2? How about $500/ft2? And how seriously should we take the sales value that we calculate by setting numbers for bedrooms and livingArea into the model?"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#learning-checks",
    "href": "NTI/NTI-Lesson19.html#learning-checks",
    "title": "Math 300R NTI Lesson 19",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#setup-1",
    "href": "NTI/NTI-Lesson19.html#setup-1",
    "title": "Math 300R NTI Lesson 19",
    "section": "Setup",
    "text": "Setup\nThe math300 package will be needed for lessons 20 through 39.\n\nlibrary(math300)\nlibrary(moderndive)\nlibrary(NHANES)"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section",
    "href": "NTI/NTI-Lesson19.html#section",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.1",
    "text": "19.1\nConsider the moderndive::evals data that records students’ evaluations (score, on a 1-5 scale) of the professors in each of several courses (the course ID), as well as the age, “average beauty rating” (bty_avg) of the professor, enrollment in the course (cls_students) and the level o the course (cls_level). Each row in the data frame is an individual course section.\n\n\n\n\n \n  \n    ID \n    score \n    age \n    bty_avg \n    cls_students \n    cls_level \n  \n \n\n  \n    329 \n    2.7 \n    64 \n    2.333 \n    22 \n    upper \n  \n  \n    313 \n    4.2 \n    42 \n    2.667 \n    86 \n    upper \n  \n  \n    430 \n    4.5 \n    33 \n    5.833 \n    120 \n    lower \n  \n  \n    95 \n    4.2 \n    48 \n    4.333 \n    33 \n    upper \n  \n  \n    209 \n    4.8 \n    60 \n    3.667 \n    34 \n    upper \n  \n  \n    442 \n    3.6 \n    61 \n    3.333 \n    39 \n    lower \n  \n  \n    351 \n    4.6 \n    50 \n    3.333 \n    26 \n    lower \n  \n  \n    317 \n    3.7 \n    52 \n    6.500 \n    44 \n    upper \n  \n  \n    444 \n    4.1 \n    52 \n    4.500 \n    111 \n    lower \n  \n  \n    315 \n    3.8 \n    52 \n    6.000 \n    88 \n    upper \n  \n\n\n\n\n\nThe following commands model score versus age and plots the data as a point plot.\n\nlm(score ~ age, data = moderndive::evals) %>% coefficients()\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\nopenintro::evals %>% gf_point(score ~ age, alpha=0.2 )\n\n\n\n\n\nExplain why some of the dots are darker than others?\n\n\n\n\n\n\n\nSolution\n\n\n\nAll the ages have integer values—e.g., 43, 44, 45—so the dots line up in vertical lines.\nSimilarly, the scores have values only to one decimal place—e.g., 3.1, 3.2, 3.3—so the dots line up in horizontal lines. If there are two or more rows in evals that have the same age and score, the dots will be plotted over one another. Since transparency (alpha = 0.2) is being used, points where there is a lot of overplotting will appear darker.\n\n\n\nRemake the plot, but using gf_jitter() instead of gf_point(). Explain what’s different about the jittered plot. (Hint: Almost all of the dots are the same lightness.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ age, alpha=0.2 )\n\n\n\n\n“Jittering” means to shift each dot by a small random amount. This reduces the number of instances where dots are overplotted.\n\n\n\nNow make a jitter plot of score versus class level (cls_level).\n\nWhat do the tick-mark labels on the horizontal axis describe? Are they numerical?\nTo judge from the plot, are their more lower-level than upper-level courses? Explain briefly what graphical feature lets you answer this question at a glance.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ cls_level)\n\n\n\n\n\nThe tick-mark labels are the levels of the categorical variable cls_level. The are words, not numbers.\nThere are many more dots in the right column than in the left. Since lower level class are shown in the left column, there are fewer lower-level courses than upper-level courses.\n\n\n\n\nThe two columns of points in the plot you made in (3) are not separated by very much empty space. You can fix this by giving gf_jitter() an argument width=0.2. Try different numerical values for width and report which one you find most effective at making the two columns clearly separated while avoiding overplotting.\nAre the scores, on average, different for the lower- vs upper-level classes? It’s hard to get more than a rough idea of the distribution of scores by looking at the “density” of points. The reason is that the number of points differs in the two columns. But there is an easy fix: add a layer to the graphic that shows the distribution (more or less like a histogram displays a distribution of values). You can do this by piping the jitter plot layer into a geom called a “violin,” like this:\n\n\nopenintro::evals %>% \n  gf_jitter(score ~ cls_level) %>%\n  gf_violin(fill=\"blue\", alpha=0.2, color=NA)\n\n\n\n\nExplain how to read the violins."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section-1",
    "href": "NTI/NTI-Lesson19.html#section-1",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.2",
    "text": "19.2\nThe openintro::promotions data comes the the 1970s and records the gender of 38 people along with the result of a decision to promote (or not) the person. =\nChapter 2 of ModernDive suggests graphically depicting decision versus gender by using a bar plot. There are two ways to make the bar plot, depending on which variable you assign to the horizontal axis and which to the fill color.\n\npromotions %>% gf_bar(~ decision, fill=~ gender)\npromotions %>% gf_bar(~ gender, fill=~decision)\n\n\n\n\nFigure 1: Two different ways to plot promotion outcome and gender\n\n\n\n\n\n\n\nFigure 2: Two different ways to plot promotion outcome and gender\n\n\n\n\nPlots like those in ?@fig-promotion-bars might be attractive or not, depending on your taste. What they don’t accomplish is to make sure which is the response variable and which the explanatory variable.\nThe choice of response and explanatory variables depends, of course, on what you are trying to display. But everyday English gives a big hint. For instance, you might describe the question at hand as, “Does gender affect promotion decisions.” Here, the variable doing the affecting is gender, and the outcome is the decision.\nModeling decision as a function of gender is easy once you convert the response variable to a zero-one variable. Like this:\n\nmod <- lm(zero_one(decision, one=\"promoted\") ~ gender, data = promotions)\ncoefficients(mod)\n\n (Intercept) genderfemale \n   0.8750000   -0.2916667 \n\nmosaicModel::mod_eval(mod)\n\n  gender model_output\n1   male    0.8750000\n2 female    0.5833333\n\n\n\nExplain what is the relationship between the model coefficients and the model outputs.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe coefficients tell how to calculate the model output. These coefficients say that the model output will be 0.875, but subtract 0.292 if the person is female.\nThe model outputs give the probability of being promoted for each of the two genders.\n\n\n\nMake this plot and explain what the red lines show. (We don’t expect you to be able to write the command to generate such plots on your own, but we do expect you to be able to interpret them.)\n\n\npromotions %>% \n  gf_jitter(zero_one(decision) ~ gender, height=0.2, width=0.2) %>%\n  gf_errorbar(model_output + model_output ~ gender, data=mod_eval(mod), \n              color=\"red\", inherit=FALSE) %>%\n  label_zero_one()\n\n\n\n\n\n\n\nSolution\n\n\n\nThe red lines show the proportion of the people in each gender group who were promoted. The y-axis scale on the left refers to the zero-one encoding of decision, while the y-axis labels on the right make it easier to read off the numerical value of the proportion."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section-2",
    "href": "NTI/NTI-Lesson19.html#section-2",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.3",
    "text": "19.3\nThe mosaicData::Whickham data from comes from a survey of a thousand or so nurses in the UK in the 1970s. The data record the age of each nurse along with whether the nurse was still alive in a follow-up survey 20 years later (outcome).\nMake this graph from the Whickham data:\n\ngf_jitter(zero_one(outcome) ~ age, data = Whickham, alpha=0.3, height=0.1) %>% \n  label_zero_one() \n\n\n\n\n\nExplain in everyday language what the graph shows about the lives of humans.\nMake the graph again, but leave out the %>% label_zero_one(). Then explain what label_zero_one() does.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nThe graph shows that young nurses tended to be alive at the 20-year follow-up, older nurses not so much.\n%>% label_zero_one() adds an axis on the left of the graph showing that in the zero-one tranform of outcome, “Alive” is assigned the value 1 and “Dead” the value 0.\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section-3",
    "href": "NTI/NTI-Lesson19.html#section-3",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.4",
    "text": "19.4\nAbout the summarization of models. Pipe the model fit into any of four functions:\n\n%>% coefficients()\n%>% broom::tidy()\n%>% rsquared()\n%>% confint()\n\nREDO confint() so that the columns are named lower, middle, upper\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#obj.-19.3",
    "href": "NTI/NTI-Lesson19.html#obj.-19.3",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.5 (Obj. 19.3)",
    "text": "19.5 (Obj. 19.3)\nCalculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We’ll use the CO2city variable in the math300::MPG data frame. The basic calculation using the mosaic package is:\n\ndf_stats( ~ CO2city, data = math300::MPG, coverage(0.95))\n\n  response   lower   upper\n1  CO2city 276.475 684.525\n\n\nThe following figure shows a violin plot of CO2city which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.\n\n50% coverage interval -A- (c)\n75% coverage interval -A- (e)\n90% coverage interval -A- (g)\n100% coverage interval -A- (i). This extends from the min to the max, so you could have figured this out just from the figure."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#obj-19.3",
    "href": "NTI/NTI-Lesson19.html#obj-19.3",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.6 (Obj 19.3)",
    "text": "19.6 (Obj 19.3)\nThe two jitter + violin graphs below show the distribution of two different variables, X and Y. Which variable has more variability?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is about the same level of variability in variable A and variable B. This surprises some people. Remember, the amount of variability has to do with the spread of values of the variable. In variable B, those values are have a 95% prediction interval of about 30 to 65, about the same as for variable A. There are two things about plot (b) that suggest to many people that there is more variability in variable B.\n\nThe larger horizontal spread of the dots. Note that variable B is shown along the vertical axis. The horizontal spread imposed by jittering is completely arbitrary: the only values that count are on the y axis.\n\nThe scalloped, irregular edges of the violin plot.\n\nOn the other hand, some people look at the clustering of the data points in graph (b) into several discrete values, creating empty spaces in between. To them, this clustering implies less variability. And, in a way, it does. But the statistical meaning of variability has to do with the overall spread of the points, not whether they are restricted to discrete values."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#objs.-19.3-19.4",
    "href": "NTI/NTI-Lesson19.html#objs.-19.3-19.4",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.7 (Objs. 19.3 & 19.4)",
    "text": "19.7 (Objs. 19.3 & 19.4)\nThe graphs below show a violin plot of body mass index (BMI) for adults and children. One of the graphs shows a correct 95% coverage interval on BMI, the other does not.\nIdentify the incorrect graph and say what feature of the graph led to your answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nGraph (b) is correct. In graph (a), you can see that the interval fails to include a lot of the low BMI children and extends too high. For adults, the graph (a) interval extends too far low and doesn’t go high enough."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#e",
    "href": "NTI/NTI-Lesson19.html#e",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.E",
    "text": "19.E\nThere are two equivalent formats describing an interval numerically that are widely used:\n\nSpecify the lower and upper endpoints of the interval, e.g. 7 to 13.\nSpecify the center and half-width of the interval, e.g. 10 ± 3, which is just the same as 7 to 13.\n\nComplete the following table to show the equivalences between the two notations.\n\n\n\n\n \n  \n    Interval \n    bottom-to-top \n    plus-or-minus \n  \n \n\n  \n    (a) \n    3 to 11 \n     \n  \n  \n    (b) \n     \n    108 ± 10 \n  \n  \n    (c) \n     \n    30 ± 1 \n  \n  \n    (d) \n    97 to  100 \n     \n  \n  \n    (e) \n    -4 to  16 \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n7 ± 4\n98 to 118\n29 to 31\n98.5 ± 1.5\n6 ± 10\n\nIt’s a matter of judgement which format to use. The bottom-to-top notation highlights the range of the interval while the plus-or-minus notation emphasizes the center of the interval. As a rule of thumb, I suggest this:\n\nIf the first two digits are different between the top and bottom of the interval, use the bottom-to-top notation. So, write 387 to 393. If the first two digits are the same, use plus-or-minus. For instancer, the ratio of the mass of the Earth to that of the Moon is 81.3005678 ± 0.0000027. This is easier to take in at a glance than the equivalent 81.3005651 - 81.3005708"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#f",
    "href": "NTI/NTI-Lesson19.html#f",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.F",
    "text": "19.F"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#documenting-software",
    "href": "NTI/NTI-Lesson19.html#documenting-software",
    "title": "Math 300R NTI Lesson 19",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-11-04\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html",
    "href": "NTI/NTI-Lesson28.html",
    "title": "Math 300R NTI Lesson 28",
    "section": "",
    "text": "28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#reading",
    "href": "NTI/NTI-Lesson28.html#reading",
    "title": "Math 300R NTI Lesson 28",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#lesson",
    "href": "NTI/NTI-Lesson28.html#lesson",
    "title": "Math 300R NTI Lesson 28",
    "section": "Lesson",
    "text": "Lesson\nWe’ve talked about explanatory variables and the response variable. Sometimes, we have one or a few explanatory variables that we care about, but recognize that others may be playing a role in the formation of the outcome. The explanatory variables that we don’t care about are called covariates. A covariate is nothing more than an explanatory variable in which we don’t have a direct interest.\nToday’s lesson is about whether using covariates can change the prediction error, either for better (a smaller prediction error) or for worse (a bigger prediction error).\nTo illustrate, consider dag04 in which multiple variables contribute to an outcome:\n\ndag_draw(dag04)\n\n\n\n\nIt might seem evident that, to predict d, using a, b, and c as explantory variables will produce narrower prediction intervals than using just one or two of the variables. We can confirm this intuition—we’ll do it with out-of-sample RMS error.\n\nTraining <- sample(dag04, size=500)\nmod1 <- lm(d ~ b, data = Training)\nmod2 <- lm(d ~ a + b + c, data = Training)\nTesting <- sample(dag04, size=1000)\nmod_eval(mod1, data = Testing) %>%\n  summarize(rms = sqrt(mean((d - model_output)^2)))\n\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.72\n\nmod_eval(mod2, data = Testing) %>%\n  summarize(rms = sqrt(mean((d - model_output)^2)))\n\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.01\n\n\nUsing the covariates reduces prediction error.\n\n\n\n\n\n\nAutomating model comparison\n\n\n\nHow about in a situation like dag05:\n\ndag_draw(dag05)\n\n\n\n\n\ncompare_model_residuals(dag05, n=500, d ~ c, d ~ b, d ~ a, d ~ a + b + c)\n\n[1] 1.010311 1.508200 1.840362 1.011861\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\ndag_draw(dag06)\n\n\n\n\n\nIn dag06, which are the best explanatory variables for predicting d?\nCan d and b help in predicting a?\n\n\n\nDo these principles hold for in-sample prediction error?\n\ncompare_model_residuals(dag05, n=500, d ~ c, d ~ b, d ~ a, d ~ a + b + c,\n                        testing=\"out-of-sample\")\n\n[1] 1.013161 1.428794 1.730756 1.014568\n\n\n\nLearning Checks\n\n\n\n\n\n\nRemove these hard-coded objectives after drafting problems\n\n\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)\n\n\n\n\n28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#learning-checks",
    "href": "NTI/NTI-Lesson28.html#learning-checks",
    "title": "Math 300R NTI Lesson 28",
    "section": "Learning Checks",
    "text": "Learning Checks\n\n\n\n\n\n\nRemove these hard-coded objectives after drafting problems\n\n\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#section",
    "href": "NTI/NTI-Lesson28.html#section",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.1",
    "text": "28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#b",
    "href": "NTI/NTI-Lesson28.html#b",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.B",
    "text": "28.B\nWhenever you seek to study a partial relationship, there must be at least three variables involves: a response variable, an explanatory variable that is of direct interest, and one or more other explanatory variables that will be held constant: the covariates. Unfortunately, it’s hard to graph out models involving three variables on paper: the usual graph of a model just shows one variable as a function of a second.\nOne way to display the relationship between a response variable and two quantitative explanatory variables is to use a contour plot. The two explanatory variables are plotted on the axes and the fitted model values are shown by the contours. Figure 1 shows such a display of the fitted model of used car prices as a function of mileage and age.\n\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\nThe dots are the mileage and age of the individual cars — the model Price is indicated by the contours.\nThe total relationship between Price and mileage involves how the price changes for typical cars of different mileage.\n\nPick a dot that is a typical car with about 10,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price (in dollars)?\n\n18000, 21000, 25000, 30000\n\nPick another dot that is a typical car with about 70,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price?\n\n18000 21000, 25000, 30000\nThe total relationship between Price and mileage is reflected by this ratio: change in model price divided by change in mileage. What is that ratio (roughly)?\n\n\\(\\frac{30000 - 21000}{70000-10000}=0.15\\) dollars/mile\n\\(\\frac{70000-10000}{25000-21000}=15.0\\) dollars/mile\n\\(\\frac{25000 - 18000}{70000-10000}=0.12\\) dollars/mile\n\nIn contrast, the partial relationship between Price and mileage holding age constant is found in a different way, by comparing two points with different mileage but exactly the same age.\n\nMark a point on the graph where age is 3 years and mileage is\nKeep in mind that this point doesn’t need to be an actual car, that is, a data point in the graph typical car. There might be no actual car with an age of 3 years and mileage 10000. But using the contour model, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000 28000, 30000\n\nFind another point, one where the age is exactly the same (3 years) but the mileage is different. Again there might not be an actual car there. Let’s pick mileage as 80000. Using the contours, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000, 28000, 30000\n\nThe partial relationship between price and mileage (holding age constant) is reflected again reflected by the ratio of the change in model price divided by the change in mileage. What is that ratio (roughly)?\n\n\n\\(\\frac{80000-10000}{25000-21000} = 17.50\\) dollars/mile\n\\(\\frac{28000 - 22000}{80000-10000}=0.09\\) dollars/mile\n\\(\\frac{26000 - 24000}{80000-10000}=0.03\\) dollars/mile\n\n\nBoth the total relationship and the partial relationship are indicated by the slope of the model price function given by the contours. The total relationship involves the slope between two points that are typical cars, as indicated by the dots. The partial relationship involves a slope along a different direction. When holding age constant, that direction is the one where mileage changes but age does not (vertical in the graph).\nThere’s also a partial relationship between price and age holding mileage constant. That partial relationship involves the slope along the direction where age changes but mileage is held constant. Estimate that slope by finding the model price at a point where age is 2 years and another point where age is 5 years. You can pick whatever mileage you like, but it’s key that your two points be at exactly the same mileage.\n\nEstimate the slope of the price function along a direction where age changes but mileage is held constant (horizontally on the graph).\n\n\n100 dollars per year\n500 dollars per year\n1000 dollars per year\n2000 dollars per year\n\nThe contour plot in Figure 1 depicts a model in which both mileage and age are explanatory variables. By choosing the direction in which to measure the slope, one determines whether the slope reflects a total relationship (a direction between typical cars), or a partial relationship holding age constant (a direction where age does not change, which might not be typical for cars), or a partial relationship holding mileage constant (a direction where mileage does not change, which also might not be typical for cars).\nIn calculus, the partial derivative of price with respect to mileage refers to an infinitesimal change in a direction where age is held constant. Similarly, the partial derivative of price with respect to age refers to an infinitesimal change in a direction where mileage is held constant.\nOf course, in order for the directional derivatives to make sense, the price function needs to have both age and mileage as explanatory variables. Figure 2 shows a model in which only age has been used as an explanatory variable: there is no dependence of the function on mileage.\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nSuch a model is incapable of distinguishing between a partial relationship and a total relationship. Both the partial and the total relationship involve a ratio of the change in price and change in age between two points. For the total relationship, those two points would be typical cars of different ages. For the partial relationship, those two points would be different ages at exactly the same mileage. But, because the model depend on mileage, the two ratios will be exactly the same."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#c",
    "href": "NTI/NTI-Lesson28.html#c",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.C",
    "text": "28.C\nIn each of the following, a situation is described and a question is asked that is to be answered by modeling. Several variables are listed. Imagine an appropriate model and identify each variable as either the response variable, an explanatory variable, a covariate, or a variable to be ignored.\n\nEXAMPLE: Some people have claimed that police foot patrols are more effective at reducing the crime rate than patrols done in automobiles. Data from several different cities is available; each city has its own fraction of patrols done by foot, its own crime rate, etc. The mayor of your town has asked for your advice on whether it would be worthwhile to shift to more foot patrols in order to reduce crime. She asks, “Is there evidence that a larger fraction of foot patrols reduces the crime rate?”\n\nVariables:\n\nCrime rate (e.g., robberies per 100000 population)\nFraction of foot patrols\nNumber of policemen per 1000 population\nDemographics (e.g., poverty rate)\n\nAnswer: The question focuses on how the fraction of foot patrols might influence crime rate, so crime rate is the response variable and fraction of foot patrols is an explanatory variable.\nBut, the crime rate might also depend on the overall level of policing (as indicated by the number of policemen), or on the social conditions that are associated with crime (e.g., demographics). Since the mayor has no power to change the demographics of your town, and probably little power to change the overall level number of policemen, in modeling the data from the different cities, you would want to hold constant number of policemen and the demographics. You can do this by treating number of policemen and demographics as covariates and including them in your model.\n\n\nAlcohol and Road Safety\nFifteen years ago, your state legislature raised the legal drinking age from 18 to 21 years. An important motivation was to reduce the number of car accident deaths due to drunk or impaired drivers. Now, some people are arguing that the 21-year age limit encourages binge drinking among 18 to 20 year olds and that such binge drinking actually increases car accident deaths. But the evidence is that the number of car accident deaths has gone down since the 21-year age restriction was introduced.\nYou are asked to examine the issue: Does the reduction in the number of car-accident deaths per year point to the effectiveness of the 21-year drinking age?\nVariables:\n\nDrinking age limit. Levels: 18 or 21.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car-accident deaths per year.\n\nWhich is it? response explanatory covariate ignore\n\nPrevalence of seat-belt use.\n\nWhich is it? response explanatory covariate ignore\n\nFraction of cars with air bags.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car accidents (with or without death).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nOf direct interest is how the drinking age limit accounts for the number of deaths, so these are, respectively, explanatory and response variables. But a lower death rate might also be explained by increased use of seat belts and of air bags; these can prevent deaths in an accident and they have been increasing over the same period in which the 21-year age limit was introduced.\nIn examining how the drinking age limit might affect the number of deaths, it might be important to hold these other factors constant. So, seat belts and air bags should be covariates included in the model.\nThe number of accidents is different. It seems plausible that the mechanism by which drunk driving causes deaths is by causing accidents. If the number of accidents were included as a covariate, then the model would be examining how the death rate changes with drinking age when {} even though the point is that the higher drinking age might reduce the number of accidents. So, number of accidents ought to be left out of the model.\n\n\n\n\nRating Surgeons\nYour state government wants to guide citizens in choosing physicians. As part of this effort, they are going to rank all the surgeons in your state. You have been asked to build the rating system and you have a set of variables available for your use. These variables have been measured for each of the 342,861 people who underwent surgery in your state last year: one person being treated by one doctor. How should you construct a rating system that will help citizens to choose the most effective surgeon for their own treatment?\nVariables:\n\nOutcome score. (A high score means that the operation did whatit was supposed to. A low score reflects failure, e.g. death. Death is a very bad outcome, post-operative infection a somewhat bad outcome.)\n\nWhich is it? response explanatory covariate ignore\n\nSurgeon. One level for each of the operating surgeons.\n\nWhich is it? response explanatory covariate ignore\n\nExperience of the surgeon.\n\nWhich is it? response explanatory covariate ignore\n\nDifficulty of the case.\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe patient has a choice of doctors and wants to have the best possible outcome. So the model needs to include surgeon as an explanatory variable and outcome score as the response.\nA simple model might be misleading for informing a patient’s choice. The best doctors might take on the most difficult cases and therefore have worse outcomes than doctors who are not as good. But the patient’s condition doesn’t change depending on what doctor is selected. This means that the difficulty of the case ought to be included as a covariate. The model would thus tell what is the typical outcome for each surgeon adjusting for the difficulty of the case, that is, given the patient’s condition.\nAnother variable that might explain the outcome is the experience of the surgeon; possibly more experienced surgeons produce better outcomes. However, experience of the surgeon\nshould not be included in the model used to inform a patient’s choice. The reason is that the patient’s choice of a doctor already reflects the experience of that doctor. From the patient’s point of view, it doesn’t matter whether the doctor’s outcomes reflect a high level of talent, a lot of experience, or superior training.\nThe choice of variables and covariates depends on the purpose of the model. If the purpose of the model were to decide how much experience to require of doctors before they are licensed, then an appropriate model would have outcome as the response, experience as the explanatory variable, and difficulty of the case and surgeon as covariates.\n\n\n\n\nSchool testing\nLast year, your school district hired a new superintendent to ``shake things up.’’ He did so, introducing several controversial new policies. At the end of the year, test scores were higher than last year. A representative of the teachers’ union has asked you to examine the score data and answer this question: Is there reason to think that the higher scores were the result of the superintendent’s new policies?\nVariables:\n\nSuperintendent (levels: New or Former superintendent)\n\nWhich is it? response explanatory covariate ignore\n\nExam difficulty\n\nWhich is it? response explanatory covariate ignore\n\nTest scores\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe issue of direct interest is whether the policies of the new superintendent might have influenced the test scores, so the model should be test scores as the response and superintendent as an explanatory variable. Of course, one possible mechanism that might have improved the scores, outside of the influence of the superintendent’s policies, is the test itself. If it were easier this year than last year, then it wouldn’t be surprising that the test scores improved this year even if the superintendent’s policies had no effect. So exam difficulty should be a covariate to be included in the model.\n\n\n\n\nGravity\nIn a bizarre twist of time, you find yourself as Galileo’s research assistant in Pisa in 1605. Galileo is studying gravity: Does gravity accelerate all materials in the same way, whether they be made of metal, wood, stone, etc.? Galileo hired you as his assistant because you have brought with you, from the 21st century, a stop-watch with which to measure time intervals, a computer, and your skill in statistical modeling. All of these seem miraculous to him.\nHe drops objects off the top of the Leaning Tower of Pisa and you measure the following:\nVariables\n\nThe size of the object (measured by its diameter).\n\nWhich is it? response explanatory covariate ignore\n\nTime of fall of the object.\n\nWhich is it? response explanatory covariate ignore\n\nThe material from which the object is made (brass, lead, wood, stone).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nGalileo wants to know how the material affects the time of fall of the object. These are the explanatory and response variables respectively. But the size of the object also has an influence, due to air resistance. For instance, a tiny ball will fall more slowly than a large ball. So the size of the object should be a covariate.\n\n\n\n##28.D\nPolling organizations often report their results in tabular form, as in Figure 3. The basic question in the poll summarized in Figure 3 asked whether the respondant agrees with the statement, “The US was a better place to live in the 1990s and will continue to decline.”\n\n\n\n\n\nFigure 3: Results from a poll conducted by Time magazine. (Source: Time, July 28, 2008, p. 41)\n\n\n\n\nThe response variable here is “pessimism.” In the report, there are three explanatory variables: race/ethnicity, income, and age. The report’s breakdown is one explanatory variable at a time, meaning that it considers “total change” rather than “change holding other factors constant.” This can be misleading when there are connections among the explanatory variables. For instance, relatively few people in the 18 to 29 age group have high incomes.\nPollsters rarely make available the raw data they collected. This is unfortunate because it prevents others from looking at the data in different ways. For the purpose of this exercise, you’ll use simulated data in the frame math300::Econ_outlook_poll. Of course, the simulation doesn’t necessarily describe people’s attitudes directly, but it does let you see how the conclusions drawn from the poll might have been different if the results for each explanatory variable had been presented in a way that adjusts for the other explanatory variables.\n\nConstruct the model pessimism ~ age - 1. Look at the coefficients and choose the statement that best reflects the results. (In case you’re wondering: The -1 is convenient when the explanatory variable is categorical. It ensures that a coefficient is reported for each level of the age variable. You’ll have to compare coefficients for different age groups to see a trend.)\nMiddle aged people have lower pessimism than young or old people.\nYoung people have the least pessimism.\nThere is no relationship between age and pessimism.\nNow construct the model pessimism ~ income - 1. Look at the coefficients and choose the statement that best reflects the results:\nHigher income people are more pessimistic than low-income people.\nHigher income people are less pessimistic than low-income people.\nThere is no relationship between income and pessimism.\nConstruct a model in which you can look at the relationship between pessimism and age while adjusting for income. That is, include income as a covariate in your model. Look at the coefficients from your model and choose the statement that best reflects the results:\nHolding income constant, older people tend to have higher levels of pessimism than young people.\nHolding income constant, young people tend to have higher levels of pessimism than old people.\nHolding income constant, there is no relationship between age and pessimism.\nYou can also interpret that same model to see the relationship between pessimism and income while adjusting for age. Which of the following statements best reflects the results? (Hint: make sure to pay attention to the sign of the coefficients.)\nHolding age constant, higher income people are more pessimistic than low-income people.\nHolding age constant, higher income people are less pessimistic than low-income people.\nHolding age constant, there is no relationship between income and pessimism."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#e",
    "href": "NTI/NTI-Lesson28.html#e",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.E",
    "text": "28.E\nA study1 on drug D indicates that patients who were given the drug were less likely to recover from their condition C. Here is a table showing the overall results:\n\n\n\nDrug\n# recovered\n# died\nRecovery Rate\n\n\n\n\nGiven\n1600\n2400\n40%\n\n\nNot given\n2000\n2000\n50%\n\n\n\nStrangely, when investigators looked at the situation separately for males and females, they found that the drug improves recovery for each group:\nSex | Drug | num recovered | # died | Recovery Rate :—-::———|—–:|—–:|——: Sex | Drug | num recovered | # died | Recovery Rate Females| Given | 900 | 2100 | 30% | Not given | 200 | 800 | 20% e | | | |\nMales | Given | 700 | 300 | 70% | Not given | 1800 | 1200 | 60%\n\nAre the two tables consistent with one another in terms of the numbers reported?\nDoes the drug improve recovery or hinder recovery?\nWhat advice would you give to a physician about whether or not to prescribe the drug to her patients? Give enough of an explanation that the physician can judge whether your advice is reasonable."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#f",
    "href": "NTI/NTI-Lesson28.html#f",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.F",
    "text": "28.F\nEconomists measure the inflation rate as a percent change in price per year. Unemployment is measured as the fraction (percentage) of those who want to work who are seeking jobs.\nAccording to economists, in the short run — say, from one year to another — there is a relationship between inflation and unemployment: all other things being equal, as unemployment goes up, inflation should go down. (The relationship is called the “Phillips curve,” but you don’t need to know that or anything technical about economics to answer this question.)\n\n\nIf the Phillips-curve relationship is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative\n\n\n\nBut despite the short term relationship, economists claim that In the long run — over decades — unemployment and inflation should be unrelated.\n\n\nIf the long-run theory is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative}\n\n\n\n\nThe point of this exercise is to figure out how to arrange a model so that you can study the short-term behavior of the relationship, or so that you can study the long term relationship.\nFor your reference, Figure 4 shows inflation and unemployment rates over about 30 years in the US. Each point shows the inflation and unemployment rates during one quarter of a year. The plotting symbol indicates which of three decade-long periods the point falls into.\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\nThe relationship between inflation and unemployment seems to be different from one decade to another — that’s the short term.\n\nWhich decade seems to violate the economists’ Phillips Curve short-term relationship? A, B, C, none, all\n\nUsing the modeling language, express these different possible relationships between the variables Inflation, Unemployment, and Decade, where the variable Decade is a categorical variable with the three different levels shown in the legend for the graph.\n\nInflation depends on Unemployment in a way that doesn’t change over time.\nInflation ~ Decade\n** ~ Inflation ~ Unemployment**\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation changes with the decade, but doesn’t depend on Unemployment.\n** ~ Inflation ~ Decade**\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in the same way every decade, but each decade introduces a new background inflation rate independent of Unemployment.\nInflation ~ Decade\nInflation ~ Unemployment\n** ~ Inflation ~ Unemployment + Decade**\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in a way that differs from decade to decade.\nInflation ~ Decade\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\n** ~ Inflation ~ Unemployment * Decade**\n\n\nWhether a model examines the short-term or the long-term behavior is analogous to whether a partial change or a total change is being considered.\n\nSuppose you wanted to study the long-term relationship between inflation and unemployment. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)\nNow suppose you want to study the short-term relationship. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#sec-28-G",
    "href": "NTI/NTI-Lesson28.html#sec-28-G",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.G",
    "text": "28.G\nConsider dag03, involving three variables: x, y, g\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nThe nature of 95% confidence intervals means that even when the true coefficient is zero, 5% of the time the confidence interval will not include zero. In a class with 100 students, around 5 will see this failure to include zero. In order to avoid those students from being fooled by such accidental effects, feel free to analyze 2 or more samples.\n\nRegretably, in the real world, when working with data that have already been collected, you can’t use such multiple samples to check your work. So there is always that 5% chance that a real effect of size zero will produce a confidence interval that doesn’t include zero. This is one reason why replication of results is useful."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#h",
    "href": "NTI/NTI-Lesson28.html#h",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.H",
    "text": "28.H\nThis learning challenge is much like LC -@28-G, but uses dag11 instead of dag03.\n\nCompare the graphs of dag03 and dag11. (You can use dag_draw() to generate the graph.) Are the two DAGs equivalent or not? Describe what differences you see between the two graphs and explain whether those differences are sufficient to make the two DAGs causally different.\n\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nNow look at the graph of dag12 and speculate whether the models x ~ y and x ~ y + g will give equivalent results. (Don’t include node h in the models.) Write down your speculation. (No penalty for being wrong, it’s just a speculation!) Then, generate a sample from dag12 and, looking at the 95% confidence intervals on the coefficients, explain whether your speculation was correct or not."
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#i",
    "href": "NTI/NTI-Lesson28.html#i",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.I",
    "text": "28.I"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#documenting-software",
    "href": "NTI/NTI-Lesson28.html#documenting-software",
    "title": "Math 300R NTI Lesson 28",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html",
    "href": "NTI/NTI-Lesson29.html",
    "title": "Math 300R NTI Lesson 29",
    "section": "",
    "text": "29.1 Correctly define “covariate”.\n29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n29.4 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#reading",
    "href": "NTI/NTI-Lesson29.html#reading",
    "title": "Math 300R NTI Lesson 29",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lesson",
    "href": "NTI/NTI-Lesson29.html#lesson",
    "title": "Math 300R NTI Lesson 29",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nSummary\n\n\n\nIncluding covariates in a model can help, or can hurt. These are the conclusions we’re working toward:\n\nIn-sample, including covariates in a model always reduces the (in-sample) prediction error. The pattern is stronger the smaller the training data set.\nOut-of-sample, including covariates may or may not reduce prediction error. It depends on whether the covariates are genuinely connected to the response variable.\nIrrelevant covariates make (out-of-sample) prediction worse. This effect is strongest for small training data sets.\n\nRemember, since some of the conclusions depend on what variables are connected to what, we need to demonstrate the phenomena using a system where we know the structure.\nWe’ll come back to this topic, as a basis for ANOVA, when we do hypothesis testing. There we’ll look at the sum of squares, mean square, F and such.\n\n\nToday, we’ll work mostly with in-sample modeling. This reflects the case in the real world, where you have a data set but not usually an easy way to collect more data for testing.1\nLet’s work with dag04,dag05, and dag07 to illustrate some points about covariates.\n\ndag_draw(dag04)\n\n\n\ndag_draw(dag05)\n\n\n\ndag_draw(dag07)\n\n\n\n\nStart with dag04, where variables a, b, and c all contribute to the formation of d.\n\ncompare_model_residuals(dag04, d ~ c, d~ b + c, d ~ a + b + c, n=50)\n\n[1] 1.628050 1.375794 1.105072\n\n\nPrediction error gets smaller, the more covariates are included.\nThe situation can be different. In dag05, a, b, and c all contribute to d, but not separately. a and b communicate with d only via c. If c is in the model, a and b contribute nothing to reducing prediction error.\n\ncompare_model_residuals(dag05, d ~ c, d~ b + c, d ~ a + b + c, n=50)\n\n[1] 1.138071 1.132395 1.126588\n\n\ndag07 is a case where we have covariates, but they aren’t actually connected to d. Will they reduce prediction error? We’ll use a very small sample size, \\(n=4\\), to make the situation obvious.\n\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 1.387186 2.217092 3.216340 3.231776\n\n\n\n\n\n\n\n\nPattern shown by dag07 model\n\n\n\nConfirm these by running many simulations.\n\nThe prediction error gets smaller the more covariates are included in the model.\nThe last prediction error, with 4 terms in the model (don’t forget 1!) is zero. A perfect model?"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#learning-checks",
    "href": "NTI/NTI-Lesson29.html#learning-checks",
    "title": "Math 300R NTI Lesson 29",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#sec-LC-29-A",
    "href": "NTI/NTI-Lesson29.html#sec-LC-29-A",
    "title": "Math 300R NTI Lesson 29",
    "section": "29.A",
    "text": "29.A\nThe `math300::Hill_racing” data frame records 2236 winning times (in seconds) in Scottish hill racing competitions. Consider this model of the winning time as a function of the race distance (km) and the total climb (meters):\n\nmod <- lm(time ~ distance + climb, data=Hill_racing)\n\nThe model_eval() function provides a convenient way to evaluate the model output (.output) for each of the rows in a data frame and, at the same time, calculates row-by-row residuals (.resid) and prediction errors (.lwr and .upr). Make sure to take not of the starting periods on the names.\n\nmodel_eval(mod) %>% head()\n\n  time distance climb  .output     .resid       .lwr     .upr\n1 1630        6   240 1679.215  -49.21475  -29.56279 3387.992\n2 1655        6   240 1679.215  -24.21475  -29.56279 3387.992\n3 2391        6   240 1679.215  711.78525  -29.56279 3387.992\n4 2351        6   240 1679.215  671.78525  -29.56279 3387.992\n5 4151       14   660 4805.779 -654.77947 3097.10184 6514.457\n6 3975       14   660 4805.779 -830.77947 3097.10184 6514.457\n\n\nThe RMS residual from the model can be calculated this way:\n\nmodel_eval(mod) %>%\n  summarize(rms = sqrt(mean(.resid^2)))\n\n       rms\n1 870.4631\n\n\n\nWhat are the units of the RMS residual?\nModify the calculation to compute the sum-of-square residuals. Report the result numerically. Be sure to say what are the units.\nWhat are the units of the effect size on time with respect to climb?\nWhat are the units of the effect size on time with respect to climb?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nRMS residual has the same units as the response variable. In this case, that’s the time to run the race, with units “seconds.”\nSS residual has units that are the square of the respond variable, in this case “square-seconds.”\nRecall that the effect size on the response with respect to an explanatory variable has the units of the response variable divided by the units of the explanatory variable. The climb variable has units of meters, so the effect size has units “seconds/meters.”\nseconds/km"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.b",
    "href": "NTI/NTI-Lesson29.html#lc-29.b",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.B",
    "text": "LC 29.B\nWhich of the following models are not nested within time ~ distance + climb?\n\ntime ~ 1\ntime ~ distance + sex\ntime ~ distance\ntime ~ climb\n\n\n\n\n\n\n\nSolution\n\n\n\nThe model time ~ distance + sex is not nested in time ~ distance + climb.\nNote that time ~ 1 is indeed nested in time ~ distance + climb. The 1 corresponds to the intercept."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#c",
    "href": "NTI/NTI-Lesson29.html#c",
    "title": "Math 300R NTI Lesson 29",
    "section": "29.C",
    "text": "29.C\nIn LC -Section 5 you calculated the RMS residuals and the sum-of-square residuals by wrangling the results from mod_eval(). That’s a perfectly good way to do things, but the work becomes tedious when there are multiple models you want to compare.\nFor convenience, there is a compare_model_residuals() command, which can calculate the RMS residual or sum-of-square residual for each of a set of models. All the models must have the same response variable.\n\nHill_racing %>% \n  compare_model_residuals(time ~ 1, \n                          time ~ distance + climb, \n                          time ~ distance + climb + sex,\n                          time ~ distance,\n                          measure = \"RMS\"\n                          )\n\n[1] 3122.4821  870.4631  775.2962 1189.7148\n\n\nIt happens that all of the models in the command are a nested set. Re-order the models so that each model nests inside the following model, that is, from smaller model to bigger model.\n\nDo the RMS residuals for the nested models increase or decrease when moving from a smaller model to a larger model?\nYou can calculate the sum-of-square residual by using the argument measure=\"SS\". Do the sum-of-square residuals for the nested models increas or decrease when moving from a smaller model to a larger model.\nYou can calculate R2 by using the argument measure=\"R2\". Do the R2 for the nested models increase or decrease when moving from a smaller model to a larger model.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nDecrease\nDecrease\nIncrease. R2 tells you how big the model output is compared to the response variable. 1-R2 tells you how big the residuals are compared to the response variable."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.d",
    "href": "NTI/NTI-Lesson29.html#lc-29.d",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.D",
    "text": "LC 29.D\n::: {.callout-warning} ## Still a draft\nLook at dag07. Notice that d is not connected to any of the other variables.\nGenerate a sample of size \\(n=6\\). Compare the sum of square residual (in sample) from the nested models c ~ 1, c ~ a c ~ a + b, and c ~ a + b + d. (Use the compare_model_residuals() using the argument method=\"SS\".\nWhich, if any, of the variables a, b, or d reduces the in-sample sum-of-squared residuals compared to the previous model.\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag07, c ~ 1, c ~ a, c ~ a + b, c ~ a + b + d, \n                        n=6, measure=\"R2\")\n\n[1] 0.000000 4.123805 5.041728 3.817478\n\n\n\n\nOut of sample, the useless covariate often increases the SS error."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.1",
    "href": "NTI/NTI-Lesson29.html#lc-29.1",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.1",
    "text": "LC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50)\n\n[1] 0.9584840 0.8647784 0.7687133 0.7156887\n\n\nd seems to block effect of a and b on c.\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5)\n\n[1] 0.5045527 0.9949466 1.2849421 1.3575294"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.2",
    "href": "NTI/NTI-Lesson29.html#lc-29.2",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, \n                        testing = \"out-of-sample\")\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.3",
    "href": "NTI/NTI-Lesson29.html#lc-29.3",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.3",
    "text": "LC 29.3\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::teacher. What’s the base pay difference between a teacher with an MA and a BA degree? What’s a confidence interval on this effect size? How does the confidence interval change if you include years as a covariate."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.4",
    "href": "NTI/NTI-Lesson29.html#lc-29.4",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.4",
    "text": "LC 29.4\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::census Predict log personal income based on other variables. Eat variance using the total_family_income variable.\n\nmod <- lm(log10(total_personal_income) ~ log10(age) + sex + marital_status + log10(total_family_income), data = openintro::census %>% filter(total_personal_income > 0, total_family_income > 0))\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: log10(total_personal_income)\n                            Df Sum Sq Mean Sq  F value    Pr(>F)    \nlog10(age)                   1  5.938  5.9383  35.6102 6.660e-09 ***\nsex                          1  5.976  5.9758  35.8351 6.006e-09 ***\nmarital_status               5  4.302  0.8604   5.1596 0.0001464 ***\nlog10(total_family_income)   1 17.620 17.6198 105.6615 < 2.2e-16 ***\nResiduals                  306 51.028  0.1668                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ngf_jitter(total_personal_income ~ total_family_income | sex, \n         data =openintro::census %>% filter(total_personal_income > 3000),\n         color=~marital_status, alpha=0.3) %>% \n  gf_refine(scale_y_log10(), scale_x_log10())\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning: Removed 20 rows containing missing values (geom_point)."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#section",
    "href": "NTI/NTI-Lesson29.html#section",
    "title": "Math 300R NTI Lesson 29",
    "section": "29.5",
    "text": "29.5\n\n\n\n\n\n\nStill in draft\n\n\n\nopenintro::starbucks where do the calories come from? Find effect size of, say, protein on calories. Then see what happens if you use carbohydrates as a covariate.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ fat + carb + fiber + protein, data = openintro::starbucks) %>% confint()\n\n                2.5 %    97.5 %\n(Intercept) -2.938079 13.605279\nfat          8.591250  9.315766\ncarb         3.686593  3.997527\nfiber       -1.418966  1.370022\nprotein      3.631695  4.364091"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#in-draft-2",
    "href": "NTI/NTI-Lesson29.html#in-draft-2",
    "title": "Math 300R NTI Lesson 29",
    "section": "In draft",
    "text": "In draft\nMaybe come back to this in confounding lesson. Look for components that tend to go together\n\nwith(openintro::starbucks, cor(fat, protein))\n\n[1] 0.22347\n\nwith(openintro::starbucks, cor(fiber, protein))\n\n[1] 0.488564\n\nlm( calories ~ fiber , data = openintro::starbucks) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 276.119106 343.80739\nfiber         1.923476  24.07453\n\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ protein + fiber, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 247.891487 320.333514\nprotein       1.700777   7.996897\nfiber        -8.099007  15.978382"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#documenting-software",
    "href": "NTI/NTI-Lesson29.html#documenting-software",
    "title": "Math 300R NTI Lesson 29",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#section",
    "href": "NTI/NTI-Lesson38.html#section",
    "title": "Math 300R NTI Lesson 38",
    "section": "38.1",
    "text": "38.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#reading",
    "href": "NTI/NTI-Lesson38.html#reading",
    "title": "Math 300R NTI Lesson 38",
    "section": "Reading",
    "text": "Reading\nOne or more of these articles:\n\nA review of false discovery\nDiet and sex determination\nMost research findings false"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#lesson",
    "href": "NTI/NTI-Lesson38.html#lesson",
    "title": "Math 300R NTI Lesson 38",
    "section": "Lesson",
    "text": "Lesson\nDiscussion of article(s).\n\nWhat should the p-value become\nConsider dag07\n\n\n\n\n\nNode d is not connected to any of the other nodes. There should accordingly be a “null” relationship between d and the others. On the other hand, b and c are connected (although the connection is confounded with a).\nLet’s model d by b and look at the p-value:\n\nSample <- sample(dag07, size=50)\nlm(d ~ b, data=Sample) %>% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   0.0556    0.132      0.422   0.675\n2 b            -0.121     0.0985    -1.23    0.225\n\n\nThe p-value on the b coefficient is large, greater than the usual threshold of 0.05.\nOn the other hand, b and c are connected and the p-value (with this much data) is tiny.\n\nlm(c ~ b, data = Sample) %>% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -0.106     0.151    -0.705 4.84e- 1\n2 b             -1.55      0.113   -13.8   2.52e-18\n\n\nImagine a setting where a popular (but unproven!) hypothesis has emerged: that b and d are really related. 100 different research teams rush in to be the first to demonstrate, each generating their own experimental data. We’ll simulate this and collect the summary of the b coefficient w.r.t. d. [First show the statement without the do() to show what each row looks like. Then run the 100 trials and look for small p-values]\n\nAll_groups <- do(100) * {\n  lm(d ~ b, data=sample(dag07, size=50)) %>% \n  broom::tidy() %>%\n  filter(term == 'b')\n  }\n\nDid any of the groups get a “significant” result?\n\nAll_groups %>% \n  filter(p.value < 0.05)\n\n# A tibble: 4 × 7\n  term  estimate std.error statistic p.value  .row .index\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl> <int>  <dbl>\n1 b        0.298    0.105       2.84 0.00651     1     42\n2 b       -0.199    0.0889     -2.24 0.0297      1     76\n3 b        0.232    0.0934      2.48 0.0166      1     78\n4 b       -0.211    0.0989     -2.14 0.0375      1     99\n\n\nIn the context of 100 trials being done, it’s understandable that some of the groups happened to get a p-value < 0.05. But suppose that only the groups with small p-values publish their results? Then it looks as if they found a “significant” result.\nHow can we guard against this accidental generation of significant results? The standard answer in scientific work is to replicate the result: the labs should try again to confirm the result they got in the first study. (In practice, there are strong social/financial/career pressures against conducting such replications. These need to be overcome to guard against false discovery.)\nHere’s a simulation where each lab group runs the study twice. Do any get small p-values both times?\n\nReplicated_groups <- do(100) * {\n  do(2) * {\n    lm(d ~ b, data=sample(dag07, size=50)) %>% \n      broom::tidy() %>%\n      filter(term == 'b')\n    } %>% .$p.value\n} \nPairs <- Replicated_groups %>% \n  tidyr::pivot_wider(names_from = .row, values_from = result)\nPairs %>% filter(`1` < 0.05, `2` < 0.05)\n\n# A tibble: 0 × 3\n# … with 3 variables: .index <dbl>, 1 <dbl>, 2 <dbl>\n\n\nA better approach. As a rule of thumb, once you have a sample size \\(n\\) that gives a genuine p \\(\\approx 0.05\\), doubling \\(n\\) should reduce p by a factor of about 10. But if p is merely accidentally small, doubling the sample size won’t have any effect.\nA demonstration when there is a genuine relationship:\n\nlm(c ~ a, data = sample(dag07, size=5)) %>% broom::tidy() %>%\n  filter(term == 'a')\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 a         1.63     0.968      1.68   0.191\n\nlm(c ~ a, data = sample(dag07, size=10)) %>% broom::tidy() %>%\n  filter(term == 'a')\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 a        0.957     0.585      1.64   0.141\n\n\nLet’s re-run the simulation with \\(n\\) doubled, that is, size=100 compared to the previous size=50\n\nBigger_n <- do(100) * {\n  lm(d ~ b, data=sample(dag07, size=100)) %>% \n    broom::tidy() %>%\n    filter(term == 'b')\n  }\n\nAre these p-values smaller than in the trials with size=50?"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#learning-checks",
    "href": "NTI/NTI-Lesson38.html#learning-checks",
    "title": "Math 300R NTI Lesson 38",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#section-1",
    "href": "NTI/NTI-Lesson38.html#section-1",
    "title": "Math 300R NTI Lesson 38",
    "section": "38.1",
    "text": "38.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#documenting-software",
    "href": "NTI/NTI-Lesson38.html#documenting-software",
    "title": "Math 300R NTI Lesson 38",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "LC/LC-lesson37.html",
    "href": "LC/LC-lesson37.html",
    "title": "Learning Checks Lesson 37",
    "section": "",
    "text": "Ideas:"
  },
  {
    "objectID": "LC/LC-lesson37.html#section",
    "href": "LC/LC-lesson37.html#section",
    "title": "Learning Checks Lesson 37",
    "section": "37.1",
    "text": "37.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson23.html",
    "href": "LC/LC-lesson23.html",
    "title": "Learning Checks Lesson 23",
    "section": "",
    "text": "Vocabulary: Sampling distribution, standard error, sampling variability, sample size\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson23.html#section-1",
    "href": "LC/LC-lesson23.html#section-1",
    "title": "Learning Checks Lesson 23",
    "section": "23.2",
    "text": "23.2\nIn LC 22.2, using do(100), you displayed the sampling distribution on the x coefficient of the model y ~ x applied to data simulated from dag01. Among other things, you calculated the standard deviation of the sampling distribution. Copy over the proc1() you wrote for LC 22.2 into your Rmd document for this lesson.\nCalculate the standard deviation of the sampling distribution in each of these situations.\n\n\n\nNumber of trials\nSample size\n\n\n\n\ndo(100)\nsize=25\n\n\ndo(100)\nsize=100\n\n\ndo(100)\nsize=400\n\n\ndo(500)\nsize=25\n\n\ndo(500)\nsize=100\n\n\ndo(500)\nsize=400\n\n\n\nIn each case, the standard deviation is somewhat random, since new simulated data is collected from dag01 each time. Nonetheless, there is a systematic pattern to how the standard deviation varies with the number of trials and with the sample size.\n\nDescribe how the standard deviation of the sampling distribution of the x coefficient varies with sample size. The general trend should be easy to see.\nDoes the standard deviation of the sampling distribution depend on the number of trials?\nGoing back to your results from (1), try to find a simple quantitative relationship that describes how the standard deviation depends on sample size. State that relationship in words."
  },
  {
    "objectID": "LC/LC-lesson23.html#section-2",
    "href": "LC/LC-lesson23.html#section-2",
    "title": "Learning Checks Lesson 23",
    "section": "23.3",
    "text": "23.3\nWe’re going to build models of prices of books based on the moderndive::amazon_books data frame. For each model, you will calculate the confidence interval of one or more coefficients in two ways:\n\nDirectly, using confint().\nIndirectly, using broom::tidy()\n\nModel 1.\n\nModel list_price versus amazon_price. Calculate the confidence intervals on the intercept and on the amazon_price coefficient.\nInterpret the amazon_price coefficient in everyday words.\n\n\n\n\n\n\n\nSolution to part 1.\n\n\n\n\nlm(list_price ~ amazon_price, data = amazon_books) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept)  3.716532 5.122737\namazon_price 1.049308 1.127471\n\nlm(list_price ~ amazon_price, data = amazon_books) %>% broom::tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      4.42    0.357       12.4 5.22e- 29\n2 amazon_price     1.09    0.0199      54.8 2.82e-165\n\n\n\nYou can read the confidence interval directly from the confint() report. For the regression report, calculate the confidence interval as the estimate \\(\\pm 2\\) times the “standard error.”\nJust to look at the amazon_price() coefficient, the list price is about 8% higher than the Amazon price. Here, “about” means 5% to 13%. But don’t forget the intercept. The list price is, on average, about $4.50 higher than the 1.08 multiplier on the Amazon price.\n\n\n\nModel 2.\n\nModel list_price versus amazon_price, including hard_paper as a covariate.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept)  3.028400 4.466996\namazon_price 1.042536 1.117826\nhard_paperH  1.786977 3.882884\n\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% broom::tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      3.75    0.366      10.3  1.62e- 21\n2 amazon_price     1.08    0.0191     56.5  9.65e-169\n3 hard_paperH      2.83    0.533       5.32 1.93e-  7\n\n\nA hardcover costs about $2 to $4 more than a paperback.\n\n\n\n\n\n\n\n\nNote in draft\n\n\n\nReturn to this example in the prediction lesson, to show how the confidence interval and the prediction interval are different.\nMaybe also use it in one of the side-exercises on interaction terms. (The plan is not to strongly emphasize interaction terms but to refer to them in the occasional exercise.)"
  },
  {
    "objectID": "LC/LC-lesson23.html#objective-23.1",
    "href": "LC/LC-lesson23.html#objective-23.1",
    "title": "Learning Checks Lesson 23",
    "section": "23.4 (Objective 23.1)",
    "text": "23.4 (Objective 23.1)\nWe’re going to work with a very short dataset so that you can see directly what resampling a data frame does. (Ordinarily, you use resampling on an entire dataset, but here we are trying to make a point about the mechanism of resampling.)\n\nCreate a data frame Five that consists of the first five rows of moderndive::mythbusters_yawn. (Hint: Use head().) Put the code for doing this into your Rmd homework paper. Note that Five contains the data from subjects 1 through 5.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nFive <- mythbusters_yawn %>% head(5)\n\n\n\n\nUse resample() to generate a new data frame from Five. At this point, you are just going to look at the result, processing it “by eye.” How many distinct human subjects are reported in the resampled data? (Your answer will likely differ from your classmates’, since resampling is done at random.)\nRepeat (2) ten times. Each time, count the number of distinct human subjects.\n\nReport those ten numbers on your write-up.\nThere will usually be one or more subjects repeated in the output. Look at these repeats carefully to check whether the variables have the same value for all the repeats or whether sometimes a repeated subject has different values for group or yawn.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nMost of the time there will be 2, 3, or 4 distinct subject. The balance of the five rows will be repeats of other subjects. When a subject is repeated, the entire row is identical for all instances of that subject.\n\n\nGoing further (optional). It’s pretty easy to automate the process of generating the resample and counting the number of distinct human subjects. Like this:\n\n{resample(Five) %>% unique() %>% nrow()}\n\n[1] 3\n\n\nUsing do(1000), carry out 1000 trials of this process, saving the overall results in a data frame named Trials. What is the mean number of unique human subjects across the 1000 trials? What fraction is this of the five subjects.\nDo the same again, but instead of using Five, use the whole mythbusters_yawn data frame (which has 50 rows). What fraction of the 50 human subjects, on average, shows up in the resamples?\n\n\n\n\n\n\nSolution\n\n\n\n\nTrials <- do(1000) * {resample(mythbusters_yawn) %>% unique() %>% nrow()}\nTrials %>% summarize(mn = mean(result)/nrow(mythbusters_yawn))\n\n       mn\n1 0.63552"
  },
  {
    "objectID": "LC/LC-lesson23.html#objective-23.1-1",
    "href": "LC/LC-lesson23.html#objective-23.1-1",
    "title": "Learning Checks Lesson 23",
    "section": "23.5 (Objective 23.1)",
    "text": "23.5 (Objective 23.1)\nReturn to the amazon_books data frame and the model list_price ~ amazon_price. In Exercise 23.3 you used the regression report to calculate the confidence intervals on the intercept and on the amazon_price coefficient. Now you are going to repeat the calculation in a different way, using randomization, a process called “bootstrapping”.\nThe basic process is to train a model using resampled data, like this:\n\nlm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()\n\n (Intercept) amazon_price \n    5.457509     1.034772 \n\n\nThen, using do(500), carry out 500 trials, saving the result in a data frame named Trials.\nProcess Trials to calculate both the mean and the standard deviation of the intercept and amazon_price columns. How do those results compare to the “standard error” results from the same model (without resampling) as you found in LC 23.3?\n\n\n\n\n\n\nSolution\n\n\n\nSOMETHING IS WRONG HERE. The means are about the same as from the regression report (as they should be) but the standard deviations are 3-4 times larger. WHAT GIVES?\nTHE PROBLEM IS a handful of books where the Amazon price is very different from the list price, because the book itself is very expensive (e.g. $100). Remedy\n\nSwitch to another data example, maybe doing both the regression report and the bootstrapping in one exercise.\nThis is an object lesson in outliers. Since the dollar discount is presumably proportional to the price, we should have used log transforms.\n\n\nTrials <- do(500) * {lm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()}\nTrials %>% summarize(m1 = mean(Intercept), \n                     m2 = mean(amazon_price), \n                     sintercept = sd(Intercept), \n                     samazon_price = sd(amazon_price))\n\n        m1       m2 sintercept samazon_price\n1 4.205343 1.105907  0.8832918     0.0772529"
  },
  {
    "objectID": "LC/LC-lesson22.html",
    "href": "LC/LC-lesson22.html",
    "title": "Learning Checks Lesson 22",
    "section": "",
    "text": "Consider these three data frames:\n\nOne <- sample(dag01, size=25)\nTwo <- do(10) * {\n  lm(y ~ x, data = sample(dag01, size=25)) %>%\n    coefficients()\n  }\nThree <- Two %>% \n  summarize(mx = mean(x), sx = sd(x))\n\n\nBoth One and Two have columns called x, but they stand for different things. Explain what the unit of observation is and what the values in x represent..\nThree does not have a column named x, but it is a summary of the x column from Two. What kind of summary.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nIn One, the x column contains the simulated of the x variable from dag01. The unit of observation is a single case, for instance a person for whom observations were made of x and y. The simulation involves generating 25 rows of data: one row for each of 25 people.\nIn Two, the x column is the regression coefficient on x from the simulation. Each row of Two corresponds to one trial in which regression is being performed on a sample of size 25 of simulated data from dag01.\nThree is a summary of the 10 trials in Two. The columns, named mx and sx, tell about the distribution of x across all the trials."
  },
  {
    "objectID": "LC/LC-lesson22.html#obj-21.3",
    "href": "LC/LC-lesson22.html#obj-21.3",
    "title": "Learning Checks Lesson 22",
    "section": "22.2 (Obj 21.3)",
    "text": "22.2 (Obj 21.3)\nPart 1\nYou are going to write a procedure that automates the following process:\n\nsampling from a DAG, specifically dag01, using sample() with a size of 25.\nfitting a model y ~ x using lm()\nreporting the coefficient on x using coefficients().\n\nCall the procedure proc1().\nTo do this fill in the following template in your Rmd document:\nproc1 <- function() {\n  # your statements go here\n}\nOnce you have proc1() ready, you can carry out the procedure by giving a simple command:\nproc1()\nPart 2\nNow that you have proc1() ready and have tried it out, you are going to run the procedure 100 times repeatedly and look at the distribution in the x coefficient.\nOf course, you could laboriously give the command proc1() 5 times, and write down the x coefficient each time. Far better, though, to automate the process of repeating and collecting the x coefficient.\nYou can do this easily by using do(5) in conjunction with proc1().\n\nWhat’s the form in which the coefficients are collected when using do()?\nIs the x coefficient the same from trial to trial? Explain why or why not.\nChange your statement to run 100 trials rather than just 5, and to store the collected results in a data frame called Trials. Use appropriate graphics to display the distribution of the x coefficient. Summarize the distribution in a sentence or two.\nCreate a consise summary of the x column of Trials using summarize() with sd(x) to calculate the standard deviation. Compare the size of the standard deviation to the graphical display in (3).\n\n\nSolution\nPart 1\n\nproc1 <- function() {\n  Dat <- sample(dag01, size=25)\n  Mod <- lm(y ~ x, data = Dat)\n  coefficients(Mod)\n}\n\nor, more concisely\n\nproc1 <- function() {\n  sample(dag01, size=25) %>%\n    lm(y ~ x, data = .) %>%\n    coefficients()\n}\n\nPart 2\n\ndo(5) * proc1()\n\n  Intercept        x\n1  3.993889 1.823723\n2  3.781726 1.192576\n3  4.152893 1.328287\n4  3.663743 1.329055\n5  3.718211 1.620334\n\n\n\nThe results of the five trials are collected into a data frame.\nThe x coefficients varies from trial to trial.\n\nCollect 100 trials\n\nTrials <- do(100) * proc1()\n\n\nAn appropriate graphical display of the trials:\n\n\nggplot(Trials, aes(x)) + geom_density(fill=\"blue\", alpha=0.3)\n\n\n\n\nThe x coefficient varies from near 0.5 to near 2.5 in a bell-shaped form.\n\nSummarize the trials by the standard deviation.\n\n\nTrials %>% summarize(s = sd(x))\n\n          s\n1 0.2005022\n\n\nThe standard deviation is about 1/4 the width of the distribution."
  },
  {
    "objectID": "LC/LC-lesson36.html",
    "href": "LC/LC-lesson36.html",
    "title": "Learning Checks Lesson 36",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson20.html",
    "href": "LC/LC-lesson20.html",
    "title": "Learning Checks Lesson 20",
    "section": "",
    "text": "Only for use in drafting questions\n\n\n\n20.1. Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\n20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples."
  },
  {
    "objectID": "LC/LC-lesson20.html#a",
    "href": "LC/LC-lesson20.html#a",
    "title": "Learning Checks Lesson 20",
    "section": "20.A",
    "text": "20.A\nYou’re having a conversation about your statistics course with your engineer aunt at Thanksgiving. You tell her about how the course uses gaming (e.g. simulated data from DAGs) to develop an understanding of statistical methodology. She says (sensibly), “What? Simulated data? Isn’t statistics supposed to be about real data? It’s not a game.”\n\nWrite a paragraph-long response to your aunt explaining the point of gaming in learning statistics. Your paragraph should make a compelling case for gaming.\nWrite another paragraph that expresses any concerns you have about using games in a course that’s supposed to be about methods for extracting information about the world from real data."
  },
  {
    "objectID": "LC/LC-lesson20.html#b",
    "href": "LC/LC-lesson20.html#b",
    "title": "Learning Checks Lesson 20",
    "section": "20.B",
    "text": "20.B\nWrite statements using the computer commands covered in the first half of the course to calculate the size of the variability of these quantities:\n\nThe age variable from the Cherry_race_longitudinal data frame.\nThe Height variable from the NHANES data frame.\nCompute the body-mass index from the formula \\(\\text{BMI} = \\frac{w^2}/h\\) where \\(w\\) is the person’s weight and \\(h\\) is the person’s height. Then calculate the size of the variability of the BMI you calculate from the NHANES data."
  },
  {
    "objectID": "LC/LC-lesson20.html#c",
    "href": "LC/LC-lesson20.html#c",
    "title": "Learning Checks Lesson 20",
    "section": "20.C",
    "text": "20.C\n\n\n\n\n\n\nIn draft\n\n\n\nShow pictures of data together with violins. Ask students to estimate the “size” of the variation and to mark on the graph an annotation reflecting the “size.”\nIs “size” a single number or a pair of numbers?\nContrast “size” with the coverage interval: why is one of them two numbers and the other just a single number."
  },
  {
    "objectID": "LC/LC-lesson20.html#section",
    "href": "LC/LC-lesson20.html#section",
    "title": "Learning Checks Lesson 20",
    "section": "20.1",
    "text": "20.1\n?@sec-size-of-variable (in the reading for this variable) describes two very closely related summary quantities used to measures of the “size” of a variable: a) the variance and b) the “standard deviation” (which is the square root of the variance).\n\nUsing software, what is the variance of the XXX variable in the YYY data frame? Make sure to include the units.\nWhat is the “standard deviation” of the XXX variable? Calculate this in two different ways: i. “by-hand” taking of the square root of the variance; ii. using the sd() software directly.\n\n[Repeat for a number of variables from different data frames.]\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson20.html#d",
    "href": "LC/LC-lesson20.html#d",
    "title": "Learning Checks Lesson 20",
    "section": "20.D",
    "text": "20.D\n\n\n\n\n\n\nIn draft\n\n\n\nIn arithmetic notation, parentheses are used to group operations. So, \\(4*(3+2)\\) is different from \\(4*3 + 2\\). With do(), a similar logic applies, but we use curly braces—not parentheses—for the grouping. Another difference is that with do() and summarization, some configurations may not work at all because using braces or not will produce different data frames with different names.\n\nTry the following two commands, which differ in how curly braces are used.\n\ndo(5) * {sample(Galton, size=50)} %>% summarize(m = mean(height))\ndo(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))}\nOne command produces an error message and the other doesn’t. Explain what’s wrong with the erroneous command.\n\nTry these two statements. Again, one will work and the other won’t. Diagnose the\n\n\n{ do(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))} } %>% summarize(sz=sd(m))\n\n         sz\n1 0.7253725\n\ndo(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))} %>% summarize(sz = sd(m))\n\n  sz\n1 NA\n2 NA\n3 NA\n4 NA\n5 NA"
  },
  {
    "objectID": "LC/LC-lesson20.html#e",
    "href": "LC/LC-lesson20.html#e",
    "title": "Learning Checks Lesson 20",
    "section": "20.E",
    "text": "20.E\nWrite single-line computer statements that will do the following (related) tasks:\n\nDraw a sample of size 5 from the mosaicData::TenMileRace data\nDraw a sample (but now of size 100) and compute the “size” of variation of the net variable (which gives the net time to complete the race, start line to finish line).\nCarry out 300 trials of (2) using the do() operator. (The result should be a data frame with entries that vary.) Note: Surround the statement for a single trial with curly braces: { and }. Also, arrange for the name of the column in the overall result to be sz. This will appear in the summarize() command.\nAdd on to (3) the computations needed to calculate the mean and the standard deviation of the trials? Note: Surround the statement from (3) with a pair of curly braces so that the summarize() command will look at all 300 trials as a single data frame.\n\nIs the mean thus calculated a single number comprising all trials or a number for each trial? Briefly justify your answer in terms of what the mean of the trials should be about.\nEexplain what the standard deviation of the trials captures and why it’s different from the standard deviation on one trial (as from (2)).\n\nRepeat (4), but now with a sample size of 400 instead of 100. With the larger sample size, how do the mean and the standard deviation compare to what you got with a sample size of 100.\nRepeat (4) but this time use 1200 trials instead of 300. With more trials, how do the mean and standard deviation compare to what you got with 300 trials?\n\n::: {.callout-note} ## Solution\n\n# sample of size 100 from TenMileRace\nsample(TenMileRace, size=5)\n\n     state time  net age sex orig.id\n5200    VA 5826 5622  26   F    5200\n3621    NY 3710 3710  52   M    3621\n4319    VA 5187 5162  16   F    4319\n6236    DC 5086 4898  31   F    6236\n4850    VA 5005 4928  25   F    4850\n\nsample(TenMileRace, size=100) %>% summarize(sz = sd(net))\n\n        sz\n1 989.7872\n\ndo(300)* {sample(TenMileRace, size=100) %>% summarize(sz = sd(net))}\n\n           sz\n1    935.2835\n2    864.1209\n3    991.9426\n4   1033.4450\n5   1107.1699\n6    999.3019\n7    979.6108\n8    984.8824\n9    893.9452\n10   896.9353\n11   909.7440\n12  1082.4825\n13   869.6667\n14   909.8586\n15   841.8605\n16   899.8305\n17  1098.6771\n18   903.9756\n19  1017.5757\n20  1009.8084\n21   947.4528\n22   866.5563\n23  1111.8237\n24   920.9881\n25  1114.2142\n26   989.2198\n27   904.5219\n28   942.2985\n29   975.0523\n30  1005.0565\n31  1045.0649\n32   804.7149\n33   984.5017\n34   931.1376\n35   987.1799\n36   966.1105\n37  1176.1742\n38   997.8880\n39   939.1997\n40   964.8751\n41  1023.9575\n42   894.5466\n43  1078.5738\n44   890.7672\n45   913.1032\n46   957.8719\n47   938.4455\n48  1198.2465\n49   960.2679\n50  1093.0112\n51   878.1397\n52   902.9950\n53   959.2964\n54   955.9949\n55   994.5670\n56   999.6098\n57   894.5494\n58   944.2706\n59   841.5120\n60   971.9427\n61   866.6388\n62   884.4254\n63  1005.0672\n64   859.3656\n65   869.6465\n66   928.4984\n67   882.3493\n68   929.3066\n69   961.0110\n70   954.7666\n71   995.1881\n72   910.8695\n73  1086.1882\n74   866.8875\n75   829.8607\n76   978.2121\n77   915.6285\n78   960.1390\n79  1075.9175\n80   806.8149\n81  1038.5203\n82   916.7681\n83   909.2274\n84   884.5920\n85   847.5868\n86  1000.2599\n87  1013.9214\n88   952.3603\n89  1032.1450\n90   960.9476\n91   938.9403\n92  1124.0317\n93  1117.8027\n94   813.1756\n95   899.5693\n96  1106.8892\n97   805.3640\n98   895.5305\n99  1041.5698\n100 1001.3925\n101 1033.3263\n102  967.3543\n103  992.1093\n104 1099.2348\n105 1006.7875\n106 1013.3553\n107 1100.3269\n108  821.2211\n109 1042.6789\n110  988.7332\n111  905.7325\n112 1025.8348\n113  909.8306\n114  880.0523\n115 1151.6035\n116  759.0052\n117 1021.5409\n118  887.0085\n119  915.8048\n120  882.4327\n121 1108.2071\n122  961.0029\n123  917.1378\n124 1022.6772\n125 1019.5482\n126 1034.1278\n127 1008.5512\n128  866.9532\n129  937.5872\n130  969.3005\n131  963.5933\n132  940.6535\n133  865.5862\n134  979.6695\n135  855.7436\n136  831.7585\n137  904.6226\n138  939.4044\n139  964.5623\n140  923.7244\n141 1035.8546\n142 1020.1612\n143  871.0645\n144  779.6602\n145 1018.4503\n146  980.4815\n147  895.1516\n148  903.2798\n149  933.7116\n150 1088.0078\n151  946.0003\n152 1085.4115\n153  836.6267\n154 1096.0999\n155 1001.8983\n156  937.0384\n157  932.4856\n158 1071.9284\n159  964.6650\n160  948.0608\n161  961.7802\n162 1086.2583\n163  881.6392\n164 1074.1256\n165 1014.8427\n166 1019.0670\n167  762.5020\n168  956.9975\n169 1013.7822\n170  961.2855\n171 1064.2289\n172  896.5258\n173  876.9779\n174 1002.3865\n175  902.0062\n176 1087.5041\n177  969.0666\n178  949.3226\n179  937.5604\n180  967.8846\n181  962.7566\n182  842.6570\n183  975.8517\n184  860.3817\n185  934.1456\n186  966.8410\n187 1054.9238\n188  864.7210\n189  897.6808\n190  975.0652\n191 1034.4298\n192 1028.2902\n193  994.6110\n194  864.5749\n195  909.5564\n196  917.8059\n197  785.3245\n198  869.8035\n199  948.1110\n200 1041.2221\n201  938.3919\n202  837.3169\n203  938.4763\n204  997.6952\n205  853.5572\n206  856.8444\n207  953.4682\n208  897.4692\n209  935.7548\n210  994.2567\n211  830.4893\n212 1009.4564\n213 1021.2338\n214  916.7626\n215  961.3999\n216 1026.8420\n217  955.5354\n218 1092.4335\n219  871.2790\n220 1022.1029\n221 1012.0510\n222  941.3925\n223  899.5408\n224  944.8411\n225  948.6405\n226 1013.6389\n227  979.0452\n228  923.8670\n229  919.9923\n230 1059.0503\n231  849.6150\n232 1115.5983\n233  948.7558\n234  823.0285\n235 1149.5049\n236 1012.8591\n237 1093.9288\n238  959.5887\n239 1001.5643\n240  830.0468\n241  984.8155\n242  962.4929\n243  920.9400\n244  874.5202\n245  933.4128\n246  899.6460\n247  939.2989\n248  971.5205\n249 1076.3141\n250 1077.4322\n251  915.0616\n252 1004.6463\n253  918.4548\n254 1053.7438\n255  984.9317\n256 1012.3069\n257  969.1277\n258  791.9972\n259  996.1261\n260  955.9223\n261 1010.5353\n262  906.0977\n263 1013.5866\n264  975.9976\n265  824.0274\n266  895.0678\n267  857.9002\n268  921.7147\n269  867.7880\n270 1114.6959\n271 1016.6237\n272 1010.3011\n273  916.3437\n274  942.8530\n275  990.4020\n276 1022.8936\n277 1010.1219\n278  945.0536\n279  962.7322\n280  966.1271\n281  769.4918\n282 1031.4658\n283  920.7142\n284  966.3896\n285 1046.1081\n286 1009.1230\n287  929.2753\n288 1011.5554\n289  856.8500\n290  916.3724\n291  962.7453\n292  865.7287\n293 1008.0288\n294 1032.9117\n295  898.6383\n296  871.3332\n297  985.6596\n298  911.9377\n299 1178.4605\n300  836.0317\n\n{do(300)* {sample(TenMileRace, size=100) %>% summarize(sz = sd(net))}} %>% summarize(m = mean(sz), s=sd(sz))\n\n         m        s\n1 964.5374 78.20796\n\n{do(300)* {sample(TenMileRace, size=400) %>% summarize(sz = sd(net))}} %>% summarize(m = mean(sz), s=sd(sz))\n\n         m        s\n1 972.0645 40.66924\n\n\n\nUsing a sample size that’s four times larger doesn’t affect the mean, but it reduces the standard deviation by a factor of two.\nIncreasing the number of trials does have any noticeable effect on either the mean or standard deviation."
  },
  {
    "objectID": "LC/LC-lesson20.html#e-1",
    "href": "LC/LC-lesson20.html#e-1",
    "title": "Learning Checks Lesson 20",
    "section": "20.E",
    "text": "20.E\nThroughout this course, you’re going to be using lm() to build models. Often, to demonstrate “sampling variation.” you will use sample() on a dataset or a DAG to generate a random sample and then send the result as the data= argument to lm(),\nHere are three computer commands that use the data= argument in different ways. One of them doesn’t work at all. Which one?\nsample(mtcars, size=10) %>% lm(mpg ~ wt + hp, data=.)\nsample(mtcars, size=10) %>% lm(mpg ~ wt + hp)\nlm(mpg ~ wt + hp, data=sample(mtcars, size=10))\n::: {.callout-warning} ## In draft\nThis problem would be come irrelevant if the fitmodel() command described in the [Lesson 19 NTI]{../NTI/NTI-Lesson19.html} is being used."
  },
  {
    "objectID": "LC/LC-lesson34.html",
    "href": "LC/LC-lesson34.html",
    "title": "Learning Checks Lesson 34",
    "section": "",
    "text": "Stat2Data::FaithfulFaces. Come back to it concerning prevalence."
  },
  {
    "objectID": "LC/LC-lesson34.html#section",
    "href": "LC/LC-lesson34.html#section",
    "title": "Learning Checks Lesson 34",
    "section": "34.1",
    "text": "34.1\nTo illustrate how stratification is used to build a classifier, consider this very simple, unrealistically small, made-up data frame listing observations of animals:\n\n\n\n\n \n  \n    species \n    size \n    color \n  \n \n\n  \n    A \n    large \n    reddish \n  \n  \n    B \n    large \n    brownish \n  \n  \n    B \n    small \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n\n\n\n\n\nYou are going to build classifiers using the data. The output of the classifier will be the probability that the species is A. The classifier itself will be a simple table: each row lists the different levels of the explanatory variable(s) the the classifier output (as a probability that the species is A).\n\nUse just size as an explanatory variable. Since there are two levels for size, the classifier can take the form of a simple table, giving the proportion of rows for each of the two sizes. Fill in the table to reflect the data.\n\n\n\n\n\n \n  \n    size \n    prop_of_A \n  \n \n\n  \n    large \n     \n  \n  \n    small \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are three rows where the size is large, of which one is species A. The classifier output is thus 2/3 for large.\nSimilarly, there is only one row where the size is small, none of which are species A. The classifier output is 0/1 for small.\n\n\n\nRepeat (1), but instead of “size”, use just “color” as an explanatory variable.\n\n\n\n\n\n \n  \n    color \n    prop_of_A \n  \n \n\n  \n    reddish \n     \n  \n  \n    brownish \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are three rows where the color is brownish, of which two are species A. The classifier output is thus 1/3 for brownish.\nThere is only one row where the color is reddish, and it is species A. The classifier output is 1/1 for reddish.\n\n\n\nAgain build a classifier, but use both color and size as explanatory variables.\n\n\n\n\n\n \n  \n    color \n    size \n    prop_of_A \n  \n \n\n  \n    reddish \n    large \n     \n  \n  \n    reddish \n    small \n     \n  \n  \n    brownish \n    large \n     \n  \n  \n    brownish \n    small \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is just one row in which color is reddish and size is large, and it is species A. The classifier output is thus 1/1.\nThere are two rows in which color is brownish and size is large, one of which is species A. The classifier output is thus 1/2.\nThere is one row in which color is brownish and size is small. It is species B. The classifier output is 1/1.\nThere are no rows in which color is reddish and size is small. A classifier output of 0/0 is meaningless. So our classifier has nothing to say for these inputs.\n\n\n\nFinally, build the “null model”, a no-input classifier. This means there is just one group, which has all four rows. -A- Of the four rows, two are species A, so the classifier output is 2/4."
  },
  {
    "objectID": "LC/LC-lesson34.html#section-1",
    "href": "LC/LC-lesson34.html#section-1",
    "title": "Learning Checks Lesson 34",
    "section": "34.2",
    "text": "34.2\nThe graph below shows data on marital status versus age from National Health and Nutrition Evaluation Survey data. You can see that the probability of the various possibilities are a function of age.\n\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:mosaic':\n\n    cross\n\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n\n\n\n\nA very simple classifier can be constructed just by indicating at each age which marital status is the most likely, as seen in the figure below.\n\n\nmaximum number of iterations reached -0.002303255 -0.002335247\n\n\n\n\n\nA classifier output should be a probability, not a categorical level. On the blank graph below, sketch out a plausible form for probability vs age for each of three categorical levels shown in the above plot. (Hint: At an age where, say, “NeverMarried” is the categorical output, the probability for “NeverMarried” will be higher than the other categories.)\n\n\n\n\n\n\nPresumably the probability output for each category varies somewhat smoothly. There are two constraints:\n\nAt any age/sex, one probability will be the highest of the three. That one should correspond to the category shown in the first graph.\nThe probabilities should add up to 1.\n\nHere’s one possibility. Note that for females, the highest probability around age 80 is “widowed”.\n\n\n\n\n\n\n\nFrom CPS §32.6: openintro::possum. Let’s investigate the possum data set again. This time we want to model a binary outcome variable. As a reminder, the common brushtail possum of the Australia region is a bit cuter than its distant cousin, the American opossum. We consider 104 brushtail possums from two regions in Australia, where the possums may be considered a random sample from the population. The first region is Victoria, which is in the eastern half of Australia and traverses the southern coast. The second region consists of New South Wales and Queensland, which make up eastern and northeastern Australia.\nWe use logistic regression to differentiate between possums in these two regions. The outcome variable, called pop, takes value Vic when a possum is from Victoria and other when it is from New South Wales or Queensland. We consider five predictors: sex, head_l, skull_w, total_l, and tail_l.\nExplore the data by making histograms or boxplots of the quantitative variables, and bar charts of the discrete variables.\n\nAre there any outliers that are likely to have a very large influence on the logistic regression model?\nBuild a logistic regression model with all the variables. Report a summary of the model.\nUsing the p-values decide if you want to remove a variable(s) and if so build that model.\nFor any variable you decide to remove, build a 95% confidence interval for the parameter.\nExplain why the remaining parameter estimates change between the two models.\nWrite out the form of the model. Also identify which of the following variables are positively associated (when controlling for other variables) with a possum being from Victoria: head_l, skull_w, total_l, and tail_l.\nSuppose we see a brushtail possum at a zoo in the US, and a sign says the possum had been captured in the wild in Australia, but it doesn’t say which part of Australia. However, the sign does indicate that the possum is male, its skull is about 63 mm wide, its tail is 37 cm long, and its total length is 83 cm. What is the reduced model’s computed probability that this possum is from Victoria? How confident are you in the model’s accuracy of this probability calculation?"
  },
  {
    "objectID": "LC/LC-lesson34.html#section-2",
    "href": "LC/LC-lesson34.html#section-2",
    "title": "Learning Checks Lesson 34",
    "section": "34.3",
    "text": "34.3\nThe HELPrct date frame (in the mosaicData package) is about a clinical trial (that is, an experiment) conducted with adult inpatients recruited from a detoxification unit. The response variable of interest reflects the success or failure of the detox treatment, namely, did the patient continue use of the substance abused after the treatment.\nFigure @ref(fig:giraffe-fall-door-1) shows the output of a simple classifier (maybe too simple!) of the response given these inputs: the average number of alcoholic drinks consumed per day in the past 30 day (before treatment); and the patient’s self-perceived level of social support from friends. (The scale for social support is zero to fourteen, with a higher number meaning more support.)\n\n\n\n\n\nClassifier based on data from a clinical trial\n\n\n\n\n\nWhat’s the probability of treatment failure for a patient who has 25 alcoholic drinks per day? Does the probability depend on the level of social support? -A- Probability of failure is 75%, and doesn’t depend on the level of social support.\nFor a patient at 0 to 10 alcoholic drinks per day, what’s the probability of treatment failure? Does the probability depend on the level of social support? -A- The probability of failure ranges from about 72% for those with no social support to 82% for those with high social support?\nYou are thinking about a friend who has roughly five alcoholic drinks per day. You are concerned that he will go on to substance abuse. Do the data from the clinical trial give good reason for your concern? Explain why or why not.\n\n\n\n\n\n\n\nSolution\n\n\n\nIt’s always a good idea to be concerned for your friend, but the data reported here are not a basis for that concern. These data are from a population consisting of inpatients from a detoxification unit. These are people who have already shown strong substance abuse. The classifier is not generalizable to your friend, unless he is an inpatient from a detox unit.\n\n\n\nExplain what’s potentially misleading about the y-axis scale selected for the plot.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe selected scale doesn’t include zero and so tends to over-emphasize what amount to small differences in the probability of failure."
  },
  {
    "objectID": "LC/LC-lesson35.html",
    "href": "LC/LC-lesson35.html",
    "title": "Learning Checks Lesson 35",
    "section": "",
    "text": "Given some classifier summaries, calculate the false-positive and false-negative rates as well as the sensitivity and specificity\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson35.html#q",
    "href": "LC/LC-lesson35.html#q",
    "title": "Learning Checks Lesson 35",
    "section": "35.Q",
    "text": "35.Q\nTITLE GOES HERE: Customize the classifiers in (ref:ant-take-room) for a population in which species A is three times as common as species B."
  },
  {
    "objectID": "LC/LC-lesson35.html#solution-1",
    "href": "LC/LC-lesson35.html#solution-1",
    "title": "Learning Checks Lesson 35",
    "section": "Solution",
    "text": "Solution\nFollow the same procedure as in (ref:ant-take-room), but duplicate each of the A rows two times, so that the data show a world in which A is three times as common as B. That is,\n\n\n\n\n \n  \n    species \n    size \n    color \n  \n \n\n  \n    A \n    large \n    reddish \n  \n  \n    A \n    large \n    reddish \n  \n  \n    A \n    large \n    reddish \n  \n  \n    B \n    large \n    brownish \n  \n  \n    B \n    small \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n\n\n\n\n\nFor the classifier using just size as an explanatory variable … There are 7 rows for which size is large. Of these six are species A so the classifier output is 6/7. For size small, there is just one row, which is B, so the classifier output is 0/1.\nFor the classifier using just color as an explanatory variable … There are five rows for which color is brownish. Of these, 2 are species A. So the classifier output is 2/5 for brownish. For reddish, the classifier output is 3/3."
  },
  {
    "objectID": "LC/LC-lesson35.html#r",
    "href": "LC/LC-lesson35.html#r",
    "title": "Learning Checks Lesson 35",
    "section": "35.R",
    "text": "35.R\nThe two tables below are different summaries of the Univ. of California Berkeley graduate admissions data from the 197e fall quarter. (Data frame: UCB_applicants) Each of the tables is about conditional probabilities about admission and sex.\n\n\n\n\nTable A\n \n  \n      \n    admitted \n    rejected \n  \n \n\n  \n    female \n    31.7 \n    46.1 \n  \n  \n    male \n    68.3 \n    53.9 \n  \n\n\n\n\n\n\n\nTable B\n \n  \n      \n    female \n    male \n  \n \n\n  \n    admitted \n    30.4 \n    44.5 \n  \n  \n    rejected \n    69.6 \n    55.5 \n  \n\n\n\n\n\n\nWhich table displays p(sex given admit)?\n\n\n\n\n\n\n\nSolution Table\n\n\n\nA\n\n\n\nWhich table displays p(admit given sex)?\n\n\n\n\n\n\n\nSolution\n\n\n\nTable B\n\n\n\nWhich of these statements is true?\n\nTable A shows that admitted students are more likely to be male.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nTrue\n\n\nb. Table A shows that rejected students are more likely to be male. \n\n\n\n\n\n\nSolution\n\n\n\nTrue\n\n\nc. Table A shows that females are less likely to be admitted than rejected. \n\n\n\n\n\n\nSolution False.\n\n\n\nThere’s nothing in Table A to tell us what fraction of applicants were admitted. Because table A stratifies by admitted/rejected, we don’t know how large those two groups are with respect to one another.\n\n\nd. Table B shows that admitted students are more likely to be male. \n\n\n\n\n\n\nSolution False.\n\n\n\nTable B is stratified on female/male. As a result, there’s no information in Table B about what fraction of applicants is male.\n\n\ne. Table B shows that females are less likely to be admitted than males. \n\nSuppose you are interested in the possibility of discrimination against women in graduate admissions (in Berkeley in 1973). Which of these questions is the right one to ask, and which table gives you the information needed to answer the question?\n\nWhat is the probability of an admitted student being a female compared to the probability of a rejected student being a female?\n\n\n\n\n\n\n\n\nSolution False.\n\n\n\nThe question is about the relative admissions probability of females and males.\n\n\nb. What is the probability of a female applicant being admitted compared to the probability that an admitted student is male? \n\n\n\n\n\n\nSolution False. You want to compare like with like. The group involved in “the probability of a female student being admitted” is females, whereas the group involved in “the probability that an admitted student is male” is admitted students. There’s little if any meaning in comparing a probability among the group of females to a probability among group of admitted students.\n\n\n\n\n\n\nc. What is the probability of  a female applicant being admitted compared to the probability of a male applicant being admitted? \n\n\n\n\n\n\nSolution True.\n\n\n\nWe want to compare two groups: females and males. Here, we’re comparing the probability of being admitted in each of the comparison groups. Table B is formatted to enable this comparison."
  },
  {
    "objectID": "LC/LC-lesson21.html",
    "href": "LC/LC-lesson21.html",
    "title": "Learning Checks Lesson 21",
    "section": "",
    "text": "The following command will generate a data frame with 1000 rows from dag00 and calculate the variance of the x and y variables:\n\nsample(dag00, size=1000) %>%\n  summarize(vx = var(x), vy = var(y))\n\n# A tibble: 1 × 2\n     vx    vy\n  <dbl> <dbl>\n1  3.83 0.947\n\n\nCompare this result to the DAG tilde expressions\n\ndag00\n\n[[1]]\nx ~ eps(2) + 5\n\n[[2]]\ny ~ eps(1) - 7\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nIn the tilde expressions, eps(2) means to generate noise of magnitude 2.0.\n\nIs the argument to eps() specified in terms of the variance or the standard deviation?\nThe tilde expression for x specifies that the constant 5 is to be added to eps(2). Similarly, the constant -7 is added to y. How do these constants relate to the calculated magnitudes of x and y?\n\n\n\n\nThe standard deviation. For instance, x has noise of magnitude 2. The variance of x is 4, the square of 2.\nThe standard deviation (and therefore the variance) ignore such added constants."
  },
  {
    "objectID": "LC/LC-lesson21.html#section-1",
    "href": "LC/LC-lesson21.html#section-1",
    "title": "Learning Checks Lesson 21",
    "section": "21.2",
    "text": "21.2\n?@sec-signal-and-noise introduces the idea that variables consist of components. A simple breakdown is into two components: i. the part of the variable that is determined by other variables in the system (“signal”) and ii. the random part of the variable (“noise”). The section uses dag01 as an illustration of how a variable can be partly determined and partly random noise.\n\nWrite and execute a command that will generate 500 rows of simulated data from dag01 and will calculate the standard deviation of x and of y.\nWhat’s the magnitude of x in the simulated data? What’s the magnitude of y?\nDoes this change if you use data with 1000 or 20000 rows?\n\n\nSolution\n\nsample(dag01, size=500) %>% summarize(sx = sd(x), sy=sd(y))\nThe standard deviation of x is about 1, the standard deviation of y is about 1.8.\nNo, the values are roughly the same regardless of the size of the sample."
  },
  {
    "objectID": "LC/LC-lesson21.html#section-2",
    "href": "LC/LC-lesson21.html#section-2",
    "title": "Learning Checks Lesson 21",
    "section": "21.3",
    "text": "21.3\n[DRAW several DAG-like graphs, one of which should be undirected in all edges, one should be undirected on one or two edges (but not all), and one should be cyclic and another acyclic.]\nReferring to the graphs in the figure, say which ones are DAGs. If a graph is not a DAG, say whether that’s because it’s not directed or because it’s not cyclic.\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson21.html#a",
    "href": "LC/LC-lesson21.html#a",
    "title": "Learning Checks Lesson 21",
    "section": "21.A",
    "text": "21.A\nA DAG (directed acyclic graph) is a mathematical object used to state hypothetical causal relationships between variables. Explain briefly (e.g. a few sentences overall) what each of the words “directed,” “acyclic,” and “graph” mean in the context of a DAG.\n\n\n\n\n\n\nSolution\n\n\n\n\nA graph is a relationship between discrete elements (called “nodes” abstraction) each of which for us represents a hypothetical quantity, that is, a variable. In addition to the nodes, the graph contains edges which represent the connections between variables.\nA directed graph is one whose arrows have a direction. For instance \\(A \\rightarrow B \\leftarrow C\\) means that \\(A\\) and \\(C\\) together cause \\(B\\), but \\(B\\) has no influence on \\(A\\) and \\(C\\).\nAn acyclic graph is one where it is impossible to start on any given node and, by following the directed edges, return back to that node."
  },
  {
    "objectID": "LC/LC-lesson21.html#b",
    "href": "LC/LC-lesson21.html#b",
    "title": "Learning Checks Lesson 21",
    "section": "21.B",
    "text": "21.B\nDraw these DAGs:\n\n“April showers bring May flowers.”\n“Price of a rug is determined by the size and the quality of materials.”\n“The weight of an automobile is reflected in the MPG fuel economy, as is the speed of the car, and inflation level of the tires.”\n“Plants tend to grow in the direction of the sunlight.”\n“An ice-cream shop owner needs to plan staffing based on the season, day of the week, and holidays.”"
  },
  {
    "objectID": "LC/LC-lesson21.html#c",
    "href": "LC/LC-lesson21.html#c",
    "title": "Learning Checks Lesson 21",
    "section": "21.C",
    "text": "21.C"
  },
  {
    "objectID": "LC/LC-lesson21.html#section-3",
    "href": "LC/LC-lesson21.html#section-3",
    "title": "Learning Checks Lesson 21",
    "section": "21.4",
    "text": "21.4\nGenerate simulated data from dag01 with 1000 rows. Fit the regression model y ~ x to the data and examine the coefficients.\n\nHow do the coefficients relate to the tilde expressions that define dag01?\nInstead of using the regression model y ~ x, where y is the response variable, try the regression model x ~ y. Do the coefficients from x ~ y correspond in any simple way to the tilde expressions that define dag01?\n\n\nSolution\n\nsample(dag01, size=1000) %>%\n  lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      3.998        1.583  \n\n\nThe intercept corresponds to the additive constant (4) in the y tilde expression. The x coefficient corresponds to the multiplier on x in the tilde expression.\nThe formula for x isn’t reflected by the coefficients.\nUsing x as the response variable:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .)\n\n\nCall:\nlm(formula = x ~ y, data = .)\n\nCoefficients:\n(Intercept)            y  \n    -1.8432       0.4616  \n\n\nThese coefficients do not appear in the dag01 tilde expressions."
  },
  {
    "objectID": "LC/LC-lesson21.html#objective-21.2",
    "href": "LC/LC-lesson21.html#objective-21.2",
    "title": "Learning Checks Lesson 21",
    "section": "21.5 (Objective 21.2)",
    "text": "21.5 (Objective 21.2)\nYou are trying to understand why automobile fuel economy varies from model to model. Using the mtcars data frame (documentation at help(\"mtcars\")) …\n\nWhat’s an appropriate choice of a response variable?\nPick two explanatory variables of interest to you. Build an appropriate model from the data, extracting the coefficients() of the model. Explain what the coefficients mean in everyday terms that your cylinder-head uncle would approve of.\nWhich of the other variables are covariates? iv.Pick a covariate that your intuition suggests would be important. Include that covariate in the model from (ii) and say whether the covariate shows up as important in the model coefficients.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nAll of the other variables are covariates. A covariate is merely a potential explanatory variable that you are not directly interested in. Of course, not all covariates play an important role in the system."
  },
  {
    "objectID": "LC/LC-lesson21.html#obj-21.3",
    "href": "LC/LC-lesson21.html#obj-21.3",
    "title": "Learning Checks Lesson 21",
    "section": "21.6 (Obj 21.3)",
    "text": "21.6 (Obj 21.3)\nGenerate a sample of size \\(n=100\\) from dag03. Use the data to construct a model of y versus x. But instead of using coefficients() to look at the model coefficients, use confint(). While coefficients() reports a single value for each coefficient, confint() reports a plausible interval for coefficients that is consistent with the data.\n\nFor \\(n=100\\), how wide is the interval reported by confint() on the x coefficient.\nRepeat the process of sampling and modeling, but this time use \\(n=400\\). How wide is the interval reported by confint() on the x coefficient.\nAgain repeat the process of sampling and modeling, this time using \\(n=1600\\). iv. Does the interval reported depend systematically on the size \\(n\\) of the sample? Describe what pattern you see."
  },
  {
    "objectID": "LC/LC-lesson21.html#section-4",
    "href": "LC/LC-lesson21.html#section-4",
    "title": "Learning Checks Lesson 21",
    "section": "21.7",
    "text": "21.7\nA short report from the British Broadcasting Company (BBC) was headlined “Millennials’ pay ‘scarred’ by the 2008 banking crisis.”\n\nPay for workers in their 30s is still 7% below the level at which it peaked before the 2008 banking crisis, research has suggested. The Resolution Foundation think tank said people who were in their 20s at the height of the recession a decade ago were worst hit by the pay squeeze. It suggested the crisis had a lasting “scarring” effect on their earnings.\n\n\nThe foundation said people in their 30s who wanted to earn more should move to a different employer. The research found those who stayed in the same job in 2018 had real wage growth of 0.5%, whereas those who found a different employer saw an average increase of 4.5%.\n\nThe phrase “should move” in the second paragraph of the report suggests causation. A corresponding DAG would look like this:\n\nmove_to_new_employer \\(\\rightarrow\\) higher_wage\n\nFor the sake of discussion, let’s add two more variables to the DAG:\n\neffectiveness—standing for how productive the employee is.\nqualifications—standing for whether the employee is a good candidate attractive to potential new employers.\n\nConstruct a DAG with all four variables that represent plausible causal connections between them. The DAG should not contain a direct causal connection between move_to_new_employer and higher_wage.\nIf the world worked this way, would it necessarily be good advice to switch to a new employer with the aim of earning a higher wage?"
  },
  {
    "objectID": "LC/LC-lesson25.html",
    "href": "LC/LC-lesson25.html",
    "title": "Learning Checks Lesson 25",
    "section": "",
    "text": "Just while in draft\n\n\n\n25.1 Given a data frame, construct a predictor function for a specified response variable.\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error. Relate this to a predition interval.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "LC/LC-lesson25.html#section",
    "href": "LC/LC-lesson25.html#section",
    "title": "Learning Checks Lesson 25",
    "section": "25.2",
    "text": "25.2\nThe data frame moderndive::house_prices lists the sales prices of 21,613 houses in King County, Washington (which includes Seattle) sold from May 2014 and May 2015. Often, with price or income data, economists work with the logarithm of the price or income or income-related quantity such as house living area. We are going to do here, but this problem is not about logarithms, so once you create the “logged” data frame, you’ll just be modeling the data using the usual methods.\nTo create the “logged” data, add these two new variables to the data frame, which we will call Seattle.\n\nSeattle <- moderndive::house_prices %>% \n  mutate(log_price = log10(price),\n         log_area = log10(sqft_living))\n\n\nBuild a model of log_price ~ log_area using the Seattle data. Store the model under the name pmod.\n\n\n\n\n\n\n\nSolution\n\n\n\n\npmod <- lm(log_price ~ log_area, data = Seattle)\n\n\n\n\nImagine that you are moving to Seattle in August 2014. Housing is expensive in the Seattle area, so you might decide to live in a small house, say 750 square feet. The log_area of such a house is 2.87. Using mod_eval(), predict the log_price of such a house. (Note that mod_eval() puts the model output in a column named model_output, not log_price.) In your Rmd file, give the command and show the output. If you’re curious about what the predicted price is in dollars (rather than log-dollars), simply raise 10 to the log-dollar amount. For instance, if the model_output were 5, the dollar amount will be \\(10^5 = \\$100,000\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 2.87)\n\n  log_area model_output\n1     2.87     5.324298\n\n\nIn dollar terms, the predicted price is 105.324 = 2.1086281^{5}.\n\n\n\nRepeat (2), but for a house with lots of space: 1500 square feet. The log_area of such a house is 3.18. As in (2), give the command and show the output in your Rmd file.\nYour budget is $200,000. In log dollars this budget is log10(200000) = 5.3. The predicted price of a 750 square-foot house is somewhat beyond your budget. But you figure that some 750-square foot house will be within your budget. To see if this is likely, look at the prediction interval of the house price. You can do this by adding the interval=\"prediction\" to the mod_eval() command. Is your budget (5.3 log dollars) within the prediction interval? Show your command and the result in your Rmd file and also give a sentence stating your conclusion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 2.87, interval=\"prediction\")\n\n  log_area model_output    lower   upper\n1     2.87     5.324298 4.993416 5.65518\n\n\nYeah! Your budget of 5.3 log dollars is near the center of the prediction interval.\n\n\n\nOn a hunch, you decide to see whether you might find a 1500 square foot (log_area = 3.18) might also fall within the prediction interval. Will it? Show your command, the result, and a sentence interpreting the result.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 3.18, interval=\"prediction\")\n\n  log_area model_output    lower    upper\n1     3.18     5.583697 5.252851 5.914543\n\n\nStrictly speaking, your budget (5.3 log dollars) is within the prediction interval. But it is very close to the lower bound of 5.25 log dollars. So there will likely be few houses of 1500 square feet within your budget. So plan that the house you will end up purchasing will be somewhere in the range 750-1500 square feet."
  },
  {
    "objectID": "LC/LC-lesson25.html#xx",
    "href": "LC/LC-lesson25.html#xx",
    "title": "Learning Checks Lesson 25",
    "section": "25.XX",
    "text": "25.XX\nYou are a bus dispatcher in New York City. The Department of Education bus logistics office has called to say that a school bus has broken down and the students need to be offloaded onto a functioning bus to take them to school. Unfortunately, the DOE officer didn’t tell you how many students are on the bus. You need to make a quick prediction in order to decide what kind and how many busses you will need for the pickup.\nYou go to the NYC OpenData site bus breakdown page to get the historical data on how many students are on the bus. There are more than 200,000 bus events listed, each one of them including the number of students. You make a jitter/violin plot of the number of students on each of the 200,000 busses.\n\n\n\n\n\n\nThe violin plot looks like an upside-down T. Explain what’s going on. (Hint: How many students fit on a school bus?) -A- As very often happens, the data file contains data-entry or other mistakes producing outliers. Almost all of the 200,000 bus incidents fall into the horizontal line near zero. There are only 164 with a number above 100 students. In the US, the legal maximum capacity for a school bus is 72 students.\n\nOne of the ways of handling outliers is to delete them from the data. A softer way is to trim the outliers, giving them a value that is distinct but not so far from the mass of values. The figure below shows a violin plot where any record where the number of students is greater than 20 is trimmed to 21.\n\n\n\n\n\n\nIf you sent a small school bus (capacity 14), what fraction of the time would you be able to handle all the students on the school bus? -A- Only about 5% of the area of the violin plot is above 14.\nIf you sent one 14-passenger school bus with another on stand-by (just in case the first bus doesn’t have sufficient capacity), what fraction of the time could you handle all the students?\n\n-A- It’s tempting to say that the 2 x 14 = 28 passenger capacity could handle all the cases, but remember, the cases at 21 stand for “21 or more passengers.” We can’t tell from the violin plot how many of those have more than 28 students on board.\n\nNotice that the violin plot is jagged. Explain why. -A- The number of passengers is an integer, e.g. 1, 2, 3, …. It can’t be a number like 4.5."
  },
  {
    "objectID": "LC/LC-lesson25.html#yyy",
    "href": "LC/LC-lesson25.html#yyy",
    "title": "Learning Checks Lesson 25",
    "section": "25.YYY",
    "text": "25.YYY\nAt a very large ballroom dance class, you are to be teamed up with a randomly selected partner. There are 200 potential partners. The figure below shows their heights.\nFrom the data plotted, calculate a 95% prediction interval on the height of your eventual partner. (Hint: You can do this by counting.)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n59 to 74 inches.\nSince there are 200 points, a 95% interval should exclude the top five cases and the bottom five cases. So draw the bottom boundary of the interval just above the bottom five points, and the top boundary just below the top five points."
  },
  {
    "objectID": "LC/LC-lesson31.html",
    "href": "LC/LC-lesson31.html",
    "title": "Learning Checks Lesson 31",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson19.html",
    "href": "LC/LC-lesson19.html",
    "title": "Learning Checks Lesson 19",
    "section": "",
    "text": "The math300 package will be needed for lessons 20 through 39.\n\nlibrary(math300)\nlibrary(moderndive)\nlibrary(NHANES)"
  },
  {
    "objectID": "LC/LC-lesson19.html#section",
    "href": "LC/LC-lesson19.html#section",
    "title": "Learning Checks Lesson 19",
    "section": "19.1",
    "text": "19.1\nConsider the moderndive::evals data that records students’ evaluations (score, on a 1-5 scale) of the professors in each of several courses (the course ID), as well as the age, “average beauty rating” (bty_avg) of the professor, enrollment in the course (cls_students) and the level o the course (cls_level). Each row in the data frame is an individual course section.\n\n\n\n\n\nID\nscore\nage\nbty_avg\ncls_students\ncls_level\n\n\n\n\n329\n2.7\n64\n2.333\n22\nupper\n\n\n313\n4.2\n42\n2.667\n86\nupper\n\n\n430\n4.5\n33\n5.833\n120\nlower\n\n\n95\n4.2\n48\n4.333\n33\nupper\n\n\n209\n4.8\n60\n3.667\n34\nupper\n\n\n442\n3.6\n61\n3.333\n39\nlower\n\n\n351\n4.6\n50\n3.333\n26\nlower\n\n\n317\n3.7\n52\n6.500\n44\nupper\n\n\n444\n4.1\n52\n4.500\n111\nlower\n\n\n315\n3.8\n52\n6.000\n88\nupper\n\n\n\n\n\nThe following commands model score versus age and plots the data as a point plot.\n\nlm(score ~ age, data = moderndive::evals) %>% coefficients()\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\nopenintro::evals %>% gf_point(score ~ age, alpha=0.2 )\n\n\n\n\n\nExplain why some of the dots are darker than others?\n\n\n\n\n\n\n\nSolution\n\n\n\nAll the ages have integer values—e.g., 43, 44, 45—so the dots line up in vertical lines.\nSimilarly, the scores have values only to one decimal place—e.g., 3.1, 3.2, 3.3—so the dots line up in horizontal lines. If there are two or more rows in evals that have the same age and score, the dots will be plotted over one another. Since transparency (alpha = 0.2) is being used, points where there is a lot of overplotting will appear darker.\n\n\n\nRemake the plot, but using gf_jitter() instead of gf_point(). Explain what’s different about the jittered plot. (Hint: Almost all of the dots are the same lightness.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ age, alpha=0.2 )\n\n\n\n\n“Jittering” means to shift each dot by a small random amount. This reduces the number of instances where dots are overplotted.\n\n\n\nNow make a jitter plot of score versus class level (cls_level).\n\nWhat do the tick-mark labels on the horizontal axis describe? Are they numerical?\nTo judge from the plot, are their more lower-level than upper-level courses? Explain briefly what graphical feature lets you answer this question at a glance.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ cls_level)\n\n\n\n\n\nThe tick-mark labels are the levels of the categorical variable cls_level. The are words, not numbers.\nThere are many more dots in the right column than in the left. Since lower level class are shown in the left column, there are fewer lower-level courses than upper-level courses.\n\n\n\n\nThe two columns of points in the plot you made in (3) are not separated by very much empty space. You can fix this by giving gf_jitter() an argument width=0.2. Try different numerical values for width and report which one you find most effective at making the two columns clearly separated while avoiding overplotting.\nAre the scores, on average, different for the lower- vs upper-level classes? It’s hard to get more than a rough idea of the distribution of scores by looking at the “density” of points. The reason is that the number of points differs in the two columns. But there is an easy fix: add a layer to the graphic that shows the distribution (more or less like a histogram displays a distribution of values). You can do this by piping the jitter plot layer into a geom called a “violin,” like this:\n\n\nopenintro::evals %>% \n  gf_jitter(score ~ cls_level) %>%\n  gf_violin(fill=\"blue\", alpha=0.2, color=NA)\n\n\n\n\nExplain how to read the violins."
  },
  {
    "objectID": "LC/LC-lesson19.html#section-1",
    "href": "LC/LC-lesson19.html#section-1",
    "title": "Learning Checks Lesson 19",
    "section": "19.2",
    "text": "19.2\nThe openintro::promotions data comes the the 1970s and records the gender of 38 people along with the result of a decision to promote (or not) the person. =\nChapter 2 of ModernDive suggests graphically depicting decision versus gender by using a bar plot. There are two ways to make the bar plot, depending on which variable you assign to the horizontal axis and which to the fill color.\n\npromotions %>% gf_bar(~ decision, fill=~ gender)\npromotions %>% gf_bar(~ gender, fill=~decision)\n\n\n\n\nFigure 1: Two different ways to plot promotion outcome and gender\n\n\n\n\n\n\n\nFigure 2: Two different ways to plot promotion outcome and gender\n\n\n\n\nPlots like those in ?@fig-promotion-bars might be attractive or not, depending on your taste. What they don’t accomplish is to make sure which is the response variable and which the explanatory variable.\nThe choice of response and explanatory variables depends, of course, on what you are trying to display. But everyday English gives a big hint. For instance, you might describe the question at hand as, “Does gender affect promotion decisions.” Here, the variable doing the affecting is gender, and the outcome is the decision.\nModeling decision as a function of gender is easy once you convert the response variable to a zero-one variable. Like this:\n\nmod <- lm(zero_one(decision, one=\"promoted\") ~ gender, data = promotions)\ncoefficients(mod)\n\n (Intercept) genderfemale \n   0.8750000   -0.2916667 \n\nmosaicModel::mod_eval(mod)\n\n  gender model_output\n1   male    0.8750000\n2 female    0.5833333\n\n\n\nExplain what is the relationship between the model coefficients and the model outputs.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe coefficients tell how to calculate the model output. These coefficients say that the model output will be 0.875, but subtract 0.292 if the person is female.\nThe model outputs give the probability of being promoted for each of the two genders.\n\n\n\nMake this plot and explain what the red lines show. (We don’t expect you to be able to write the command to generate such plots on your own, but we do expect you to be able to interpret them.)\n\n\npromotions %>% \n  gf_jitter(zero_one(decision) ~ gender, height=0.2, width=0.2) %>%\n  gf_errorbar(model_output + model_output ~ gender, data=mod_eval(mod), \n              color=\"red\", inherit=FALSE) %>%\n  label_zero_one()\n\n\n\n\n\n\n\nSolution\n\n\n\nThe red lines show the proportion of the people in each gender group who were promoted. The y-axis scale on the left refers to the zero-one encoding of decision, while the y-axis labels on the right make it easier to read off the numerical value of the proportion."
  },
  {
    "objectID": "LC/LC-lesson19.html#section-2",
    "href": "LC/LC-lesson19.html#section-2",
    "title": "Learning Checks Lesson 19",
    "section": "19.3",
    "text": "19.3\nThe mosaicData::Whickham data from comes from a survey of a thousand or so nurses in the UK in the 1970s. The data record the age of each nurse along with whether the nurse was still alive in a follow-up survey 20 years later (outcome).\nMake this graph from the Whickham data:\n\ngf_jitter(zero_one(outcome) ~ age, data = Whickham, alpha=0.3, height=0.1) %>% \n  label_zero_one() \n\n\n\n\n\nExplain in everyday language what the graph shows about the lives of humans.\nMake the graph again, but leave out the %>% label_zero_one(). Then explain what label_zero_one() does.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nThe graph shows that young nurses tended to be alive at the 20-year follow-up, older nurses not so much.\n%>% label_zero_one() adds an axis on the left of the graph showing that in the zero-one tranform of outcome, “Alive” is assigned the value 1 and “Dead” the value 0.\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson19.html#section-3",
    "href": "LC/LC-lesson19.html#section-3",
    "title": "Learning Checks Lesson 19",
    "section": "19.4",
    "text": "19.4\nAbout the summarization of models. Pipe the model fit into any of four functions:\n\n%>% coefficients()\n%>% broom::tidy()\n%>% rsquared()\n%>% confint()\n\nREDO confint() so that the columns are named lower, middle, upper\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson19.html#obj.-19.3",
    "href": "LC/LC-lesson19.html#obj.-19.3",
    "title": "Learning Checks Lesson 19",
    "section": "19.5 (Obj. 19.3)",
    "text": "19.5 (Obj. 19.3)\nCalculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We’ll use the CO2city variable in the math300::MPG data frame. The basic calculation using the mosaic package is:\n\ndf_stats( ~ CO2city, data = math300::MPG, coverage(0.95))\n\n  response   lower   upper\n1  CO2city 276.475 684.525\n\n\nThe following figure shows a violin plot of CO2city which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.\n\n50% coverage interval -A- (c)\n75% coverage interval -A- (e)\n90% coverage interval -A- (g)\n100% coverage interval -A- (i). This extends from the min to the max, so you could have figured this out just from the figure."
  },
  {
    "objectID": "LC/LC-lesson19.html#obj-19.3",
    "href": "LC/LC-lesson19.html#obj-19.3",
    "title": "Learning Checks Lesson 19",
    "section": "19.6 (Obj 19.3)",
    "text": "19.6 (Obj 19.3)\nThe two jitter + violin graphs below show the distribution of two different variables, X and Y. Which variable has more variability?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is about the same level of variability in variable A and variable B. This surprises some people. Remember, the amount of variability has to do with the spread of values of the variable. In variable B, those values are have a 95% prediction interval of about 30 to 65, about the same as for variable A. There are two things about plot (b) that suggest to many people that there is more variability in variable B.\n\nThe larger horizontal spread of the dots. Note that variable B is shown along the vertical axis. The horizontal spread imposed by jittering is completely arbitrary: the only values that count are on the y axis.\n\nThe scalloped, irregular edges of the violin plot.\n\nOn the other hand, some people look at the clustering of the data points in graph (b) into several discrete values, creating empty spaces in between. To them, this clustering implies less variability. And, in a way, it does. But the statistical meaning of variability has to do with the overall spread of the points, not whether they are restricted to discrete values."
  },
  {
    "objectID": "LC/LC-lesson19.html#objs.-19.3-19.4",
    "href": "LC/LC-lesson19.html#objs.-19.3-19.4",
    "title": "Learning Checks Lesson 19",
    "section": "19.7 (Objs. 19.3 & 19.4)",
    "text": "19.7 (Objs. 19.3 & 19.4)\nThe graphs below show a violin plot of body mass index (BMI) for adults and children. One of the graphs shows a correct 95% coverage interval on BMI, the other does not.\nIdentify the incorrect graph and say what feature of the graph led to your answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nGraph (b) is correct. In graph (a), you can see that the interval fails to include a lot of the low BMI children and extends too high. For adults, the graph (a) interval extends too far low and doesn’t go high enough."
  },
  {
    "objectID": "LC/LC-lesson19.html#e",
    "href": "LC/LC-lesson19.html#e",
    "title": "Learning Checks Lesson 19",
    "section": "19.E",
    "text": "19.E\nThere are two equivalent formats describing an interval numerically that are widely used:\n\nSpecify the lower and upper endpoints of the interval, e.g. 7 to 13.\nSpecify the center and half-width of the interval, e.g. 10 ± 3, which is just the same as 7 to 13.\n\nComplete the following table to show the equivalences between the two notations.\n\n\n\n\n\nInterval\nbottom-to-top\nplus-or-minus\n\n\n\n\n(a)\n3 to 11\n\n\n\n(b)\n\n108 ± 10\n\n\n(c)\n\n30 ± 1\n\n\n(d)\n97 to 100\n\n\n\n(e)\n-4 to 16\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n7 ± 4\n98 to 118\n29 to 31\n98.5 ± 1.5\n6 ± 10\n\nIt’s a matter of judgement which format to use. The bottom-to-top notation highlights the range of the interval while the plus-or-minus notation emphasizes the center of the interval. As a rule of thumb, I suggest this:\n\nIf the first two digits are different between the top and bottom of the interval, use the bottom-to-top notation. So, write 387 to 393. If the first two digits are the same, use plus-or-minus. For instancer, the ratio of the mass of the Earth to that of the Moon is 81.3005678 ± 0.0000027. This is easier to take in at a glance than the equivalent 81.3005651 - 81.3005708"
  },
  {
    "objectID": "LC/LC-lesson19.html#f",
    "href": "LC/LC-lesson19.html#f",
    "title": "Learning Checks Lesson 19",
    "section": "19.F",
    "text": "19.F"
  },
  {
    "objectID": "LC/LC-lesson30.html",
    "href": "LC/LC-lesson30.html",
    "title": "Learning Checks Lesson 30",
    "section": "",
    "text": "Dags with longer confounding pathways. Is there mixing when leaving out an element in the pathway. Mix up the directions of the arrows and show that the mixing occurs when the covariate is included in the model.\nRegression to the mean example.\nCollider?\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson24.html",
    "href": "LC/LC-lesson24.html",
    "title": "Learning Checks Lesson 24",
    "section": "",
    "text": "Just to help when writing problems\n\n\n\n24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation."
  },
  {
    "objectID": "LC/LC-lesson24.html#objective-24.1",
    "href": "LC/LC-lesson24.html#objective-24.1",
    "title": "Learning Checks Lesson 24",
    "section": "24.1 (Objective 24.1)",
    "text": "24.1 (Objective 24.1)\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\nSolution\n\nPrediction and (2) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price."
  },
  {
    "objectID": "LC/LC-lesson24.html#objective-24.1-1",
    "href": "LC/LC-lesson24.html#objective-24.1-1",
    "title": "Learning Checks Lesson 24",
    "section": "24.2 (Objective 24.1)",
    "text": "24.2 (Objective 24.1)\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\nSolution\n\nPrediction\nRelationship\nRelationship\n\n\nSOME IDEAS FOR EXERCISE MODES\n\nUse mod_plot() and look at the slope of lines and offsets. Compare to the model coefficients.\nGenerate data from a DAG and look at the confidence interval on the effect size. Then make new samples and see if the effect size in those samples is consistent with the confidence interval.\nIn text, maybe look at the confidence intervals across new samples and show that they tend to overlap. Only a few of them don’t touch a common line. This is basically just a review of confidence intervals, but why not?\nInteraction. Show that when there is an interaction term, the effect size (as calculated by mod_effect()) is not constant, as it is for models with purely linear terms."
  },
  {
    "objectID": "LC/LC-lesson24.html#lc-24.1",
    "href": "LC/LC-lesson24.html#lc-24.1",
    "title": "Learning Checks Lesson 24",
    "section": "LC 24.1",
    "text": "LC 24.1\nThe Computational Probability and Statistics text describes an early study on human-to-human heart transplantation:\n\n“The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. [[Not in data set: Another variable called [MISSING] was used to indicate whether or not the patient was alive at the end of the study.]]”\n\nThe data frame is called Transplants. [NEED TO MOVE TO PACKAGE]\n\n\n\nYou’re going to build a model of outcome vs group based on the data in Transplants. The outcome variable has levels \"Dead\" and \"Alive\", that is, it is a two-level categorical variable. Consequently, the model output will be the probability that the transplant candidate was alive at the end of the study.\n\nBuild a model outcome == \"Alive\" ~ group from the Transplants data. Pay close attention to the left-hand side of the tilde expression: it is a calculation that produces a 1 if outcome is \"Alive\" and zero otherwise. Notice the double equal signs and the quotes around \"Alive\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod <- lm(outcome == \"Alive\" ~ group, data = Transplants)\n\n\n\n\nThe sole explanatory variable here, group also is categorical. It has levels \"Control\" and \"Treatment\".\n\nUsing eval_mod(), find the probability of being alive at the end of the study for the Control group and for the treatment group.\nThe two probabilities in (ii) do not add up to zero. Explain why.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(mod, group=\"Treatment\")\n\n      group model_output\n1 Treatment    0.3478261\n\nmod_eval(mod, group=\"Control\")\n\n    group model_output\n1 Control    0.1176471\n\n\n\n\n\nFind the effect size of the treatment. All you need is your results from (2)?\nUse mod_effect(modelname, ~ group) to calculate the effect size.\n\nIs the result consistent with what you found in (3).\nExplain in everyday language what this effect size means.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_effect(mod, ~ group)\n\n     change     group to_group\n1 -0.230179 Treatment  Control"
  },
  {
    "objectID": "LC/LC-lesson24.html#section",
    "href": "LC/LC-lesson24.html#section",
    "title": "Learning Checks Lesson 24",
    "section": "24.2",
    "text": "24.2\nEffect sizes generally come with units and you have to take into account the units in order to know if the effect is important or not.\n\n\n\n\n\n\nIn draft\n\n\n\nMove the Loans data into the math300 package. But for now …\n\nLoans <- readr::read_csv(\"data/loans.csv\")\n\n\n\nA case in point is provided by the Loans data frame, which records dozens of variables on each of 10,000 loans made through the Lending Club. The interest rate at which the loans are made varies substantially from loan to loan. Presumably, higher interest rates reflect a higher perception of risk of default (which would lead to the lender losing his or her money).\nHere’s a model of the interest rate. (This is for the borrowers who have a low debt-to-income percent; we won’t worry about the few very high debt-to-income cases.) We’re only interested in this problem with the effect size, which is the same as the coefficients on the model.\n\nmod <- lm(interest_rate ~ homeownership + debt_to_income + \n            account_never_delinq_percent + verified_income, \n          data = Loans %>% filter(debt_to_income<50))\nmod %>% confint()\n\n                                     2.5 %      97.5 %\n(Intercept)                    15.31467948 17.26502804\nhomeownershipOWN                0.16056412  0.73009641\nhomeownershipRENT               1.01571903  1.41695574\ndebt_to_income                  0.09565833  0.11523598\naccount_never_delinq_percent   -0.09149666 -0.07122369\nverified_incomeSource Verified  1.37798985  1.79909554\nverified_incomeVerified         2.88724295  3.39001806\n\n\nThere are two quantitative explanatory variables—debt_to_income and account_never_delinq_percent—both of which are measured in percent.\nThere are two categorical explanatory variables: homeownership and verified_income. The levels for homeownership are “MORTGAGE” (meaning money is still owed on the house), “OWN” (without a mortgage), and “RENT” (meaning the borrower rents rather than owning a home). The levels for verified_income are “Not Verified”, “Verified”, “Source Verified”.\nFor the categorical explanatory variables (and the intercept) the effect-size units are “percent interest.” For the quantitative explanatory variables, the effect-size units are “percent interest per percent,” so that when multiplied by the debt_to_income percent or the account_never_delinq_percent the result will be in “percent interest.”\n\nAccording to the model, who pays the higher interest rate (on average): people who OWN their home, people who RENT, or people who have a mortgage on their home? How much higher than the lowest-interest rate category.\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople who rent pay the highest interest rate, a little more than 1 percentage point higher than people who have mortgages. It’s interesting that people who own their homes outright pay (on average) pay about 0.45 percentage points more than people who own outright. This might be because having a mortgage means you also have a credit history.\n\n\n\nAccording to the model, who pays the higher interest rate (on average): people whose income is “not verified,” people whose income is “verified,” or people who have the source of income verified (level: “source verified”)?\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople whose income is verified pay about 3 percentage points higher interest than people whose income is “not verified.” This seems surprising, but it may be that people who have higher perceived default risk are also the people who are asked to verify their income. Things get complicated when explanatory variables are linked to each other.\n\n\n\nThe coefficients on debt_to_income and account_never_delinq_percent are the smallest numerically. Does this mean that the effects of debt_to_income and account_never_delinq_percent are smaller than the other two explanatory variables in the model? Explain why or why not. (Hint: Look at the distribution of debt_to_income and account_never_delinq_percent to get an idea for the range of values these variables take on.)"
  },
  {
    "objectID": "LC/LC-lesson24.html#solution-7",
    "href": "LC/LC-lesson24.html#solution-7",
    "title": "Learning Checks Lesson 24",
    "section": "Solution",
    "text": "Solution\ndebt_to_income varies over about 25 percentage points. The variation in account_never_delinq_percent is about the same, varying from about 80 to 100 percentage points. The effect of the variables (in percent interest) is determined by multiplying the coefficients by the amount of variation in the variables. So, from one extreme to the other, the effect of debt_to_income is about 2 perentage points of interest, and roughly the same for debt_to_income."
  },
  {
    "objectID": "LC/LC-lesson24.html#section-1",
    "href": "LC/LC-lesson24.html#section-1",
    "title": "Learning Checks Lesson 24",
    "section": "24.4",
    "text": "24.4\nThe logic of effect size is to investigate the change in output of a model when one input variable is changed, holding all other things constant. This problem is about the extent to which we mean “all”.\nThe figure shows yearly CO2 production of individual gasoline-fueled passenger vehicles stratified by the number of engine 4 or 6 cylinders.\n\n\n\n\n\n\nThe effect size is the difference between the output variable when a change is made to an input. Consider the effect size of changing from a six-cylinder engine to a four-cylinder engine. Here’s a subtly incorrect way of calculating something like an effect size: Pick a dot from the six-cylinder group and another dot from the four-cylinder group. Subtract the 4-cylinder point’s CO2 value from the six-cylinder point’s CO2 value, and divide by the change in the input, that is, -2 cylinders.\n\nFollow this procedure for the top-most dot in each cloud and calculate the effect size. -A- (5100 kg - 4100 kg) / (-2 cylinders) = -500 kg/cylinder.\nFollow the procedure for the bottom-most dot in each cloud and calculate the effect size. -A- (3400 kg - 2500 kg) / (-2 cylinders) = -450 kg/cylinder.\nFollow the procedure for the bottom-most dot in the four-cylinder cloud and the top-most dot in the six-cylinder cloud. -A- (5100 kg - 3400 kg) / (-2 cylinders) = -850 kg/cylinder.\nDo the same as in (c) but use the bottom-most six-cylinder dot and the top-most four-cylinder dot. -A- (3400 kg - 4100 kg) / (-2 cylinders) = +350 kg/cylinder. In other words, switching from the six\n\nYou can imagine the (tedious) process of repeating the calculation for every possible pair of dots and getting a distribution of effect sizes. What would be the range of this distribution: from the smallest effect size to the biggest? (Hint: You can figure it out from your answers to (1).) -A- -850 kg/cylinder to +350 kg/cylinder\n\nFrom your result in (2), you might be tempted to conclude that the effect size is highly uncertain: it might be negative or it might be positive. But there is a problem, the procedure used in (1) and (2) fails to incorporate the notion of all other things being equal. This notion applies not just to known variables, but to all the other unknown factors that shape the data.\nFor the purpose of envisioning the concept, imagine that we actually had a measurement of all the factors that shaped CO2 emissions from a vehicle. We’ll call this imaginary measurement “all other things”. The figure below shows a conceptualization of what CO2 emissions as a function of the number of cylinders and “all other things” would look like.\n\n\n\n\n\n\nUsing the graph of CO2_year versus “all other things”, calculate the effect size of a change from six to four cylinders. Remember to hold “all other things” constant in your calculations. So do the effect size calculation at each of several values of “all other things”. What is the effect size and about how much does it vary from one value of “all other things” to another? -A- At “all other things being 0.25, the CO2 emissions for the 6- and 4-cylinder cars is 2800 kg and 3800 kg. The effect size is therefore (3800 kg - 2800 kg) / -2 cylinders = -500 kg/cylinder. The value of the effect size for other levels of”all other things” will be about the same, since the position of the six-cylinder dots is more-or-less constant compared to the corresponding four-cylinder dot.\n\nThis exercise is intended to give you a way of thinking about effect size and “all other things”. In reality, of course, we do not have a way to measure “all other things”. Instead, we calculate the effect size not directly from the data but from the model output (indicated by the statistic layer in the first graphic). As we saw in Lesson 22, there is actually some uncertainty about the model output stemming from sampling variability that can be summarized by a “confidence interval”. For the relationship between the number of cylinders and CO2 emissions, the extent of that uncertainty is indicated by the interval layer in the first graphic. A fair depiction for the corresponding uncertainty in the effect size can be had by repeating the process of (2), but only for points falling into the confidence interval.\n\n\n\n\n\n\nDo we need this?\n\n\n\nNote to readers who know something about car engines: As you know, switching out a six-cylinder engine for a four-cylinder engine is likely to change many other things: the weight of the vehicle, the displacement of the engine, the power available, etc. So it’s not useful to insist that everything other than the number of cylinders be held constant. Instead, we’ll be looking at the effect of that whole constellation of changes associated with a change in the number of cylinders."
  },
  {
    "objectID": "LC/LC-lesson24.html#x",
    "href": "LC/LC-lesson24.html#x",
    "title": "Learning Checks Lesson 24",
    "section": "24.X",
    "text": "24.X\nIn very simple settings, you don’t need access to the original data: a simple summary will do.\n\n\n\n\n\n\nIn draft\n\n\n\nUse the data/bloodthinner.csv data from CPS chapter 19. Create the table of counts and calculate the effect size/probabilities from that."
  },
  {
    "objectID": "LC/LC-lesson24.html#z",
    "href": "LC/LC-lesson24.html#z",
    "title": "Learning Checks Lesson 24",
    "section": "24.Z",
    "text": "24.Z\nThe National Cancer Institute publishes an online, interactive “Breast Cancer Risk Assessment Tool”, also called the “Gail model.” The output of the model is a probability that the woman whose characteristics are used as input will develop breast cancer in the next five years. The inputs include:\n\nage\nrace/ethnicity\nage at first menstrual period\nage at birth of first child (if any)\nhow many of the woman’s close relatives (mother, sisters, or children) have had breast cancer\n\nAs a baseline, consider a 55-year-old, African-American woman who has never had a breast biopsy or any history of breast cancer, who doesn’t know her BRCA status, and whose close relatives have no history of breast cancer, whose first menstrual period was at age 13 and first child at age 23. No “subrace” or “place of birth” is specified.\n\nFollow the link above to the cancer risk assessment tool, and enter the baseline values into the assessment tool using the link above. According to the assessment tool, what is the probability (“risk”) of developing breast cancer in the next five years? What is the lifetime risk of developing breast cancer? -A- 1.4% risk in the next five years, 7.8% lifetime risk.\nWhat is the effect size on our baseline subject of finding out that:\n\n\none of her close relatives has developed breast cancer? -A- Using the baseline values for the inputs, but changing the number of close relatives who have developed breast cancer to 1, the new five year risk is 2.2% and the lifetime risk is 12.3%. This is a change of five year risk of 0.8 percentage points and 4.5 percentage points for lifetime risk. (In Lesson 33, you’ll see that expressing effect size of a probability is often done using “log-odds”.)\nmore than one of her close relatives have developed breast cancer? -A- This has a dramatic effect, with five-year risk increasing to 4.8 percentage points and lifetime risk increasing to 29.4%.\n\n\nWhat is the effect size associated with comparing the baseline conditions to a woman with the same conditions but a race of white? -A- Five year risk goes down by 0.3 percentage points, lifetime risk goes down by 0.4 percentage points.\nWhat is the effect size of age? Compare the baseline 55-year old woman to herself when she is 65, without the other inputs changing. Since age is quantitative, Report the effect size as percentage-points-per-year. -A- Five year risk goes up to 1.5%, an increase of 0.1 percentage points per year. Lifetime risk goes down to 5.6%, a rate of -0.67 percentage points per year.\n\n\nPossibly Stat2Data::ArcheryData and ask about effect size."
  },
  {
    "objectID": "LC/LC-lesson24.html#zz",
    "href": "LC/LC-lesson24.html#zz",
    "title": "Learning Checks Lesson 24",
    "section": "24.ZZ",
    "text": "24.ZZ\nThe graphs shows world record times in the 100m and 200m butterfly swim race as a function of year, sex, and race length. (Data frame: math300::Butterfly)\n\n\n\n\n\n\n\n\n\nIn 1980, what is the effect size of race distance? (Make sure to give the units.)\nIn 1980, what is the effect size of sex? (Make sure to give the units.)\nIn 2000, what is the effect size of race distance? (Be sure to give the units.)"
  },
  {
    "objectID": "LC/LC-lesson24.html#ww",
    "href": "LC/LC-lesson24.html#ww",
    "title": "Learning Checks Lesson 24",
    "section": "24WW",
    "text": "24WW\nFigure 1 shows a model of the running time of winners of Scottish hill races as a function of climb, distance, and sex.\n\n\n\n\n\nFigure 1: A model of running time in Scottish hill racing versus climb, distance, and sex of the runner\n\n\n\n\nUsing as a baseline a distance of 10 km, a climb of 500m, and sex female, calculate each of these effect sizes. Remember for quantitative inputs to format the effect size as a rate, for instance seconds-per-meter-of-climb.\n\nThe effect size of climb. Consider an increase of climb by 500m. - For females running a 10 km course, an increase in climb from 500m (the specified baseline for comparison) to 1000m is roughly 2000 seconds. Since climb is quantitative, this should be reported as a rate, the change of output (2000 s) divided by the change in input (500 m). This comes to 4 seconds per meter of climb.\nThe effect size of distance. Consider an increase in distance of 10 km. -A- The model output at baseline is about 3500 seconds. The model output for a distance of 20 km (an additional 10 km above baseline) is about 6000 s. The effect size is a rate: (6000 - 3500) s / 10 km, or 250 s/km. For comparison, the fastest 20 km road run by a woman (at the time this is being written) is 1 hour 1 minute 25 seconds, that is, 3685 seconds. This amounts to an average of about 180 s/km. A walker would cover 1 km in about 720 seconds. So the racers are quite fast.\nIs there any evidence of an interaction between sex and climb? -A- Yes. Note that the lines for different climb amounts are vertically spaced more widely for females than for males. This means that the effect size of climb differs between the sexes. That’s an interaction between climb and sex."
  },
  {
    "objectID": "LC/LC-lesson24.html#r",
    "href": "LC/LC-lesson24.html#r",
    "title": "Learning Checks Lesson 24",
    "section": "24.R",
    "text": "24.R\nOver the past hundred years, attitudes toward sex have changed. Let’s examine one way that this change might show up in data: the age at which people first had sex. The National Health and Nutrition Evaluation Survey includes a question about the age at which a person first had sex. For the moment, let’s focus just on that subset of people who have had sex at least once.\n\n\n\n\n\n\nWhat is the effect size of year born on age at first sex? Use units of years-over-years. -A- The model output is 17 years for those born round about 1940, and slightly more than 16 years for those born around 1990. The effect size is therefore about negative 1 year per 50 years\nIs there evidence for an interaction between gender and year born? -A- No, the slopes of the lines are essentially identical for males and females.\nTranslate the effect size in (1) to the units weeks-over-years. -A- 1 year per 50 years is the same as 52 weeks per 50 years, or about one week per year."
  },
  {
    "objectID": "LC/LC-lesson32.html",
    "href": "LC/LC-lesson32.html",
    "title": "Learning Checks Lesson 32",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson26.html",
    "href": "LC/LC-lesson26.html",
    "title": "Learning Checks Lesson 26",
    "section": "",
    "text": "Ideas\nAt the number of beers you found in (1), what fraction of volunteers will be above the 0.08 level?\nTo have a non-zero guideline, we have to allow that the guideline will put a small fraction of people above the 0.08 BAC level. Suppose that we decide to use the standard 95% level for the prediction interval. Construct the 95% prediction interval on BAC for each of the inputs 1 to 5 beers. Which number of beers will keep the upper limit of the prediction interval below the 0.08 BAC limit?"
  },
  {
    "objectID": "LC/LC-lesson26.html#section",
    "href": "LC/LC-lesson26.html#section",
    "title": "Learning Checks Lesson 26",
    "section": "26.1",
    "text": "26.1\nThe openintro::bac data frame records an experiment with sixteen student volunteers at Ohio State University who each drank a randomly assigned number of cans of beer (beers). These students were evenly divided between men and women, and they differed in weight and drinking habits. Thirty minutes later, a police officer measured their blood alcohol content (bac) in grams of alcohol per deciliter of blood.\nConstruct a model of bac ~ beers using the openintro::bac data.\n\nmod <- lm(bac ~ beers, data = openintro::bac)\n\n\nFederal and state laws typically specify a legal upper limit for blood alcohol content of a driver of 0.08%. According to the model function, how many beers corresponds to this upper limit?"
  },
  {
    "objectID": "LC/LC-lesson26.html#solution",
    "href": "LC/LC-lesson26.html#solution",
    "title": "Learning Checks Lesson 26",
    "section": "Solution",
    "text": "Solution\nOne way to calculate this is to guess at the number of beers, then modify your guess according to whether it’s high or low.\n\nmod_eval(mod, beers=4) # too low\n\n  beers model_output\n1     4   0.05915444\n\nmod_eval(mod, beers=6) # too high\n\n  beers model_output\n1     6   0.09508197\n\n# next guess should be around 5\n\nAnother way, for the avid calculus student, is to turn the model into a function, then use Zeros() to find where the output is 0.08.\n\nf <- makeFun(mod)\nmosaicCalc::Zeros(f(beers) - 0.08 ~ beers, mosaicCalc::bounds(beers=0:10))\n\n# A tibble: 1 × 2\n  beers .output.\n  <dbl>    <dbl>\n1  5.16        0\n\n\nSimplest of all, just graph the model function and read backwards from the vertical axis."
  },
  {
    "objectID": "LC/LC-lesson26.html#gg",
    "href": "LC/LC-lesson26.html#gg",
    "title": "Learning Checks Lesson 26",
    "section": "26.GG",
    "text": "26.GG\nThe town where you live has just gone through a so-called 100-year rain storm, which caused flooding of the town’s sewage treatment plant and consequent general ickiness. The city council is holding a meeting to discuss install flood barriers around the sewage treatment plant. The are trying to decide how urgent it is to undertake this expensive project. When will the next 100-year storm occur.\nTo address the question, the city council has enlisted you, the town’s most famous data scientist, to do some research to find the soonest that a 100-year flood can re-occcur.\nYou look at the historical weather records for towns that had a 100-year flood at least 20 years ago. The records start in 1900 and you found 1243 towns with a 100-year flood that happened 20 or more years ago. The plot shows, for all the towns that had a 100-year flood at least 20 years ago, how long it was until the next flood occurred. Those town for which no second flood occurred are shown in a different color.\nYou explain to the city council what a 95% prediction interval is and that you will put your prediction in the form of a probability of 2.5% that the flood will occur sooner than the date you give. You show them how to count dots on a jitter plot to find the 2.5% level.\n\n\nWarning: Removed 1110 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\nSince the town council is thinking of making the wall-building investment in the next 10 years, you also have provided a zoomed-in plot showing just the floods where the interval to the next flood was less than ten years.\n\nYou have n = 1243 floods in your database. How many is 2.5% of 1243? -A- 31\nUsing the zoomed-in plot, starting at the bottom count the number of floods you calculated in part (a). A line drawn where the counting stops is the location of the bottom of the 95% coverage interval. Where is the bottom of the 95% interval.-A- About 2.5 years.\nA council member proposes that the town act soon enough so that there is a 99% chance that the next 100-year flood will not occur before the work is finished. It will take 1 year to finish the work, once it is started. According to your data, when should the town start work? -A- Find the bottom limit that excludes 1% of the 1243 floods in your data. This will be between the 12th and 13th flood, counting up from the bottom. This will be at about 1.25 years, that is 15 months. So the town has 3 months before work must begin. That answer will be a big surprise to those who think the next 100-year flood won’t come for about 100 years.\nA council member has a question. “Judging from the graph on the left, are you saying that the next 100-year flood must come sometime within the next 120 years?” No, that’s not how the graph shold be read. Explain why. -a- Since the records only start in 1900, the longest possible interval can be 120 years, that is, from about 2020 to 1900. About half of the dots in the plot reflect towns that haven’t yet had a recurrence 100-year flood. Those could happen at any time, and presumably many of them will happen after an interval of, say, 150 years or even longer."
  },
  {
    "objectID": "LC/LC-lesson26.html#ww",
    "href": "LC/LC-lesson26.html#ww",
    "title": "Learning Checks Lesson 26",
    "section": "26.WW",
    "text": "26.WW\n\n\n\n\n\n\nIn draft\n\n\n\nThis exercise will be about the mean square error when modeling BMI versus weight. Weight is not the same thing as BMI, though it is closely related. You can see from the data the amount of variation there is in BMI at any given weight.\nAdd in Height as an explanatory variable. Mean square error gets smaller. (R^2 goes from .85 to .95)\nFOR EXTRA CREDIT? Model log BMI against log Weight + log Height. RMS residual is zero."
  },
  {
    "objectID": "LC/LC-lesson26.html#nn",
    "href": "LC/LC-lesson26.html#nn",
    "title": "Learning Checks Lesson 26",
    "section": "26.NN",
    "text": "26.NN\nThe violin plot shows gasoline consumption (in miles-per-gallon for city driving) stratified by the number of passenger doors for the SDSdata::MPG data frame. (For many vehicles, the values are missing. These are marked “NA”.)\n\n\n\n\n\n\nSketch in 95% prediction intervals for gasoline consumption stratified by the number of passenger doors.\n\n::: {.callout-note} ## Solution\n\n\n\n\n\n:::\n\nWhich category – 2 doors, 4 doors, or NA – has the largest number of vehicles?\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is no way to tell from the violin graph. Each violin shows the distribution of values within its category. There’s no information about how many data rows are comprised by an individual violin.\n\n\nThe unit of observation in the MPG data frame is “a model of car”. There are 1154 different models included in MPG, thus 1154 rows. Imagine that you had another data frame with the same variables as MPG, but where the unit of observation were “a registered vehicle”. There are roughly 290 million vehicles registered in the US, so the data frame would have 290 million rows. Also imagine (probably contrary to reality at present) that car models with relatively high miles per gallon are much more popular than cars with low miles per gallon.\n\nSketch out what the MPG ~ doors violins would look like for the 290 million-row table.\n\n\n\n\n\n\n\nSolution\n\n\n\nCompared to the 1154 data frame, the violins would be fatter at the higher miles-per-gallon and correspondingly thinner at low miles-per-gallon. If they are fatter in one place, they must be thinner in another so that the overall area of each violin stays the same.\nThe graph shows an imagined scenario where cars with a 10 mile-per-gallon increase in fuel efficiency triples the popularity of a car model."
  },
  {
    "objectID": "LC/LC-lesson26.html#r",
    "href": "LC/LC-lesson26.html#r",
    "title": "Learning Checks Lesson 26",
    "section": "26.R",
    "text": "26.R\nYou’ve been told that Jenny is in an elementary school that covers grade K through 6. Predict how old is Jenny.\n\nPut your prediction in the format of assigning a probability to each of the possible outcomes, as listed below. Remember that the sum of your probabilities should be 1. (You don’t have to give too much thought to the details. Anything reasonable will do.)\n\nAge         | 3 or under | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12  | 13 | 14 | 15+\n------------|------------|---|---|---|---|---|---|----|----|-----|----|----|-----\nprobability |            |   |   |   |   |   |   |    |    |     |    |    |\n\n\n\n\n\n\nSolution\n\n\n\nPerhaps something like the following, where the probabilities are given in percentage points.\nAge         | 3 or under  | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15+\n------------|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|------\nprobability |      0      | 2.5 | 12  | 12  | 12  | 12  | 12  | 12  | 12  | 11  | 2   | 0.5 | 0\nAges 5 through 12 are equally likely, with a small possibility of 4-year olds or 14 year olds.\n\n\n\nTranslate your set of probabilities to a 95% prediction interval.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe 95% prediction interval 5 to 12 years old.\nA 95% interval should leave out 2.5% of the total probability on either end. Below age 5 there is 2.5% and above age 12 there is 2.5%.\nIf you wrote your own probabilities so that there’s no cut-off that gives exactly 2.5%, then set the interval to come as close as possible to 2.5%."
  },
  {
    "objectID": "LC/LC-lesson27.html",
    "href": "LC/LC-lesson27.html",
    "title": "Learning Checks Lesson 27",
    "section": "",
    "text": "This is a QR day."
  },
  {
    "objectID": "LC/LC-lesson33.html",
    "href": "LC/LC-lesson33.html",
    "title": "Learning Checks Lesson 33",
    "section": "",
    "text": "See medGPA from the Stat2Data package. Maybe come back to this in Lesson 34.\nMaybe Data2Stats::FlightResponse\nMaybe space shuttle Challenger o-ring data."
  },
  {
    "objectID": "LC/LC-lesson33.html#section",
    "href": "LC/LC-lesson33.html#section",
    "title": "Learning Checks Lesson 33",
    "section": "33.1",
    "text": "33.1\n\nConvert probability to odds and log odds, and vice versa.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson33.html#xxx",
    "href": "LC/LC-lesson33.html#xxx",
    "title": "Learning Checks Lesson 33",
    "section": "33.XXX",
    "text": "33.XXX\nA major company in big trouble is planning to lay off 20% of its employees. You work in the personnel office and have constructed a multivariate model of the probability of an individual being laid off. The effect size of the different input factors in the model are as follows:\n\nage over 50 years: change in log-odds 1\nsoftware engineer: change in log-odds -0.5\npay level: change in log-odd rate 0.2 per $10,000 above the company average of $40K\n\n\nWhat are the odds of being laid off for an employee for whom you have no information about age, engineering capabilities, or pay level. -A- The risk of being laid off is 20% so the probability of not being laid off is 80%. The odds are therefore 20/80 = 0.25.\nWhat are the log odds of an employee in (1) being laid off? -A- Simply the logarithm of the odds, so \\(\\log(0.25) = -1.4\\).\nConsider a 30-year old software engineer making $50,000 per year.\n\nWhat are the log odds of her being laid off? -A- At baseline, her log odds is -1.4. Being a software engineer, her log odds are lower by 0.5. But she makes more than the average pay by $10,000, which adds 0.2 to her log odds. Altogether, -1.4 - 0.5 + 0.2 = -1.7.\nTranslate the log odds into an absolute risk of her being laid off. -A- Undoing the logarithm on -1.7 gives an odds of 0.18, which corresponds to a probability of 0.18/(1 + 0.18) or 15%.\n\n\n\nIn LC 24.2, we took as baseline inputs for [this online breast-cancer risk assessment model]((https://bcrisktool.cancer.gov/calculator.html), a 55-year-old, African-American woman who has never had a breast biopsy or any history of breast cancer, who doesn’t know her BRCA status, and whose close relatives have no history of breast cancer, whose first menstrual period was at age 13 and first child at age 23.\nFor this baseline case, the output of the model was a probability of 1.4% five-year risk of developing breast cancer. Finding out that one close relative has developed breast cancer elevates this risk to 2.2%.\nThese are absolute risks: probabilities. The effect size is 2.2 - 1.4 = 0.8 percentage points. When risks are specified as probabilities, differences in risks are in “percentage points.”\n\nWhat risk ratio corresponds to the 1.4 to 2.2 change of risk? -A- This is a simple ratio: 2.2% / 1.4% which is 1.57.\nSuppose (hypothetically) that the risks were ten times higher: 14% and 22%.\n\nWhat would be the risk ratio? -A- Still 1.57.\nWhat would be the percentage point change in absolute risk? -A- 8 percentage points.\n\nPut yourself in the place of the baseline woman who has just found out that a close relative developed breast cancer. Which way of reporting a change of risk is more pertinent to your personal life? -A- The change in absolute risk. Even though the risk ratios are the same in (1) and (2), the change in absolute risk is much greater in (2)."
  },
  {
    "objectID": "LC/first-half-LC.html",
    "href": "LC/first-half-LC.html",
    "title": "Additional LC for first half of course",
    "section": "",
    "text": "The US Department of Transportation has a program called the Fatality Analysis Reporting System. FARS has a web site which publishes data. Figure Figure 1 shows partial screen shot of their web page.\n\n\n\n\n\nFigure 1: National statistics from the US on motor0-vehicle accident-related fatalities. Source: https://www-fars.nhtsa.dot.gov/Main/index.aspx.\n\n\n\n\nFor several reasons, the table is not in tidy form.\n\nSome of the rows serve as headers for the next several rows, but don’t contain any data. Identify several of those headers. -A- “Motor vehicle traffic crashes”, “Traffic crash fatalities”, “Vehicle occupants”, “Non-motorists”, “Other national statistics”, “National rates: fatalities”\nIn tidy data, all the entries in a column should describe the same kind of quantity. You can see that all of the columns contain numbers. But the numbers are not all the same kind of quantity. Referring to the 2016 column:\n\nWhat kind of thing is the number 34,439? -A- A number of crashes\nWhat kind of thing is 18,610? -A- A number of drivers\nWhat kind of thing is 1.18? -A- A rate: fatalities per 100-million miles.\n\nIn tidy data, there is a definite unit of observation that is the same kind of thing for every row. Give an example of two rows that are not the same kind of thing. -A- For example, “Registered vehicles” and “Licensed drivers”. The first is a count of cars, the second a count of drivers.\nIdentify a few rows that are summaries of other rows. Such summaries are not themselves a unit of observation. -A- “Sub Total1”, “Sub Total2”, “Total**”"
  },
  {
    "objectID": "LC/first-half-LC.html#graphics",
    "href": "LC/first-half-LC.html#graphics",
    "title": "Additional LC for first half of course",
    "section": "Graphics",
    "text": "Graphics\nThe three graphs below show the distribution of weights broken down by sex. The violin layer is the same in each graph. Each graph has a coverage interval at one of these levels: 25% 50%, 80%, 95%. Which graph has which coverage interval? Which of the listed coverage intervals is not shown in any graph?\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum"
  },
  {
    "objectID": "LC/first-half-LC.html#violin-plots",
    "href": "LC/first-half-LC.html#violin-plots",
    "title": "Additional LC for first half of course",
    "section": "Violin plots",
    "text": "Violin plots\nThe graph below is a violin plot. Using a pencil and your intuition, add a few dozen dots to the graphic as they would appear in a data layer superimposed on the violin layer. The dots should be jittered and be consistent with the shape of the violins.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nWhere the violin is wider, there is a greater concentration of dots. In a jittered plot, the exact horizontal position of the dots has no significance."
  },
  {
    "objectID": "LC/first-half-LC.html#untidy-data",
    "href": "LC/first-half-LC.html#untidy-data",
    "title": "Additional LC for first half of course",
    "section": "Untidy data",
    "text": "Untidy data\nList what’s not tidy about this table.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nUnits ought to be in the codebook.\nThe “length of year” variable is in a mixture of units. Some rows are (Earth) days, others are (Earth) years.\nThe numbers have commas, which are intended for human consumption. Data tables are for machine consumption and the commas are a nuisancwe.\nThe \\(\\frac{1}{4}\\) in the “length of year” column is not a standard computer numeral. Write 365.25 instead."
  },
  {
    "objectID": "LC/first-half-LC.html#calculating-new-variables-from-old.",
    "href": "LC/first-half-LC.html#calculating-new-variables-from-old.",
    "title": "Additional LC for first half of course",
    "section": "Calculating new variables from old.",
    "text": "Calculating new variables from old.\nTITLE GOES HERE: Often, a data scientist needs to calculate some new quantity from the quantities already available in a data table. Proper data computing software makes this easy to do in a manner that is clear even to an inexperienced reader. For example, the following statement will take a data frame named Fatality_data with the structure of @tab-pine-hit-pants and augment it with a new variable total_fatalities that gives the total number of fatalities in each year\n\nFARS <-\n  FARS %>%\n  mutate(total_fatalities = drivers + passengers + unknown)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\ncrashes\ndrivers\npassengers\nunknown\nmiles\nresident_pop\ntotal_fatalities\n\n\n\n\n2016\n34439\n18610\n6407\n79\n3174\n323128\n25096\n\n\n2015\n32539\n17613\n6213\n71\n3095\n320897\n23897\n\n\n2014\n30056\n16470\n5766\n71\n3026\n318563\n22307\n\n\n\n\n\nThe mutate() function does the work of creating the new variable. The text between the opening parenthesis and the corresponding closing parenthesis is the argument to mutate(), which gives the specifics of what to do.\nFor each of the following, write the argument for a mutate() statement that will produce the desired new variable. You can assume that total_fatalities is already one of the existing variables. (We created it above!)\n\nFatalities per crash. ::: {.callout-note} ##Solution\n\n\nfatalities_per_crash = total_fatalities / crash\n\n:::\n\nFatalities per million vehicle miles. ::: {.callout-note} ##Solution\n\n\nfatalities_per_distance = total_fatalities / (miles * 1000)\n\nDivide the total number of fatalities by the number of vehicle miles travelled.\nWhy the multiplication by 1000? Recall that miles is in billions of vehicle miles, while we want the units of fatalities_per_distance to be fatalities per million vehicle miles. The 1000 performs the conversion from billions to millions. :::\n\nNumber of crashes per million vehicle miles. ::: {.callout-note} ##Solution\n\n\ncrashes_per_distance = crashes / (miles * 1000)\n\n::: d. Referring back to the original data shown in Figure @ref(fig:bear-ride-pants-1), you can see that the calculation of total fatalities in the introduction to this problem left out the number of motorcyclist and nonmotorist fatalities. Modify the calculation of total fatalities to include these, assuming they are represented by motorcyclist and nonmotorist respectively. ::: {.callout-note} ##Solution\n\ntotal_fatalities = drivers + passengers + unknown +\n  motorcyclist + nonmotorist\n\n:::\n——–="
  },
  {
    "objectID": "LC/first-half-LC.html#unit-of-observation",
    "href": "LC/first-half-LC.html#unit-of-observation",
    "title": "Additional LC for first half of course",
    "section": "Unit of observation",
    "text": "Unit of observation\nThe data table below records activity at a neighborhood car repair shop.\n\n\n\nmechanic\nproduct\nprice\ndate\n\n\n\n\nAnne\nstarter\n170.00\n2019-01-12\n\n\nBeatrice\nshock absorber\n78.42\n2019-01-12\n\n\nAnne\nalternator\n385.95\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nClarisse\nbrake shoe\n39.50\n2019-01-12\n\n\nBeatrice\nradiator hose\n17.90\n2019-02-12\n\n\n\nThe codebook for a data table should describe what is the unit of observation. For the purpose of this exercise, your job is to comment on each of the following possibilities and say why or why not this is plausibly the unit of observation.\n\na day. -A- There must be more to it than that, since the same date may be repeated with different values for the other variables.\na mechanic. -A- No. The same mechanic appears multiple times, so the unit of observation is not simply a mechanic.\na car part used in a repair. -A- Could be, for instance if every time a mechanic installs a part a new entry is added to the table describing the part, its price, the date, and the mechanic doing the work."
  },
  {
    "objectID": "LC/first-half-LC.html#graphics-and-data",
    "href": "LC/first-half-LC.html#graphics-and-data",
    "title": "Additional LC for first half of course",
    "section": "Graphics and data",
    "text": "Graphics and data\nThe graphic below contains a single data layer. Four of the data points are annotated with letters in order to identify them specifically.\n\n\n\n\n\n\n\n\n\nPart 1\n\nIs the income level of “a” greater than “b”? -A- no\nIs the income level of “d” greater than “a”? -A- no\nIs the number of rooms greater for “b” than for “a”? -A- no\nIs the number of rooms greater for “c” than for “a”? -A- no\n\nPart 2\nHere is the data plotted in the figure.\n\n\n\n\n \n  \n    row \n    income \n    number_of_rooms \n  \n \n\n  \n    1 \n    0.90 \n    1 \n  \n  \n    2 \n    1.00 \n    3 \n  \n  \n    3 \n    0.31 \n    3 \n  \n  \n    4 \n    0.85 \n    1 \n  \n  \n    5 \n    1.09 \n    3 \n  \n  \n    6 \n    1.19 \n    2 \n  \n  \n    7 \n    1.01 \n    1 \n  \n  \n    8 \n    1.09 \n    3 \n  \n  \n    9 \n    1.16 \n    2 \n  \n  \n    10 \n    2.86 \n    2 \n  \n\n\n ... and so on for 2,765 rows altogether.\n\n\n\n\nThe points a, b, c, and d, are shown in the table. For each of a, b, c, d, say which row corresponds to the point. -A- a is row 8, b is row 7, c is row 2, d is row 1"
  },
  {
    "objectID": "LC/Learning-checks.html",
    "href": "LC/Learning-checks.html",
    "title": "Learning Checks from Modern Dive",
    "section": "",
    "text": "LC 1.1 Block 1 Day 1\n\n\n\nRepeat the earlier installation steps, but for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package for data wrangling, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for generating easy-to-read tables in R. We’ll use these packages in the next section.\n\n\n\n\n\n\n\n\nLC 1.2 Block 1 Day 1\n\n\n\n“Load” the dplyr, nycflights13, and knitr packages as well by repeating the earlier steps.\n\n\nRun View(flights) in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note the uppercase V in View(). R is case-sensitive, so you’ll get an error message if you run view(flights) instead of View(flights)\n\n\n\n\n\n\nLC 1.3 Block 1 Day 1\n\n\n\nWhat does any ONE row in this flights dataset refer to?\n\nA. Data on an airline\nB. Data on a flight\nC. Data on an airport\nD. Data on multiple flights\n\n\n\n\n\n\n\n\n\nLC 1.4 Block 1 Day 1\n\n\n\nWhat are some other examples in this dataset (flights) of categorical variables? What makes them different than quantitative variables?\n\n\n\n\n\n\n\n\nLC 1.5 Block 1 Day 1\n\n\n\nWhat properties of each airport do the variables lat, lon, alt, tz, dst, and tzone describe in the airports data frame? Take your best guess.\n\n\n\n\n\n\n\n\nLC 1.6 Block 1 Day 1\n\n\n\nProvide the names of variables in a data frame with at least three variables where one of them is an identification variable and the other two are not. Further, create your own tidy data frame that matches these conditions.\n\n\n\n\n\n\n\n\nLC 1.7 Block 1 Day 1\n\n\n\nLook at the help file for the airports data frame. Revise your earlier guesses about what the variables lat, lon, alt, tz, dst, and tzone each describe."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-2-visualization",
    "href": "LC/Learning-checks.html#chapter-2-visualization",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 2: Visualization",
    "text": "Chapter 2: Visualization\n\n\n\n\n\n\nLC 2.1 Block 1 Day 2\n\n\n\nTake a look at both the flights and alaska_flights data frames by running View(flights) and View(alaska_flights). In what respect do these data frames differ? For example, think about the number of rows in each dataset.\n\n\n\n\n\n\n\n\nLC 2.2-2.6 Block 1 Day 2\n\n\n\nWhat are some practical reasons why dep_delay and arr_delay have a positive relationship?\nWhat variables in the weather data frame would you expect to have a negative correlation (i.e., a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here. Hint: Explore the weather dataset by using the View() function.\nWhy do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaska Air flights?\nWhat are some other features of the plot that stand out to you?\nCreate a new scatterplot using different variables in the alaska_flights data frame by modifying the example given.\n\n\n\n\n\n\n\n\nLC 2.7-2.8 Block 1 Day 2\n\n\n\nWhy is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot?\nAfter viewing Figure @ref(fig:alpha), give an approximate range of arrival delays and departure delays that occur most frequently. How has that region changed compared to when you observed the same plot without alpha = 0.2 set in Figure @ref(fig:noalpha)?\n\n\n\n\n\n\n\n\nLC 2.9-2.10 Block 1 Day 3\n\n\n\nLC 2.9 Take a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather). In what respect do these data frames differ?\nLC 2.10 View() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement, whereas the hour variable does not?\n\n\n\n\n\n\n\n\nLC 2.11-2.13 Block 1 Day 3\n\n\n\nLC 2.11 Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis?\nLC 2.12 Why are linegraphs frequently used when time is the explanatory variable on the x-axis?\nLC 2.12 Plot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013.\n\n\n\n\n\n\n\n\nLC 2.18-2.21 Block 1 Day 3\n\n\n\nWhat other things do you notice about this faceted plot? How does a faceted plot help us see relationships between two variables?\nWhat do the numbers 1-12 correspond to in the plot? What about 25, 50, 75, 100?\nFor which types of datasets would faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics.\nLC 2.21 Does the temp variable in the weather dataset have a lot of variability? Why do you say that?\n\n\n\n\n\n\n\n\nLC 2.22-2.25 Boxplots Block 1 Day 4\n\n\n\nLC 2.22 What does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point.\nLC 2.23 Which months have the highest variability in temperature? What reasons can you give for this?\nLC 2.24 We looked at the distribution of the numerical variable temp split by the numerical variable month that we converted using the factor() function in order to make a side-by-side boxplot. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative?\nLC 2.25 Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram?\n\n\n\n\n\n\n\n\nLC 2.26-2.29 Histograms Block 1 Day 4\n\n\n\nLC 2.26 Why are histograms inappropriate for categorical variables?\nLC 2.27 What is the difference between histograms and barplots?\nLC 2.28 How many Envoy Air flights departed NYC in 2013?\nLC 2.29 What was the 7th highest airline for departed flights from NYC in 2013? How could we better present the table to get this answer quickly?\n\n\n\n\n\n\n\n\nLC 2.30-2.31 Pie charts Block 1 Day 4\n\n\n\nLC 2.30 Why should pie charts be avoided and replaced by barplots?\nLC 2.31 Why do you think people continue to use pie charts?\n\n\n\n\n\n\n\n\nLC 2.32-2.37 Block 1 Day 4\n\n\n\nLC 2.32 What kinds of questions are not easily answered by looking at Figure @ref(fig:flights-stacked-bar) (2.23)?\nLC 2.33 What can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights?\nLC 2.34 Why might the side-by-side barplot be preferable to a stacked barplot in this case?\nLC 2.35 What are the disadvantages of using a dodged barplot, in general?\nLC 2.36 Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case?\nLC 2.37 What information about the different carriers at different airports is more easily seen in the faceted barplot?"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-3-wrangling",
    "href": "LC/Learning-checks.html#chapter-3-wrangling",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 3: Wrangling",
    "text": "Chapter 3: Wrangling\n\n\n\n\n\n\nLC 3.1 Block 1 Day 5\n\n\n\nWhat’s another way of using the “not” operator ! to filter only the rows that are not going to Burlington, VT nor Seattle, WA in the flights data frame? Test this out using the previous code.\n\n\n\n\n\n\n\n\nLC 3.2 Block 1 Day 5\n\n\n\nSay a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five-year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach?\n\n\n\n\n\n\n\n\nLC 3.3 Block 1 Day 5\n\n\n\nModify the earlier summarize() function code that creates the summary_temp data frame to also use the n() summary function: summarize(... , count = n()). What does the returned value correspond to?\n\n\n\n\n\n\n\n\nLC 3.4 Block 1 Day 5\n\n\n\nWhy doesn’t the following code work? Run the code line-by-line instead of all at once, and then look at the data. In other words, run summary_temp <- weather %>% summarize(mean = mean(temp, na.rm = TRUE)) first.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nsummary_temp <- weather %>%   \n  summarize(mean = mean(temp, na.rm = TRUE)) %>% \n  summarize(std_dev = sd(temp, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\nLC 3.5 Block 1 Day 6\n\n\n\nRecall from Chapter @ref(viz) when we looked at temperatures by months in NYC. What does the standard deviation column in the summary_monthly_temp data frame tell us about temperatures in NYC throughout the year?\n\n\n\n\n\n\n\n\nLC 3.6 Block 1 Day 6\n\n\n\nWhat code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC?\n\n\n\n\n\n\n\n\nLC 3.7 Block 1 Day 6\n\n\n\nRecreate by_monthly_origin, but instead of grouping via group_by(origin, month), group variables in a different order group_by(month, origin). What differs in the resulting dataset?\n\n\n\n\n\n\n\n\nLC 3.8 Block 1 Day 6\n\n\n\nHow could we identify how many flights left each of the three airports for each carrier?\n\n\n\n\n\n\n\n\nLC 3.9 Block 1 Day 6\n\n\n\nHow does the filter() operation differ from a group_by() followed by a summarize()?\n\n\n\n\n\n\n\n\nLC 3.10 Block 1 Day 6\n\n\n\nWhat do positive values of the gain variable in flights correspond to? What about negative values? And what about a zero value?\n\n\n\n\n\n\n\n\nLC 3.11 Block 1 Day 6\n\n\n\nCould we create the dep_delay and arr_delay columns by simply subtracting dep_time from sched_dep_time and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in flights.\n\n\n\n\n\n\n\n\nLC 3.12 Block 1 Day 76\n\n\n\nWhat can we say about the distribution of gain? Describe it in a few sentences using the plot and the gain_summary data frame values.\n\n\n\n\n\n\n\n\nLC 3.13 Block 1 Day 7\n\n\n\nLooking at Figure @ref(fig:reldiagram), when joining flights and weather (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of year, month, day, hour, and origin, and not just hour?\n\n\n\n\n\n\n\n\nLC 3.14 Block 1 Day 7\n\n\n\nWhat surprises you about the top 10 destinations from NYC in 2013?\n\n\n\n\n\n\n\n\nLC 3.15 Block 1 Day 7\n\n\n\nWhat are some advantages of data in normal forms? What are some disadvantages?\n\n\n\n\n\n\n\n\nLC 3.16 Block 1 Day 7\n\n\n\nWhat are some ways to select all three of the dest, air_time, and distance variables from flights? Give the code showing how to do this in at least three different ways.\n\n\n\n\n\n\n\n\nLC 3.17 Block 1 Day 7\n\n\n\nHow could one use starts_with(), ends_with(), and contains() to select columns from the flights data frame? Provide three different examples in total: one for starts_with(), one for ends_with(), and one for contains().\n\n\n\n\n\n\n\n\nLC 3.18 Block 1 Day 7\n\n\n\nWhy might we want to use the select function on a data frame?\n\n\n\n\n\n\n\n\nLC 3.19 Block 1 Day 7\n\n\n\nCreate a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013.\n\n\n::: {.callout-note icon=false} ## LC 3.20 Block 1 Day 7 Let’s now put your newly acquired data wrangling skills to the test!\nAn airline industry measure of a passenger airline’s capacity is the available seat miles, which is equal to the number of seats available multiplied by the number of miles or kilometers flown summed over all flights.\nFor example, let’s consider the scenario in Figure 1. Since the airplane has 4 seats and it travels 200 miles, the available seat miles are \\(4 \\times 200 = 800\\).\n\n\n\n\n\nFigure 1: Example of available seat miles for one flight.\n\n\n\n\nExtending this idea, let’s say an airline had 2 flights using a plane with 10 seats that flew 500 miles and 3 flights using a plane with 20 seats that flew 1000 miles, the available seat miles would be \\(2 \\times 10 \\times 500 + 3 \\times 20 \\times 1000 = 70,000\\) seat miles.\nUsing the datasets included in the nycflights13 package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints:\n\nCrucial: Unless you are very confident in what you are doing, it is worthwhile not starting to code right away. Rather, first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level pseudocode that is informal yet detailed enough to articulate what you are doing. This way you won’t confuse what you are trying to do (the algorithm) with how you are going to do it (writing dplyr code).\nTake a close look at all the datasets using the View() function: flights, weather, planes, airports, and airlines to identify which variables are necessary to compute available seat miles.\nFigure @ref(fig:reldiagram) showing how the various datasets can be joined will also be useful.\nConsider the data wrangling verbs in Table @ref(tab:wrangle-summary-table) as your toolbox! ::"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-4-tidy",
    "href": "LC/Learning-checks.html#chapter-4-tidy",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 4: Tidy",
    "text": "Chapter 4: Tidy\n::: {.callout-note icon=false} ## LC 4.1 Block 1 Day 8 What are common characteristics of “tidy” data frames? ::\n::: {.callout-note icon=false} ## LC 4.2 Block 1 Day 8 What makes “tidy” data frames useful for organizing data? ::\n::: {.callout-note icon=false} ## LC 4.3 Block 1 Day 8 Take a look at the airline_safety data frame included in the fivethirtyeight data package. Run the following:\n\nairline_safety\n\nAfter reading the help file by running ?airline_safety, we see that airline_safety is a data frame containing information on different airline companies’ safety records. This data was originally reported on the data journalism website, FiveThirtyEight.com, in Nate Silver’s article, “Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?”. Let’s only consider the variables airlines and those relating to fatalities for simplicity:\n\nairline_safety_smaller <- airline_safety %>% \n  select(airline, starts_with(\"fatalities\"))\nairline_safety_smaller\n\n# A tibble: 56 × 3\n   airline               fatalities_85_99 fatalities_00_14\n   <chr>                            <int>            <int>\n 1 Aer Lingus                           0                0\n 2 Aeroflot                           128               88\n 3 Aerolineas Argentinas                0                0\n 4 Aeromexico                          64                0\n 5 Air Canada                           0                0\n 6 Air France                          79              337\n 7 Air India                          329              158\n 8 Air New Zealand                      0                7\n 9 Alaska Airlines                      0               88\n10 Alitalia                            50                0\n# … with 46 more rows\n\n\nThis data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, in particular so that it has a variable fatalities_years indicating the incident year and a variable count of the fatality counts? ::\n::: {.callout-note icon=false} ## LC 4.4 Block 1 Day 9 Convert the dem_score data frame into a “tidy” data frame and assign the name of dem_score_tidy to the resulting long-formatted data frame. ::\n::: {.callout-note icon=false} ## LC 4.5 Block 1 Day 9 Read in the life expectancy data stored at https://moderndive.com/data/le_mess.csv and convert it to a “tidy” data frame. ::"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-5-regression",
    "href": "LC/Learning-checks.html#chapter-5-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 5: Regression",
    "text": "Chapter 5: Regression\n\n\n\n\n\n\nLC 5.1 Block 2 Day 1\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) being score but with age as the new explanatory variable \\(x\\). Remember, this involves three things:\n\nLooking at the raw data values.\nComputing summary statistics.\nCreating data visualizations.\n\nWhat can you say about the relationship between age and teaching scores based on this exploration?\n\n\n\n\n\n\n\n\nLC 5.2 Block 2 Day 1\n\n\n\nFit a new simple linear regression using lm(score ~ age, data = evals_ch5) where age is the new explanatory variable \\(x\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your earlier exploratory data analysis?\n\n\n\n\n\n\n\n\nLC 5.3 Block 2 Day 1\n\n\n\nGenerate a data frame of the residuals of the model where you used age as the explanatory \\(x\\) variable.\n\n\n\n\n\n\n\n\nLC 5.4 Block 2 Day 2\n\n\n\nConduct a new exploratory data analysis with the same explanatory variable \\(x\\) being continent but with gdpPercap as the new outcome variable \\(y\\). What can you say about the differences in GDP per capita between continents based on this exploration?\n\n\n\n\n\n\n\n\nLC 5.5 Block 2 Day 2\n\n\n\nFit a new linear regression using lm(gdpPercap ~ continent, data = gapminder2007) where gdpPercap is the new outcome variable \\(y\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis?\n\n\n\n\n\n\n\n\nLC 5.6 Block 2 Day 2\n\n\n\nUsing either the sorting functionality of RStudio’s spreadsheet viewer or using the data wrangling tools you learned in Chapter @ref(wrangling), identify the five countries with the five smallest (most negative) residuals? What do these negative residuals say about their life expectancy relative to their continents’ life expectancy?\n\n\n\n\n\n\n\n\nLC 5.7 Block 2 Day 2\n\n\n\nRepeat this process, but identify the five countries with the five largest (most positive) residuals. What do these positive residuals say about their life expectancy relative to their continents’ life expectancy?\n\n\n\n\n\n\n\n\nLC 5.8 Block 2 Day 3\n\n\n\nNote in Figure @fig:three-lines there are 3 points marked with dots and:\n\nThe “best” fitting solid regression line in blue\nAn arbitrarily chosen dotted red line\nAnother arbitrarily chosen dashed green line\n\n\n\n\n\n\nFigure 2: Regression line and two others.\n\n\n\n\nCompute the sum of squared residuals by hand for each line and show that of these three lines, the regression line in blue has the smallest value."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-6-multiple-regression",
    "href": "LC/Learning-checks.html#chapter-6-multiple-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 6: Multiple regression",
    "text": "Chapter 6: Multiple regression\n\n\n\n\n\n\nLC 6.1 Block 2 Day 4\n\n\n\nCompute the observed values, fitted values, and residuals not for the interaction model as we just did, but rather for the parallel slopes model we saved in score_model_parallel_slopes.\n\n\n\n\n\n\n\n\nLC 6.2\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age?\n\n\n\n\n\n\n\n\nLC 6.3\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age?\n\n\n\n\n\n\n\n\nLC 6.4\n\n\n\nFit a new simple linear regression using lm(debt ~ credit_rating + age, data = credit_ch6) where credit_rating and age are the new numerical explanatory variables \\(x_1\\) and \\(x_2\\). Get information about the “best-fitting” regression plane from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis?"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-7-sampling",
    "href": "LC/Learning-checks.html#chapter-7-sampling",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 7: Sampling",
    "text": "Chapter 7: Sampling\n\n\n\n\n\n\nLC 7.1 Block 3 Day 1\n\n\n\nWhy was it important to mix the bowl before we sampled the balls?\n\n\n\n\n\n\n\n\nLC 7.2 Block 3 Day 1\n\n\n\nWhy is it that our 33 groups of friends did not all have the same numbers of balls that were red out of 50, and hence different proportions red?\n\n\n\n\n\n\n\n\nLC 7.3 Block 3 Day 1\n\n\n\nWhy couldn’t we study the effects of sampling variation when we used the virtual shovel only once? Why did we need to take more than one virtual sample (in our case 33 virtual samples)?\n\n\n\n\n\n\n\n\nLC 7.4 Block 3 Day 1\n\n\n\nWhy did we not take 1000 “tactile” samples of 50 balls by hand?\n\n\n\n\n\n\n\n\nLC 7.5 Block 3 Day 1\n\n\n\nLooking at Figure @ref(fig:samplingdistribution-virtual-1000), would you say that sampling 50 balls where 30% of them were red is likely or not? What about sampling 50 balls where 10% of them were red?\n\n\n\n\n\n\n\n\nLC 7.6 Block 3 Day 1\n\n\n\nIn Figure 7.9, we used shovels to take 1000 samples each, computed the resulting 1000 proportions of the shovel’s balls that were red, and then visualized the distribution of these 1000 proportions in a histogram. We did this for shovels with 25, 50, and 100 slots in them. As the size of the shovels increased, the histograms got narrower. In other words, as the size of the shovels increased from 25 to 50 to 100, did the 1000 proportions\n\nA. vary less,\nB. vary by the same amount, or\nC. vary more?\n\n\n\n\n\n\n\n\n\nLC 7.7 Block 3 Day 1\n\n\n\nWhat summary statistic did we use to quantify how much the 1000 proportions red varied?\n\nA. The interquartile range\nB. The standard deviation\nC. The range: the largest value minus the smallest.\n\n\n\n\n\n\n\n\n\nLC 7.8 Block 3 Day 2\n\n\n\nIn the case of our bowl activity, what is the population parameter? Do we know its value?\n\n\n\n\n\n\n\n\nLC 7.9 Block 3 Day 2\n\n\n\nWhat would performing a census in our bowl activity correspond to? Why did we not perform a census?\n\n\n\n\n\n\n\n\nLC 7.10 Block 3 Day 2\n\n\n\nWhat purpose do point estimates serve in general? What is the name of the point estimate specific to our bowl activity? What is its mathematical notation?\n\n\n\n\n\n\n\n\nLC 7.11 Block 3 Day 2\n\n\n\nHow did we ensure that our tactile samples using the shovel were random?\n\n\n\n\n\n\n\n\nLC 7.12 Block 3 Day 2\n\n\n\nWhy is it important that sampling be done at random?\n\n\n\n\n\n\n\n\nLC 7.13 Block 3 Day 2\n\n\n\nWhat are we inferring about the bowl based on the samples using the shovel?\n\n\n\n\n\n\n\n\nLC 7.14 Block 3 Day 2\n\n\n\nWhat purpose did the sampling distributions serve?\n\n\n\n\n\n\n\n\nLC 7.15 Block 3 Day 2\n\n\n\nWhat does the standard error of the sample proportion \\(\\widehat{p}\\) quantify?\n\n\n\n\n\n\n\n\nLC 7.16 Block 3 Day 2\n\n\n\nThe table that follows is a version of Table @ref(tab:comparing-n-2) matching sample sizes \\(n\\) to different standard errors of the sample proportion \\(\\widehat{p}\\), but with the rows randomly re-ordered and the sample sizes removed. Fill in the table by matching the correct sample sizes to the correct standard errors.\nStandard errors of \\(\\hat{p}\\) based on n = 25, 50, 100\n\n\n\nSample size\nStandard error of \\(\\hat{p}\\)\n\n\n\n\n\\(n=\\)\n0.94\n\n\n\\(n=\\)\n0.45\n\n\n\\(n=\\)\n0.69\n\n\n\nFor the following four Learning checks, let the estimate be the sample proportion \\(\\widehat{p}\\): the proportion of a shovel’s balls that were red. It estimates the population proportion \\(p\\): the proportion of the bowl’s balls that were red.\n\n\n\n\n\n\n\n\nLC 7.17 Block 3 Day 2\n\n\n\nWhat is the difference between an accurate and a precise estimate?\n\n\n\n\n\n\n\n\nLC 7.18 Block 3 Day 2\n\n\n\nHow do we ensure that an estimate is accurate? How do we ensure that an estimate is precise?\n\n\n\n\n\n\n\n\nLC 7.19 Block 3 Day 2\n\n\n\nIn a real-life situation, we would not take 1000 different samples to infer about a population, but rather only one. Then, what was the purpose of our exercises where we took 1000 different samples?\n\n\n\n\n\n\n\n\nLC 7.20 Block 3 Day 2\n\n\n\nFigure @ref(fig:accuracy-vs-precision) with the targets shows four combinations of “accurate versus precise” estimates. Draw four corresponding sampling distributions of the sample proportion \\(\\widehat{p}\\), like the one in the leftmost plot in Figure @ref(fig:comparing-sampling-distributions-3).\n\n\n\n\n\n\n\n\nLC 7.21 Block 3 Day 3\n\n\n\nThe Royal Air Force wants to study how resistant all their airplanes are to bullets. They study the bullet holes on all the airplanes on the tarmac after an air battle against the Luftwaffe (German Air Force).\n\n\n\n\n\n\n\n\nLC 7.22 Block 3 Day 3\n\n\n\nImagine it is 1993, a time when almost all households had landlines. You want to know the average number of people in each household in your city. You randomly pick out 500 phone numbers from the phone book and conduct a phone survey.\n\n\n\n\n\n\n\n\nLC 7.23 Block 3 Day 3\n\n\n\nYou want to know the prevalence of illegal downloading of TV shows among students at a local college. You get the emails of 100 randomly chosen students and ask them, “How many times did you download a pirated TV show last week?”.\n\n\n\n\n\n\n\n\nLC 7.24 Block 3 Day 3\n\n\n\nA local college administrator wants to know the average income of all graduates in the last 10 years. So they get the records of five randomly chosen graduates, contact them, and obtain their answers."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-8-confidence-intervals",
    "href": "LC/Learning-checks.html#chapter-8-confidence-intervals",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 8: Confidence intervals",
    "text": "Chapter 8: Confidence intervals\n\n\n\n\n\n\nLC 8.1 Block 3 Day 5\n\n\n\nWhat is the chief difference between a bootstrap distribution and a sampling distribution?\n\n\n\n\n\n\n\n\nLC 8.2 Block 3 Day 5\n\n\n\nLooking at the bootstrap distribution for the sample mean in Figure @ref(fig:one-thousand-sample-means), between what two values would you say most values lie?\n\n\n\n\n\n\n\n\nLC 8.3 Block 3 Day 6\n\n\n\nWhat condition about the bootstrap distribution must be met for us to be able to construct confidence intervals using the standard error method?\n\n\n\n\n\n\n\n\nLC 8.4 Block 3 Day 6\n\n\n\nSay we wanted to construct a 68% confidence interval instead of a 95% confidence interval for \\(\\mu\\). Describe what changes are needed to make this happen. Hint: we suggest you look at Appendix @ref(appendix-normal-curve) on the normal distribution.\n\n\n\n\n\n\n\n\nLC 8.5 Block 3 Day 8\n\n\n\nConstruct a 95% confidence interval for the median year of minting of all US pennies. Use the percentile method and, if appropriate, then use the standard-error method."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-9-hypothesis-testing",
    "href": "LC/Learning-checks.html#chapter-9-hypothesis-testing",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 9: Hypothesis testing",
    "text": "Chapter 9: Hypothesis testing\n\n\n\n\n\n\nLC 9.1 Block 4 Day 2\n\n\n\nWhy does the following code produce an error? In other words, what about the response and predictor variables make this not a possible computation with the infer package?\n\nlibrary(moderndive)\nlibrary(infer)\nnull_distribution_mean <- promotions %>%\n  specify(formula = decision ~ gender, success = \"promoted\") %>% \n  hypothesize(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\n\n\n\n\n\n\n\n\nLC 9.2 Block 4 Day 2\n\n\n\nWhy are we relatively confident that the distributions of the sample proportions will be good approximations of the population distributions of promotion proportions for the two genders?\n\n\n\n\n\n\n\n\nLC 9.3 Block 4 Day 2\n\n\n\nUsing the definition of p-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the promotion rates for males and females.\n\n\n\n\n\n\n\n\nLC 9.4 Block 4 Day 2\n\n\n\nDescribe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between the promotion rate of males and females using this study.\n\n\n\n\n\n\n\n\nLC 9.5 Block 4 Day 3\n\n\n\nWhat is wrong about saying, “The defendant is innocent.” based on the US system of criminal trials?\n\n\n\n\n\n\n\n\nLC 9.6 Block 4 Day 3\n\n\n\nWhat is the purpose of hypothesis testing?\n\n\n\n\n\n\n\n\nLC 9.7 Block 4 Day 3\n\n\n\nWhat are some flaws with hypothesis testing? How could we alleviate them?\n\n\n\n\n\n\n\n\nLC 9.8 Block 4 Day 3\n\n\n\nConsider two \\(\\alpha\\) significance levels of 0.1 and 0.01. Of the two, which would lead to a more liberal hypothesis testing procedure? In other words, one that will, all things being equal, lead to more rejections of the null hypothesis \\(H_0\\).\n\n\n\n\n\n\n\n\nLC 9.9\n\n\n\nConduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating. What was different and what was the same?\n\n\n\n\n\n\n\n\nLC 9.10\n\n\n\nWhat conclusions can you make from viewing the faceted histogram looking at rating versus genre that you couldn’t see when looking at the boxplot?\n\n\n\n\n\n\n\n\nLC 9.11\n\n\n\nDescribe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies.\n\n\n\n\n\n\n\n\nLC 9.12\n\n\n\nWhy are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres?\n\n\n\n\n\n\n\n\nLC 9.13\n\n\n\nUsing the definition of \\(p\\)-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the mean rating of romance to action movies.\n\n\n\n\n\n\n\n\nLC 9.14\n\n\n\nWhat is the value of the \\(p\\)-value for the hypothesis test comparing the mean rating of romance to action movies?\n\n\n\n\n\n\n\n\nLC 9.15\n\n\n\nTest your data wrangling knowledge and EDA skills:\n\nUse dplyr and tidyr to create the necessary data frame focused on only action and romance movies (but not both) from the movies data frame in the ggplot2movies package.\nMake a boxplot and a faceted histogram of this population data comparing ratings of action and romance movies from IMDb.\nDiscuss how these plots compare to the similar plots produced for the movies_sample data."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-10-inference-for-regression",
    "href": "LC/Learning-checks.html#chapter-10-inference-for-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 10: Inference for regression",
    "text": "Chapter 10: Inference for regression\n\n\n\n\n\n\nLC 10.1 Block 4 Day 7\n\n\n\nContinuing with our regression using age as the explanatory variable and teaching score as the outcome variable.\n\nUse the get_regression_points() function to get the observed values, fitted values, and residuals for all 463 instructors.\nPerform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here.\n\n\n\n\n\n\n\n\n\nLC 10.2 Block 4 Day 8\n\n\n\nRepeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of stat = \"correlation\" in the calculate() function of the infer package."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-11-tell-your-story-with-data",
    "href": "LC/Learning-checks.html#chapter-11-tell-your-story-with-data",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 11: Tell your story with data",
    "text": "Chapter 11: Tell your story with data\n\n\n\n\n\n\n\nLC 11.1 Block 4 Day 2\n\n\n\nRepeat the regression modeling in Subsection 11.2.3 and the prediction making you just did on the house of condition 5 and size 1900 square feet in Subsection 12.2.4, but using the parallel slopes model you visualized in Figure 11.6. Show that it’s $524,807!\n\n\n\n\n\n\n\n\nLC 11.2\n\n\n\nWhat date between 1994 and 2003 has the fewest number of births in the US? What story could you tell about why this is the case?"
  },
  {
    "objectID": "LC/LC-lesson29.html",
    "href": "LC/LC-lesson29.html",
    "title": "Learning Checks Lesson 29",
    "section": "",
    "text": "The `math300::Hill_racing” data frame records 2236 winning times (in seconds) in Scottish hill racing competitions. Consider this model of the winning time as a function of the race distance (km) and the total climb (meters):\n\nmod <- lm(time ~ distance + climb, data=Hill_racing)\n\nThe model_eval() function provides a convenient way to evaluate the model output (.output) for each of the rows in a data frame and, at the same time, calculates row-by-row residuals (.resid) and prediction errors (.lwr and .upr). Make sure to take not of the starting periods on the names.\n\nmodel_eval(mod) %>% head()\n\n  time distance climb  .output     .resid       .lwr     .upr\n1 1630        6   240 1679.215  -49.21475  -29.56279 3387.992\n2 1655        6   240 1679.215  -24.21475  -29.56279 3387.992\n3 2391        6   240 1679.215  711.78525  -29.56279 3387.992\n4 2351        6   240 1679.215  671.78525  -29.56279 3387.992\n5 4151       14   660 4805.779 -654.77947 3097.10184 6514.457\n6 3975       14   660 4805.779 -830.77947 3097.10184 6514.457\n\n\nThe RMS residual from the model can be calculated this way:\n\nmodel_eval(mod) %>%\n  summarize(rms = sqrt(mean(.resid^2)))\n\n       rms\n1 870.4631\n\n\n\nWhat are the units of the RMS residual?\nModify the calculation to compute the sum-of-square residuals. Report the result numerically. Be sure to say what are the units.\nWhat are the units of the effect size on time with respect to climb?\nWhat are the units of the effect size on time with respect to climb?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nRMS residual has the same units as the response variable. In this case, that’s the time to run the race, with units “seconds.”\nSS residual has units that are the square of the respond variable, in this case “square-seconds.”\nRecall that the effect size on the response with respect to an explanatory variable has the units of the response variable divided by the units of the explanatory variable. The climb variable has units of meters, so the effect size has units “seconds/meters.”\nseconds/km"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.b",
    "href": "LC/LC-lesson29.html#lc-29.b",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.B",
    "text": "LC 29.B\nWhich of the following models are not nested within time ~ distance + climb?\n\ntime ~ 1\ntime ~ distance + sex\ntime ~ distance\ntime ~ climb\n\n\n\n\n\n\n\nSolution\n\n\n\nThe model time ~ distance + sex is not nested in time ~ distance + climb.\nNote that time ~ 1 is indeed nested in time ~ distance + climb. The 1 corresponds to the intercept."
  },
  {
    "objectID": "LC/LC-lesson29.html#c",
    "href": "LC/LC-lesson29.html#c",
    "title": "Learning Checks Lesson 29",
    "section": "29.C",
    "text": "29.C\nIn LC -Section 1 you calculated the RMS residuals and the sum-of-square residuals by wrangling the results from mod_eval(). That’s a perfectly good way to do things, but the work becomes tedious when there are multiple models you want to compare.\nFor convenience, there is a compare_model_residuals() command, which can calculate the RMS residual or sum-of-square residual for each of a set of models. All the models must have the same response variable.\n\nHill_racing %>% \n  compare_model_residuals(time ~ 1, \n                          time ~ distance + climb, \n                          time ~ distance + climb + sex,\n                          time ~ distance,\n                          measure = \"RMS\"\n                          )\n\n[1] 3122.4821  870.4631  775.2962 1189.7148\n\n\nIt happens that all of the models in the command are a nested set. Re-order the models so that each model nests inside the following model, that is, from smaller model to bigger model.\n\nDo the RMS residuals for the nested models increase or decrease when moving from a smaller model to a larger model?\nYou can calculate the sum-of-square residual by using the argument measure=\"SS\". Do the sum-of-square residuals for the nested models increas or decrease when moving from a smaller model to a larger model.\nYou can calculate R2 by using the argument measure=\"R2\". Do the R2 for the nested models increase or decrease when moving from a smaller model to a larger model.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nDecrease\nDecrease\nIncrease. R2 tells you how big the model output is compared to the response variable. 1-R2 tells you how big the residuals are compared to the response variable."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.d",
    "href": "LC/LC-lesson29.html#lc-29.d",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.D",
    "text": "LC 29.D\n::: {.callout-warning} ## Still a draft\nLook at dag07. Notice that d is not connected to any of the other variables.\nGenerate a sample of size \\(n=6\\). Compare the sum of square residual (in sample) from the nested models c ~ 1, c ~ a c ~ a + b, and c ~ a + b + d. (Use the compare_model_residuals() using the argument method=\"SS\".\nWhich, if any, of the variables a, b, or d reduces the in-sample sum-of-squared residuals compared to the previous model.\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag07, c ~ 1, c ~ a, c ~ a + b, c ~ a + b + d, \n                        n=6, measure=\"R2\")\n\n[1] 0.0000000 0.4390922 0.6557035 0.7487311\n\n\n\n\nOut of sample, the useless covariate often increases the SS error."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.1",
    "href": "LC/LC-lesson29.html#lc-29.1",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.1",
    "text": "LC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50)\n\n[1] 0.9916292 0.8672413 0.8119626 0.7180731\n\n\nd seems to block effect of a and b on c.\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5)\n\n[1] 1.136096 1.106351 1.446975 1.423358"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.2",
    "href": "LC/LC-lesson29.html#lc-29.2",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, \n                        testing = \"out-of-sample\")\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.3",
    "href": "LC/LC-lesson29.html#lc-29.3",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.3",
    "text": "LC 29.3\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::teacher. What’s the base pay difference between a teacher with an MA and a BA degree? What’s a confidence interval on this effect size? How does the confidence interval change if you include years as a covariate."
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.4",
    "href": "LC/LC-lesson29.html#lc-29.4",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.4",
    "text": "LC 29.4\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::census Predict log personal income based on other variables. Eat variance using the total_family_income variable.\n\nmod <- lm(log10(total_personal_income) ~ log10(age) + sex + marital_status + log10(total_family_income), data = openintro::census %>% filter(total_personal_income > 0, total_family_income > 0))\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: log10(total_personal_income)\n                            Df Sum Sq Mean Sq  F value    Pr(>F)    \nlog10(age)                   1  5.938  5.9383  35.6102 6.660e-09 ***\nsex                          1  5.976  5.9758  35.8351 6.006e-09 ***\nmarital_status               5  4.302  0.8604   5.1596 0.0001464 ***\nlog10(total_family_income)   1 17.620 17.6198 105.6615 < 2.2e-16 ***\nResiduals                  306 51.028  0.1668                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ngf_jitter(total_personal_income ~ total_family_income | sex, \n         data =openintro::census %>% filter(total_personal_income > 3000),\n         color=~marital_status, alpha=0.3) %>% \n  gf_refine(scale_y_log10(), scale_x_log10())\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning: Removed 20 rows containing missing values (geom_point)."
  },
  {
    "objectID": "LC/LC-lesson29.html#section",
    "href": "LC/LC-lesson29.html#section",
    "title": "Learning Checks Lesson 29",
    "section": "29.5",
    "text": "29.5\n\n\n\n\n\n\nStill in draft\n\n\n\nopenintro::starbucks where do the calories come from? Find effect size of, say, protein on calories. Then see what happens if you use carbohydrates as a covariate.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ fat + carb + fiber + protein, data = openintro::starbucks) %>% confint()\n\n                2.5 %    97.5 %\n(Intercept) -2.938079 13.605279\nfat          8.591250  9.315766\ncarb         3.686593  3.997527\nfiber       -1.418966  1.370022\nprotein      3.631695  4.364091"
  },
  {
    "objectID": "LC/LC-lesson29.html#in-draft-2",
    "href": "LC/LC-lesson29.html#in-draft-2",
    "title": "Learning Checks Lesson 29",
    "section": "In draft",
    "text": "In draft\nMaybe come back to this in confounding lesson. Look for components that tend to go together\n\nwith(openintro::starbucks, cor(fat, protein))\n\n[1] 0.22347\n\nwith(openintro::starbucks, cor(fiber, protein))\n\n[1] 0.488564\n\nlm( calories ~ fiber , data = openintro::starbucks) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 276.119106 343.80739\nfiber         1.923476  24.07453\n\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ protein + fiber, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 247.891487 320.333514\nprotein       1.700777   7.996897\nfiber        -8.099007  15.978382"
  },
  {
    "objectID": "LC/LC-lesson28.html",
    "href": "LC/LC-lesson28.html",
    "title": "Learning Checks Lesson 28",
    "section": "",
    "text": "Remove these hard-coded objectives after drafting problems\n\n\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "LC/LC-lesson28.html#section",
    "href": "LC/LC-lesson28.html#section",
    "title": "Learning Checks Lesson 28",
    "section": "28.1",
    "text": "28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution\n:::"
  },
  {
    "objectID": "LC/LC-lesson28.html#b",
    "href": "LC/LC-lesson28.html#b",
    "title": "Learning Checks Lesson 28",
    "section": "28.B",
    "text": "28.B\nWhenever you seek to study a partial relationship, there must be at least three variables involves: a response variable, an explanatory variable that is of direct interest, and one or more other explanatory variables that will be held constant: the covariates. Unfortunately, it’s hard to graph out models involving three variables on paper: the usual graph of a model just shows one variable as a function of a second.\nOne way to display the relationship between a response variable and two quantitative explanatory variables is to use a contour plot. The two explanatory variables are plotted on the axes and the fitted model values are shown by the contours. Figure 1 shows such a display of the fitted model of used car prices as a function of mileage and age.\n\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\nThe dots are the mileage and age of the individual cars — the model Price is indicated by the contours.\nThe total relationship between Price and mileage involves how the price changes for typical cars of different mileage.\n\nPick a dot that is a typical car with about 10,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price (in dollars)?\n\n18000, 21000, 25000, 30000\n\nPick another dot that is a typical car with about 70,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price?\n\n18000 21000, 25000, 30000\nThe total relationship between Price and mileage is reflected by this ratio: change in model price divided by change in mileage. What is that ratio (roughly)?\n\n\\(\\frac{30000 - 21000}{70000-10000}=0.15\\) dollars/mile\n\\(\\frac{70000-10000}{25000-21000}=15.0\\) dollars/mile\n\\(\\frac{25000 - 18000}{70000-10000}=0.12\\) dollars/mile\n\nIn contrast, the partial relationship between Price and mileage holding age constant is found in a different way, by comparing two points with different mileage but exactly the same age.\n\nMark a point on the graph where age is 3 years and mileage is\nKeep in mind that this point doesn’t need to be an actual car, that is, a data point in the graph typical car. There might be no actual car with an age of 3 years and mileage 10000. But using the contour model, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000 28000, 30000\n\nFind another point, one where the age is exactly the same (3 years) but the mileage is different. Again there might not be an actual car there. Let’s pick mileage as 80000. Using the contours, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000, 28000, 30000\n\nThe partial relationship between price and mileage (holding age constant) is reflected again reflected by the ratio of the change in model price divided by the change in mileage. What is that ratio (roughly)?\n\n\n\\(\\frac{80000-10000}{25000-21000} = 17.50\\) dollars/mile\n\\(\\frac{28000 - 22000}{80000-10000}=0.09\\) dollars/mile\n\\(\\frac{26000 - 24000}{80000-10000}=0.03\\) dollars/mile\n\n\nBoth the total relationship and the partial relationship are indicated by the slope of the model price function given by the contours. The total relationship involves the slope between two points that are typical cars, as indicated by the dots. The partial relationship involves a slope along a different direction. When holding age constant, that direction is the one where mileage changes but age does not (vertical in the graph).\nThere’s also a partial relationship between price and age holding mileage constant. That partial relationship involves the slope along the direction where age changes but mileage is held constant. Estimate that slope by finding the model price at a point where age is 2 years and another point where age is 5 years. You can pick whatever mileage you like, but it’s key that your two points be at exactly the same mileage.\n\nEstimate the slope of the price function along a direction where age changes but mileage is held constant (horizontally on the graph).\n\n\n100 dollars per year\n500 dollars per year\n1000 dollars per year\n2000 dollars per year\n\nThe contour plot in Figure 1 depicts a model in which both mileage and age are explanatory variables. By choosing the direction in which to measure the slope, one determines whether the slope reflects a total relationship (a direction between typical cars), or a partial relationship holding age constant (a direction where age does not change, which might not be typical for cars), or a partial relationship holding mileage constant (a direction where mileage does not change, which also might not be typical for cars).\nIn calculus, the partial derivative of price with respect to mileage refers to an infinitesimal change in a direction where age is held constant. Similarly, the partial derivative of price with respect to age refers to an infinitesimal change in a direction where mileage is held constant.\nOf course, in order for the directional derivatives to make sense, the price function needs to have both age and mileage as explanatory variables. Figure 2 shows a model in which only age has been used as an explanatory variable: there is no dependence of the function on mileage.\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nSuch a model is incapable of distinguishing between a partial relationship and a total relationship. Both the partial and the total relationship involve a ratio of the change in price and change in age between two points. For the total relationship, those two points would be typical cars of different ages. For the partial relationship, those two points would be different ages at exactly the same mileage. But, because the model depend on mileage, the two ratios will be exactly the same."
  },
  {
    "objectID": "LC/LC-lesson28.html#c",
    "href": "LC/LC-lesson28.html#c",
    "title": "Learning Checks Lesson 28",
    "section": "28.C",
    "text": "28.C\nIn each of the following, a situation is described and a question is asked that is to be answered by modeling. Several variables are listed. Imagine an appropriate model and identify each variable as either the response variable, an explanatory variable, a covariate, or a variable to be ignored.\n\nEXAMPLE: Some people have claimed that police foot patrols are more effective at reducing the crime rate than patrols done in automobiles. Data from several different cities is available; each city has its own fraction of patrols done by foot, its own crime rate, etc. The mayor of your town has asked for your advice on whether it would be worthwhile to shift to more foot patrols in order to reduce crime. She asks, “Is there evidence that a larger fraction of foot patrols reduces the crime rate?”\n\nVariables:\n\nCrime rate (e.g., robberies per 100000 population)\nFraction of foot patrols\nNumber of policemen per 1000 population\nDemographics (e.g., poverty rate)\n\nAnswer: The question focuses on how the fraction of foot patrols might influence crime rate, so crime rate is the response variable and fraction of foot patrols is an explanatory variable.\nBut, the crime rate might also depend on the overall level of policing (as indicated by the number of policemen), or on the social conditions that are associated with crime (e.g., demographics). Since the mayor has no power to change the demographics of your town, and probably little power to change the overall level number of policemen, in modeling the data from the different cities, you would want to hold constant number of policemen and the demographics. You can do this by treating number of policemen and demographics as covariates and including them in your model.\n\n\nAlcohol and Road Safety\nFifteen years ago, your state legislature raised the legal drinking age from 18 to 21 years. An important motivation was to reduce the number of car accident deaths due to drunk or impaired drivers. Now, some people are arguing that the 21-year age limit encourages binge drinking among 18 to 20 year olds and that such binge drinking actually increases car accident deaths. But the evidence is that the number of car accident deaths has gone down since the 21-year age restriction was introduced.\nYou are asked to examine the issue: Does the reduction in the number of car-accident deaths per year point to the effectiveness of the 21-year drinking age?\nVariables:\n\nDrinking age limit. Levels: 18 or 21.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car-accident deaths per year.\n\nWhich is it? response explanatory covariate ignore\n\nPrevalence of seat-belt use.\n\nWhich is it? response explanatory covariate ignore\n\nFraction of cars with air bags.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car accidents (with or without death).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nOf direct interest is how the drinking age limit accounts for the number of deaths, so these are, respectively, explanatory and response variables. But a lower death rate might also be explained by increased use of seat belts and of air bags; these can prevent deaths in an accident and they have been increasing over the same period in which the 21-year age limit was introduced.\nIn examining how the drinking age limit might affect the number of deaths, it might be important to hold these other factors constant. So, seat belts and air bags should be covariates included in the model.\nThe number of accidents is different. It seems plausible that the mechanism by which drunk driving causes deaths is by causing accidents. If the number of accidents were included as a covariate, then the model would be examining how the death rate changes with drinking age when {} even though the point is that the higher drinking age might reduce the number of accidents. So, number of accidents ought to be left out of the model.\n\n\n\n\nRating Surgeons\nYour state government wants to guide citizens in choosing physicians. As part of this effort, they are going to rank all the surgeons in your state. You have been asked to build the rating system and you have a set of variables available for your use. These variables have been measured for each of the 342,861 people who underwent surgery in your state last year: one person being treated by one doctor. How should you construct a rating system that will help citizens to choose the most effective surgeon for their own treatment?\nVariables:\n\nOutcome score. (A high score means that the operation did whatit was supposed to. A low score reflects failure, e.g. death. Death is a very bad outcome, post-operative infection a somewhat bad outcome.)\n\nWhich is it? response explanatory covariate ignore\n\nSurgeon. One level for each of the operating surgeons.\n\nWhich is it? response explanatory covariate ignore\n\nExperience of the surgeon.\n\nWhich is it? response explanatory covariate ignore\n\nDifficulty of the case.\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe patient has a choice of doctors and wants to have the best possible outcome. So the model needs to include surgeon as an explanatory variable and outcome score as the response.\nA simple model might be misleading for informing a patient’s choice. The best doctors might take on the most difficult cases and therefore have worse outcomes than doctors who are not as good. But the patient’s condition doesn’t change depending on what doctor is selected. This means that the difficulty of the case ought to be included as a covariate. The model would thus tell what is the typical outcome for each surgeon adjusting for the difficulty of the case, that is, given the patient’s condition.\nAnother variable that might explain the outcome is the experience of the surgeon; possibly more experienced surgeons produce better outcomes. However, experience of the surgeon\nshould not be included in the model used to inform a patient’s choice. The reason is that the patient’s choice of a doctor already reflects the experience of that doctor. From the patient’s point of view, it doesn’t matter whether the doctor’s outcomes reflect a high level of talent, a lot of experience, or superior training.\nThe choice of variables and covariates depends on the purpose of the model. If the purpose of the model were to decide how much experience to require of doctors before they are licensed, then an appropriate model would have outcome as the response, experience as the explanatory variable, and difficulty of the case and surgeon as covariates.\n\n\n\n\nSchool testing\nLast year, your school district hired a new superintendent to ``shake things up.’’ He did so, introducing several controversial new policies. At the end of the year, test scores were higher than last year. A representative of the teachers’ union has asked you to examine the score data and answer this question: Is there reason to think that the higher scores were the result of the superintendent’s new policies?\nVariables:\n\nSuperintendent (levels: New or Former superintendent)\n\nWhich is it? response explanatory covariate ignore\n\nExam difficulty\n\nWhich is it? response explanatory covariate ignore\n\nTest scores\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe issue of direct interest is whether the policies of the new superintendent might have influenced the test scores, so the model should be test scores as the response and superintendent as an explanatory variable. Of course, one possible mechanism that might have improved the scores, outside of the influence of the superintendent’s policies, is the test itself. If it were easier this year than last year, then it wouldn’t be surprising that the test scores improved this year even if the superintendent’s policies had no effect. So exam difficulty should be a covariate to be included in the model.\n\n\n\n\nGravity\nIn a bizarre twist of time, you find yourself as Galileo’s research assistant in Pisa in 1605. Galileo is studying gravity: Does gravity accelerate all materials in the same way, whether they be made of metal, wood, stone, etc.? Galileo hired you as his assistant because you have brought with you, from the 21st century, a stop-watch with which to measure time intervals, a computer, and your skill in statistical modeling. All of these seem miraculous to him.\nHe drops objects off the top of the Leaning Tower of Pisa and you measure the following:\nVariables\n\nThe size of the object (measured by its diameter).\n\nWhich is it? response explanatory covariate ignore\n\nTime of fall of the object.\n\nWhich is it? response explanatory covariate ignore\n\nThe material from which the object is made (brass, lead, wood, stone).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nGalileo wants to know how the material affects the time of fall of the object. These are the explanatory and response variables respectively. But the size of the object also has an influence, due to air resistance. For instance, a tiny ball will fall more slowly than a large ball. So the size of the object should be a covariate.\n\n\n\n##28.D\nPolling organizations often report their results in tabular form, as in Figure 3. The basic question in the poll summarized in Figure 3 asked whether the respondant agrees with the statement, “The US was a better place to live in the 1990s and will continue to decline.”\n\n\n\n\n\nFigure 3: Results from a poll conducted by Time magazine. (Source: Time, July 28, 2008, p. 41)\n\n\n\n\nThe response variable here is “pessimism.” In the report, there are three explanatory variables: race/ethnicity, income, and age. The report’s breakdown is one explanatory variable at a time, meaning that it considers “total change” rather than “change holding other factors constant.” This can be misleading when there are connections among the explanatory variables. For instance, relatively few people in the 18 to 29 age group have high incomes.\nPollsters rarely make available the raw data they collected. This is unfortunate because it prevents others from looking at the data in different ways. For the purpose of this exercise, you’ll use simulated data in the frame math300::Econ_outlook_poll. Of course, the simulation doesn’t necessarily describe people’s attitudes directly, but it does let you see how the conclusions drawn from the poll might have been different if the results for each explanatory variable had been presented in a way that adjusts for the other explanatory variables.\n\nConstruct the model pessimism ~ age - 1. Look at the coefficients and choose the statement that best reflects the results. (In case you’re wondering: The -1 is convenient when the explanatory variable is categorical. It ensures that a coefficient is reported for each level of the age variable. You’ll have to compare coefficients for different age groups to see a trend.)\nMiddle aged people have lower pessimism than young or old people.\nYoung people have the least pessimism.\nThere is no relationship between age and pessimism.\nNow construct the model pessimism ~ income - 1. Look at the coefficients and choose the statement that best reflects the results:\nHigher income people are more pessimistic than low-income people.\nHigher income people are less pessimistic than low-income people.\nThere is no relationship between income and pessimism.\nConstruct a model in which you can look at the relationship between pessimism and age while adjusting for income. That is, include income as a covariate in your model. Look at the coefficients from your model and choose the statement that best reflects the results:\nHolding income constant, older people tend to have higher levels of pessimism than young people.\nHolding income constant, young people tend to have higher levels of pessimism than old people.\nHolding income constant, there is no relationship between age and pessimism.\nYou can also interpret that same model to see the relationship between pessimism and income while adjusting for age. Which of the following statements best reflects the results? (Hint: make sure to pay attention to the sign of the coefficients.)\nHolding age constant, higher income people are more pessimistic than low-income people.\nHolding age constant, higher income people are less pessimistic than low-income people.\nHolding age constant, there is no relationship between income and pessimism."
  },
  {
    "objectID": "LC/LC-lesson28.html#e",
    "href": "LC/LC-lesson28.html#e",
    "title": "Learning Checks Lesson 28",
    "section": "28.E",
    "text": "28.E\nA study1 on drug D indicates that patients who were given the drug were less likely to recover from their condition C. Here is a table showing the overall results:\n\n\n\nDrug\n# recovered\n# died\nRecovery Rate\n\n\n\n\nGiven\n1600\n2400\n40%\n\n\nNot given\n2000\n2000\n50%\n\n\n\nStrangely, when investigators looked at the situation separately for males and females, they found that the drug improves recovery for each group:\nSex | Drug | num recovered | # died | Recovery Rate :—-::———|—–:|—–:|——: Sex | Drug | num recovered | # died | Recovery Rate Females| Given | 900 | 2100 | 30% | Not given | 200 | 800 | 20% e | | | |\nMales | Given | 700 | 300 | 70% | Not given | 1800 | 1200 | 60%\n\nAre the two tables consistent with one another in terms of the numbers reported?\nDoes the drug improve recovery or hinder recovery?\nWhat advice would you give to a physician about whether or not to prescribe the drug to her patients? Give enough of an explanation that the physician can judge whether your advice is reasonable."
  },
  {
    "objectID": "LC/LC-lesson28.html#f",
    "href": "LC/LC-lesson28.html#f",
    "title": "Learning Checks Lesson 28",
    "section": "28.F",
    "text": "28.F\nEconomists measure the inflation rate as a percent change in price per year. Unemployment is measured as the fraction (percentage) of those who want to work who are seeking jobs.\nAccording to economists, in the short run — say, from one year to another — there is a relationship between inflation and unemployment: all other things being equal, as unemployment goes up, inflation should go down. (The relationship is called the “Phillips curve,” but you don’t need to know that or anything technical about economics to answer this question.)\n\n\nIf the Phillips-curve relationship is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative\n\n\n\nBut despite the short term relationship, economists claim that In the long run — over decades — unemployment and inflation should be unrelated.\n\n\nIf the long-run theory is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative}\n\n\n\n\nThe point of this exercise is to figure out how to arrange a model so that you can study the short-term behavior of the relationship, or so that you can study the long term relationship.\nFor your reference, Figure 4 shows inflation and unemployment rates over about 30 years in the US. Each point shows the inflation and unemployment rates during one quarter of a year. The plotting symbol indicates which of three decade-long periods the point falls into.\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\nThe relationship between inflation and unemployment seems to be different from one decade to another — that’s the short term.\n\nWhich decade seems to violate the economists’ Phillips Curve short-term relationship? A, B, C, none, all\n\nUsing the modeling language, express these different possible relationships between the variables Inflation, Unemployment, and Decade, where the variable Decade is a categorical variable with the three different levels shown in the legend for the graph.\n\nInflation depends on Unemployment in a way that doesn’t change over time.\nInflation ~ Decade\n** ~ Inflation ~ Unemployment**\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation changes with the decade, but doesn’t depend on Unemployment.\n** ~ Inflation ~ Decade**\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in the same way every decade, but each decade introduces a new background inflation rate independent of Unemployment.\nInflation ~ Decade\nInflation ~ Unemployment\n** ~ Inflation ~ Unemployment + Decade**\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in a way that differs from decade to decade.\nInflation ~ Decade\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\n** ~ Inflation ~ Unemployment * Decade**\n\n\nWhether a model examines the short-term or the long-term behavior is analogous to whether a partial change or a total change is being considered.\n\nSuppose you wanted to study the long-term relationship between inflation and unemployment. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)\nNow suppose you want to study the short-term relationship. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)"
  },
  {
    "objectID": "LC/LC-lesson28.html#sec-28-G",
    "href": "LC/LC-lesson28.html#sec-28-G",
    "title": "Learning Checks Lesson 28",
    "section": "28.G",
    "text": "28.G\nConsider dag03, involving three variables: x, y, g\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nThe nature of 95% confidence intervals means that even when the true coefficient is zero, 5% of the time the confidence interval will not include zero. In a class with 100 students, around 5 will see this failure to include zero. In order to avoid those students from being fooled by such accidental effects, feel free to analyze 2 or more samples.\n\nRegretably, in the real world, when working with data that have already been collected, you can’t use such multiple samples to check your work. So there is always that 5% chance that a real effect of size zero will produce a confidence interval that doesn’t include zero. This is one reason why replication of results is useful."
  },
  {
    "objectID": "LC/LC-lesson28.html#h",
    "href": "LC/LC-lesson28.html#h",
    "title": "Learning Checks Lesson 28",
    "section": "28.H",
    "text": "28.H\nThis learning challenge is much like LC -@28-G, but uses dag11 instead of dag03.\n\nCompare the graphs of dag03 and dag11. (You can use dag_draw() to generate the graph.) Are the two DAGs equivalent or not? Describe what differences you see between the two graphs and explain whether those differences are sufficient to make the two DAGs causally different.\n\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nNow look at the graph of dag12 and speculate whether the models x ~ y and x ~ y + g will give equivalent results. (Don’t include node h in the models.) Write down your speculation. (No penalty for being wrong, it’s just a speculation!) Then, generate a sample from dag12 and, looking at the 95% confidence intervals on the coefficients, explain whether your speculation was correct or not."
  },
  {
    "objectID": "LC/LC-lesson28.html#i",
    "href": "LC/LC-lesson28.html#i",
    "title": "Learning Checks Lesson 28",
    "section": "28.I",
    "text": "28.I"
  },
  {
    "objectID": "LC/data/documentation.html",
    "href": "LC/data/documentation.html",
    "title": "Documentation for data sets",
    "section": "",
    "text": "The variables are:\n\nAccept Status: A=accepted to medical school or D=denied admission\nAcceptance: Indicator for Accept: 1=accepted or 0=denied\nSex: F=female or M=male\nBCPM: Bio/Chem/Physics/Math grade point average\nGPA: College grade point average\nVR: Verbal reasoning (subscore)\nPS: Physical sciences (subscore)\nWS: Writing sample (subcore)\nBS: Biological sciences (subscore)\nMCAT: Score on the MCAT exam (sum of CR+PS+WS+BS)\nApps: Number of medical schools applied to\n\n\nBuild a logistic regression model to predict if a student where denied admission from GPA and Sex.\nGenerate a 95% confidence interval for the coefficient associated with GPA.\nFit a model with a polynomial of degree 2 in the GPA. Drop Sex from the model. 4. Does a quadratic fit improve the model?\nFit a model with just GPA and interpret the coefficient.\nTry to add different predictors to come up with your best model.\nGenerate a confusion matrix for the best model you have developed.\nFind a 95% confidence interval for the probability a female student with a 3.5 GPA, a BCPM of 3.8, a verbal reasoning score of 10, a physical sciences score of 9, a writing sample score of 8, a biological score of 10, a MCAT score of 40, and who applied to 5 medical schools.”\n\n\n\n\n\nFootnotes\n\n\n”Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎"
  },
  {
    "objectID": "LC/LC-lesson38.html",
    "href": "LC/LC-lesson38.html",
    "title": "Learning Checks Lesson 38",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "Objectives/Obj-lesson-26.html",
    "href": "Objectives/Obj-lesson-26.html",
    "title": "Objectives (Day 26)",
    "section": "",
    "text": "26.2 Interpret prediction bands as a series of intervals, one for each value of the model input.\n26.3 Identify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t."
  },
  {
    "objectID": "Objectives/Obj-lesson-32.html",
    "href": "Objectives/Obj-lesson-32.html",
    "title": "Objectives (Day 32)",
    "section": "",
    "text": "32.2 Correctly re-draw DAG for an ideal experimental intervention.\n32.3 Use blocking to set assignment to treatment or control."
  },
  {
    "objectID": "Objectives/Obj-lesson-33.html",
    "href": "Objectives/Obj-lesson-33.html",
    "title": "Objectives (Day 33)",
    "section": "",
    "text": "33.2. Calculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\n33.3. Interpret effect size as stated in log odds."
  },
  {
    "objectID": "Objectives/Obj-lesson-27.html",
    "href": "Objectives/Obj-lesson-27.html",
    "title": "Objectives (Day 27)",
    "section": "",
    "text": "This is a QR day."
  },
  {
    "objectID": "Objectives/Obj-lesson-19.html",
    "href": "Objectives/Obj-lesson-19.html",
    "title": "Objectives (Day 19)",
    "section": "",
    "text": "19.2 Understand the covering of a variable by an interval specified by a coverage level (e.g. 0.95) and make a data graphic annotated by such intervals.\n19.3 Be able to produce point plots overlaid with “violin” plots to display the density of a variable. (This should really go in the first half of the course, but ModernDive doesn’t do it.)\n19.4 Convert categorical variables to a 0-1 encoding. a. Generate graphics appropriate to a categorical encoding. (jitter plots) b. Apply modeling techniques to 0-1 encodings of binomial response variables."
  },
  {
    "objectID": "Objectives/Obj-lesson-31.html",
    "href": "Objectives/Obj-lesson-31.html",
    "title": "Objectives (Day 31)",
    "section": "",
    "text": "31.2 Construct appropriate DAG to match a narrative hypothesis."
  },
  {
    "objectID": "Objectives/Obj-lesson-25.html",
    "href": "Objectives/Obj-lesson-25.html",
    "title": "Objectives (Day 25)",
    "section": "",
    "text": "25.2 Use the predictor function to estimate prediction error and summarize with root mean square (RMS) error. Relate this to a prediction interval.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "Objectives/Obj-lesson-24.html",
    "href": "Objectives/Obj-lesson-24.html",
    "title": "Objectives (Day 24)",
    "section": "",
    "text": "a. **Prediction**: predict an outcome for an individual\nb. **Relationship**: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\nGiven a research question, identify whether it corresponds to a prediction setting or a relationship setting.\n24.2 Estimate an effect size from a regression model of one and two variables.\n24.3 Construct a confidence interval on the effect size and evaluate whether a confidence interval indicates that estimated effect size is consistent with a specified value."
  },
  {
    "objectID": "Objectives/Obj-lesson-30.html",
    "href": "Objectives/Obj-lesson-30.html",
    "title": "Objectives (Day 30)",
    "section": "",
    "text": "30.2 Choose whether to include covariate depending on form of DAG"
  },
  {
    "objectID": "Objectives/Obj-lesson-34.html",
    "href": "Objectives/Obj-lesson-34.html",
    "title": "Objectives (Day 34)",
    "section": "",
    "text": "34.2. Cross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\n34.3. Calculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome."
  },
  {
    "objectID": "Objectives/Obj-lesson-20.html",
    "href": "Objectives/Obj-lesson-20.html",
    "title": "Objectives (Day 20)",
    "section": "",
    "text": "20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples."
  },
  {
    "objectID": "Objectives/Obj-lesson-21.html",
    "href": "Objectives/Obj-lesson-21.html",
    "title": "Objectives (Day 21)",
    "section": "",
    "text": "21.2 Having selected a response and one or more explanatory variables, identify other DAG nodes as covariates.\n21.3 Generate data from simulations and use the data to model the relationships."
  },
  {
    "objectID": "Objectives/Obj-lesson-35.html",
    "href": "Objectives/Obj-lesson-35.html",
    "title": "Objectives (Day 35)",
    "section": "",
    "text": "35.2 Understand sensitivity and specificity as conditional probabilities.\n35.3 Calculate false-positive and false-negative rates for a given prevalence."
  },
  {
    "objectID": "Objectives/Obj-lesson-23.html",
    "href": "Objectives/Obj-lesson-23.html",
    "title": "Objectives (Day 23)",
    "section": "",
    "text": "23.2 Infer sampling variation from a regression table: “standard error” of a model coefficient.\n23.3 Construct and interpret confidence intervals on a model coefficient and relate the interval to the sampling distribution.\n23.4. Understand and use scaling of confidence interval length as a function of \\(n\\)."
  },
  {
    "objectID": "Objectives/Obj-lesson-37.html",
    "href": "Objectives/Obj-lesson-37.html",
    "title": "Objectives (Day 37)",
    "section": "",
    "text": "37.2 Interpret correctly from regression/ANOVA reports\n37.3 Traditional names for hypothesis tests in different “textbook” settings.\n37.4. Distinguish between p-value and effect size, that is, “significance” and “substance.”"
  },
  {
    "objectID": "Objectives/Obj-lesson-36.html",
    "href": "Objectives/Obj-lesson-36.html",
    "title": "Objectives (Day 36)",
    "section": "",
    "text": "36.2 Contrast hypothesis testing versus Bayesian framework."
  },
  {
    "objectID": "Objectives/Obj-lesson-22.html",
    "href": "Objectives/Obj-lesson-22.html",
    "title": "Objectives (Day 22)",
    "section": "",
    "text": "22.2 Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\n22.3 Graphically display the distribution of summaries and generate a compact numerical description of the sampling distribution."
  },
  {
    "objectID": "Objectives/Obj-lesson-38.html",
    "href": "Objectives/Obj-lesson-38.html",
    "title": "Objectives (Day 38)",
    "section": "",
    "text": "38.2 Estimate how overall p-value should change when study is replicated."
  },
  {
    "objectID": "Objectives/Obj-lesson-29.html",
    "href": "Objectives/Obj-lesson-29.html",
    "title": "Objectives (Day 29)",
    "section": "",
    "text": "29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n29.4 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "Objectives/Obj-lesson-28.html",
    "href": "Objectives/Obj-lesson-28.html",
    "title": "Objectives (Day 28)",
    "section": "",
    "text": "28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html",
    "href": "Reading-notes/Reading-notes-lesson-28.html",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "",
    "text": "Note: in draft\n\n\n\nUse KidsFeet to illustrate the importance of incorporating covariates into models. Story: It was claimed that girls’ feet are narrower than boys. Do the data show this?\n\ndf_stats(width ~ sex, data = KidsFeet, ci.mean)\n\n  response sex   lower    upper\n1    width   B 8.97856 9.401440\n2    width   G 8.54631 9.022111\n\n\nBarely any overlap in the two intervals, so there does seem to be a difference between the groups. Or, in the language of models:\n\nlm(width ~ sex, data = KidsFeet) %>% confint()\n\n                 2.5 %      97.5 %\n(Intercept)  8.9758882  9.40411181\nsexG        -0.7125476 -0.09903131\n\n\nThe confidence interval on the effect size of sex does not include zero, so it’s reasonable to conclude that sex is related to foot width.\n\n\n\n\n\n\nPossible LC or Student-notes item\n\n\n\nDescribe the meaning of the reports in this section in everyday English to someone not trained in statistics.\n\n\nWith covariates: The motivating question is not really about the width of feet but the shape of feet. If boys’ and girls’ feet have the same shape, there’s no obvious anatomical reason for shoes to be divided into types according to sex.\nOne way to get at this is to use a response variable that serves the actual needs of the question, which is about shape. The “aspect ratio”—width divided by length—is a simple summary of shape. Does the aspect ratio differ between the sexes?\n\nKidsFeet %>%\n  mutate(aspect_ratio = width/length) %>%\n  df_stats(aspect_ratio ~ sex, ci.mean)\n\n      response sex     lower     upper\n1 aspect_ratio   B 0.3585847 0.3742274\n2 aspect_ratio   G 0.3530213 0.3700862\n\n\nAnother way to see the same thing is to look at the confidence interval on the effect size of sex on aspect_ratio:\n\nKidsFeet %>%\n  mutate(aspect_ratio = width/length) %>%\n  lm(aspect_ratio ~ sex, data=.) %>%\n  confint()\n\n                  2.5 %     97.5 %\n(Intercept)  0.35861279 0.37419929\nsexG        -0.01601772 0.00631307\n\n\nThe confidence interval includes zero, so it’s reasonable to conclude that sex is not a predictor of foot shape.\nNotice that constructing an appropriate data analysis to address the real-world question at hand involved using three variables: sex, width, and length. For the question at hand, examining just sex and width isn’t adequate. length isn’t a variable of direct interest here, but we can’t get an appropriate sense of the relationship between sex and width unless we take length into account. Variables that aren’t of direct interest, but which play an important role in the overall system, are called “covariates.”\nIn the above, we used the covariate length to generate a more meaningful response variable. This is clever and appropriate. But more generally, covariates play the role of an explanatory variable. To illustrate the general use of covariates, we can construct a different model of width which shows that sex is not an explanatory factor once we take length into account.\n\nlm(width ~ sex + length, data = KidsFeet) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept)  1.1048182 6.17751841\nsexG        -0.4947759 0.02974084\nlength       0.1202348 0.32181513\n\n\nThe confidence interval on the effect size of sexG on width includes zero, so the data do not justify a conclusion that sex is related to width."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#total-and-partial-relationships",
    "href": "Reading-notes/Reading-notes-lesson-28.html#total-and-partial-relationships",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Total and Partial Relationships",
    "text": "Total and Partial Relationships\nThe common phrase “all other things being equal” is an important qualifier in describing relationships. To illustrate: A simple claim in economics is that a high price for a commodity reduces the demand. For example increasing the price of heating fuel will reduce demand as people turn down thermostats in order to save money. But the claim can be considered obvious only with the qualifier all other things being equal. For instance, the fuel price might have increased because winter weather has increased the demand for heating compared to summer. Thus, higher prices may be associated with higher demand. Unless you hold other variables constant – e.g., weather conditions – increased price may not in fact be associated with lower demand.\nIn fields such as economics, the Latin equivalent of “all other things being equal” is sometimes used: “ceteris paribus”. So, the economics claim would be, “higher prices are associated with lower demand, ceteris paribus.”\nAlthough the phrase “all other things being equal” has a logical simplicity, it’s impractical to implement “all.” Instead of the blanket “all other things,” it’s helpful to be able to consider just “some other things” to be held constant, being explicit about what those things are. Other phrases along these lines are “taking into account …” and “controlling for ….” Such phrases apply when you want to examine the relationship between two variables, but there are additional variables that may be coming into play. The additional variables are called “covariates” or “confounders” .\nA covariate is just an ordinary variable. The use of the word “covariate” rather than “variable” highlights the interest in holding this variable constant, to indicate that it’s not a variable of primary interest."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#example-covariates-and-death",
    "href": "Reading-notes/Reading-notes-lesson-28.html#example-covariates-and-death",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Example: Covariates and Death",
    "text": "Example: Covariates and Death\nThis news report appeared in 2007:\n\nHeart Surgery Drug Carries High Risk, Study Says. A drug widely used to prevent excessive bleeding during heart surgery appears to raise the risk of dying in the five years afterward by nearly 50 percent, an international study found. The researchers said replacing the drug – aprotinin, sold by Bayer under the brand name Trasylol – with other, cheaper drugs for a year would prevent 10,000 deaths worldwide over the next five years.\nBayer said in a statement that the findings are unreliable because Trasylol tends to be used in more complex operations, and the researchers’ statistical analysis did not fully account for the complexity of the surgery cases. The study followed 3,876 patients who had heart bypass surgery at 62 medical centers in 16 nations. Researchers compared patients who received aprotinin to patients who got other drugs or no antibleeding drugs. Over five years, 20.8 percent of the aprotinin patients died, versus 12.7 percent of the patients who received no antibleeding drug. [This is a 64% increase in the death rate.] When researchers adjusted for other factors, they found that patients who got Trasylol ran a 48 percent higher risk of dying in the five years afterward. The other drugs, both cheaper generics, did not raise the risk of death significantly. The study was not a randomized trial, meaning that it did not randomly assign patients to get aprotinin or not. In their analysis, the researchers took into account how sick patients were before surgery, but they acknowledged that some factors they did not account for may have contributed to the extra deaths. - Carla K. Johnson, Associated Press, 7 Feb. 2007 The report involves several variables. Of primary interest is the relationship between (1) the risk of dying after surgery and (2) the drug used to prevent excessive bleeding during surgery. Also potentially important are (3) the complexity of the surgical operation and (4) how sick the patients were before surgery. Bayer disputes the published results of the relationship between (1) and (2) holding (4) constant, saying that it’s also important to hold variable (3) constant.\n\nIn the aprotinin drug example, the total relationship involves a death rate of 20.8 percent of patients who got aprotinin, versus 12.7 percent for others. This implies an increase in the death rate by a factor of 1.64. When the researchers looked at a partial relationship (holding constant the patient sickness before the operation), the death rate was seen to increase by less: a factor of 1.48. In evaluating the drug, it’s best to examine its effects holding other factors constant. So, even though the data directly show a 64% increase in the death rate, 48% is a more meaningful number since it adjusts for covariates such as patient sickness. The difference between the two estimates reflect that sicker patients tended to be given aprotinin. As the last paragraph of the story indicates, however, the researchers did not take into account all covariates. Consequently, it’s hard to know whether the 48% number is a reliable guide for decision making.\n\nThe term “partial relationship” describes a relationship with one or more covariates being held constant. A useful thing to know in economics might be the partial relationship between fuel price and demand with weather conditions being held constant. Similarly, it’s a partial relationship when the article refers to the effect of the drug on patient outcome in those patients with a similar complexity of operation.\nIn contrast to a partial relationship where certain variables are being held constant, there is also a “total relationship”: how an explanatory variable is related to a response variable letting those other explanatory variables change as they will. (The corresponding Latin phrase is “mutatis mutandis”.)\nHere’s an everyday illustration of the difference between partial and total relationships. I was once involved in a budget committee that recommended employee health benefits for the college at which I work. At the time, college employees who belonged to the college’s insurance plan received a generous subsidy for their health insurance costs. Employees who did not belong to the plan received no subsidy but were instead given a moderate monthly cash payment. After the stock-market crashed in year 2000, the college needed to cut budgets. As part of this, it was proposed to eliminate the cash payment to the employees who did not belong to the insurance plan. This proposal was supported by a claim that this would save money without reducing health benefits. I argued that this claim was about a partial relationship: how expenditures would change assuming that the number of people belonging to the insurance plan remained constant. I thought that this partial relationship was irrelevant; the loss of the cash payment would cause some employees, who currently received health benefits through their spouse’s health plan, to switch to the college’s health plan. Thus, the total relationship between the cash payment and expenditures might be the opposite of the partial relationship: the savings from the moderate cash payment would trigger a much larger expenditure by the college.\nPerhaps it seems obvious that one should be concerned with the “big picture,” the total relationship between variables. If eliminating the cash payment increases expenditures overall, it makes no sense to focus exclusively on the narrow savings from the suspending the payment itself. On the other hand, in the aprotinin drug example, for understanding the impact of the drug itself it seems important to take into account how sick the various patients were and how complex the surgical operations. There’s no point ascribing damage to aprotinin that might instead be the result of complicated operations or the patient’s condition.\nWhether you wish to study a partial or a total relationship is largely up to you and the context of your work. But certainly you need to know which relationship you are studying."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#example-used-car-prices",
    "href": "Reading-notes/Reading-notes-lesson-28.html#example-used-car-prices",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Example: Used Car Prices",
    "text": "Example: Used Car Prices\nFigure @ref(fig:price-vs-mileage) shows a scatter plot of the price of used Honda Accords versus the number of miles each car has been driven. The graph shows a pretty compelling relationship: the more miles that a car goes, the lower the price. This can be summarized by a simple linear model: price ~ mileage. Fitting such a model gives this model formula\nprice = 20770 - 0.10 × mileage.\nKeeping in mind the units of the variables, the price of these Honda Accords typically falls by about 10 cents per mile driven. Think of that as the cost of the wear and tear of driving: depreciation.\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe price of used cars falls with increasing miles driven. The gray diagonal line shows the best fitting linear model. Price falls by about $10,000 for 100,000 miles, or 10 cents per mile driven.\n\n\n\n\nAs the cars are being driven, other things are happening to them. They are wearing out, they are being involved in minor accidents, and they are getting older. The relationship shown in Figure @ref(fig:price-vs-mileage) takes none of these into account. As mileage changes, the other variables such as age are changing as they will: a total relationship.\nIn contrast to the total relationship, the partial relationship between price and mileage holding age constant tells you something different than the total relationship. The partial relationship would be relevant, for instance, if you were interested in the cost of driving a car. This cost includes gasoline, insurance, repairs, and depreciation of the car’s value. The car will age whether or not you drive it; the extra depreciation due to driving it will be indicated by the partial relationship between price and mileage holding age constant.\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe relationship between price and mileage for cars in different age groups, indicating the partial relationship between price and mileage holding age constant.\n\n\n\n\nThe most intuitive way to hold age constant is to look at the relationship between price and mileage for a subset of the cars; include only those cars of a given age. This is shown in Figure @ref(fig:price-vs-mileage2). The cars have been divided into age groups (less than 2 years old, between 3 and 4 years old, etc.) and the data for each group has been plotted separately together with the best fitting linear model for the cars in that group. From the figure it’s easy to see that the slope of the fitted models for each group is shallower than the slope fitted to all the cars combined. Instead of the price falling by about 10 cents per mile as it does for all the cars combined, within the 4-8 year old group the price decrease is only about 7 cents per mile, and only 3 cents per mile for cars older than 8 years.\nBy looking at the different age groups individually, you are holding age approximately constant in your model. The relationship you find in this way between price and mileage is a partial relationship. Of course, there are other variables that you didn’t hold constant. So, to be precise, you should describe the relationship you found as a partial relationship with respect to age."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#models-and-partial-relationships",
    "href": "Reading-notes/Reading-notes-lesson-28.html#models-and-partial-relationships",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Models and Partial Relationships",
    "text": "Models and Partial Relationships\nModels make it easy to estimate the partial relationship between a response variable and an explanatory variable, holding one or more covariates constant.\nThe first step is to fit a model using both the explanatory variable and the covariates that you want to hold constant. For example, to find the partial relationship between car price and miles driven, holding age constant, fit the model price ~ mileage+age. For the car-price data from Figure @ref(fig:price-vs-mileage), this gives the model formula\nprice = 21330 - 0.077 × mileage - 538 × age\nThe second step is to interpret this model as a partial relationship between price and mileage holding age constant. A simple way to do this is to plug in some particular value for age, say 1 year. With this value plugged in, the formula for price as a function of mileage becomes\nprice = 21330 - 0.077 × mileage - 538 × 1 = 20792 - 0.077 × mileage\nThe partial relationship is that price goes down by 0.077 dollars per mile, holding age constant.\nNote the use of the phrase “estimate the partial relationship” in the first paragraph of this section. The model you fit creates a representation of the system you are studying that incorporates both the variable of interest and the covariates in explaining the response values. In this mathematical representation, it’s easy to hold the covariates constant. If you don’t include the covariate in the model, you can’t hold it constant and so you can’t estimate the partial relationship between the response and the variable of interest while holding the covariate constant. But even when you do include the covariates in your model, there is a legitimate question of whether your model is a faithful reflection of reality; holding a covariate constant in a model is not the same thing as holding it constant in the real world. These issues, which revolve around the idea of the causal relationship between the covariate and the response, are discussed in Chapter 18."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#aside-partial-change-and-partial-derivatives",
    "href": "Reading-notes/Reading-notes-lesson-28.html#aside-partial-change-and-partial-derivatives",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Aside: Partial change and partial derivatives",
    "text": "Aside: Partial change and partial derivatives\nIf you are familiar with calculus and “partial derivatives”, you may notice that this rate is the partial derivative of price with respect to mileage. Using partial derivatives allows one to interpret more complicated models relatively easily. For example, Figure @ref(fig:price-vs-mileage2) shows pretty clearly that the price vs mileage lines have different slopes for different age group. To capture this in your model, you might choose to include an interaction term between mileage and age. This gives a model with four terms:\nprice = 22140 - 0.094 × mileage - 750× age + 0.0034 × mileage × age\nFor this model, the partial relationship between price and mileage is not just the coefficient on mileage. Instead it is the partial derivative of price with respect to mileage, or:\n∂price / ∂mileage = −0.094 + 0.0034 × age\nTaking into account the units of the variables, this means that for a new car (age = 0), the price declines by $0.09/mile, that is, 9.4 cents per mile. But for a 10-year old car, the decline is less rapid: −0.094 + 10×0.0034 = −0.060 – only six cents a mile."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#adjustment",
    "href": "Reading-notes/Reading-notes-lesson-28.html#adjustment",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Adjustment",
    "text": "Adjustment\nThe table contains the data on professional and sales employees of a large mid-western US trucking company: the annual earnings in 2007, sex, age, job title, how many years of employment with the company. Data such as these are sometimes used to establish whether or not employers discriminate on the basis of sex.\n\n\n\nsex\nearnings\nage\ntitle\nhiredyears\n\n\n\n\nM\n35000\n25\nPROGRAMMER\n0\n\n\nF\n36800\n62\nCLAIMS ADJUSTER\n5\n\n\nF\n25000\n34\nRECRUITER\n1\n\n\nM\n45000\n44\nCLAIMS ADJUSTER\n0\n\n\nM\n40000\n30\nPROGRAMMER\n5\n\n\n\n\n\n\n\n\nFigure 1: The distribution of annual earnings broken down by sex for professional and sales employees of a trucking company.\n\n\n\n\nThe data in Figure 1 reveals a clear pattern: men are being paid more than women. Fitting the model earnings ~ sex indicates the average difference in earnings between men and women:\n\nlm(earnings ~ sex, data = mosaicModel::Trucking_jobs) %>% coefficients()\n\n(Intercept)        sexM \n  35501.250    4735.098 \n\n\nSince earnings are in dollars per year, men are being paid, on average, $4735 more per year than women. This difference reflects the total relationship between earnings and sex, letting other variables change as they will.\nNotice from the boxplot that even within the male or female groups, there is considerable variability in annual earnings from person to person. Evidently, there is something other than sex that influences the wages.\nAn important question is whether you should be interested in the total relationship between earnings and sex, or the partial relationship, holding other variables constant. This is a difficult issue. Clearly there are some legitimate reasons to pay people differently, for example different levels of skill or experience or different job descriptions, but it’s always possible that these legitimate factors are being used to mask discrimination.\nFor the moment, take as a covariate something that can stand in as a proxy for experience: the employee’s age. Unlike job title, age is hardly something that can be manipulated to hide discrimination. Figure 2 shows the employees’ earnings plotted against age. Also shown are the fitted model values of wages against age, fitted separately for men and women.\n\n\n\n\n\nFigure 2: Annual earnings versus age. The lines show fitted models made separately for men (top) and women (bottom).\n\n\n\n\nIt’s evident that for both men and women, earnings tend to increase with age. The model design imposes a straight line structure on the fitted model values. The formulas for the two lines are:\n\n# For females\nlm(earnings ~ age, data = mosaicModel::Trucking_jobs %>% filter(sex==\"F\")) %>%\n  coefficients()\n\n(Intercept)         age \n  17178.160     529.952 \n\n# For males\nlm(earnings ~ age, data = mosaicModel::Trucking_jobs %>% filter(sex==\"M\")) %>%\n  coefficients()\n\n(Intercept)         age \n 16734.9949    609.0916 \n\n\nFrom the graph, you can see the partial relationship between earnings and sex, holding age constant. Pick an age, say 30 years. At 30 years, according to the model, the difference in annual earnings is $1931, with men making more. At 40 years of age, the difference between the sexes is even more ($2722), at 20 years of age, the difference is less ($1140). All of these partial differences (holding age constant) are substantially less than the difference when age is not taken into account ($4735).\nOne way to summarize the differences in earnings between men and women is to answer this question: How would the earnings of the men have been different if the men were women? Of course you can’t know all the changes that might occur if the men were somehow magically transformed, but you can use the model to calculate the change assuming that all other variables except sex are held constant. This process is called “adjustment”.\nTo find the men’s wages adjusted as if they were women, take the age data for the men and plug them into the model formula for women. The difference between the earnings of men and women, adjusting for age, is $2125. This is much smaller than the difference, $4735, when earnings are not adjusted for age. Differences in age between the men and women in the data set appear to account for more than half of the overall earnings difference between men and women.\nOf course, before you draw any conclusions, you need to know how precise these coefficients are. For instance, it’s a different story if the sex difference is 2125 \\(\\pm\\) 10 or if it is 2125 \\(\\pm\\) 5000. In the latter case, it would be sensible to conclude only that the data leave the matter of wage difference undecided. Later chapters in this book describe how to characterize the precision of an estimate.\nAnother key matter is that of causation. $2125 indicates a difference, but doesn’t say completely where the difference comes from. By adjusting for age, the model disposes of the possibility that the earnings difference reflects differences in the typical ages of male and female workers. It remains to be found out whether the earnings difference might be due to different skill sets, discrimination, or other factors."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#simpsons-paradox",
    "href": "Reading-notes/Reading-notes-lesson-28.html#simpsons-paradox",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nSometimes the total relationship is the opposite of the partial relationship. This is “Simpson’s paradox”.\nOne of the most famous examples involves graduate admissions at the University of California in Berkeley. It was observed that graduate admission rates were lower for women than for men overall. This reflects the total relationship between admissions and sex. But, on a department-by-department basis, admissions rates for women were consistently as high or higher than for men. The partial relationship, taking into account the differences between departments, was utterly different from the total relationship."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#example-cancer-rates-increasing",
    "href": "Reading-notes/Reading-notes-lesson-28.html#example-cancer-rates-increasing",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Example: Cancer Rates Increasing?",
    "text": "Example: Cancer Rates Increasing?\nConsider another example of partial versus total relationships. In 1962, naturalist author Rachel Carson published Silent Spring [@silent-spring], a powerful indictment of the widespread use of pesticides such as DDT. Carson pointed out the links between DDT and dropping populations of birds such as the bald eagle. She also speculated that pesticides were the cause of a recent increase in the number of human cancer cases. The book’s publication was instrumental in the eventual banning of DDT.\nThe increase in deaths from cancer over time is a total relationship between cancer deaths and time. It’s relevant to consider a partial relationship between the number of cancer deaths and time, holding the population constant. This partial relationship can be indicated by a death rate: say, the number of cancer deaths per 100,000 people. It seems obvious that the covariate of population size ought to be held constant. But there are still other covariates to be held constant. The decades before Silent Spring had seen a strong decrease in deaths at young ages from other non-cancer diseases which now were under greater control. It had also seen a strong increase in smoking. When adjusting for these covariates, the death rate from cancer was actually falling, not increasing as Carson claimed.[@tierney-NYT-june-5-2007]"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#explicitly-holding-covariates-constant",
    "href": "Reading-notes/Reading-notes-lesson-28.html#explicitly-holding-covariates-constant",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Explicitly Holding Covariates Constant",
    "text": "Explicitly Holding Covariates Constant\nThe distinction between explanatory variables and covariates is in the modeler’s mind. When it comes to fitting a model, both sorts of variables are considered on an equal basis when calculating the residuals and choosing the best fitting model to produce a model function. The way that you choose to interpret and analyze the model function is what determines whether you are examining partial change or total change.\nThe intuitive way to hold a covariate constant is to do just that. Experimentalists arrange their experimental conditions so that the covariates are the same. Think of Galileo using balls of the same diameter and varying only the mass. In a clinical trial of a new drug, perhaps you would test the drug only on women so that you don’t have to worry about the covariate sex.\nWhen you are not doing an experiment but rather working with observational data, you can hold a covariate constant by throwing out data. Do you want to see the partial relationship between price and mileage while holding age constant? Then restrict your analysis to cars that are all the same age, say 3 years old. Want to know the relationship between breath-holding time and body size holding sex constant? Then study the relationship in just women or in just men.\nDividing data up into groups of similar cases, as in Chapter 4, is an intuitive way to study partial relationships. It can be effective, but it is not a very efficient way to use data.\nThe problem with dividing up data into groups is that the individual groups may not have many cases. For example, for the used cars shown in Figure @ref(fig:price-vs-mileage2) there are only a dozen or so cases in each of the groups. To get even this number of cases, the groups had to cover more than one year of age. For instance, the group labeled “age < 8” includes cars that are 5, 6, 7, and 8 years old. It would have been nice to be able to consider six-year old cars separately from seven-year old cars, but this would have left me with very few cases in either the six- or seven-year old group.\nAt the same time, it seems reasonable to think that 5- and 7-year old cars have something to say about 6-year old cars; you would expect the relationship between price and mileage to shift gradually with age. For instance, the relationship for 6-year old cars should be intermediate to the relationships for 5- and for 7-year old cars.\nModeling provides a powerful and efficient way to study partial relationships that does not require studying separate subsets of data. Just include multiple explanatory variables in the model. Whenever you fit a model with multiple explanatory variables, the model gives you information about the partial relationship between the response and each explanatory variable with respect to each of the other explanatory variables."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#example-sat-scores-and-school-spending",
    "href": "Reading-notes/Reading-notes-lesson-28.html#example-sat-scores-and-school-spending",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Example: SAT Scores and School Spending",
    "text": "Example: SAT Scores and School Spending\nChapter 7 showed some models relating school expenditures to SAT scores. The model sat ~ 1 + expend produced a negative coefficient on expend, suggesting that higher expenditures are associated with lower test scores. Including another variable, the fraction of students who take the SAT (variable frac) reversed this relationship.\nThe model sat ~ 1 + expend + frac attempts to capture how SAT scores depend both on expend and frac. In interpreting the model, you can look at how the SAT scores would change with expend while holding frac constant. That is, from the model formula, you can study the partial relationship between SAT and expend while holding frac constant.\nThe example also looked at a couple of other fiscally related variables: student-teacher ratio and teachers’ salary. The total relationship between each of the fiscal variables and SAT was negative – for instance, higher salaries were associated with lower SAT scores. But the partial relationship, holding frac constant, was the opposite: Simpson’s Paradox.\nFor a moment, take at face value the idea that higher teacher salaries and smaller class sizes are associated with better SAT scores as indicated by the following models:\nsat = 988 + 2.18 salary - 2.78 frac\nsat = 1119 - 3.73 ratio - 2.55 frac\nIn thinking about the impact of an intervention – changing teachers’ salaries or changing the student-teacher ratio – it’s important to think about what other things will be changing during the intervention. For example, one of the ways to make student-teacher ratios smaller is to hire more teachers. This is easier if salaries are held low. Similarly, salaries can be raised if fewer teachers are hired: increasing class size is one way to do this. So, salaries and student-teacher ratio are in conflict with each other.\nIf you want to anticipate what might be the effect of a change in teacher salary while holding student-teacher ratio constant, then you should include ratio in the model along with salary (and frac, whose dominating influence remains confounded with the other variables if it is left out of the model):\nsat = 1058 + 2.55 salary - 4.64 ratio - 2.91 frac\nComparing this model to the previous ones gives some indication of the trade-off between salaries and student-teacher ratios. When ratio is included along with salary, the salary coefficient is somewhat bigger: 2.55 versus 2.18. This suggests that if salary is increased while holding constant the student-teacher ratio, salary has a stronger relationship with SAT scores than if salary is increased while allowing student-teacher ratio to vary in the way it usually does, accordingly.\nOf course, you still need to have some way to determine whether the precision in the estimate of the coefficients is adequate to judge whether the detected difference in the salary coefficient is real – 2.18 in one model and 2.55 in the other. Such issues are introduced in Chapter 12."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#aside-divide-and-be-conquered",
    "href": "Reading-notes/Reading-notes-lesson-28.html#aside-divide-and-be-conquered",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Aside: Divide and Be Conquered!",
    "text": "Aside: Divide and Be Conquered!\nEfficiency starts to be a major issue when there are many covariates. Consider a study of the partial relationship between lung capacity and smoking, holding constant all these covariates: sex, body size, smoking status, age, physical fitness. There are two sexes and perhaps three or more levels of body size (e.g., small, medium, large). You might divide age into five different groups (e.g., pre-teens, teens, young adults, middle aged, elderly) and physical fitness into three levels (e.g., never exercise, sometimes, often). Taking the variables altogether, there are now 2 × 3 × 5 × 3 = 90 groups. It’s very inefficient to treat these 90 groups completely separately, as if none of the groups had anything to say about the others. A model of the form\nlung capacity ~ body size + sex + smoking status + age + fitness\ncan not only do the job more efficiently, but avoids the need to divide quantitative variables such as body size or age into categories.\nTo illustrate, consider this news report:\n\nHigher vitamin D intake has been associated with a significantly reduced risk of pancreatic cancer, according to a study released last week. Researchers combined data from two prospective studies that included 46,771 men ages 40 to 75 and 75,427 women ages 38 to 65. They identified 365 cases of pancreatic cancer over 16 years. Before their cancer was detected, subjects filled out dietary questionnaires, including information on vitamin supplements, and researchers calculated vitamin D intake. After statistically adjusting for [that is, holding constant] age, smoking, level of physical activity, intake of calcium and retinol and other factors, the association between vitamin D intake and reduced risk of pancreatic cancer was still significant. Compared with people who consumed less than 150 units of vitamin D a day, those who consumed more than 600 units reduced their risk by 41 percent. - New York Times, 19 Sept. 2006, p. D6. There are more than 125,000 cases in this study, but only 365 of them developed pancreatic cancer. If those 365 cases had been scattered around dozens or hundreds of groups and analyzed separately, there would be so little data in each group that no pattern would be discernible."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#adjustment-and-truth",
    "href": "Reading-notes/Reading-notes-lesson-28.html#adjustment-and-truth",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Adjustment and Truth",
    "text": "Adjustment and Truth\nIt’s tempting to think that including covariates in a model is a way to reach the truth: a model that describes how the real world works, a model that can correctly anticipate the consequences of interventions such as medical treatments or changes in policy, etc. This overstates the power of models.\nA model design – the response variable and explanatory terms – is a statement of a hypothesis about how the world works. If this hypothesis happens to be right, then under ideal conditions the coefficients from the fitted model will approximate how the real world works. But if the hypothesis is wrong, for example if an important covariate has been left out, then the coefficients may not correctly describe how the world works.\nIn certain situations – the idealized “experiment” – researchers can create a world in which their modeling hypothesis is correct. In such situations there can be good reason to take the model results as indicating how the world works. For this reason, the results from studies based on experiments are generally taken as more reliable than results from non-experimental studies. But even when an experiment has been done, the situation may not be ideal; experimental subjects don’t always do what the experimenter tells them to and uncontrolled influences can sometimes remain at play.\nIt’s appropriate to show some humility about models and recognize that they can be no better than the assumptions that go into them. Useful object lessons are given by the episodes where conclusions from modeling (with careful adjustment for covariates) can be compared to experimental results. Some examples (from [@freedman-editorial-2008]):\n\nDoes it help to use telephone canvassing to get out the vote? Models suggest it does, but experiments indicate otherwise.\nIs a diet rich in vitamins, fruits, vegetables and low in fat protective against cancer, heart disease or cognitive decline? Models suggest yes, but experiments generally do not.\n\nThe divergence between models and experiment suggests that an important covariate has been left out of the models."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#from-sds-book",
    "href": "Reading-notes/Reading-notes-lesson-28.html#from-sds-book",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "From SDS book",
    "text": "From SDS book\nOur method for modeling a real-world system is to identify a single response variable and treat that variable as a function of one, several, or many explanatory variables. Effect size (Lesson 24) lets you focus attention on an individual explanatory variable and how changes in that explanatory variable correspond to changes in the response variable.\nInsofar as the logic of effect size is to isolate a single explanatory variable of interest, you might wonder why not build a model that conditions the response variable just on that one explanatory variable? As you’ll see, this is generally the wrong way to go about things. The role of each explanatory variable takes place in a context set by other explanatory factors.\nThe context-setting factors are called “covariates”. In the sense of data, covariates are perfectly ordinary variables. They are called covariates only to highlight the role of these variables for putting in context some other explanatory variables of particular interest to the modeler."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#example-covariates-and-context-in-educational-outcomes",
    "href": "Reading-notes/Reading-notes-lesson-28.html#example-covariates-and-context-in-educational-outcomes",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Example: Covariates and context in educational outcomes",
    "text": "Example: Covariates and context in educational outcomes\nTo illustrate how covariates set context, consider an issue of interest to public policy-makers in many societies: How much money to spend on children’s education? In the United States, for instance, educational budget policy is set mainly on a state-by-state level. State lawmakers are understandably concerned with the quality of the public education provided, but they also have other concerns and constraints and constituencies who give budget priority to other matters.\nIn evaluating the various trade-offs they face, lawmakers would be helped by knowing how increased educational spending will shape educational outcomes. What can available data tell us? Unfortunately, there are various political constraints that work against states adopting and publishing data on a common measure of genuine educational outcome. Instead, we have high-school graduation rates, student grades, etc. These have some genuine meaning but also can reflect the way the system is gamed by administrators and teachers and which cannot be easily compared across states. At a national level, we have college admissions tests such as the ACT and SAT. Perhaps because these tests are administered by private organizations and not state governments, it’s possible to gather data on test-score outcomes on a state-by-state basis and collate these with public spending information.\nFigure 3 shows average SAT score in 2010 in each state versus expenditures per pupil in public elementary and secondary schools. Laid on top of the data is a flexible linear model (and its confidence band) of SAT score versus expenditure. The overall impression given by the model is that the relationship is negative, with lower expenditures corresponding to higher SAT scores. But the confidence bands are broad and it is possible to find a smooth path through the confidence band that has almost zero slope. Either way, the conventional wisdom that higher spending produces better school outcomes is not supported by this graph.\n\n\n\n\n\nFigure 3: State by state data (from 2010) on average score on the SAT college admissions test and expenditures for public education.\n\n\n\n\nThere are other factors that play a role in shaping education outcomes: poverty levels, parental education, how the educational money is spent (higher pay for teachers or smaller class sizes? administrative bloat?), and so on. Modeling educational outcomes solely by expenditures ignores these other factors.\nAt first glance, it’s tempting to ignore these additional factors. We may not have data on them. And insofar as our interest is in understanding the relationship between expenditures and education outcomes, we are not directly concerned with the additional factors. This lack of direct concern, however, doesn’t imply that we should totally ignore them but that we should do what we can to “hold them constant”.\nTo illustrate, let’s consider a factor on which we do have data: the fraction of eligible students (those in their last year of high school) who actually take the test. This varies widely from state to state. In a poor state where few students go to college the fraction can be very small (Alabama 8%, Arkansas 5%, Mississippi 4%, Louisiana 8%). In some states, the large majority of students take the SAT (Maine 93%, Massachusetts 89%, New York 89%). In states with low SAT participation rates, the students who do take the test are applying to schools with competitive admissions. Such strong students can be expected to be get high scores. In contrast, the scores in states with high participation rates reflect both strong and weak students; they will be lower on average than in the low-participation states.\nPutting the relationship between expenditure and SAT scores in the context of the fraction taking the SAT can be done by using fraction as a co-variate, that is, building the model SAT ~ expenditure + fraction rather than just SAT ~ expenditure. Figure 4) shows a model with fraction taken into account.\n\n\n\n\n\nFigure 4: The model of SAT score versus expenditures, including as a covariate the fraction of eligible students in the state who take the SAT.\n\n\n\n\nNote that the effect size of spending on SAT scores is positive when the expenditure level is less than $10,000 per pupil. And notice that when the fraction taking the SAT is near 0, the average scores don’t depend on expenditure. This suggests that among elite students, expenditure doesn’t make a discernable difference: it’s the students, not the schools that matter.\nThe relationship shown in Figure 3 is genuine. So is the very different relationship seen in Figure 4. How can the same data be consistent with two utterly different displays? The answer, perhaps unexpectedly, has to do with the connections among the explanatory variables. Whatever the relationship between each individual explanatory variable and the response variable, the appearance of that relationship will depend on how explanatory variables are connected to each other."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#connections-among-explanatory-variables",
    "href": "Reading-notes/Reading-notes-lesson-28.html#connections-among-explanatory-variables",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Connections among explanatory variables",
    "text": "Connections among explanatory variables\nTo demonstrate that the apparent relationship between an explanatory variable and a response variable – for instance, school expenditures and education outcomes – depends on the connections of the explanatory variable with other explanatory variables, let’s move away from the controversies of political issues and study some systems where everyone can agree exactly how the variables are connected. We’ll look at data produced by simulations where we specify exactly what the connections are.\nA simulation implements a hypothesis: a statement about that might or might not be true about the real world. As a starting point for our simulation, let’s imagine that education outcomes increase with school expenditures in a very simple way: each $1000 increase in school expenditures per pupil results in an average increase of 10 points in the SAT score: an effect size of 0.01 points per dollar. Thus, the imagined relationship is:\n\\[\\mbox{sat} = 1100 + 0.01 * \\mbox{dollar expenditure}\\]\nLet’s also imagine that the fraction of students taking the SAT test also influences the average test score with an effect size of -4 sat points per percentage point. Adding this effect into the simulation leads to an imagined relationship of\n\\[\\mbox{sat} = 1100 + 0.01 * \\mbox{dollar expenditure} - 4 * \\mbox{participation percentage} .\\]\nAnd, of course, there are other factors, but we’ll treat their effect as random with a typical size of \\(\\pm\\) 50 points.\nTo complete the simulation, we’ll need to set values for dollar expenditures and participation percentage. We’ll let the dollar expenditures vary randomly from $7000 to $18,000 from one state to another and the participation percentage vary randomly from 1 to 100 percentage points.\nNotice that in this simulation, both participation percentage and expenditures affect education outcomes, but there is no connection at all between the two explanatory variables. That is, the graphical causal network is that shown in Figure @ref(fig:school-sim-1).\n\ndag_school1\n\n[[1]]\nexpenditure ~ unif(7000, 18000)\n\n[[2]]\nparticipation ~ unif(1, 100)\n\n[[3]]\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + eps(50)\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\ndag_draw(dag_school1)\n\n\n\n\nFigure 5: A graphical causal network relating expenditures, participation percentage, and education outcome, where there is no connection between expenditures and participation.\n\n\n\n\nWe can generate simulated data and use the data to train models. ?@fig-school-data-1 shows the data and two different models.\n\nDat1 <- sample(dag_school1, size=500)\nmod1_1 <- lm(outcome ~ ns(expenditure,2), data = Dat1)\nmod1_2 <- lm(outcome ~ ns(expenditure,2) * participation, data = Dat1)\nmod_plot(mod1_1, interval=\"prediction\") %>%\n  gf_point(outcome ~ expenditure, data = Dat1)\nmod_plot(mod1_2, interval=\"prediction\") %>%\n  gf_point(outcome ~ expenditure, alpha=~participation, data = Dat1, inherit=FALSE)\n\n\n\n\nFigure 6: Data and models of the relationship between expenditures and education outcomes from a simulation in which expenditures and participation rate are unconnected as in Figure 5. - (a) The model outcome ~ expenditure - (b) The model with participation as a covariate: outcome ~ expenditure + participation Both models (a) and (b) show the same effect size for outcome with respect to expenditure.\n\n\n\n\n\n\n\nFigure 7: Data and models of the relationship between expenditures and education outcomes from a simulation in which expenditures and participation rate are unconnected as in Figure 5. - (a) The model outcome ~ expenditure - (b) The model with participation as a covariate: outcome ~ expenditure + participation Both models (a) and (b) show the same effect size for outcome with respect to expenditure.\n\n\n\n\nThe relationship between outcome and expenditure can be quantified by the effect size, which appears as the slope of the function. You can see that when the explanatory variables are unconnected, as in Figure 5, the functions have the same slope.\nNow consider a somewhat different simulation. Rather than expenditures and participation being unconnected (as in the causal diagram shown in Figure 5), in this new situation we will posit a connection between the two explanatory variables. We’ll image that there is some broad factor, labeled “culture” in ?@fig-school-sim-2, that influences both the amount of expenditure and the participation in the tests used to measure education outcome. For instance, “culture” might be the importance that the community places on education or the wealth of the community.\n\ndag_school2\n\n[[1]]\nculture ~ unif(-1, 1)\n\n[[2]]\nexpenditure ~ 12000 + 4000 * culture + eps(1000)\n\n[[3]]\nparticipation ~ (50 + 30 * culture + eps(15)) %>% pmax(0) %>% \n    pmin(100)\n\n[[4]]\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + eps(50)\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\ndag_draw(dag_school2)\n\n\n\n\nFigure 8: A DAG for school outcomes that links participation and expenditure as a function of culture.\n\n\n\n\nAgain, using data from this simulation, we can train models:\n\n\noutcome ~ expenditures, which has no covariates.\n\n\noutcome ~ expenditures + participation, which includes participation as a covariate.\n\n\n?@fig-school-data-2 shows the data from the new simulation (which is the same in both subplots) and the form of the function trained on the data. Now model (a) shows a very different relationship between expenditures and outcome than model (b).\n\n\n\n\n\nFigure 9: Similar to ?@fig-school-data-1 but using the simulation in which the explanatory variables – expenditure and participation – are connected by a common cause. The two models show very different relationships between outcomes and expenditures. Model (b) matches the mechanism used in the simulation, while that mechanism is obscured in model (a).\n\n\n\n\n\n\n\nFigure 10: Similar to ?@fig-school-data-1 but using the simulation in which the explanatory variables – expenditure and participation – are connected by a common cause. The two models show very different relationships between outcomes and expenditures. Model (b) matches the mechanism used in the simulation, while that mechanism is obscured in model (a).\n\n\n\n\nSince we know the exact mechanism in the simulation—outcome increases with expenditure—we know that model (b) matches the workings of the simulation while model (a) does not.\nFor the simulation where expenditure and participation share a common cause, failing to stratify on participation – that is, looking at the points in @fig:-school-data-2 (a) but ignoring color – gives an utterly different result than if the stratification includes participation."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#always-include-covariates",
    "href": "Reading-notes/Reading-notes-lesson-28.html#always-include-covariates",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Always include covariates?",
    "text": "Always include covariates?\nIt might be tempting at this point to conclude that your models should always include covariates. After all, for both simulations the model that included participation as a covariate gave the correct effect size of expenditures on outcomes. That turns out to be an over-simplification, as you’ll see in Learning Check XXX [about a collider]."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#causality-correlation",
    "href": "Reading-notes/Reading-notes-lesson-28.html#causality-correlation",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Causality & Correlation",
    "text": "Causality & Correlation\nCausality is about relationships among entities in the world, e.g. the immunological properties of the drug acetaminophen lead to a reduction in fever. Correlation is about relationships that are evident in data, which might or might not be due to direct causal connections. For example, people who take acetaminophen tend to have fever, but this is not because acetaminophen causes fever. Instead, people who are unwell, and perhaps have fever, are more likely to take acetaminophen than those who are asymptomatic.\nCorrelations are properly part of the evidence to support a claim or quantification of causation. Indeed, whenever there is a correlation between two variables, it’s likely that there is some chain of causal connections that links the two variables, even if that chain is not directly from one variable to the other. For instance, taking the flu vaccine is correlated with reduced mortality. Some of this correlation is due to the immunological properties of the vaccine itself. But some of the correlation results from healthy people being more likely to take the vaccine than sick people, and healthy people having a lower mortality than sick people.\nSeen as a pessimist, this chapter can help you understand some of the ways that correlations can be present without a direct causal pathway, and how you can be badly mislead if you rely purely on data without any causal theory of the way your system works in the real world.\nSeen as an optimist, this chapter is about ways of calculating effect sizes from data that allow you to incorporate knowledge of the causal connections amongst the variables in your data.\nThe field of statistics comprises both optimists and pessimists. Perhaps to oversimplify, the pessimists think the proper domain of statistics is data and stylized mathematical models, and ought not include speculative notions of causal connections in the real world. The only sort of causal connection that the pessimists will accept is that of the experimenter who sets the values of inputs, for example by giving one “treatment” group of patients a drug and another “control” group a “placebo”. This has been a highly productive attitude in statistics, resulting in the development of clever designs for experiments that give the most information with the least laboratory effort. Unfortunately, the no-causation-without-experimentation philosophy leaves us without recourse when working with a system where a controlled experiment is not feasible.\nPerhaps the outstanding historical example of the limits of the no-causation-without-experimentation philosophy relates to the health effects of smoking. Nowadays, the morbidity and mortality caused by tobacco smoking is mainstream knowledge. Among the other proofs of the causal relationship is the decline in mortality due to lung cancer amoung populations where smoking became much less popular. Until the mid 1960s, however, some statisticians were in the vanguard of challenging the idea of a causal connection between smoking and, e.g., lung cancer. Notably, Ronald Fisher, generally considered to be the leading statistical figure of the 20th century, vehemently and influentially criticized the evidence for the causal connection.\n\n\n\n\n\nFigure 11: Insisting that “correlation is not causation” can interfere with making useful judgements, as interpreted by Randall Munroe in his XKCD cartoon series.\n\n\n\n\nThe optimists, again to oversimplify, believe it is possible to make useful statements (e.g. “the class helped” in Figure 11) about the causal connections that underlie data. They emphasize that statistics can support decision making even when knowledge of causation is incomplete and uncertain.\nThe optimists and the pessimists use the same set of mathematical and statistical tools for data analysis, particularly the calculation of effect sizes. The difference between them is the range of legitimate conclusions that can be drawn. The pessimists place in the center the idea that “correlation is not causation” and that only controlled experiment can be a justification for making causal conclusions. (We’ll study experiment in Lesson 32.) The optimists also see the difference between correlation and causation: correlation is a mathematical property, causation is a physical one. And the optimists accept that controlled experiment is an excellent way to form strong conclusions. But they accept other sources of knowledge or theoretical speculations as potentially useful, and use effect-size calculations in a way that, contingent on that knowledge or speculation, creates through the process of data analysis situations analogous to those created in the laboratory by careful experimentation."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-29.html",
    "href": "Reading-notes/Reading-notes-lesson-29.html",
    "title": "Math 300R Lesson 29 Reading Notes",
    "section": "",
    "text": "In model building, we create a function to link a response variable to one or more explanatory variables. Let’s imagine that the response variable is named y and the explanatory variable named x, a, b, and so on. By training a model on data, we create a function, let’s call it \\(f(x, a, b)\\). This function that results from training can be used in two two distinct decision-making settings:\nIn intervention mode, as we saw in Lesson 28, it’s important that the model formula reflect accurate the causal connections among the variables. The simple model y ~ x, without covariates, can sometimes give a misleading view of the effect of x on y. Including a covariate in the model might improve or worsen the estimate of effect size, depending on the causal connections in the real-world mechanism of the system.\nIn prediction mode, capturing the real-world causal connections in the model formula is not essential. For example, even if y is the cause of x, the model formula y ~ x might do a good job of predicting y from a measured value of x.\nIn this Lesson, we’ll examine the use of covariates in constructing prediction models."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-29.html#prediction-error",
    "href": "Reading-notes/Reading-notes-lesson-29.html#prediction-error",
    "title": "Math 300R Lesson 29 Reading Notes",
    "section": "Prediction error",
    "text": "Prediction error\nThe output of a prediction model is typically somewhat different from what happens in the real world. The difference between the real-world value of the response variable and the output of the prediction model is called the prediction error. As we saw in Lesson 26, in stating the output of a prediction model, it is helpful to also be able to state a typical size for the prediction error, usually in the form of a prediction interval.\n\nAlternative accountings\nWe’ve been using RMS prediction error to quantify how well the response variable has been accounted for by the explanatory variable(s). RMS prediction error is a convenient summary of the size of the typical prediction error because 1) it is an average over all the cases in the testing data and 2) it has the same units as the response variable. But RMS is not the only the only such accounting. In this section, we’ll look at two others that are widely used in statistical reports: R2 and the “sum of squares”.\nWe’ll start with the sum of squares accounting. Recall that the letters in RMS each stand for a specific step:\n\nS: square the each of the values\nM: average over the (squared) values\nR: take the square root of the (average squared) values.\n\nThe order of the steps is important; S first, M next, then finally R.\nThe sum of squares, often written SS, is a two-step process.\n\nS: square each of the values\nS: sum (not average) the (squared) values.\n\nAgain, the order of the steps is important: square first then finally sum. (The notation SS doesn’t make the order clear, but the name “sum of squares” does. So remember that SS stands for “sum of squares”.)\n\n\n\n\n\n\nNote in draft\n\n\n\nWHEN YOU GET TO THE ANOVA REPORT, make sure to point out that the so-called “mean square” is a confusing name because it wrongly brings to mind the MS in RMS. The quantity is really the sum of squares divided by the degrees of freedom."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html",
    "href": "Reading-notes/Reading-notes-lesson-38.html",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "",
    "text": "Ask, and it shall be given you; seek, and ye shall find; knock, and it shall be opened unto you: For every one that asketh receiveth; and he that seeketh findeth; and to him that knocketh it shall be opened. – Matthew 7:7-8\nThe modeling techniques we’ve covered are surprisingly powerful at identifying patterns in data. With power comes responsibility. This chapter is about how spurious patterns can arise in data and processes you can use to help ensure that the patterns your models identify are genuine.\nIt’s well known that people are particularly adept at finding patterns. To see this, spend a minute or two with Figure 1, which shows x-y pairs generated by a complex mathematical procedure called the Mersenne-Twister algorithm. How many of the structures created by Mersenne-Twister algorithm can you identify by eye? Take five of the stronger-looking patterns: clusters of points, large empty areas, strings of dots, etc. Write down a list of the patterns you spotted, including the coordinate location of each, a short description (e.g. “arc of dots”), and your subjective sense of how strong or convincing that pattern is.\nWith your list in hand, look at Figure 2 at the end of this section, which displays another n = 1000 x-y pairs generated by the same mathematical procedure. You’re going to check which of the patterns you found in the testing data are confirmed by the training data. Go through your list, looking at each location where you found a pattern in the training data and checking whether a similar pattern appears at that location in the testing data.\nWere any of the patterns you saw in the training data confirmed by the testing data?\nThere’s no denying that the patterns you saw were in the data. But the Mersenne-Twister algorithm is specifically designed not to produce regular patterns. Any that you saw were accidental alignments in the particular sample of data from the algorithm.\nThe “patterns” abstractly referred to in the previous paragraphs appear in data. In data used for modeling, a pattern might be a relationship or correlation between two or more variables, or a cluster of rows in a data frame that have similar values for a response variable and explanatory variables.\nTraining models on data can encode the underlying patterns. For instance, a pattern in the data might result in a model generating detailed predictions or demonstrating a strong effect size of one variable on another.\nA valid pattern is one that steadily appears from one sample of data to another (so long as the sample is big enough). Such consistency suggests that the pattern reflects some genuine aspect of the system generating the data. A false or accidental pattern is one that appears in a sample of data, but is unlikely to show up in another sample. This inconsistency indicates that conclusions based on this pattern are unlikely to be applicable in the future or in new situations.\nThe obvious, direct way to check the validity of a pattern encoded by a model is to see if the same pattern occurs in new data, data that was not used in building the initial model. Lesson 22 took this approach by constructing a sampling distribution of a statistic such as an effect size. To create a sampling distribution, we train many models on different subsets of a data set.\nWhen working with prediction models, the sign of a valid pattern is that the quality of the predictions – perhaps measured with a root-mean-square-error or a sensitivity/specificity – remains consistent when we calculate it on new data. A prediction that shows very small error on the data used to train the model but large error on new data is not a prediction that we can rely on in new settings.\nThe historical rapid growth in data analysis activity and the construction of data sets with large numbers of explanatory variables has made it easier to capture with models both valid patterns and false patterns. This makes it important to recognize that the false detection of patterns is possible whenever you train a model, to be aware of the characteristics of models and data that make false detection more likely, and to adopt procedures to mitigate the risk that the results of your work may not generalize beyond the particular sample of data you have in hand."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#example-falsely-discovering-purchasing-habits",
    "href": "Reading-notes/Reading-notes-lesson-38.html#example-falsely-discovering-purchasing-habits",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "Example: Falsely discovering purchasing habits",
    "text": "Example: Falsely discovering purchasing habits\nYou are a data scientist for an internet retailer, Potomac.com, which has just bought a national grocery chain, Austin Foods. You’re part of the team that is connecting the customer loyalty card data from Austin Foods with Potomac’s own large record of purchases. This is accomplished by offering a 10% Xdiscount for an item on Potomac to people who enter their Austin loyalty card number.\nPotomac’s management wants to create a cross-marketing program in which a customer shopping at Potomac will be offered coupons for Austin products. The hope is that the coupon discount will attract new customers to start shopping at Austin’s. In order for this to work, it’s best if the coupons are for products that the customer finds attractive.\nYour job is to build the coupon assignment system, that is, to figure out how to choose which products a customer is most likely to find attractive. To do this, you’ll create a set of classifiers that indicates the interest of a Potomac customer in an Austin product.\nYou’ve got data on 10,000 Potomac/Austin customers, that is, people whose records from Potomac and from Austin you can bring together. There are ten popular Austin products for which coupons can be offered. Among the 10,000 customers, about 16% have actually bought any given Austin product. You have built ten classifiers, one for each of the ten products. The input to the classifiers is 100 standard measures of a customer’s Potomac activity. The output of each classifier is the probability that the customer actually bought the corresponding Austin product.\nThe no-input classifier gives a probability of about 16% that the customer will buy the product. Management hopes that you will be able to segment the market to identify the products that a given person is much more likely to buy.\nIt’s a lot to ask of a person to sort through 100 potential explanatory variables to identify those that are predictive of buying a product. But it’s straightforward to use a model family that can learn on its own which variables are informative. You train the ten classifiers using a tree family of models.\n\nHeads up! The “data” has been created using random numbers, so that there are no actual relationships between the explanatory variables and the purchase outcomes. That is, no actual relationships aside from the accidental ones, such as the patterns encountered in Figure 1.\n\nTo illustrate how the coupon assignment system works, Table @ref(tab:some-results) shows an intermediate step in the calculation, where a probability for each of the ten products is calculated for each customer.\n\n\n\n\n\n\nTable @ref(tab:some-results) shows the output of the classifiers for just the first fifteen customers out of the 10,000 used to build the coupon selection system. For each person, all ten classifiers have been applied to estimate the probability that the person would buy each of the ten products. Highlighted in green are those products with a purchase probability greater than 40%.\nThe final output of the coupon assignment system is, for each customer, the identification of the specific products for which the probability is large. Reading Table @ref(tab:some-results), you’ll see that for person 1, product 9 merits a coupon. For person 2, products 2 and 10 merit a coupon. A winning product has not been identified for every customer, but you can’t please everyone.\n\n\n\nAttaching package: 'formattable'\n\n\nThe following objects are masked from 'package:scales':\n\n    comma, percent, scientific\n\n\n\n\n(ref:some-results-cap)\n \n\n\nCustomer ID\n\n  \n    product \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n  \n \n\n  \n    1 \n    9 \n    13 \n    10 \n    13 \n    5 \n    13 \n    8 \n    7 \n    13 \n    5 \n    44 \n    15 \n    8 \n    13 \n    13 \n  \n  \n    2 \n    16 \n    14 \n    7 \n    13 \n    11 \n    16 \n    13 \n    10 \n    11 \n    71 \n    8 \n    14 \n    73 \n    0 \n    16 \n  \n  \n    3 \n    89 \n    13 \n    10 \n    13 \n    11 \n    12 \n    100 \n    12 \n    10 \n    11 \n    10 \n    8 \n    14 \n    0 \n    9 \n  \n  \n    4 \n    9 \n    13 \n    22 \n    10 \n    14 \n    10 \n    3 \n    6 \n    12 \n    12 \n    12 \n    11 \n    14 \n    10 \n    8 \n  \n  \n    5 \n    14 \n    3 \n    10 \n    7 \n    11 \n    8 \n    11 \n    11 \n    15 \n    10 \n    11 \n    11 \n    14 \n    0 \n    8 \n  \n  \n    6 \n    16 \n    10 \n    7 \n    11 \n    64 \n    9 \n    25 \n    10 \n    8 \n    7 \n    5 \n    11 \n    11 \n    15 \n    12 \n  \n  \n    7 \n    10 \n    5 \n    9 \n    69 \n    13 \n    20 \n    13 \n    8 \n    8 \n    13 \n    73 \n    20 \n    6 \n    10 \n    13 \n  \n  \n    8 \n    14 \n    12 \n    15 \n    5 \n    4 \n    14 \n    13 \n    12 \n    4 \n    10 \n    14 \n    21 \n    14 \n    14 \n    13 \n  \n  \n    9 \n    12 \n    12 \n    14 \n    12 \n    12 \n    12 \n    14 \n    62 \n    12 \n    12 \n    80 \n    9 \n    0 \n    9 \n    12 \n  \n  \n    10 \n    9 \n    13 \n    6 \n    10 \n    12 \n    6 \n    17 \n    16 \n    6 \n    6 \n    25 \n    9 \n    5 \n    15 \n    7 \n  \n\n\n\n\n\n(ref:some-results-cap) The output of the ten classifiers for the first 15 customers. Green highlighting is used for those products which a given customer is likely to buy.\n\n\nWarning: `group_by_()` was deprecated in dplyr 0.7.0.\nℹ Please use `group_by()` instead.\nℹ See vignette('programming') for more help\nℹ The deprecated feature was likely used in the dplyr package.\n  Please report the issue at <\u001b]8;;https://github.com/tidyverse/dplyr/issues\u0007https://github.com/tidyverse/dplyr/issues\u001b]8;;\u0007>.\n\n\nTo test the performance of the system, we can look at the product/customer combinations for which a coupon was merited, and check how many of them actually corresponded to a purchase: it’s 74%. But for the combinations with no coupon, the purchase rate was only 11%.\nThe results are impressive. For about half of the customers, the coupon assignment system has identified customers/product combinations with a purchase probability of more than 40%. Often, the probability of purchase is considerably higher than 40%. Targeting each customer with a coupon for the right product is likely to generate a lot of new sales!\n\nSince data was generated using random numbers, we know that the “success” of the coupon assignment system is illusory. Later, we’ll see how the process was able to uncover so many accidental patterns from random data and list some things to look out for when modeling. But first, let’s provide a reliable method for you to identify when your results are based in accidental patterns: using testing data.\n\nA true measure of the performance of a model should be based not on the data on which the model was trained, but data which have been held back for use in testing and not used in training. For this example, we’ll use testing data consisting of 10,000 customers for whom we have the same 100 explanatory variables from the Potomac database and for whom we know if each customer purchased any of the ten products from Austin Foods. Only about 1 in 6 customers bought any single product from Austin. We want to see if the classifier assigns a high probability to those customers who did buy the product. If so, it means we can use just the 100 explanatory variables to find winning products for customers for whom we have no Austin purchasing data.\n\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\nℹ The deprecated feature was likely used in the tibble package.\n  Please report the issue at <\u001b]8;;https://github.com/tidyverse/tibble/issues\u0007https://github.com/tidyverse/tibble/issues\u001b]8;;\u0007>.\n\n\n\n\n(ref:purchase-test-cap)\n \n\n\nCustomer ID\n\n  \n    product \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n  \n \n\n  \n    1 \n    13 \n    7 \n    13 \n    17 \n    12 \n    8 \n    58 \n    13 \n    7 \n    7 \n    13 \n    9 \n    9 \n    13 \n    6 \n  \n  \n    2 \n    8 \n    19 \n    86 \n    13 \n    13 \n    9 \n    13 \n    18 \n    9 \n    75 \n    16 \n    13 \n    8 \n    13 \n    71 \n  \n  \n    3 \n    25 \n    12 \n    16 \n    0 \n    8 \n    10 \n    16 \n    10 \n    71 \n    8 \n    7 \n    80 \n    10 \n    10 \n    9 \n  \n  \n    4 \n    15 \n    13 \n    12 \n    11 \n    10 \n    9 \n    17 \n    14 \n    8 \n    3 \n    17 \n    14 \n    12 \n    11 \n    3 \n  \n  \n    5 \n    8 \n    10 \n    10 \n    38 \n    9 \n    13 \n    11 \n    10 \n    7 \n    11 \n    11 \n    9 \n    7 \n    14 \n    7 \n  \n  \n    6 \n    9 \n    10 \n    8 \n    60 \n    6 \n    4 \n    89 \n    9 \n    7 \n    11 \n    8 \n    10 \n    12 \n    8 \n    20 \n  \n  \n    7 \n    9 \n    15 \n    6 \n    20 \n    8 \n    100 \n    5 \n    10 \n    10 \n    9 \n    9 \n    13 \n    72 \n    8 \n    9 \n  \n  \n    8 \n    21 \n    12 \n    14 \n    14 \n    12 \n    12 \n    3 \n    11 \n    14 \n    13 \n    13 \n    14 \n    12 \n    12 \n    14 \n  \n  \n    9 \n    12 \n    9 \n    8 \n    9 \n    21 \n    6 \n    9 \n    71 \n    8 \n    8 \n    9 \n    8 \n    10 \n    9 \n    14 \n  \n  \n    10 \n    12 \n    16 \n    14 \n    6 \n    10 \n    12 \n    15 \n    10 \n    9 \n    17 \n    19 \n    7 \n    12 \n    9 \n    8 \n  \n\n\n\n\n\n(ref:purchase-test-cap) Similar to Table @ref(tab:some-results) but for the testing data.\n\n\n\nA valid evaluation of the performance of the system involves using the testing data rather than the training data. Figure @ref(fig:purchase-test) shows the assignment of coupons for the customers in the test data. Although coupons are assigned to these customers, the purchase rate for these items is only 16%, no different than the probability of purchase for no-coupon items. In other words, the coupon assignment system doesn’t work at all!"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#sources-of-false-discovery",
    "href": "Reading-notes/Reading-notes-lesson-38.html#sources-of-false-discovery",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "Sources of false discovery",
    "text": "Sources of false discovery\nHow did the coupon classifier system identify so many accidental patterns, patterns that existed in the training data but not in the testing data?\nOne source of false discovery stems from having multiple potential response variables. In the Potomac/Austin example, there were ten different classifiers at work, one for each of the ten Austin products. Even if the probability of finding an accidental pattern in one classifier is small, looking in ten different places dramatically increases the odds of finding something.\nSimilarly, having a large number of explanatory variables – we had 100 in the coupon classifier – provides many opportunities for false discovery. The probability of an accidental pattern between one outcome and one explanatory variable is small, but with many explanatory variables each being considered it’s much more likely to find something.\nA third source of false discovery at work in the coupon classifier relates to the family of models selected to implement the classifier. We used a tree model classifier capable of searching through the (many) explanatory variables to find ones that are associated with the response outcome. Unbridled, the tree model is capable of very fine stratification. Each coupon classifiers stratified the customers into about 200 levels. On average, then, there were about 50 customers in each strata. But there is variation, so many of the strata are much smaller, with ten or fewer customers. The small groups were constructed by the tree-building algorithm to have similar outcomes among the members, so it’s not surprising to see a very strong pattern in each group. For each classifier, about 15% of all customers fall into a strata with 20 or fewer customers.\nTo illustrate, Figure 3 shows the shape of the tree model for a typical coupon classifier. Each of the splits reflects an accidental alignment of the response variable with one of the explanatory variables. As more splits are made, the group of customers contained in the split becomes smaller. Many of the leaves on the tree contain just a handful of customers who accidentally had similar values for the several explanatory variables used in the splits.\n\n\n\n\n\nFigure 3: A sketch of one of the classifiers constructed for the coupon selection system. The tree-growing algorithm was allowed to keep going until the customer data was split up into very small strata.\n\n\n\n\nThe tree is too complex to be plausible as a real-world mechanism. None of the details in Figure 3 have any validity beyond the training data itself."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#identifying-false-discovery",
    "href": "Reading-notes/Reading-notes-lesson-38.html#identifying-false-discovery",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "Identifying false discovery",
    "text": "Identifying false discovery\nWe use data to build statistical models and systems such as the coupon-assignment machine. False discovery occurs when a pattern or model performance seen with one set of data does not generalize to other potential data sets.\nThe basic technique to avoid false discovery is called cross validation. One simple approach to cross validation splits the data frame into two randomly selected non-overlapping sets of rows: one for training and the other for testing. Use the training data to build the system. Use the testing data to evaluate the system’s performance.\nMost often, cross validation is used to test model prediction performance such as the root-mean-square error or the sensitivity and specificity of a classifier. This can be accomplished by taking the trained model and providing as input the explanatory variables from the testing data, then comparing the model output to the actual response variable values in the testing data. Note that using testing data in this way does not involve retraining the model on the testing data.\nHow big should the training set be compared to the testing set? For now, we’ll keep things simple and encourage use of a 50:50 split or something very close to that.\nThis is a simple and reliable approach that should always be used."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#false-discovery-and-multiple-testing",
    "href": "Reading-notes/Reading-notes-lesson-38.html#false-discovery-and-multiple-testing",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "False discovery and multiple testing",
    "text": "False discovery and multiple testing\nWhen the main interest is in an effect size, standard procedure calls for calculating a confidence interval on the effect. For example, a 2008 study examined the possible relationship between a woman’s diet before conception and the sex of the conceived child. The popular press was particularly taken by this result from the study:\n\nWomen producing male infants consumed more breakfast cereal than those with female infants. The odds ratio for a male infant was 1.87 (95% CI 1.31, 2.65) for women who consumed at least one bowl of breakfast cereal daily compared with those who ate less than or equal to one bowlful per week. [@fetal-sex-2008]\n\nThe model here is a classifier of the sex of the baby based on the amount of breakfast cereal eaten. The effect size tells the change in the odds of a male when the explanatory variable changes from one bowlful of cereal per week to one bowl per day (or more). This effect size is sensibly reported as a ratio of the two odds. A ratio bigger than one means that boys are more likely outcomes for the one-bowl-a-day potential mother than the one-bowl-a-week potential mother. The 95% confidence interval is given as 1.31 to 2.65. This confidence interval does not contain 1. In a conventional interpretation, this provides compelling evidence that the relationship between cereal consumption and sex is not a false pattern.\nBut the confidence interval is not the complete story. The authors are clear in stating their methodology: “Data of the 133 food items from our food frequency questionnaire were analysed, and we also performed additional analyses using broader food groups.” In other words, the authors had available more than 133 potential explanatory variables. For each of these explanatory variables, the study’s authors constructed a confidence interval on the odds ratio. Most of the confidence intervals included 1, providing no compelling evidence of a relationship between that food item and the sex of the conceived child. As it happens, breakfast cereal produced the confidence interval that was the most distant from an odds ratio of 1.\nLet’s look at the range of confidence intervals that can be found from studying 100 potential random variables that are each unrelated to the response variable. We’ll simulate a response randomly generated “sex” G and B where the odds of G is 1. Similarly, each explanatory variable will be a randomly generated “consumption” high or low where the odds of high is 1. A simple stratification of sex by consumption will generate the odds of G for those cases with consumption Y and also the odds of G for those cases with consumption N. Taking the ratio of these odds gives, naturally enough, the odds ratio. We can also calculate from the stratified data a 95% confidence interval on the odds ratio.\nSo that the results will be somewhat comparable to the results in @fetal-sex-2008, we’ll use a similar sample size, that is, n = 740. Table @ref(tab:sex-consumption-1) shows one trial of the simulation.\n\n\n\n\n\n\n(ref:sex-consumption-1-cap)\n\n\n\n\n\n\nhigh\n\n\nlow\n\n\n\n\n\n\nB\n\n\n165\n\n\n182\n\n\n\n\nG\n\n\n211\n\n\n182\n\n\n\n\n\n(ref:sex-consumption-1-cap) A stratification of sex outcome (B or G) on consumption (high or low) for one trial of the simulation described in the text.\nReferring to Table @ref(tab:sex-consumption-1), you can see that the odds of G when consumption is low is 182 / 182 = 1. The odds of G when consumption is high is 211/165 = 1.28. The 95% confidence interval on the odds ratio can be calculated. It is 0.95 to 1.73. Since that includes 1, the data underlying Table @ref(tab:sex-consumption-1) provide little or no evidence for a relationship between sex and consumption. This is exactly what we expect, since the simulation involves entirely random data.\nFigure 4 shows the 95% confidence interval on the odds ratio for 133 trials like that in Table @ref(tab:sex-consumption-1). The confidence interval from each trial is shown as a horizontal line. The large majority of them include 1. That’s to be expected because the data have been generated so that sex and consumption have no relationship except those arising by chance.\n\n\nWarning: geom_vline(): Ignoring `mapping` because `xintercept` was provided.\n\n\n\n\n\nFigure 4: Confidence intervals on the odds ratio comparing female and male birth rates for many trials of simulated data with no genuine relationship between the explanatory and response variables.\n\n\n\n\nNonetheless, out of 133 simulations there are six where the confidence interval does not include 1. These are shown in red. By necessity, one of the intervals will be the most extreme. If instead of numbering the simulations, we had labelled them with food items – e.g. grapefruit, breakfast cereal, toast – we would have a situation very similar to what seems to have happened in the sex-vs-food study. (For a more detailed analysis of the impact of multiple testing in @fetal-sex-2008, see @young-2009.)\nSuppose now that half of the data used in @fetal-sex-2008 had been held back as testing data. Using the training data, it would be an entirely legitimate practice to generate hypotheses about which specific food items might be related to the sex of the baby. The validity of any one selected hypothesis could then be established using the testing data without the ambiguity introduced by multiple testing. The testing data confidence interval can be taken at face value; the training data confidence interval cannot."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#example-organic-discovery",
    "href": "Reading-notes/Reading-notes-lesson-38.html#example-organic-discovery",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "Example: Organic discovery?",
    "text": "Example: Organic discovery?\nIt’s easy to find organic foods in many large grocery stores. Advocates of an organic diet are attracted by a view that it is sustainable, promotes small farms, and helps avoid contact with pesticides. There are also nay-sayers who make valid points, but that is not our purpose here. Informally, I find that many people and news reports point to the health benefits of an organic diet. Usually they believe that these benefits are an established fact.\nA 2018 New York Times article observed:\n\nPeople who buy organic food are usually convinced it’s better for their health, and they’re willing to pay dearly for it. But until now, evidence of the benefits of eating organic has been lacking. [@NYT-2018-10-23-Rabin]\n\nThe new evidence of health benefits is reported in an article in the Journal of the American Medical Association: Internal Medicine [@baudry-2018]\nDescribing the findings of the research, the Times article continued:\n\nEven after these adjustments [for covariates], the most frequent consumers of organic food had 76 percent fewer lymphomas, with 86 percent fewer non-Hodgkin’s lymphomas, and a 34 percent reduction in breast cancers that develop after menopause.\n\nThe study warrants being taken seriously: it involved about 70,000 French adults among whom 1340 cancers were noted. The summary of organic foot consumption was a scale from 0 to 32 and included 16 labeled products including dairy, meat and fish, eggs, coffee and tea, wine, vegetable oils, and sweets such as chocolate. Adjustment was made for a substantial number of covariates: age, sex, educational level, marital status, income, physical activity, smoking, alcohol intake, family history of cancer, body mass index, hormonal treatment for menopause, and others.\nYet … the reseach displays many of the features that can lead to false discovery. For instance, results were reported for four different types of cancer: breast, prostate, skin, lymphomas. The study reports p-values and hazard ratios1 comparing cancer rates among the four quartiles of the organic consumption index.\nComparing the most organic (average organic index 19.36/32) and the least organic (average index 0.72/32) groups the 95% confidence interval on the relative risk and p-values given in the study’s Table 4 are:\n\nBreast cancer: 0.66 - 1.16 (p = 0.38)\nProstate cancer: 0.61- 1.73 (p = 0.39)\nSkin cancer: 0.49 - 1.28 (p = 0.11)\nLymphomas: 0.07 - 0.69 (p = 0.05)\n\nYou might be surprised to see that the confidence interval on the relative risk for breast cancer includes 1.0, which suggests no evidence for an effect. As clearly stated in the report, the risk reduction for breast cancer is seen only in a subgroup of study participants: those who are postmenopausal. And even then, the confidence intervals continue to include 1.0:\n\nBreast cancer pre-menopausal: 0.67 - 1.52 (p = 0.85)\nBreast cancer post-menopausal: 0.53 - 1.18 (p = 0.18)\n\nSo where is the claimed 34% reduction in breast cancer cited in the New York Times article. It turns out the the study used two different indices of organic food consumption. The 0 to 32 scale which includes many items for which the amount consumed is very small (e.g., coffee, chocolate) and a “simplified, plant derived organic food score.” It’s only when you look at the full 0 to 32 scale that you see the reduction in post-menopausal breast cancer: the confidence interval is 0.45 to 0.96 (p = 0.03).\nWhat about cancer rates overall? For the 0 to 32 scale the risk ratio was 0.58 - 1.01 (p = 0.10). To see the claimed reduction clearly you need to look at the simplified food score which gives 0.63 - 0.89 (p < 0.005). And it’s only in comparing the highest-index quarter of participants with the lowerest quarter participants that any difference at all is seen in any type of cancer: the middle-half of participants show no difference in relative risk from the lowest-organic quarter of participants. (Because of this, had the study compared the highest quarter to the next highest quarter, they would have seen basically the same relative risks reported in the highest-to-lowest quarter comparison. Then the conclusion would have had a different flavor, perhaps to be reported as “Typical organic food consumption levels show no cancer benefits.”)\nA further source of potential false discovery stems from the study’s starting and stop times. It’s not clear that these were pre-defined; the reported results are intermediate to a longer follow up. The choice to report intermediate results is another way that the number of opportunities for false discovery is increased. And the choice is important: for the follow-up time used, about 2% of the participants developed cancer. In an earlier study of more than 600,000 middle-aged UK women (average age 59), the incidence of cancer was four times larger: 8.6%. [@bradbury-2014] That study did not find any relationship between organic food consumption and overall cancer rates, and found no relationship for 15 out of 16 different types of cancer. The exception is extremely interesting: non-Hodgkin lymphoma for which a similar result was found in the French study.\nSo is the study reported in the New York Times a matter of false discovery? It’s emotionally unsatisfying to discount a result about organic food and non-Hodgkin lymphoma simply because it’s part of a larger study that looked at many different combinations of cancer types and organic food indices. What if the researchers had only studied non-Hodgkin lymphoma – they would have gotten the same result and it wouldn’t have the deficiencies of being the strongest result of many possibilities. It would have stood on its own. But it doesn’t and we are left in a state of doubt."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#p_values",
    "href": "Reading-notes/Reading-notes-lesson-38.html#p_values",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "p-values and “significance”",
    "text": "p-values and “significance”\nFalse discovery is not a new problem. The traditional logic can be traced back to 1710, when John Arbuthnot was examining London birth records from 1629 to 1710. Arbuthnot was surprised to find that for each year males were more common than females. In interpreting this finding, Arbuthnot refered to the conventional wisdom that births of males and females are equally likely. If this were the case, in any one year there might, by chance, be more females than males or the other way around. While it’s theoretically possible that chance might produce the string of 82 years with more males, it’s very unlikely. “From whence it follows, that it is Art, not Chance, that governs,” Arburthnot wrote. In more modern language, Arburthnot concluded that the hypothesis of equal rates of male and female births was not consistent with the data. Arbuthnot’s “Chance” corresponds to false discovery, while “Art” is a valid discovery.\nArburthnot’s logic became a standard component of statistical method.\n\nSummarize the data into a single number called a “test statistic”. For Arburthnot the test statistic was the number of years where male births predominated, out of the 82 years being examined. The observed value of the test statistic was 82.\nState a “null hypothesis”, typically something that is the conventional wisdom. For Arburthnot, the null hypothesis was that male and female births are equally likely.\nCalculate a hypothetical quantity based on the null hypothesis: the probability that the test statistic produced in a world in which the null hypothesis holds true would be at least as large as the test statistic.\nIf the probability in (3) is small, one is entitled to “reject the null hypothesis.” Typically, “small” is defined as 0.05 or less.\n\nIn the 1890s, statistical pioneer Karl Pearson invented a test statistic he called \\(\\chi^2\\) (“chi”-squared, with “chi” pronounced “ki” as in “kite”) that can be applied in a variety of settings. In 1900, Pearson published a table [@pearson-1900] that makes it an easy matter to look up the probability in step (3) above. He called this theoretical probability “P”, a name that has stuck but is conventionally written as lower-case “p”.\nData scientists tend to work with “big data”, but for many applications of statistics, data is so scarce that use of separate training and testing data is impractical. For such small data, the calculation of a p-value can be a sensible guard against false discovery. Still, a p-value does not address any of the sources of false discovery outlined in the previous sections of this chapter. When used with small data and simple modeling methods, those sources of false discovery are not so much of a problem. In small data there won’t be multiple explanatory variables that can be searched and there won’t be a choice of response variables. This doesn’t eliminate all problems, since in small data results can depend critically on the inclusion or exclusion of a single row of data. The name “p hacking” has been given to the various ways that researchers can manipulate p-values to get them below 0.05.\nAnother problem with p-values stems from misinterpretation of the admittedly difficult logic that underlies them. The misinterpretations are encouraged by the use of the term “tests of significance” to the p-value method. Particularly galling is the use of the description “statistically significant” to describe a result where p < 0.05. The everyday meaning of “significant” as something of importance is in no way justified by p < 0.05. Instead, the practical importance or not is more clearly signaled by examining an effect size. (It’s extremely disappointing that journalists, who are writing for an audience that for the most part has no understanding of p-value methodology, use “significant” when reporting on the statistics of research findings. It would be more honest to use a neutral term such as “null-validated” or “p-validated” which does not confuse the statistical result with actual practical importance.)\nThe p-value methodology has little or nothing to contribute to data science practice. When data is big there is a much more straightforward method to guard against false discovery: cross validation. And when data is big there is another, more fundamental problem with p-values. They are calculated with reference to a specific null hypothesis of “no effect” or “no relationship.” More realistically, they should be calculated with respect to a hypothesis of “trivial (but potentially non-zero) effect”. There are all sorts of mechanisms in the world (such as common causes) that can create the appearance of some effect or relationship. No matter how trivial in size this is, with sufficient data the p-value will become small. To illustrate, Figure 5 shows the p-value as a function of the sample size n in a system with an R-squared of 0.001, which in most settings would be of no practical signficance.\n\n\nWarning: geom_hline(): Ignoring `mapping` because `yintercept` was provided.\n\n\n\n\n\nFigure 5: The p-value as a function of sample size n when the test statistic R-squared has the trivial value 0.001. The horizontal line shows the usual threshold for “significance” of p < 0.05."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#notes-in-draft",
    "href": "Reading-notes/Reading-notes-lesson-38.html#notes-in-draft",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "NOTES IN DRAFT",
    "text": "NOTES IN DRAFT\n“Statistical crisis” in science\nhttps://www.americanscientist.org/article/the-statistical-crisis-in-science\nGarden of the Forking Paths\nIonedes"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-21.html",
    "href": "Reading-notes/Reading-notes-lesson-21.html",
    "title": "Math 300R Lesson 21 Reading Notes",
    "section": "",
    "text": "Imagine being transported back to June 1940 . You and your family might be sitting around a radio receiver, having just switched on set and waited for it to warm up in time to hear a news broadcast. I’ve selected a newscast for you, recording 103. The recording covers the surrender of the French in the face of the German invasion. Press the play button in the box below and listen.\nThere are many other recordings on the site which are worth listening to. I’m directing you to #103 as an example of a radio phenomenon: noise (or, in slang, “static”). You can clearly make out the spoken words from the recording. But there is also a background sound, something like the sound made by the act of crumpling paper.\nModern radio engineering has more-or-less eliminated noise, mainly by the use of digital technology. (Many of the recordings on the radio archive site have been “cleaned” so the noise is not so evident.) But if you have ever talked to a friend at a sporting event, you have probably had to shout to get your message over the noise of the crowd. At the receiving end, your friend intuitively filters out the noise (unless it is too loud) and recovers your words.\nEngineers and others make a distinction between signal and noise. The signal is the spoken words of the 1940 broadcast, the noise is the hiss and clicks. You can intuitively separate the signal and the noise in this recording, focusing attention on the signal and ignoring the noise.\nSeparating signal from noise—or, at least trying to reduce the noise—is a common problem in all sorts of settings. Historically, statistics emerged from a confluence of two needs: i) the need to summarize the resources and activities of countries and states (whence comes the “stat” in “statistics”) and ii) the need to filter out noise so that the signal becomes clearer."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-21.html#sec-signal-and-noise",
    "href": "Reading-notes/Reading-notes-lesson-21.html#sec-signal-and-noise",
    "title": "Math 300R Lesson 21 Reading Notes",
    "section": "Signal and noise",
    "text": "Signal and noise\nTo illustrate the statistical problem of signal and noise, let’s turn to a DAG simulation: dag01. Here’s a sample from dag01:\n\nsample(dag01, size=2)\n\n# A tibble: 2 × 2\n       x     y\n   <dbl> <dbl>\n1 -0.326  2.84\n2  0.552  5.04\n\n\nThe DAG simulation implements a relationship between x and y. In statistics, this relationship is the signal.\n\nLook at the 2-row sample from the simulation and make a guess about what the relationship is.\n\nYour guess will be exactly that: a guess. Any of an infinite number of possible relationships could account for the x and y data. The noise reduction problem of statistics is to make the guess as good as possible. For a sample of size \\(n=2\\), as good as possible is not very good!\nTo have a better shot at revealing the relationship hidden by the noise, we need more data, a bigger sample. Here’s a sample of size \\(n=10\\):\n\nsample(dag01, size=10)\n\n# A tibble: 10 × 2\n         x     y\n     <dbl> <dbl>\n 1 -0.786  1.89 \n 2  0.0547 4.12 \n 3 -1.17   2.36 \n 4 -0.167  6.33 \n 5 -1.87   0.933\n 6 -0.120  2.93 \n 7  0.826  5.70 \n 8  1.19   5.90 \n 9 -1.09   2.13 \n10 -0.375  4.23 \n\n\nLooking carefully at the two rows of data you may be able to see some patterns. x is never larger than, say, 2 in magnitude and can be positive or negative. y is always positive. And notice that when x is negative, the corresponding y value is relatively small compared to the y values for positive x.\nWith the bigger sample size, \\(n=10\\) versus \\(n=2\\), we can make a more informed guess about the relationship between variables x and y.\nHuman cognition is not well suited to looking a long columns of numbers. Often, we can make better use of our natural human talents by translating the sample into a graphic, like this:\n\n\n\n\n\nCollecting more data can make the relationship clearer. Here’s a graph of a sample of size \\(n=10,000\\) with the smaller \\(n=10\\) sample shown in orange:\n\n\n\n\n\nFigure 1: With \\(n=10,000\\) rows, the relationship between x and y is evident graphically.\n\n\n\n\nThere are many ways to describe the relationship between x and y indicated by the graph of the large sample. For instance, we can see that when x is positive, y is almost always greater than 4, but for negative x the value of y tends to be less than 4.\nIn this course, we prefer to make quantitative summaries of relationships. We do this by fitting models to the data. Here’s the relationship that’s shown by the original sample of 10:\n\nlm(y ~ x, data = original) %>% coefficients() # size 10 sample\n\n(Intercept)           x \n   4.262846    1.741758 \n\n\nThe coefficients in this model correspond to the mathematical relationship \\(y = 4.26 + 1.74 x\\). On it’s own, this formula doesn’t tell us the extent to which we have filtered out the noise in the simulation.\nWith more data, say, the larger sample of \\(n=10,000\\), the relationship becomes more evident:\n\nlm(y ~ x, data = larger) %>% coefficients() # size 10 sample\n\n(Intercept)           x \n   4.008928    1.495904 \n\n\nBecause these data come from a DAG simulation, we can look at the formulas to see exactly what relationship is behind the data:\n\nprint(dag01)\n\n[[1]]\nx ~ eps()\n\n[[2]]\ny ~ 1.5 * x + 4 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nComparing the two models to the DAG formula for y, we can see that the larger sample produced coefficients that are much closer to the formula than did the smaller sample. Closer, but not exactly the same. Even in the coefficients calculated from the large sample, there is still a legacy of the noise in the original relationship.\nThe lesson here is simple: More data can give a better view of the relationships.\nThe challenge we face when working with data generated in the real world is that it is not often possible to open the black box that generated the data; all we have is the data! So how can we tell whether the data we have at hand are sufficient for giving a faithful description of the actual relationships?\nThe general idea is to use the variation within the sample to accomplish two things at once: i. make a description of the relationship, and ii. estimate how much inherited noise there is in the description. The result of (ii) is important, since it can tell us whether or not our description (i) is good enough for the purpose we seek to serve.\nTo get started, let’s explore how to measure the amount of variation in the data. This can give us an idea of the size of the overall signal+noise, which we will do in the next section of this Lesson. In Lesson 22 we will use DAG simulations to get an idea of how to estimate the amount of inherited noise in the description of the relationship. The DAG simulation is useful because we have access to the internal mechanism of the DAG, so it is easy to see how close the description is to the actual relationship.\nIn Lesson 23, we will take off the DAG training wheels and learn how to estimate the size of the inherited noise in the description directly from data, without having to open the black box of the mechanism that generated the data. If you think about it, it is an amazing claim that we can estimate how close our data-driven description is to the actual mechanism, without having to know the actual mechanism!"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-21.html#simple-dags",
    "href": "Reading-notes/Reading-notes-lesson-21.html#simple-dags",
    "title": "Math 300R Lesson 21 Reading Notes",
    "section": "Simple DAGs",
    "text": "Simple DAGs\nA DAG is a hypothesis, a statement that might or might not be true. DAGs are part of the statistical apparatus for thinking responsibly about causality. You use a DAG—or, potentially, multiple DAGs—when the issue of what causes what is relevant to your work.\nWhen there are only two variables involved in the system under consideration—we’ll call them X and Y for simplicity—there are only two possible DAGs:\n\\[X \\rightarrow Y\\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\ X \\leftarrow Y\\]\nBut there are additional DAG possibilities once we accept that there might be other factors that positioned in the middle of the relationship between X and Y, for instance a variable C.\n\\[X \\rightarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\leftarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\rightarrow C \\leftarrow Y\\]\nActually, there are many other configurations of DAGs involving three variables. To keep things simple, we’ll restrict things to DAGs where X might or might not cause Y, but Y never causes X. (We don’t lose anything from this restriction because you get to make the choice of what real-world variable correspond to X and which one to Y.) Figure 2 shows the 10 configurations of 3-variable DAGs where Y doesn’t cause X.\n\n\n\n\n\nFigure 2: Ten DAG configurations involving three variables X, Y, and C and where there is no causal arrow between Y and X.\n\n\n\n\nThis enumeration of the ten configurations is useful for two reasons. First, the appropriate ways of analyzing data to quantify the causal links depends on which DAGs are hypothesized to be relevant. (We’ll get to this matter in Lesson 30.) Second, however strong may be your conviction that a particular DAG describes the way the world works, it’s worthwhile to look at each of the other DAGs as possibilities and try to imagine scenarios where they might be an appropriate choice. This exercise can reveal alternative reasonable hypotheses about the real-world connections among variables.\nThe behavior of such three-variable DAGs is sufficiently rich that we won’t have to systematically enumerate the possibilities for four- and higher-variable DAGs. On the other hand, in framing hypotheses about connections in the real world, you will often have occasion to connect multiple factors.\nAlways keep in mind that DAGs never involve loops (“cycles”). ?@fig-two-cycles shows examples of connections that have cycles. The middle A or DAG stands for “acyclic,” that is, without cycles.\n\n\n\n\n\n\nFigure 3: Examples of networks with cycles. These are not DAGs.\n\n\n\n\n\n\n\nFigure 4: Examples of networks with cycles. These are not DAGs.\n\n\n\n\n\nWhat about systems that involve feedback, for instance an economic model where today’s prices determine tomorrow’s supply which in turn shapes the prices on the next day? Keep in mind that “today’s prices” and “the next day’s prices” are different variables and therefore not the same node in a directed graph."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-34.html",
    "href": "Reading-notes/Reading-notes-lesson-34.html",
    "title": "Math 300R Lesson 34 Reading Notes",
    "section": "",
    "text": "We all face many yes/no situations. A patient has a disease or does not. A credit card transaction is genuine or fraudulent. A classifier is a statistical model designed to predict the unknown outcome of a yes/no situation from information that is already available.\nConsider a credit-card company might building a classifier to predict at the time of the transaction whether a purchase of gasoline is fraudulent. The company knows how often and how much gasoline the individual cardholders buys, where the cardholder lives, whether the cardholder travels extensively, typical times of day for a purchase, and so on. Feature engineering is the process of using existing data—including, in our example, whether the purchase turned out to be fraudulent—to develop potential markers or signals of the outcome. For simplicity, imagine the features selected are the number of days since the last gasoline purchase and the distance from the last place of purchase.\nOnce potential features have been proposed, the engineers building the classifier assemble training and testing data sets. Suppose, for the purpose of illustration, that the training data has 2000 fraudulent transactions and 4000 non-fraudulent ones, and the testing set is about the same.\nThe word “assemble” was used intentionally to describe how the testing and training data were collected: a case-control study. Since the objective is to detect fraud, it is reasonable to have a lot of “yes” cases in the data. The “no” cases serve as a kind of control; they were included specifically to have balance in the data. If data had been collected as a simple random sample of credit card transactions, there would have been many, many more “no” cases than “yes.”\nWith such training data it is easy to build a statistical model with Fraud as the response variable. That model can then be evaluated on the testing data to produce a model output for each row:\nIt’s understandable that a classifier may not have perfect performance. After all, it iss trying to make a prediction based on limited data, and randomness may play a role.\nThere are different ways of making a mistake, and these different ways have very different consequences. One kind of mistake, called a “false positive”, involves a classifier output that’s positive (i.e. the classifier indicates fraud) but which is wrong. The consequence of this sort of mistake in the present example is a customer who has to find another way to pay for gasoline.\nThe other kind of mistake is called a “false negative”. Here, the classifer output is that the transaction is not fraudulent, but in actuality it was. The consequence of this kind of mistake is different: a successful theft.\nThe nomenclature signals that a mistake has been made with the word “false.” The kind of mistake is either “positive” or “negative”, corresponding to the output of the classifier.\nWhen the classifier gets things right, that is a “true” result. As with the false results, a true result is possible both for a “positive” and a “negative” classifier output. So the two ways of getting things right are called “true positive” and “true negative”.\nTabulating all 6000 rows of the testing data might produce something like this:"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-34.html#incidence",
    "href": "Reading-notes/Reading-notes-lesson-34.html#incidence",
    "title": "Math 300R Lesson 34 Reading Notes",
    "section": "Incidence",
    "text": "Incidence"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-34.html#sensitivity-and-specificity",
    "href": "Reading-notes/Reading-notes-lesson-34.html#sensitivity-and-specificity",
    "title": "Math 300R Lesson 34 Reading Notes",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\nExample: Accuracy of airport security screening\n\n\n\nAirplane passengers have, for decades, gone through a security screening process involving identity checks, “no fly” lists, metal detection, imaging of baggage, random pat-downs, and such. How accurate is such screening? Almost certainly, the accuracy is not as good as an extremely simple, no-input, alternative process: automatically identify every passenger as “not a security problem.” We can estimate the accuracy of the “not a security problem” classifier by guessing what fraction of airplane passengers are indeed a threat to aircraft. In the US alone, there are about 2.5 million airplane passengers each day and security problems of any sort rarely happen. So the accuracy of the no-input classifier is something like 99.999%.\nThe actual screening system, using metal detectors, baggage x-rays, etc. will have a lower accuracy. We know this since it regularly mis-identifies innocent people as security problems.\nThe problem here is not with airport security screening, but with the flawed use of accuracy as a measure of performance. Indeed, achieving super-high accuracy is not the objective of the security screening process. Instead, the objective is to deter security problems by convincing potential terrorists that they are likely to get caught before they can get on a plane. This has to do with the sensitivity of the system. The specificity of the system, although important to the everyday traveller, is not what deters the terrorist."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html",
    "href": "Reading-notes/Reading-notes-lesson-20.html",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "",
    "text": "This Lesson introduces two ideas. The first is how to measure variation. This is important, as you can see from the definition of statistical thinking given in the previous Lesson:\nVariation is what we’re trying to explain/describe. To do this, it helps to be able to measure variation.\nThe second idea is also fundamental to statistical thinking. Often, but not always, our interest in studying data is to reveal the causal connections between variables. This is important, for instance, if we are planning to make an intervention in the world and want to anticipate the consequences. Interventions are things like “increase the dose of medicine,” “stop smoking!”, “lower the budget,” “add more cargo to a plane (which will increase fuel consumption and reduce the range).”\nHistorically, statisticians were hostile to the idea of using data to explore causal relationships. The one exception was experiment, where the data come from an actual intervention in the world. (See Lesson 32.) Statistics teachers encouraged students to use phrases like “associated with” or “correlated with” and reminded them that “correlation is not causation.”\nRegretably, this attitude made statistics irrelevant to that part of the real world where intervention was the matter of interest and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists were seeing evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, “smoking is associated with lung cancer.” He speculated that smoking and lung cancer might have a common cause, perhaps genetic. To establish causation, it would be necessary to run an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result.\nFortunately, around 1960 a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. This was one of the first developments in a field called “causal inference”"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html#using-do",
    "href": "Reading-notes/Reading-notes-lesson-20.html#using-do",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "Using do( )",
    "text": "Using do( )\nTHERE’s a matter of the order of precedence of the operations in a pipe. Use the CURLY BRACES in the same way you would use parentheses in an arithmetic expression.\nWHY CURLY BRACES? Because parentheses mean something else in R: either application of a function to arguments or the usual arithmetic meaning."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-36.html",
    "href": "Reading-notes/Reading-notes-lesson-36.html",
    "title": "Math 300R Lesson 36 Reading Notes",
    "section": "",
    "text": "So far, we’ve used only the first three columns of the regression report: the name of the term to which the remaining entries belong, the estimate of the coefficient on that term, and the standard error of that estimate.\nThere are two more columns to go. The fourth column is labelled “statistic” (short for “test statistic”) and the fifth column is the “p-value.”\nIt’s the p-value that concerns us here, the “statistic” is just an intermediate on the way to calculating the p-value. Both are reported because, in some fields people are accustomed to reading the statistic to draw quick conclusions. But in every field, the p-value is used.\nThe p-value is at once incredibly simple to interpret and impossibly difficult to make sense of. This contradiction is the reason many statisticians have called for moving away from the p-value as a summary of a result. We will discuss the reasons for the controversy in Lesson 38. In Lesson 37, we’ll show how the p-value is computed from the test statistic and introduce another report summarizing a model, the “ANOVA report,” which is useful for many purposes.\nIn this Lesson, we’ll explain the “incredibly simple” interpretation of the p-value and the subtle logic behind it. This is important because frequently (pretty close to “always”) people mistake the p-value as addressing a completely different question than the question it actually pertains to. It’s useful to know about this misconception, because it points to a different question that often more directly addresses the needs of researchers and decision makers."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-36.html#incredibly-simple-interpretation",
    "href": "Reading-notes/Reading-notes-lesson-36.html#incredibly-simple-interpretation",
    "title": "Math 300R Lesson 36 Reading Notes",
    "section": "“Incredibly simple” interpretation",
    "text": "“Incredibly simple” interpretation\nAs you will see, the p-value is always a number between zero and one. When the p-value is small, the conclusion is that the corresponding explanatory variable is contributing to explaining the variation in the response variable. That is, a p-value that’s small is justification for believing that there is a connection of some sort between the explanatory variable and the response variable.\n“Small” in the phrase “when the p-value is small” is most usually taken to mean \\(p < 0.05\\). But different fields have different standards for defining small. For instance, it’s common in psychology to consider \\(p < 0.10\\) as fairly small, while in physics, “small” means perhaps \\(p < 0.001\\) or even \\(p < 0.000001\\).\nIt may seem odd that there is no universal agreement about “small.” The reason is that p-values are part of a standard operating procedure for evaluating research results to know if they are worthy of publication.\nIn physics, laws and models are meant to be exact or close to exact. Lord Rutherford (1871-1935), an important physicist who won the Nobel prize in 1908, famously disparaged the use of statistics, reportedly saying, “If your experiment needs statistics, you should have done a better experiment.” This was in an era where the p-value standard operating procedure had not yet been invented. Today, when p-values are common in most fields, Rutherford’s distaste for statistical method is reflected in p-value thresholds like \\(p < 0.000001\\).\nIn other fields such as economics or psychology or clinical medicine, models are sought that are useful but without any expectation that they be exact. (In the 19th and early 20th century, psychologists and economists sometimes used the vocabulary of “law” to describe their findings, but “model” is more appropriate, because, unlike physics, the laws are not strictly enforced!) Often, in economics or psychology or medicine, the size of a sample used to train a model is less than, say, \\(n=100\\). And the units of observation—people or countries, for instance—are different one from the other, quite unlike, say, electrons, which are all the same. Consequently, sampling variation is often an important source of noise, obscuring relationships or even suggesting relationships that are not really there. (See Lesson 31.) This situation—small sample size, variation in observational units, and large sampling variation—would cause many useful findings to go unreported, as would happen if \\(p < 0.000001\\) were the standard. So a less stringent threshold for publication is used, most commonly 0.05."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-36.html#what-is-a-p-value",
    "href": "Reading-notes/Reading-notes-lesson-36.html#what-is-a-p-value",
    "title": "Math 300R Lesson 36 Reading Notes",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nA p-value is the result of a calculation based on data, but also involving a special hypothesis, called the “Null hypothesis.” The Null hypothesis is almost always a statement in line with the claim that “there is no relationship between these variables” or “nothing is going on.” For example, in a study about the effectiveness of a new drug, the Null hypothesis will be that the drug has no effect at all. Another example: In an economics study about the possible relationship between a country’s “corruption index” and interest rates, the Null hypothesis would be “corruption is unrelated to interest rates.”\nPerhaps it is helpful to envision the Null hypothesis as the belief of a skeptical devil, standing on the researcher’s shoulder and constantly whispering to the research that, “this study is useless, a waste of time, the result purely of sampling variation.” Note that in a world where researchers always took the devil’s advice, no study would be done. What motivates a researcher is a belief that the study will indeed produce results that are useful and represent something about the real world other than sampling variation, e.g. a relationship between two variables.\nThe calculation that results in a p-value is done under the assumption that the devil is right. The point is to see if the data are consistent with the devil’s skeptical position. If they are consistent, then the p-value will be large. If not, the p-value will be “small.”\nThe format of a p-value is that of a conditional probability. The condition is that the devil is right. The probability is that of seeing what the data analysis shows—typically summarized as a model coefficient—if the devil were right.\nActually, the probability reported in the p-value is not that of seeing the exact value of the model coefficient shown in the regression report. The probability also includes the events where the coefficient was larger in magnitude than the coefficient. Why? Because larger coefficients are stronger evidence that the devil is not right."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-36.html#the-world-of-the-null-hypothesis",
    "href": "Reading-notes/Reading-notes-lesson-36.html#the-world-of-the-null-hypothesis",
    "title": "Math 300R Lesson 36 Reading Notes",
    "section": "The world of the Null hypothesis",
    "text": "The world of the Null hypothesis\nRecall that the Null hypothesis is the claim that “nothing is going on.” For a regression model, this amounts to saying that “there is no relationship between an explanatory variable and the response variable.” In order to help clarify the description in the previous section, let’s do an example calculation of a p-value. We will use for the example the possible relationship between a car’s fuel economy (mpg) and the maximum horsepower (hp) of the engine.\nA skeptic, such as the imaginary devil from the previous section, might argue this way: “The maximum horsepower is hardly ever used by a car. Instead, the driver throttles the engine so that it generates only that power needed to move the car along under the current conditions: acceleration, speed, wind, slope of the road. The maximum horsepower just affects the range of conditions under which the car can operate. But the fuel economy is based on a standard set of conditions which is the same for every car, regardless of the horsepower.”\nWe will pick up the action at the point where the study has been designed and the design implemented to produce data. For the example, we have the mtcars data frame in hand. As should be familiar at this point, the data are modeled and the model coefficient on the explanatory variable of interest is recorded. Looking at the regression report presented at the beginning of this Lesson, that coefficient is -0.0318 mpg/horsepower.\nThe data were collected in the real world, but that is not the world that’s relevant to the Null hypothesis. The world of the Null hypothesis is one where fuel economy is utterly unrelated to horsepower. To calculate the p-value, we construct a simulation of the Null-hypothesis world. But it is not sufficient for the simulation to generate Null-hypothesis data out of the blue. We want the simulation to be as much like the actual data as possible, except that there is no relationship between mpg and hp.\nPerhaps surprisingly, there is a very simple device for accomplishing this. It involves creating a new variable to use in place of hp in the model, but which is unrelated to mpg. Let’s call this new variable hp_null. We can generate hp_null by taking the values in hp and shuffling them. This randomized version of hp has no relationship to mpg because it is being dealt out to each row of the data frame at random.\nHere’s what the shuffling looks like, pretending for readability that there wee only ten rows in mtcars.\n\nSamp <- mtcars %>% \n  select(mpg, wt, hp) %>%\n  sample_n(size=10) %>%\n  mutate(hp_null = shuffle(hp))\nSamp\n\n                    mpg    wt  hp hp_null\nMerc 450SLC        15.2 3.780 180     230\nPorsche 914-2      26.0 2.140  91      52\nToyota Corolla     33.9 1.835  65     175\nHonda Civic        30.4 1.615  52     175\nCadillac Fleetwood 10.4 5.250 205      91\nPontiac Firebird   19.2 3.845 175     180\nHornet Sportabout  18.7 3.440 175      62\nDatsun 710         22.8 2.320  93      65\nMerc 240D          24.4 3.190  62      93\nChrysler Imperial  14.7 5.345 230     205\n\n\nWe’ll use such data, replacing the actual hp with the shuffled hp, to find the model coefficient on hp. This can be done concisely:\n\nset.seed(112)\nlm(mpg ~ wt + shuffle(hp), data = mtcars) %>%\n  broom::tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  34.2      2.93        11.7  1.65e-12\n2 wt           -4.95     0.626       -7.90 1.03e- 8\n3 shuffle(hp)   0.0120   0.00894      1.34 1.90e- 1\n\n\nIn this trial, the coefficient on the shuffled hp is 0.0120. Of course the coefficient might well be different if the trial were repeated. Let’s run 1000 trials, from each of which we’ll extract the coefficient on the shuffled hp.\n\nTrials <- do(1000) * { \n  lm(mpg ~ wt + shuffle(hp), data = mtcars) %>%\n  broom::tidy() %>%\n  filter(term == \"shuffle(hp)\") %>%\n  select(estimate)\n}\n\nFigure 1 shows the distribution of the shuffled hp coefficient, compared to the coefficient we found from the original, unshuffled data.\n\ngf_jitter(estimate ~ 1, data = Trials, alpha=0.3, width=0.3) %>%\n  gf_violin(color=NA, fill=\"blue\", alpha=0.5, width=0.1) %>%\n  gf_lims(x=c(0,2)) %>%\n  gf_hline(yintercept = ~ -0.03177295, color=\"red\")\n\n\n\n\nFigure 1: The sampling distribution of the shuffled hp coefficient\n\n\n\n\nYou can see in Figure 1 that the coefficient on the shuffled hp is near zero, as would be expected since we enforced hp_null to be unrelated to mpg. Almost always, the coefficients on hp_null are in the interval \\(\\pm 0.02\\). That is to say, even though hp_null is unrelated to mpg, sampling variation will spread out the estimated coefficient away from the ideal of zero. The amount of spread due to sampling variation is \\(\\pm 0.02\\).\nThe estimated coefficient on hp in the original, unshuffled data is shown as a red horizontal line. You can see that this is farther from zero than any of the null-hypothesis trials. Since there were 1000 trials, the extreme nature of the coefficient from the original data let’s us eyeball the probability of that coefficient (or larger) coming out of the Null hypothesis simulation is something on the order of one-in-a-thousand. A detailed calculation—refer to the regression table at the start of this Lesson—puts that probability at \\(p = 0.0015\\)."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-36.html#what-to-conclude",
    "href": "Reading-notes/Reading-notes-lesson-36.html#what-to-conclude",
    "title": "Math 300R Lesson 36 Reading Notes",
    "section": "What to conclude?",
    "text": "What to conclude?\nRemember always that the p-value is a probability calculated in a hypothetical world the world of the Null hypothesis. In the calculation, we are able to place the data in this hypothetical world by shuffling the explanatory variable.\nNo calculation done in the Null hypothesis world is going to tell us whether that hypothesis is correct or not. Nonetheless, that the Null hypothesis simulation did not generate a coefficient as large as that in the actual data suggests that the data are inconsistent with the Null hypothesis, that we can in the case of the hp coefficient regard the Null hypothesis as an implausible candidate to account for the data.\nAlmost always, newcomers to this p-value based scheme of hypothesis testing misinterpret the p-value to be the probability that the Null hypothesis is right. Small p-value would thus mean a small probability that the Null is right.\nBut suppose we want to do a calculation to produce something in the format “the probability that the Null is right?” The probability that decision-makers are usually interested in is the relative conditional probabilities for each of a set of hypotheses of interest. The “condition” under which these probabilities are calculated is, “given the data at hand.” Returning to the notation of Lesson 34, this is \\(p(H | \\text{data})\\), where \\(H\\) stands for each of the hypotheses of interest, say, that a drug has a large effect, a medium effect, no effect at all, or even a negative effect. The framework for calculation is called “Bayesian” statistics, the ideas of which date from the very beginnings of the emergence of statistical method.\nTo illustrate the Bayesian approach, return to Lesson 35 when we were evaluating the performance of classifiers. There, we had two hypotheses that were relevant. In the context of health, these might be \\(H_\\text{sick}\\) and \\(H_\\text{healthy}\\). The quantity of ultimate interest to the patient is p(sick given the test result). To calculate this probability we need to take a round-about route. We first find two completely different probabilities: p(test result given sick) and p(test result given healthy). In practice, these two probabilities are accessible: take a group of sick patients and see what fraction of them have positive tests, and take a different group of people who are healthy and see what fraction of that group have positive tests. With those two probabilities in hand, we take an estimate of the prevalence of sickness: p(sick). Then the probability of clinical interest, p(sick given test result) can be calculated using the Bayesian formula, just as we did in Lesson 35.\nIn contrast, the p-value is a probability in a different format: \\(p(\\text{summary(data)} | H_0)\\). Here, \\(H_0\\), the Null hypothesis, is indeed a specific hypothesis, but not any hypothesis that motivates the research. The quantity “summary(data)” is a particular summary computed from the data, say the sample mean or a regression coefficient.\nThe p-value probability is bound to be confusing on first sight (and, for most people, on second, third, and later sightings). After all, we know exactly what is the “summary(data)”; we just calculated it from the data! The probability of “summary(data)” is therefore 1, at least until you understand what is the event being summarized by the p-value probability.\nFor the p-value, the random event that lies behind the probability is a number generated by a process: Go to the world of the Null hypothesis, that boring world of “nothing happening” or “no relationship between variables.” Figure 2 lays out the different worlds involved in statistical inference using the metaphor of planets.\n\n\n\n\n\n\n\n(a) Planet Alt\n\n\n\n\n\n\n\n(b) The real world\n\n\n\n\n\n\n\n(c) Planet Sample\n\n\n\n\n\n\n\n(d) Planet Null\n\n\n\n\nFigure 2: The four planets of the statistical solar system.\n\n\nWhat motivates the work of collecting and modeling data is a hypothesis about the world. Typically, such hypotheses are simplistic, cartoon-like ideas about the shape of things.\nNaturally, our ultimate interest is in the real world. But we don’t have the whole Earth at hand; we have only a sample from it. The sample is something like the real world, but being a sample it is somewhat patchy, assembled from the \\(n\\) cases in our sample. Planet Sample lacks the detail of the real world, but each point on Planet Sample comes from a genuine place on Planet Earth.\nThe p-value is a probability computed on Planet Null, that boring world where nothing is going on and any perceived patterns are illusions, the appearance produced by random and shifting gusts of the winds of chance.\nAlmost all the work of calculating a p-value takes place on Planet Null. That work consists of simulation trials. Each trial involves taking a sample from Planet Null, summarizing it, and recording the result for later comparison the the summary calculated from Planet Sample.\nIt may seem perverse to base conclusions for real-world data on an imagined planet of no direct interest. And it is! At a minimum, we should put into competition at least two hypotheses: for instance Planet Alt and Planet Null. But in the world of the first half of the 20th century, when statistical analysis of data was just coming into the mainstream, it was impractical to compute the competing probabilities of the Bayesian style of reasoning. The reason: the computers and algorithms we use now had not been invented.\nIn addition, those early statisticians put a big premium on what they called “objectivity.” They did not think the subjective beliefs of researchers—the cartoon alternative hypothesis—should play any role in data analysis. The method they ended up inventing, p-values, was based only on a hypothesis that everyone could agree might be in play: the Null hypothesis. Unfortunately, the only valid conclusions that can be drawn from p-values are 1) “reject the Null hypothesis” and 2) “fail to reject the Null hypothesis.” These conclusions don’t guide us to favor any other particular hypothesis and so are inadequate to support decision-making in the real world. But the p-value conclusions can be the basis for a standard operating procedure: If the conclusion is “fail to reject the Null hypothesis,” don’t allow the work to be published.\nSo, standard operating procedures were based on the tools at hand. We will return to the mismatch between hypothesis testing and the contemporary world in Lesson 38."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-36.html#more-metaphors",
    "href": "Reading-notes/Reading-notes-lesson-36.html#more-metaphors",
    "title": "Math 300R Lesson 36 Reading Notes",
    "section": "More metaphors?",
    "text": "More metaphors?\nUse this from Section 19.4 of Computational Probability and Statistics?\n\nA US court considers two possible claims about a defendant: she is either innocent or guilty. Imagine you are the prosecutor. If we set these claims up in a hypothesis framework, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. Your job as the prosecutor is to use evidence to demonstrate to the jury that the alternative hypothesis is the reasonable conclusion.\n\n\nThe jury considers whether the evidence under the null hypothesis, innocence, is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis).\n\n\nJurors examine the evidence under the assumption of innocence to see whether the evidence is so unlikely that it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person’s innocence. They are simply not convinced of the alternative that the person is guilty.\n\n\nThis is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.\n\n\nThere are two types of mistakes possible in this scenario, letting a guilty person go free and sending an innocent person to jail. The criteria for making the decision, reasonable doubt, establishes the likelihood of those errors.\n\n\nHypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.\n\n\nThere are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized below.\n\n\\[\n\\begin{array}{cc|cc} & & \\textbf{Test Conclusion} &\\\\\n& & \\text{do not reject } H_0 &  \\text{reject } H_0 \\text{ in favor of }H_A  \\\\\n\\textbf{Truth} & \\hline H_0 \\text{ true} & \\text{Correct Decision} &  \\text{Type 1 Error}  \\\\\n& H_A \\text{true} & \\text{Type 2 Error} & \\text{Correct Decision}  \\\\\n\\end{array}\n\\]\n\nA Type 1 error, also called a false positive, is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination (from the Case Study) and the commercial length studies, it is possible that we made a Type 1 error in one or both of those studies. A Type 2 error, also called a false negative, is failing to reject the null hypothesis when the alternative is actually true. A Type 2 error was not possible in the gender discrimination or commercial length studies because we rejected the null hypothesis.\n\n\n\n\n\n\n\nIn DRAFT\n\n\n\nRecast the previous paragraph to tie it to classifiers. Point out that in a hypothesis test, unlike a court, we never “accept the Null hypothesis.” Neither is there any definite notion of “true,” since neither the Null nor the Alternative are strictly speaking correct: they are both models of the world."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html",
    "href": "Reading-notes/Reading-notes-lesson-23.html",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "",
    "text": "Case : a row in a data frame\nSample: a data frame.\nSummarized sample: lm(model, data=dataframe) %>% summary()\n\nThis is as far as we can go with real data. DAG simulation (gaming) let us go further:\n\nSample: a draw of \\(n\\) cases from the DAG: sample(DAG, size=50)\nTrial: a summarized sample: r trial <- function(n=50) {   lm(tilde, data=sample(DAG, size=n) %>% summary() }\nRepeated trials to see sampling:distribution: do(100) * trial()\nSummarize the repeated trials: e.g. standard deviation of trial-by-trial coefficients r     do(100) * trial() %>% summarize(sd = sd(coef_on_x)) We found that the standard deviation of trial-by-trial coefficient\n\n\nIs smaller if \\(n\\) is bigger.\nSpecifically, is proportional to \\(\\frac{1}{\\sqrt{n}}\\).\n\n\n\n\n\n\n\nFormulas for sampling distributions\n\n\n\nStatistics textbooks often give formulas for the standard deviation of the sampling distribution. The formulas have been constructed for many of the standard situations. To give you an idea of how this is done, let’s go over the very simplest situation.\nSystem: \\(\\epsilon \\longrightarrow y\\) with tilde xy~ 1.\nInterpretation: The coefficient on 1, that is, the “intercept” is an estimate of the mean of \\(y\\).\nTHIS IS JUST A SKETCH.\nWhen \\(n=1\\), that is, the mean of a sample of size \\(n=1\\), the standard deviation of the sampling distribution is just \\(sd(y)\\). Of course, we can’t estimate this from a single sample of size \\(n=1\\) because we need at least \\(n=2\\) to calculate a standard deviation. If we knew the DAG behind the data, we could read \\(sd(y)\\) from the DAG. Let’s imagine that we do and use the name \\(\\sigmal\\) for the standard deviation from many trials on the DAG. But if we have \\(n > 1\\), we could calculate the SD from the sample. Let’s call that \\(s\\), our estimate of sigma.\nWe also know that the SD of the sampling distribution scales as \\(\\frac{1}{n}\\).\n\n\n\nSample size\nSD of Intercept coefficient\n\n\n\n\n\\(n=1\\)\n\\(\\sigma\\) as stipulated\n\n\n\\(n=2\\)\n\\(\\sigma/\\sqrt{2}\\) estimated by \\(s/\\sqrt{2}\\)\n\n\n\\(n=3\\)\n\\(\\sigma/\\sqrt{3}\\) estimated by \\(s/\\sqrt{3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(\\sigma/\\sqrt{n}\\) estimated by \\(s/\\sqrt{n}\\)\n\n\n\nGIVE FORMULAS for y ~ 1, y ~ yesno, y ~ x\nThe challenge faced by the traditional statistics student is to look up the correct formula for the situation at hand. But the computer can figure out the right formula directly from the tilde model."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#bootstrapping",
    "href": "Reading-notes/Reading-notes-lesson-23.html#bootstrapping",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#regression-table",
    "href": "Reading-notes/Reading-notes-lesson-23.html#regression-table",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Regression table",
    "text": "Regression table"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#is-sampling-variation-the-issue",
    "href": "Reading-notes/Reading-notes-lesson-23.html#is-sampling-variation-the-issue",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Is sampling variation the issue?",
    "text": "Is sampling variation the issue?\n\nFrom the 2018 StatPREP newsletter\n\nIn 1996 my department chair handed me the first statistics textbook I had ever seen. That single gesture constituted my college’s faculty development program for teaching statistics. One of the earliest examples in the book was about the importance of random sampling. It included a picture of President Truman holding up the Chicago Tribune’s infamous “Dewey Defeats Truman” headline. It’s a good story, but hardly timely, having taken place 48 years earlier. Few of my students knew who Truman was and none of them knew anything about Dewey.\nOur students have grown up in an era of “scientific” polling. Being scientific, the results are reported with a margin of error, often ±3 percentage points, to help us know when conclusions are warranted and when not. Many of our statistics courses feature units on constructing a margin of error on a sample proportion, often with explicit reference to political polls. But, like Dewey defeating Truman, the story is no longer timely. The “error” in the “margin of error” is now only a small part of the unreliability of polls. Why?\nIn an unprecedented opening up of the process of polling, The New York Times is letting us observe, live, their polling for the 2018 mid-term elections. You’ll find a description of the project in a September 2018 column and the live action here. It’s worth watching.\nFor those of you reading this after the polling ends, I’ll describe the action. As I write this, 2,070,469 telephone calls have been made. In each Congressional district, the results from the past calls are laid out in a long line of circles, filled red or blue depending on the the recipient’s response. But only 1 or 2% of the dots are filled. The large majority are empty: no response. Each new call generates a wiggling box at the head of the line of dots. It wiggles until the end of the call. Almost always, the box turns into an unfilled circle.\nThe poll I’m watching now, New Jersey 3rd district, is in its early stage. 4250 calls producing 62 responses. The margin of error? There’s a simple but meaningful statement laid right on top of the grayed-out tally so far: “Don’t take this poll seriously until we reach at least 250 people. We’re at 62.”\nThe calls are made based on a random selection from the phone numbers known to be in the district. But the random selection hardly generates a random sample when the response rate is 2%. To get something that resembles the population, pollsters weight their results. The New York Times is weighting “by age, party registration, gender, likelihood of voting, race, education and region, mainly using data from voting records files compiled by L2, a nonpartisan voter file vendor.” And then there’s the “likely voter” model, an informed guess about what fraction of people in each weighting strata will actually vote. There’s a detailed explanation in this article on the site, where the faulty results from the 2016 presidential election are attributed to a failure to weight by education level.\nSeeing the polling process in such detail reveals our misconceptions about what’s important in statistics. The so-called “margin of error” is not an adequate indicator of the reliability of the poll. Instead, we need to be thinking about the factors used in weighting and the extent to which they capture the current configuration of political schisms. Polls are now about big, multivariable data (the “voting records compiled by L2”) and building models of turnout based on previous elections."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#margin-of-error",
    "href": "Reading-notes/Reading-notes-lesson-23.html#margin-of-error",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Margin of error",
    "text": "Margin of error\n\none_trial <- function(n=2) {\n  vals <- rnorm(n)\n  tibble(m = mean(vals), s = sd(vals))\n}\n\nThe confidence interval from each trial will be \\(m \\pm \\beta s\\), where \\(\\beta\\) is a number yet to be determined. How to do so, we want to select \\(\\beta\\) so that, across all trials, 95% will include the mean of the distribution from which the data values were drawn.\n\n# vary beta until 95% of the trials have a left value smaller than zero.\nn <- 10000\nbeta <- 0.02\nTrials <- do(1000) * one_trial(n=n) %>% \n  mutate(left = m - beta*s, right = m + beta*s) \nTrials %>% \n  summarize(coverage = sum(sign(left*right) < 0)/n())\n\n# A tibble: 1 × 1\n  coverage\n     <dbl>\n1    0.956\n\n\nFor sample size \\(n=10\\), \\(\\beta\\) needs to be 0.72, while for a sample size \\(n=100\\), \\(\\beta\\) needs to be 0.20. For \\(n=1000\\), the multiplier needs to be 0.062, and so on. For \\(n=10000\\), the multiplier needs to be 0.02\n\n\n\nn\n\\(\\beta\\)\n\\(t = \\beta / \\sqrt{\\strut n}\\)\n\n\n\n\n10\n0.72\n2.26\n\n\n15\n0.55\n2.14\n\n\n20\n0.47\n2.09\n\n\n50\n0.28\n2.01\n\n\n100\n0.20\n1.98\n\n\n500\n0.088\n1.96\n\n\n1000\n0.062\n1.96\n\n\n10000\n0.20\n1.96\n\n\n\nNotice that as \\(n\\) gets bigger, the size of \\(\\beta\\) to cover 95% of the trials gets smaller. More than a century ago, it was known that the multiplier for any sample size \\(n\\) is effectively \\(2/\\sqrt{n}\\). Consequently, the confidence interval for the mean of \\(n\\) values is approximately\n\\[\\mathtt{CI} = \\mathtt{mean(x)}\\pm \\underbrace{\\frac{2}{\\sqrt{n}} \\mathtt{sd(x)}}_\\text{margin of error}\\]\nThe quantity following the \\(\\pm\\) is called the “margin of error.” Because of the \\(\\pm\\), he overall length of the confidence interval is twice the margin of error.\nIt’s much easier to remember \\(2/\\sqrt{n}\\) than a list of \\(\\beta\\) values that change from one \\(n\\) to the next. Another ubiquitous memory aid involves another technical term, the standard error. This involves a simple re-arrangement of the equation for the confidence interval:\n\\[\\mathtt{CI} = \\mathtt{mean(x)}\\pm 2\\underbrace{\\frac{\\mathtt{sd(x)}}{\\sqrt{n}}} _\\text{standard error}\\]\nIt’s standard in statistical software to report the standard error of a coefficient. Usually abbreviated se or std.error or something similar. The software is doing the divide-by-\\(\\sqrt{n}\\) for you, so all you need to construct the margin of error is multiply the standard error by 2. That’s convenient, but it comes at the cost of yet another use of the words “standard” and “error,” which can be confusing.\nHere’s an example of a typical software output summarizing a model in the format called a “regression report.” Here’s an example, looking at the fuel economy of cars (mpg) as a function the car’s weight (wt) and horsepower (hp).\n\nlm(mpg ~ wt + hp, data = mtcars) %>% \n  broom::tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  37.2      1.60        23.3  2.57e-20\n2 wt           -3.88     0.633       -6.13 1.12e- 6\n3 hp           -0.0318   0.00903     -3.52 1.45e- 3\n\n\nAccording to this report, each additional 1000 lbs of weight decreases fuel economy by an estimated 3.9 miles per gallon. But since the model is based on a sample of data, it’s important to report the precision of that number in the face of sampling variation. The confidence interval is the standard format for that precision. It will be the estimate plus-or-minus two times the standard error, that is: \\(-3.88 \\pm 2\\times0.633\\), that is, -5.15 to -2.61 mpg per 1000 lbs. Similarly, each addition horsepower (hp) lowers fuel economy by \\(-0.032 \\pm 2 \\times 0.009\\), that is, -0.05 to 0.013 mpg per horsepower.\nEven more convenient is to calculate the confidence interval with confint() which handles all the computations, including the ones for tiny \\(n\\) described in ?@sec-tiny-n.\n\n\n\n\n\n\nHow many digits?\n\n\n\nNotice that the estimate of the wt coefficient in the above regression report is -3.87783074. That seems like an awful lot of digits to report when the confidence interval is -5.15 to -2.61. Or, rather, an awful lot of digits for the human reader.\nIt is of course easy for the human to ignore the last several digits of the number. This makes reading more reliable; there are not as many digits to confuse. Even worse, the many digits suggest a level of precision that is belied by the width of the confidence interval. (When the number is going to be part of a continuing computation, that is, the “reader” is a computer, mis-interpretion or faulty reading is not an issue, which is why the software calculates so many digits .)\nSo how many digits ought to be reported for a human reader? There is an easy procedure to determine this.\n\nLook at the standard error in the regression report and multiply by 2 to get the margin of error. For example, for the hp coefficient, the margin of error is \\(2 \\times 0.63273349 = 1.265467\\).\nIt is always the case that no more than two digits of the margin of error have any meaning. (Even the second digit would suffer sampling variation.) So round the margin of error to two digits, that is 1.3 for the hp standard error.\nNotice the location of the second digit of the rounded standard error. For hp, the second digit is 3 and it’s located in the one-tenths place. Round the coefficient to this place. So, the hp coefficient -3.87783074 will round to -3.9.\nThe confidence interval, formatted for the human reader, will be the rounded coefficient plus-or-minus the rounded standard error. For hp, the confidence interval will be \\(-3.9 \\pm 1.3\\) or -5.2 to -2.6."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#tiny-n-optional",
    "href": "Reading-notes/Reading-notes-lesson-23.html#tiny-n-optional",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Tiny \\(n\\) (optional)",
    "text": "Tiny \\(n\\) (optional)\nWhen you have a very small sample size—say, \\(n=2\\)—the values may coincidentally be very close together. Around 1907, William Gosset, a scientist at Guinness, discovered that such coincidences force \\(\\beta\\) to be much larger than \\(2/\\sqrt{n}\\) in order to produce confidence intervals that cover the mean of the data-generating process. Gosset’s particular interest was in making sense of Guinness’s standard testing protocols, which involve averaging the results from three small batches of beer ingredients. Contacting the leading statisticians of the day, Gosset was told that such small \\(n\\) is “brewing, not statistics.” Nonetheless, Gosset had to work within Guinness’s testing protocols, which were indeed brewing but still needed statistical interpretation.\nGosset carried out something very much like the trials we used above, but—amazingly—by hand, since this was the age before electronic computers. To see the problem he observed, let’s look at the confidence intervals calculated in 1000 trials with \\(n=3\\).\nSHOW THE FOLLOWING FOR n=3, n=2 (the worst case), and n=10\n\nn=10\nbeta <- 2 / sqrt(n)\nTrials <- do(100) * one_trial(n=n) %>% \n  mutate(left = m - beta*s, right = m + beta*s) \ngf_errorbarh(.index ~ left + right, data = Trials, alpha=0.5) %>%\n  gf_errorbarh(.index ~ left + right, \n               data = Trials %>% filter(left > 0 | right < 0)) %>%\n  gf_vline(xintercept = ~ 0, color=\"blue\", inherit=FALSE)\n\n\n\n\nGosset effectively tabulated the \\(\\beta\\) multipliers\n\n\n\nn\n\\(\\beta\\)\n\\(t = \\beta / \\sqrt{\\strut n}\\)\n\n\n\n\n2\n8.98\n12.7\n\n\n3\n2.48\n4.30\n\n\n4\n1.59\n3.18\n\n\n5\n1.24\n2.78\n\n\n6\n1.04\n2.57\n\n\n7\n0.92\n2.44\n\n\n\\(\\vdots\\)\n\n\n\n\n10\n0.72\n2.26\n\n\n15\n0.55\n2.14\n\n\n20\n0.47\n2.09\n\n\n50\n0.28\n2.01\n\n\n100\n0.20\n1.98\n\n\n500\n0.088\n1.96\n\n\n1000\n0.062\n1.96\n\n\n\nYou can see that for \\(n\\) bigger than 10 or 20, the \\(t\\) multiplier is 2. But for very small \\(n\\), the t-multiplier can be considerably larger.\nYou can see the wisdom of brewers here. They made tests by averaging measurements from three small batches of beer. If they had used only two batches, the confidence interval would be almost three times larger than for \\(n=3\\), making it very hard to conclude anything about whether the tests show the ingredients to be within the quality-control standards.\nGosset’s work was published under the pseudonym “Student,” since Guinness forbade employees to publish under their own names. Statisticians, recognizing the value of the work (and knowing the name behind the pseudonym), came to use the name \\(t\\), perhaps because tea was considered more refined than “beer.” In many statistics texts, you will see the phrase “Student t” to refer to how Gosset’s work is used."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-33.html",
    "href": "Reading-notes/Reading-notes-lesson-33.html",
    "title": "Math 300R Lesson 33 Reading Notes",
    "section": "",
    "text": "A probability—a number between 0 and 1—is the most used measure of the chances that something will happen, but it is not the only way nor the best for all purposes.\nAlso part of everyday language is the word “odds,” as in, “What are the odds?” to express surprise at an unexpected event.\nOdds are usually expressed in terms of two numbers, as in “3 to 2” or “100 to 1”, written more compactly as 3:2 and 100:1 respectively. The setting for odds is an even that might happen or not: the horse Fortune’s Chance might win the race, otherwise not; it might rain today, otherwise not; the Red Sox might win the World Series, otherwise not.\nThe format of a probability assigns a number between 0 and 1 to the chances that Fortune’s Chance will win, or that it will rain, or that the Red Sox will come out on top. If that number is called \\(p\\), then the chances of the “otherwise outcome” must be \\(1-p\\). The event with probability \\(p\\) would be reformatted into odds as \\(p:(1-p)\\). No information is lost if we treat the odds as a single number, the result of the division \\(p/(1-p)\\). Thus, when \\(p=0.25\\) the corresponding odds will be \\(0.25/0.75\\), in other words, 1/3.\nA big mathematical advantage to using odds is that the odds number can be anything from zero to infinity; it’s not bounded within 0 to 1. Even more advantageous for the purposes of accumulating risk is the logarithm of the odds, called “log odds.” We will come back to this later."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-33.html#staying-in-bounds",
    "href": "Reading-notes/Reading-notes-lesson-33.html#staying-in-bounds",
    "title": "Math 300R Lesson 33 Reading Notes",
    "section": "Staying in bounds",
    "text": "Staying in bounds\n\n\n\n\n\n\nNote in draft\n\n\n\nMaybe move this to a earlier lesson. Not clear.\n\n\nThe linear models (lm()) we have mostly been using up until now accumulate the model output as a linear combination of model inputs. Consider, for instance, a simple model of fuel economy based on the horsepower and weight of a car:\n\nmpg_mod <- lm(mpg ~ hp + wt, data = mtcars) \nmpg_mod %>% coefficients()\n\n(Intercept)          hp          wt \n37.22727012 -0.03177295 -3.87783074 \n\n\nThese coefficients mean that the model output is a sum. For instance, a 100 horsepower car weighting 2500 pounds has a predicted fuel economy of 37.2 - 0.032*100 - 3.88*2.5=24.3 miles per gallon.1 If we’re interested in making a prediction, we often hide the arithmetic behind a computer function, but it is exactly this arithmetic:\n\nmod_eval(mpg_mod, hp = 100, wt = 2.5)\n\n   hp  wt model_output\n1 100 2.5      24.3554\n\n\nThe arithmetic, in principle, let’s us evaluate the model for any inputs, even ridiculous ones like a 10,000 hp car weighing 50,000 lbs. There is no such car, but there is a model output.2\n\nmod_eval(mpg_mod, hp=10000, wt = 50)\n\n     hp wt model_output\n1 10000 50    -474.3937\n\n\nThe prediction reported here means that such a car goes negative 474 miles on a gallon of gas. That’s silly. One way to deal with such silliness is to restrict the inputs to “reasonable” values.\nOften, a better way to avoid the silliness is to structure the model so that unreasonable outputs—such as negative miles per gallon—cannot happen. Figuring out how to do this draws on mathematical experience. In this case, modeling the logarithm of mpg means that a numerically negative output still corresponds to a positive mpg. If we want the model output denominated in miles-per-gallon rather than logarithmic units, we just need to exponentiate (exp()) the logarithmic output to return to the world of miles-per-gallon:\n\nmod_logmpg <- lm(log(mpg) ~ hp + wt, data = mtcars)\nmod_eval(mod_logmpg, hp=10000, wt=50) %>%\n  mutate(model_mpg = exp(model_output))\n\n     hp wt model_output   model_mpg\n1 10000 50     -21.6327 4.02753e-10\n\n\nThis “trick” of modeling the logarithm of output keeps the model output in bounds so far as mpg is concerned. There will never be a negative mpg output.3 ## Modeling log odds\nWhen a model output is intended to be interpreted as a probability, we have a similar problem. THATS what the LOG-ODDS transformation DOES. WITH LOG-ODDS we can model probability using linear combinations of inputs."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-33.html#probability-as-prediction",
    "href": "Reading-notes/Reading-notes-lesson-33.html#probability-as-prediction",
    "title": "Math 300R Lesson 33 Reading Notes",
    "section": "Probability as prediction?",
    "text": "Probability as prediction?\nDOES IT MAKE SENSE TO FRAME a PREDICTION IN TERMS OF A PROBABILITY? So long as the probability is not exactly zero or one, observing either of the two kinds of events—e.g., yes/no, alive/dead, diseased/healthy—does not contradict the model output. So how can we judge if a model is on-target or not. Or, equivalently, how can we decide which of two models is better.\nIn Lesson 26 we introduced the idea of a prediction interval OUTPUT SHOULD ALMOST ALWAYS (95% of the time) be within the interval.\nThe problem for us now is to create something like the PREDICTION INTERVAL when the model output is a probability.\nINTRODUCE the variance as a measure of uncertainty. The variance of a probability is \\(p(1-p)\\).\n\n\n\n\n\n\nExample: A bookies’ calculations [NEEDS FIXING]\n\n\n\nThe most familiar use of “odds” is in gambling. For instance, a famous song lyric puts the odds of Valentine winning the horse rate “at five to nine.” Less musically, this odds is \\(5/9 = 0.5555\\), but the two-number format makes particular sense for keeping track of bets. Five-to-nine describes a bet of one unit. The second number, 9, specifies the amount the gambler is staking on the outcome. On a loss, the gambler loses that stake. On a win, the gambler gets back the stake and, in addition, gets the amount specified by the first number. So a winner at five to nine would leave the racetrack with an extra $5. But on a loss, the gambler leaves $9 behind.\nA “bookie” is someone who provides a service. You can go to a bookie to lay a bit. In drama, this might be done by telephone: “Lay $90 on Valentine” is all the gambler needs to communicate. No money has to change hands. On a win, the bookie will return $50 to the gambler. On a loss, the gambler has a debt of $90.\nA bookie is not a gambler; he’s an accountant who records numbers. The bookie arranges these numbers so that he makes money. To see this, imagine a horse race including Valentine, Paul Revere, and Epitaph. To start, the bookie specifies odds on each possible outcome, say 5:9 for Valentine, 1:3 for Paul Revere (a favorite!), and 1:2 on Epitaph.\nIf the bookie has a good nose, about a third of the stakes will be bet on each outcome. If not, as new bets come in the bookie raises or lowers the odds to encourage or discourage bets so that the roughly one-third of stakes are placed on each outcome. Suppose at the end of the day that $500 is staked on each of the three outcomes.\nWRONG WRONG WRONG. It needs to work that the winning returned for Valentine has to be less than the stakes on the other horses, and similarly for all horses. So if $100 is bet on Valentine we need $100 staked on the other horses.\nAdded up, these odds are \\(5+1+1=7\\) on the top and \\(9+2+1=12\\) on the bottom. It’s important—for the bookie—that the odds are arranged so that the bottom number is larger than the top number: 12 is larger than 11. Note that this method of adding is simpler than combining fractions. To add the fractions \\(1/2\\) and \\(1/3\\) gives \\(5/6\\). But to combine the odds \\(1:2\\) and \\(1:3\\) gives \\(2:5\\). One more detail is needed for a real-life bookies, taking into account the size of each bet. For instance, a $5 bet at 5:9 would be recorded as 25:45.\nNow the race is run. The winner is … well … from the bookie’s point of view it doesn’t matter who wins."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-33.html#irrationality",
    "href": "Reading-notes/Reading-notes-lesson-33.html#irrationality",
    "title": "Math 300R Lesson 33 Reading Notes",
    "section": "“Irrationality”",
    "text": "“Irrationality”\n[From The Model Thinker, p. 52]\nGain Framing: You have two options\nOption A) Win $400 for certain\nOption B) Win $1000 if a fair coin comes up heads and $0 if tails\nLoss Framing: You are given $1000 and have two options:\nOption a) Lose $600 for certain\nOption b) Lose $0 if a fair coin comes up heads and lose $1000 if tails.\nHyperbolic discounting: see pp 52-43\n“Prospect theory”, Kahneman and Tversky (1979) “Prospect theory: an analysis of decisions under risk,” Econometrica 47(2):263-291 link to paper\n\n\n\n\n\n\nExample\n\n\n\nA subtle modification to the linear model architecture allows the modeller to guarantee that the output will be between zero and one. The modified architecture, called “logistic regression”, is therefore well suited to modeling categorical response variables, where the model output will be interpreted as a probability.\nFigure 1 shows a logistic model of survival as a function of age and smoking status. Notice that in the logistic model, the effect of smoking on survival is negative, particularly for people around age 50. The logistic architecture provides an intrinsic flexibility which avoids the undue influence of the very young and very old, for whom survival is close to 100% or 0 respectively regardless of smoking status.\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nFigure 1: A simple logistic model of survival versus age and smoking status.\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Fraction attributable\n\n\n\nUS Federal law forbids employment discrimination based on age. (There are some exceptions, such as air-traffic controllers, whose mandatory retirement age is 56). In a discrimination lawsuit, data on who was and who was not laid-off was used to construct a model of the probability of layoff. The effect size is, as usual for a probability model, expressed in log odds.\n\nbaseline: risk of 20%, so log odds of of -1.4.\nage over 50, add log odds of 1 ± 0.3\nsoftware engineer, subtract log odds of 0.5 ± 0.25\npaid different from company average, subtract log odds of 0.2 ± 0.1 per $10,000 high than company average.\n\nThese estimates come from a logistic regression model laid_off ~ over50 +  software_engineer + pay_above_average.\n\nFor a laid-off employee over 50, what is the fraction attributable to age?\n\n\n\n\n\n\n\nSolution\n\n\n\nThe baseline risk of being laid off is 20%. For the employee aged over 50 years, the log odds of the risk is -1.4 + 1 ± 0.3, or -0.7 to -0.1. Translating these log odds into probabilities gives a risk of 33% to 47%, with the range reflecting the uncertainty in the effect size from the model. The estimated relative risk (risk ratio) for the employees over 50 ranges from 33/20 to 47/20, that is, from 1.65 to 2.35. The attributable fraction is \\((RR - 1) / RR\\) and therefore ranges from (1.65 - 1)/1.65 to (2.35 - 1) / 2.35 or 40% to 57%.\n\n\n\nWhat fraction of all layoffs can be attributed to age over 50? (Population attributable fraction.) Assume that one-third of the employees are over 50.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-26.html",
    "href": "Reading-notes/Reading-notes-lesson-26.html",
    "title": "Math 300R Lesson 26 Reading Notes",
    "section": "",
    "text": "Which graph, (a) or (b), corresponds to the 50% interval. -A- A 50% interval will be narrower than a 95% interval, so (b) is the 50% interval.\nJudging by eye in (a) and (b), give the top and bottom of the 95% prediction interval and the 50% prediction interval when the distance is 10 km. -A- 50% interval runs from about 670 to 950 seconds. The 90% interval runs from about 500 to 1300 seconds.\n\nDrawing the prediction intervals as bands gives the misleading idea that any value between the top and bottom of the band is equally likely. But values toward the center of the band are much more likely than those toward the edges or outside of the band.\nThe following graphic gives a better idea of the relative probability of each outcome by overlaying several prediction bands: 25%, 50%, 75%, 95%. The darker regions are more likely than the lighter regions.\n\n\n\n\n\n\nAbout how much wider is the 95% band than the 75% band? -A- Almost twice as wide.\nWhat is the probability of an individual outcome being somewhere inside the 95% band, but outside the 75% band? -A- 20%. Events outside the 75% band will be seen only 25% of the time. Events outside the 95% band will be seen only 5% of the time. That leaves 20% for the region between the two bands."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-32.html",
    "href": "Reading-notes/Reading-notes-lesson-32.html",
    "title": "Math 300R Lesson 32 Reading Notes",
    "section": "",
    "text": "A theory is something nobody believes, except the person who made it. An experiment is something everybody believes, except the person who made it.\n\nA graphical causal network is a kind of theory. As a theory, it’s natural for people to be skeptical about results stem from the theory. Experiments are more persuasive. Let’s consider what an experiment looks like when represented by a graphical causal networks.\nIn an experiment, you have some real-world system and a means to intervene physically on at least one of the variables in that system and to read out the response of the system to the intervention. You don’t necessarily know much about the actual structure of the real world system. In Figure 1 the real-world system is shown in the rounded box. The intervention is on X and the output is Y.\n\n\n\n\n\nFigure 1: An experiment is a system in which there is an intervention and an output.\n\n\n\n\nNote that in Figure 1 X is, potentially, affected by other variables in the system.\nIdeally, the experiment is set up to eliminate all other effects on X except the intervention as in Figure 2. And the intervention is done in a way that none of the variables in the system can have any effect on it, for instance by assigning the intervention using a computer random-number generator. The lovely thing about this configuration is that the correct model to capture the effect of X on Y is simply Y ~ X. Whatever different people might believe about the real-world mechanism doesn’t matter. The correct model is always Y ~ X. This is why Einstein’s statement, “An experiment is something everybody believes,” is justified.\n\n\n\n\n\nFigure 2: An ideal experiment is one where the only influence on X is the intervention. Any effect on X or the intervention of the other variables in the system has been eliminated. The input paths to X from C and D that appear in Figure 1 have been deleted by the experimenter. This is not always possible in practice.\n\n\n\n\nBut there is another part to Einstein’s statement: “… except the person who made it.” Why shouldn’t the experimenter believe her own experiment? The experimenter might know that she didn’t or couldn’t conduct an ideal experiment. She wasn’t actually able to eliminate the arrows D \\(\\rightarrow\\) X and C \\(\\rightarrow\\) X. The other variables in the system might also be influencing X as in Figure 1. In this situation, the right model may not be Y ~ X. In fact, for the particular system shown in Figure 1 the correct model would be Y ~ X + C + D. But how could the experimenter know this for sure if she didn’t know all about the real-world mechanism?\nIt turns out that for either of the causal systems in Figures 1 there is always a correct model to show the link between the intervention and output: Output ~ Intervention. Rather than modeling the output by the physical quantity X, model the output by the random numbers generated by the computer that were used to set the intervention. This modeling approach is called intent to treat.\nTypically, experiments are done using a specially constructed system that is thought to resemble the system on which the intervention will actually be done. Insofar as the experimental system does resemble the real-world system, the experimental results will anticipate the effect of the real-world intervention. But often it’s hard to establish that the experimental system is a match to the system on which the real-world intervention will be applied. As such, subjective belief is still a factor in accepting that the experiment will be informative about the real-world systems we work with."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-24.html",
    "href": "Reading-notes/Reading-notes-lesson-24.html",
    "title": "Math 300R Lesson 24 Reading Notes",
    "section": "",
    "text": "SHOW makeFun() applied to a model producing a model function.\nEffect size of an input is partial derivative of model function with respect to that input.\nEffect size is a rate: the change in output per unit change in input. It’s a measure of the size of a relationship."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-30.html",
    "href": "Reading-notes/Reading-notes-lesson-30.html",
    "title": "Math 300R Lesson 30 Reading Notes",
    "section": "",
    "text": "The previous chapters introduced techniques for modeling a response variable as a function of explanatory variables. Each model is a machine for turning inputs into outputs. Change the input and the output will change correspondingly. But this does not mean that nature works in the same way. Changing an input in the real world – administering polio vaccine, eating organic food, providing bed nets to the poor – may not cause the same change in the response variable as happens when you change the input to a model.\nThe key word here is “cause.”\nStatisticians are careful to distinguish between two different interpretations of relationship: “correlation” and “causal.” Every successful prediction model Y ~ X is a demonstration that there is a correlation between the response Y and the explanatory variable X.1 But the performance of the model does not itself tell us that X causes Y in the real world. There are other possible configurations that will produce a correlation between X and Y. For instance, both X and Y may themselves have a common cause C without X being otherwise related to Y. In such a circumstance, a real-world intervention to change X will have no effect on Y. To put this in the form of a story, consider that the start of the school year and leaves changing color are correlated. But an intervention to start the school year in mid-winter will not result in leaves changing color. There’s a common cause for the school year and colorful folliage that produces the relationship: the end of summer.\nThis chapter considers simple networks of causality involving three variables, generically called X, Y, and C. Always, we’ll imagine that the modeler’s interest is in anticipating how an intervention to change X will create to a change in Y. To accomplish this, the modeler has two basic choices for structuring a model, either\n\nY ~ X, or\nY ~ X + C.\n\nIt’s surprising to many people that models (1) and (2) can have utterly different, even contradictory implications for how a change in model input X will produce a change in the model output Y. To the modeler trying to capture how the real world works, there’s a fundamental choice to be made between using model (1) or model (2) to anticipate the consequences of a real-world intervention on X.\nConsider this hypothesis: “It’s harder to learn to drive as you move into your 20s.” The hypothesis might or might not be true. The way such hypotheses are formed is often by anecdote. Say, you’re having dinner when the conversation turns to a friend who has been learning to drive in her 30s. She explains that even after taking many lessons last year, she failed her driving test twice. Others at the table, who started driving in their teens, learned in a much shorter amount of time.\nThe hypothesis suggests a practical recommendation: It will be easier to learn to drive when younger, so better to start young. Such recommendations to take an action are always rooted in causality: starting young will cause you to have less difficulty learning to drive. A diagram, or graphical causal model, representing the causal hypothesis is seen in Figure 1.\n\n\n\n\n\nFigure 1: A simple graphical causal model expressing the hypothesis that age when learning to drive is the causal factor for the difficulty of learning to drive.\n\n\n\n\nThe direction of the arrow in fig-learn-to-drive-1 is a crucial feature. The diagram asserts that a person’s age when learning to drive causes difficulty, as opposed to the other way around.\nSo, does learning to drive when young make it easier to succeed? You might collect some data, perhaps a survey asking people at what age (if ever) they learned to drive (X) and how difficult it was (Y). Suppose you build a successful model Y ~ X. This establishes a correlation X and Y (and vice versa), confirming the dinner table anecdote.\nIf you were undertaking serious study of the hypothesis, you should consider how other factors might influence the situation. For instance, it might be that people who learn to drive in their 30s were more anxious about driving when in their teens. That anxiety is why they didn’t learn in their teens. The anxiety might also influence the drivers perception of the difficulty learning. Such a situation is expressed in the graphical causal models in Figure 2.\n\n\n\n\n\nFigure 2: Two possible graphical causal models of the hypothesis, “Age causes difficulty learning to drive,” which do not incorporate a direct link from age to difficulty learning. Left: Anxiety itself leads to people deferring learning to drive, and to increased difficulty when learning. Right: Age is causally linked to difficulty, but the issue is really the diminished support available to older learners.\n\n\n\n\nAnother possibility is that older learners have busier lives and less support for learning to drive. (Figure 2(right)) It’s harder for older learners to schedule opportunities to practice or to find car owners who can help them learn.\nThere is nothing inevitable about graphical causal networks. We can, and often do, intervene in ways that alter the causal flow. For example, Figure 3 shows the network when a later-in-life friend steps in to support a mature student.\n\n\n\n\n\nFigure 3: Intervening in a system can change the structure of the graphical causal network. Here, a friend has stepped in to provide the support needed to learn to drive, severing the link that previously connected age when learning to support. (That link – now severed – reflects the kind of learning support often available to teenage students of driving, but not to older learners.)\n\n\n\n\n\n\n\n\n\n\nCausal Caution\n\n\n\nJust because you’ve calculated an effect size doesn’t mean that you have captured any sort of causal relationship between the variables. To illustrate, use dag01 and fit two different models: y ~ x and x ~ y.\n\nSample <- sample(dag01, size=500)\nlm(y ~ x, data = Sample)\n\n\nCall:\nlm(formula = y ~ x, data = Sample)\n\nCoefficients:\n(Intercept)            x  \n      3.998        1.525  \n\nlm(x ~ y, data = Sample)\n\n\nCall:\nlm(formula = x ~ y, data = Sample)\n\nCoefficients:\n(Intercept)            y  \n     -1.845        0.464  \n\n\nYou can’t tell from these coefficients whether x causes y or vice versa (or something entirely different). The words “correlation” or “association” are used when we don’t want to claim that there is a causal connection. Many statisticians will only use those words unless the data come from an experiment.\nWe’re going to use causal language (“relationship”, “effect,” etc.) because that is often the matter of concern to decision making. But using language doesn’t doesn’t make the connection causal.\n\n\n\n\n\n\nFootnotes\n\n\n“Successful” means that the prediction performance of the model is better than the performance of a no-input model.↩︎"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-31.html",
    "href": "Reading-notes/Reading-notes-lesson-31.html",
    "title": "Math 300R Lesson 31 Reading Notes",
    "section": "",
    "text": "Example: The flu vaccine\n\n\n\nAs you know, people are encouraged to get vaccinated before flu season. This recommendation is particularly emphasized for older adults, say, 60 and over.\nThe benefits of the flu vaccine have been extensively studied. It’s been found based on medical records that older adults who are vaccinated have a lower mortality rate than unvaccinated older adults. This is certainly a correlation between vaccination and (lower) mortality, but is there necessarily a causal connection.\nIn 2012, the Lancet, a leading medical journal, published a systematic examination and comparison of many previous studies. (Such a study of earlier studies is called a meta-analysis.) The Lancet article describes a hypothesis that existing flu vaccines may not be as effective as was originally found.\n\nA series of observational studies undertaken between 1980 and 2001 attempted to estimate the effect of seasonal influenza vaccine on rates of hospital admission and mortality in [adults 65 and older]. Reduction in all-cause mortality after vaccination in these studies ranged from 27% to 75%. In 2005, these results were questioned after reports that increasing vaccination in people aged 65 years or older did not result in a significant decline in mortality. Five different research groups in three countries have shown that these early observational studies had substantially overestimated the mortality benefits in this age group because of unrecognized confounding. This error has been attributed to a healthy vaccine recipient effect: reasonably healthy older adults are more likely to be vaccinated, and a small group of frail, undervaccinated elderly people contribute disproportionately to deaths, including during periods when influenza activity is low or absent."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html",
    "href": "Reading-notes/Reading-notes-lesson-25.html",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "",
    "text": "Use mod_eval()"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#a-model-as-function",
    "href": "Reading-notes/Reading-notes-lesson-25.html#a-model-as-function",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "A model as function",
    "text": "A model as function\nInconvenient because functions don’t know how to translate from the individual arguments to values taken from a data frame."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#feeding-inputs-into-a-model",
    "href": "Reading-notes/Reading-notes-lesson-25.html#feeding-inputs-into-a-model",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "Feeding inputs into a model",
    "text": "Feeding inputs into a model"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#drawing-a-model-function",
    "href": "Reading-notes/Reading-notes-lesson-25.html#drawing-a-model-function",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "Drawing a model function",
    "text": "Drawing a model function\nEvaluate the model at many inputs, then plot out model_output versus input.\nmod_plot() automates this."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#an-example-extracting-information-from-residuals",
    "href": "Reading-notes/Reading-notes-lesson-25.html#an-example-extracting-information-from-residuals",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "An example extracting information from residuals",
    "text": "An example extracting information from residuals\nAmazon discounts it’s books, but the discount is a smaller percentage for low-priced books. You can see this by building a model of log prices, extracting the most negative residuals, then looking at the prices of these negative-residual books.\nIn the PREDICTION lessons, build a model of list price versus amazon price, possibly using logarithms. Are the books with the most negative residuals different in some way from the others? (I think that they are mostly low-price books.)\n\noffsets <- lm(log(list_price) ~ log(amazon_price), data = amazon_books) %>% residuals()\nbad_deals <- which(offsets < 0.4)\namazon_books$list_price[bad_deals]\n\n  [1]  15.00   1.50  15.99  30.50  28.95  20.00  15.00  35.00  30.00  17.00\n [11]  17.00  19.95  20.00  39.95  18.99   5.95  15.00   7.50  15.95  13.00\n [21]  11.94  14.00  14.95  22.99  15.00  18.00   7.95  14.95  14.95   7.95\n [31]  15.95  20.00  14.00  29.50  11.99  13.07  13.95  15.00   4.99  13.00\n [41]  13.99  24.95  26.99  14.95   6.69   5.99  13.95  26.95  24.00  16.00\n [51]  13.99  17.99  15.00   8.99   6.99  14.54  17.99 114.95  48.20  15.95\n [61]  15.95  18.00  86.95  15.00  17.99  15.00  11.50  26.95   6.99  15.00\n [71]  16.00  53.95  15.00   6.99  12.00  21.10  13.95  14.99  30.00   7.99\n [81]   8.00  15.99  26.99  17.00   6.99  14.95  18.00  16.00   6.99  16.00\n [91]   8.99  16.00  19.99  18.00   8.99  17.00  14.00  27.95 139.95  24.95\n[101]   6.95  26.00  26.00   9.95  27.99  16.99   8.95  14.95  15.95  21.00\n[111]  14.00  20.00  15.00  14.95  15.95  14.99  14.00  12.95  24.99  15.00\n[121]  15.00  25.95  25.00   9.99  26.99  17.99  15.00  15.00  25.05  17.99\n[131]  16.00  14.99  20.00   7.99  14.00  13.29  16.00  13.83  12.00  14.95\n[141]  15.00  16.99  15.00   6.95  14.00  15.00  20.00  16.00  15.00  16.00\n[151]  15.00  15.00  16.00   8.95   5.99  16.00  15.00  14.00  35.95  14.99\n[161]   7.95  16.99  98.95  35.75  19.73  37.50  13.00  15.95  26.00   5.99\n[171]  14.99  16.00   9.99  15.95  14.99  14.00  13.95  16.00  75.00  32.95\n[181]  26.95  15.99  15.00  24.00   6.95  16.00  16.00  26.95  26.99   7.99\n[191]  17.95  35.75   6.99   9.99   6.99  15.00  12.00   9.99     NA  11.99\n[201]  15.00  24.00  16.00  16.00  12.14   7.75  14.10   4.99  13.95  22.00\n[211]  12.99  13.99   8.95   8.99   7.95  15.00  17.00  13.00  12.99  70.80\n[221]  48.20   9.99  30.00  15.00  14.00  16.99  22.00   8.42   9.99  15.00\n[231]  14.95  18.99  10.95  16.95  28.00  25.00  15.00  15.00  15.00  17.00\n[241]  13.95  16.00  11.00   8.99  17.99  16.00  15.00  12.00  17.99  21.99\n[251]  15.95  24.95  39.95  16.00  10.65  13.99  19.95  14.00  15.00  16.95\n[261]   9.99  14.95  11.00  17.00  14.95  12.00   9.99  25.00  14.00  17.99\n[271]  30.00  16.99  34.80  15.95   7.77   3.95   2.00   8.99  18.95  20.00\n[281]  20.00  15.44  22.00  14.95  17.99  16.00  14.95  25.00  14.95  26.00\n[291]  24.99  20.00  13.00  13.99  15.99  12.60  10.99  24.99  28.00  35.00\n[301]  15.95   6.99  14.95  14.95  25.00  27.00  15.00  15.00  15.95  14.95\n[311]  26.00  18.99  12.95"
  },
  {
    "objectID": "objective-list.html",
    "href": "objective-list.html",
    "title": "List of objectives",
    "section": "",
    "text": "19.1 Distinguish between the two settings for decision-making:\n\nPrediction: predict an outcome for an individual\nRelationship: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\n19.2 Given a research question, identify whether it corresponds to a prediction setting or a relationship setting.\n\n20.1. Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\n20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples.\n\n21.1 Determine whether a proposed graph is directed and acyclic.\n21.2 Having selected a response and one or more explanatory variables, identify other DAG nodes as covariates.\n21.3 Generate data from simulations and use the data to model the relationships.\n\n22.1 Implement on the computer a procedure to generate a sample, calculate a regression model, and produce a summary.\n22.2 Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\n22.3 Graphically display the distribution of summaries and generate a compact numerical description of the sampling distribution.\n\n23.1 Use bootstrapping to estimate sampling variation.\n23.2 Infer sampling variation from a regression table: “standard error” of a model coefficient.\n23.3 Construct and interpret confidence intervals on a model coefficient and relate the interval to the sampling distribution.\n23.4. Understand and use scaling of confidence interval length as a function of \\(n\\).\n\n24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation.\n\n25.1 Given a sample from a DAG simulation, construct a predictor function for a specified response variable.\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error.\n\n26.1 In evaluating a model function, generate a prediction interval.\n26.2 Interpret prediction bands as a series of intervals, one for each value of the model input.\n26.3 Identify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t.\n\nThis is a QR day.\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)\n\n29.1 Correctly define “covariate”.\n29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n\n30.1 Identify confounding in a DAG\n30.2 Choose whether to include covariate depending on form of DAG\n\n31.1 Distinguish “common cause” and “collider” forms of DAG.\n31.2 Construct appropriate DAG to match a narrative hypothesis.\n\n32.1 Properly use nomenclature of experiment.\n32.2 Correctly re-draw DAG for an ideal experimental intervention.\n32.3 Use blocking to set assignment to treatment or control.\n\n33.1. Distinguish between absolute and relative risk and identify when a change in risk is being presented as absolute or relative.\n33.2. Calculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\n33.3. Interpret effect size as stated in log odds.\n\n34.1. Build a classifier from case-control data.\n34.2. Cross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\n34.3. Calculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome.\n\n35.1 Explain why case-control data may not give an proper measure of “prevalence.”\n35.2 Understand sensitivity and specificity as conditional probabilities.\n35.3 Calculate false-positive and false-negative rates for a given prevalence.\n\n36.1 Understand and use properly hypothesis testing nomenclature: test statistic, sampling distribution under the null, Type-1 and Type-2 error, rejection threshold, p-value\n36.2 Contrast hypothesis testing versus Bayesian framework.\n\n37.1 The permutation test\n37.2 Interpret correctly from regression/ANOVA reports\n37.3 Traditional names for hypothesis tests in different “textbook” settings.\n37.4. Distinguish between p-value and effect size, that is, “significance” and “substance.”\n\n38.1 Identify signs of false discovery in a research paper.\n38.2 Estimate how overall p-value should change when study is replicated."
  },
  {
    "objectID": "lessons.html",
    "href": "lessons.html",
    "title": "Math 300R day-by-day Lessons",
    "section": "",
    "text": "See Fall 2022 repository.\nData, graphics, wrangling\n\nData with R\nScatterplots\nLinegraphs, histograms, facets\nBoxplots and barcharts\nfilter and summarize\ngroup_by, mutate, arrange\njoin, select, rename, & top n\nImporting data\nCase study/review\nGR1 (chapters 1-4)\n\nRegression\n\nSLR: Continuous x\nSLR: Discrete x\nSLR: Related topics\nMultiple regression: Numerical & discrete\nMultiple regression: Two numerical\nMultiple regression: Related topics\nMultiple regression: Conclusion/review\nGR 2 (chapters 5-6)\n\nThere are already learning checks associated with these first 18 chapters. Some additional ideas for learning checks are in this document."
  },
  {
    "objectID": "lessons.html#new-lessons",
    "href": "lessons.html#new-lessons",
    "title": "Math 300R day-by-day Lessons",
    "section": "New lessons",
    "text": "New lessons\nVariation\n\nStatistical thinking NTI : Objectives : LC : Reading : Student notes\nMeasuring and simulating variation NTI : Objectives : LC : Reading : Student notes\nSignal and noise NTI : Objectives : LC : Reading : Student notes\nSampling variation NTI : Objectives : LC : Reading : Student notes\nEstimate sampling variation from a single sample NTI : Objectives : LC : Reading : Student notes\nEffect size NTI : Objectives : LC : Reading : Student notes\nMechanics of prediction NTI : Objectives : LC : Reading : Student notes\nConstructing a prediction interval NTI : Objectives : LC : Reading : Student notes\nGR 3 (Lessons 19-26)\n\nInference\n\nCovariates NTI : Objectives : LC : Reading : Student notes\nCovariates eat variance NTI : Objectives : LC : Reading : Student notes\nConfounding NTI : Objectives : LC : Reading : Student notes\nNon-causal correlation NTI : Objectives : LC : Reading : Student notes\nExperiment and random assignment NTI : Objectives : LC : Reading : Student notes\nMeasuring and accumulating risk NTI : Objectives : LC : Reading : Student notes\nConstructing a classifier NTI : Objectives : LC : Reading : Student notes\nAccounting for prevalence NTI : Objectives : LC : Reading : Student notes\nHypothesis testing NTI : Objectives : LC : Reading : Student notes\nCalculating a p-value NTI : Objectives : LC : Reading : Student notes\nFalse discovery with hypothesis testing NTI : Objectives : LC : Reading : Student notes\nGR 4 (lessons 28-38)\nReview"
  },
  {
    "objectID": "layout.html",
    "href": "layout.html",
    "title": "Layout of this site",
    "section": "",
    "text": "As of October 2022, the revisions are for lessons 19-37. Revisions, if any, to earlier lessons may be added. Eventually, the revised lessons and the unrevised lessons should be consolidated into a single site. This might occur in December 2022.\nThere are four main directories. In each directory, the contents are split up on a lesson-by-lesson basis.\n\nObjectives: Student-facing learning goals that make explicit the skills and understandings that students are expected to develop in the course of each lesson. These files are formatted in a specific manner that provides an ID to each objective. Software reads the files so that each objective can be referred to in any document without duplication. That is, the files in this Objectives directory are the source of truth for the objectives for each of the revised lessons. Any edits or additions to the objectives must be performed in the files in this directory.\nNTI: Lesson-by-lesson “Notes To Instructors.” These are intended to guide instructors through each lesson. They usually contain references to the objectives stored in the Objectives directory. In draft form, the NTIs typically contain notes that are to be moved eventually to the “Reading notes.”\nLC: Like Objectives, this directory contains lesson-by-lesson exercises, styled “learning checks” in the style of the Statistical Inference via Data Science textbook. Not all the learning checks need to be assigned. Which ones are assigned for each lesson will be noted in the NTIs.\nReading-notes: Textbook-like readings, also organized lesson-by-lesson (as opposed to the chapter organization typically found in textbooks.) Often, several lessons in sequence refer to the same statistical topic. Nonetheless, the reading notes are divided on a lesson basis. It is anticipated that these will be provided to students in an on-line format.\nStudent-notes: “Student notes” refers to a set of student-facing Rmd files, one for each Lesson. They are usually based on the Learning Challenges Students are expected to work through the notes, answering the questions by completing code chunks and writing short free response answers. A good daily assignment would be complete the notes and compile them to PDF format."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "",
    "text": "This site holds the proposal for the Spring 2023 version of Math 300."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Background",
    "text": "Background\nUp through Spring 2022, Math 300 was organized around the Moore and Notz textbook: Statistics: Concepts and controversies 10/e. This book was designed for a non-technical audience of “consumers of statistics” but is dramatically outdated. For instance, it has no data science content and introduces only primitive statistical methods. A course with such shortcomings seems inappropriate for cadets going on to be officers who will inevitably have to work with modern data and methods.\nIn Fall 2022, Math 300 switched to a very different book, Ismay and Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. The ModernDive book introduces computing on data in an accessible but modern way. It is the only well-known statistics text based on a data-science perspective. Nonetheless, the statistical inference portions of the book regress to the same sort of primitive statistical methods from Concepts and Controversies.\nTo support the Fall 2022 course using ModernDive, a complete set of roughly 35 Notes to Instructors (NTI) was written by Prof. Bradley Warner along with problem sets and other needed materials and deployed for the course.\nThis proposal is for additional improvements to Math 300, building on the Fall 2022 course but replacing the statistical inference portions of the course with more contemporary and general-purpose inference techniques and support for concepts and methods relevant to decision-making.\nIn the following, I refer to three different versions of Math 300:\n\nThe Fall 2022 version of the course, using the ModernDive book, will be called Math 300.\nThe previous version of the course, as taught for several years before Fall 2022, will be called 300CC, which refers to the textbook then used, Concepts & Controversies.\nThe course proposed in this document, a revision of part of Math 300, will be called Math 300R. The R stands for “revised.”"
  },
  {
    "objectID": "index.html#overall-goals-of-math-300r",
    "href": "index.html#overall-goals-of-math-300r",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Overall goals of Math 300R",
    "text": "Overall goals of Math 300R\nThe design of a course revision needs to take into account several factors:\n\nThe target audience’s anticipated technical ability and motivation and, therefore, the appropriate pedagogy and the balance between theory and practice to use in the course.\nInstitutional goals that inform the prioritization of the topics included in the course.\nConstraints of class time and internal coherence of the course, that is, using later topics to reinforce student learning of the earlier topics.\n\nLater, in the rationale section section of this document, I describe how I came to the following conclusions, but for now, a simple statement will suffice.\n\nThe target audience is humanities and social science majors, many of whom will not be confident in the use of calculus but all of whom have had previous exposure to R in the core calculus course.\nInstitutional goals (as revealed by discussions with humanities and social science departments and the wording of the catalog description of Math 300) include a substantial emphasis on data science techniques (data wrangling and visualization) and the use of statistical concepts and methods to support decision-making."
  },
  {
    "objectID": "index.html#statistical-topics-and-framework",
    "href": "index.html#statistical-topics-and-framework",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Statistical topics and framework",
    "text": "Statistical topics and framework\nTransitioning from Math 300CC to Math 300 has already accomplished many data science goals. This proposal centers on the statistical topics/methods to be covered and the path through them.\nThe class-time demands of the new emphasis on data science techniques in Math 300 (and retained in Math 300R) dictate that the statistical concepts and methods be taught more compactly than in Math 300CC. Low-priority, legacy topics from Math 300CC should be dropped. (The GAISE report provides some guidance here.) We can use to advantage that students in Math 300 already see many modeling-related topics in Math 141Z/142Z. Since students already have a background in model-building and computing, we can choose statistical topics that relate well to decision-making.\nA traditional path for statistical methods starts with descriptive statistics (e.g., standard deviation) and then presents “1-sample” statistics (e.g., mean, proportion) and inferential techniques (confidence interval, hypothesis test) in that context. Next comes the inferential techniques for the analogous “2-sample” statistics (difference in mean, difference in proportion), followed by inference techniques for regression.\nThis path is unnecessarily long for our students since regression encompasses all the traditional methods.1 Framing statistical inference in the context of regression avoids the need to teach method-specific calculations or cover the variety of formats for non-regression test results. Regression is part of the data scientist’s standard toolbox and relates well to more advanced data techniques such as machine learning. The ModernDive textbook uses regression as the segue from the first block (about data wrangling and visualization) to the third block (about inference).\nAdditional streamlining comes from motivating statistical inference using a simulation approach. Simulation draws on two conceptually simple data operations: resampling and permutation (shuffling). This approach is well established in the statistics community and is considered by many (including the ModernDive authors) to be a better pedagogy than the traditional formula-and-distribution presentation of statistical methods. Since Math 300 (and 300R) students will already have worked with wrangling and visualization, they will be well prepared to work with the data generated by repeated simulation trials.\nThe focus on decision-making in 300R appears in the addition of new concepts and techniques treated minimally in traditional statistics courses. These include risk, prediction (and its close cousin classification), causality, and confounding. Introductory epidemiology courses provide a model for teaching about risk, causation, and confounding. The pedagogy for these topics in Math 300R comes from the epidemiology course I introduced at Macalester. In addition, Math 300R draws on my decade of experience teaching causality as part of an introductory statistics course. (See the causation chapter of my Statistical Modeling text.)\nThe Statistical Modeling pedagogy for causality uses directed acyclic graphs (DAGs) and causal simulations based on them. Unlike resampling and permutation, which re-arrange existing data, the DAG simulations generate synthetic data with specified properties (such as effect sizes). Simulations allow a concrete demonstration of the extent to which regression techniques can and cannot recover causal information from data.\nThe DAG-simulation approach lends itself naturally to the demonstration of statistical phenomena such as sampling variation and estimation of prediction error. As an example, consider the statistical fallacy of regression to the mean, as with Galton’s finding about comparing children’s heights to their parents’. The natural hypothesis that heights are determined by genetic and other factors is represented by this DAG:\n\\[\\epsilon \\rightarrow parent \\longleftarrow GENES \\longrightarrow child \\leftarrow \\nu\\]\nIn this DAG there is no causal mechanism included for “regression to the mean.” However, Galton’s empirical finding is replicated by data from the DAG-simulation."
  },
  {
    "objectID": "index.html#sec-broad-structure",
    "href": "index.html#sec-broad-structure",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Scope of the proposed changes",
    "text": "Scope of the proposed changes\nMath 300R will retain the first 17 lessons of Math 300. All teaching materials for this part of the course will be used unaltered. (Exception: revisions to Math 300 the Fall 2022 teaching team deems appropriate. Such revisions are not part of this proposal.)\nThe following 19 lessons are entirely refactored and based on new readings, NTIs, exams, and other materials. Objectives for each of these 19 lessons are itemized here.\n\nThe corresponding ModernDive chapters are not used in Math 300R.\nThe software used is the same as that used in the first half of the ModernDive book, specifically the ggplot2 graphics package and the tidyverse data wrangling packages. However, the infer package used in the second half of ModernDive is dropped.\n\nThe theme of the refactored 19 lessons is “informing decisions with data.” Statistical approaches that can inform decision-making include anticipating the impact of interventions, predicting individual outcomes, and the quantification of risk. These are all included in Math 300R.\nTopics to be de-emphasized are the algebra of computing confidence intervals and p-values and the (controversial) role of p-values as a guide to practical “significance.” About half of a traditional course is about the construction of confidence intervals in various settings and, more or less equivalently, the conversion of data into p-values. However, in the contemporary era, when “observational” data are collected en masse, p-values can become very small (“significant”) even when the relationship under study is slight and insubstantial.\nConfounding and methods for dealing with it (statistical adjustment, experiment) are treated substantially in Math 300R. Decision-making about interventions often relies on understanding causal effects. The possibility of confounding is a major source of skepticism about making causal judgments. In a world where much data is observational, the sweeping principles that “correlation is not causation” and “no causation without experimentation” do not support making responsible conclusions about causal connections and the need to make decisions even when data cannot provide a definitive answer. Decision-makers need this support."
  },
  {
    "objectID": "index.html#rationale",
    "href": "index.html#rationale",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Rationale for course revisions",
    "text": "Rationale for course revisions\n\nRelationship to Math 357 and Math 377\nDFMS offers three courses satisfying the statistics component of the Academy’s core requirements: Math 300, Math 357, and Math 377. In designing 300R, attention should be paid to the reasons for supporting three distinct courses. The catalog copy lays out the differences in terms of intended student major, software, mathematical background, and orientation to data science.\nIntended student major: The catalog says, “Math 300 is designed primarily for majors in the Social Sciences and Humanities.” while “Math 356 is primarily designed for cadets in engineering, science, or other technical disciplines. Math majors and Operations Research majors will take Math 377.” Math 377 is also the intended course for prospective Data Science majors, although this is not in the catalog.\nSoftware: The catalog does not describe any software component for either Math 300 or Math 357, but states that, in Math 377, “modern software appropriate for data analysis will be used.” In reality, as of Fall 2022, much the same software is used in all three courses: R with the dplyr package for data wrangling, ggplot2 for data visualization, and “R/Markdown” for creating computationally active documents.\nOne difference between Math 300 and 357/377 relates to computer programming. Both 357 and 377 include content about the underlying structure of the R language, object types, the construction of functions, and arrays and iteration. In contrast, Math 300 is based on a small set of command patterns using data frames. Students see R in Math 300 more or less as an extension of what they learned in 141Z/142Z; what’s added is a few statistical and data-wrangling functions and a handful of new graphics types.\nStudents’ mathematical background: Math 377 explicitly refers to “calculus-based probability.” Math 300 and 357 share identical catalog copy, though in reality Math 357 and Math 377 use the same textbook. Calculus is indeed necessary for the probability topics in Math 357 and 377. My interpretation is that Math 300 should serve as a safe haven for those who lack confidence in their calculus skills. Both the Fall 2022 edition of Math 300 and the proposed Math 300R serve this role as safe haven.\nOrientation to Data Science: Starting with the Fall 2022 edition, Math 300 develops and draws on data-science skills for wrangling and visualization. In this, the new Math 300 is in line with both Math 357 and 377.\nThe above analysis indicates that Math 300 and 300R should diverge from Math 357/377 in these ways:\n\nMath 300R should make little or no use of calculus operations.\nMath 300R should include little consideration of probability distributions or (non-automated) calculations with any but the simplest.\nMath 300R should be computational, but should not draw heavily on computer programming skills such as types of objects, arrays, indexing, and loop-style iteration. Use of R/Markdown documents should be considered as a pedagogical choice, and retained or discontinued based on how it contributes to student success in the other areas of the course.\n\nIn addition, I suggest that …\n\nMath 300R include some work with assembling/curating data using spreadsheets and basic data cleaning with spreadsheets. Awareness of the ubiquity of data errors and a basic understanding of how to deal with such errors is an important component of working with data. (This is not to suggest that data analysis, modeling, and graphical depiction be taught using spreadsheets, which are notoriously unreliable, difficult, and limiting for such purposes. Spreadsheets are, however, appropriate for the phase where non-tabular data is transcribed into a tabular arrangement.)\n\n\n\nInstitutional goals\nIt can be difficult to translate broadly stated institutional goals to apply them to a single course. However, catalog descriptions of programs and individual courses provide some assistance. Here is the catalog copy for Math 300 (which is identical to the catalog description of Math 357).\n\nMath 300. Introduction to Statistics. An introduction in probability and statistics for decision-makers. Topics include basic probability, statistical inference, prediction, data visualization, and data management. This course emphasizes critical thinking among decision-makers, preparing future officers to be critical consumers of data. (Emphasis added.)\n\nI interpret the final sentence as a description of the overall objective of the course:\n\nOverall objective: Prepare officers to use data to inform decisions.\n\nReturning to the idea that the topics listed in the catalog copy ought to be interpreted as serving the overall objective of the course, let’s consider those topics one at a time:\n\ndata management\ndata visualization\nprediction\nstatistical inference\nbasic probability\n\n\nStrictly speaking, as a term of art the phrase “data management” is business jargon describing enterprise-level activities that are unrelated to the other items on the list. It would be unheard of to include it, in this strict sense, in a statistics course. I believe the intent of the phrase to be better served by terms like “data wrangling,” “data cleaning,” “database querying,” and such which make up an important part of “data science.” Data wrangling is a major feature of Math 300 launched and is covered using professional level computing tools well suited to both small and large data. But whatever “data management” might reasonably be taken to mean, it was utterly ignored in Math 300CC.\n“Data visualization” is generally taken to be the process of using graphics to discover and highlight patterns shown in data. Math 300CC included only statistical graphics such as histograms, box-and-whisker plots, and basic “scatter plots.” Math 300 adds to this modern modes of graphics such as transparency, color, and faceting that make it possible to display relationships among multiple variables. The software used in Math 300 is the professional-level ggplot2 which provides the ability to increase the sophistication and generality of data display, using for example density graphics such as violin plots. As such, Math 300 is a big step on the road to rich data visualization. Some of these will be introduced in Math 300R in the second half of the course.\n“Prediction” is a central paradigm used in the important area of “machine learning.” It is also an often used method used to inform decision making and characterize risk, for instance, by indicating the distribution of plausible outcomes. Math 300CC emphasized paradigms such as hypothesis testing and confidence intervals that are not aligned with making and interpreting predictions. Math 300 focuses on these same paradigms. Math 300R will treat prediction as a central statistical path, as well as highlighting its proper use, interpretation, and evaluation.\n“Statistical inference” is traditionally taken to mean the calculation and interpretation of hypothesis tests and confidence intervals in various simple settings. Such settings include the “difference between two means,” the “correlation coefficient,” and the “slope of a regression line.” Math 300CC introduced a handful of such settings, providing distinct formulas for each of them. The “controversies” referred to in the title Concepts and controversies includes the problematic interpretation of “p-values” and the need to use random sampling and/or random assignment in data collection to get “correct” results. Math 300 retains the emphasis on confidence intervals and p-values in the simple settings, but emphasizes a more general and accessible methodology based on bootstrapping and permutation tests.\n\nUnfortunately, appealing to random sampling/assignment is often whistling past the graveyard, since these idealized data collection processes are rarely available. Instead, professionals include “covariates” in their data collection in order to “adjust” for the factors that would have been scrambled into insignificance by random sampling/assignment if it had been available. Math 300R incorporates covariate methods and highlights the importance of identifying appropriate covariates.\n\n“Basic probability” can mean different things to different people. In most introductory statistics courses it refers to the construction, calculation, and study of named distributions such as the binomial, normal, chi-squared, t, etc. Such distributions play an important role in the statistical theory of confidence intervals and hypothesis testing. That is, they are support for statistical inference. Math 300CC followed the traditional pattern of having students memorize which distribution is relevant to which setting and using printed tables for calculation. As described earlier, Math 300 provides a much more natural route to inference through bootstrapping and permutation tests.\n\nWhat’s left out in this conception of basic probability is the support for decision making. Essential to this is the proper use of “conditional probability.” Math 300R emphasizes appropriate use and interpretation of conditional probability, seen most clearly in the “classifiers” part of the course.\n\n\nFaculty opinions\nInsofar as faculty internalize the goals of the institution, their views can point to ways that existing courses do and do not reflect those goals.\nWithin DFMS and other departments, there is a general discontent that Math 300CC was not doing what it ought to. Reasons for this can be seen by examining the textbook used in Math 300CC. The book has clear deficiencies, among which are:\n\nthe material is out of date and does not reflect any of the consensus recommendations (such as GAISE) developed in the last 30 years.\nit does not use data at any level beyond hand calculation.\nit does not deal with decision making at any serious level. (The only decision formally supported is whether or not to reject the Null hypothesis.)\n\nThe opinions of faculty outside DFMS can also be an important guide to institutional priorities. In AY 2021-22 I contacted the departments in the social sciences and humanities. Three of these—history, political science, economics—responded with interest. Discussion with groups of faculty from these departments elucidated a number of points:\n\nThe faculty most highly valued the development of data-science skills such as computing for data wrangling and data visualization.\nThe then-current version of Math 300 did not contribute to the development of such skills.\nMath 357 is not seen as an appropriate alterative to Math 300, both because of perceived difficulty of 357 and because faculty do not value the emphasis on probability distributions seen in 357.\n\nFrom my experience at Macalester and in conducting reviews at many colleges, I am often wary of the motivation of faculty in other departments. These can represent a desire for service courses like Math 300 to cover discipline-specific techniques. However, the faculty I spoke to also had an eye on what their students will need for their post-graduation jobs. Particularly the USAF officers drew on their field experience in areas such as military intelligence.\nBased on these findings, the group of faculty planning for revisions to Math 300 made an easy decision: replace the textbook with one oriented to data science. We selected the ModernDive book, which is unique among introductory statistics textbooks in starting out with data wrangling and visualization. This change of textbook addresses the “use data” part of the course objective stipulated above.\nThe other part of the objective—inform decisions—remains problematic even with the switch in the Math 300 textbook. Discussions I had with the ModernDive authors made clear that their purpose in writing the book was to provide a way to introduce data science into introductory statistics, but that they stuck to the traditional hypothesis-testing/confidence-interval framework in order not to make the change too daunting for instructors thinking of adopting the text. In other words, they were not trying to turn the topic toward decision-making with data, the motivation of the ideas presented in this proposal for Math 300R."
  },
  {
    "objectID": "index.html#plan-of-work",
    "href": "index.html#plan-of-work",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Plan of work",
    "text": "Plan of work\n\nEarly October 2022: Preliminary approval, with appropriate modifications, of the proposed objectives.\nOctober 2022: DTK will draft new day-by-day NTIs for the second half of the course in the same style as the existing NTIs for the first half of the course. In the process of drafting, there will likely be some re-arrangement and modification of the objectives in (1).\nNovember 2022: With the draft NTIs in hand, a faculty team will make a more detailed examination of the proposed objectives. I recommend that this examination be structured as a set of hour-long discussions, one for each of the five divisions described in ?@sec-topics.\nNovember/December 2022: DTK (and others, as interested) will assemble student readings to replace the second half of ModernDive. Much of the content already exists in the form of a draft textbook by DTK. These will be re-arranged to correspond to the day-to-day objectives as determined in (3).\nJanuary/February 2023: The first 18 lessons of 300R will be taught as a repeat of those lessons from Math 300 Fall 2022. DTK will participate mainly as an observer.\nJanuary/February 2023: Revision and refinement will be made of the readings and NTIs in (3) and (4) above.\nMarch/April 2023: Teaching the new lessons. DTK will participate as an instructor for these lessons."
  },
  {
    "objectID": "LC-list.html",
    "href": "LC-list.html",
    "title": "List of learning checks",
    "section": "",
    "text": "This list is assembled from the individual-lesson learning check files in the LC/ directory. Make any changes in that directory.\nAt the number of beers you found in (1), what fraction of volunteers will be above the 0.08 level?\nTo have a non-zero guideline, we have to allow that the guideline will put a small fraction of people above the 0.08 BAC level. Suppose that we decide to use the standard 95% level for the prediction interval. Construct the 95% prediction interval on BAC for each of the inputs 1 to 5 beers. Which number of beers will keep the upper limit of the prediction interval below the 0.08 BAC limit?"
  },
  {
    "objectID": "LC-list.html#non-technical-resources",
    "href": "LC-list.html#non-technical-resources",
    "title": "List of learning checks",
    "section": "Non-technical resources",
    "text": "Non-technical resources\nThese are resources that can be used for an essay/reading exercise or class discussion.\n\npudding.cool\nExamples:\n\nChinese censorship of the Big Bang Theory\nComparing pocket sizes on women’s and men’s clothing\nHow artists get paid from music streaming\nRandomness and age. Perhaps suitable for Lesson 38 on “false discovery.”\nCoughing and the Academy awards (The link was through <pudding.cool>.)\nNBA draft\nWriting plainly\nmany others!\n\n\n\nNYT “What’s going on with this graph?”\nLink to index\n\nForeign born people as a fraction of the US Population\nTree rings and droughts\nmany others!\n\n\n\nExercises from CPS\n\nSection 29.6 Exercises\n\nLinear and nonlinear correlations\nR2"
  },
  {
    "objectID": "LC-list.html#lesson-19",
    "href": "LC-list.html#lesson-19",
    "title": "List of learning checks",
    "section": "Lesson 19",
    "text": "Lesson 19"
  },
  {
    "objectID": "LC-list.html#setup",
    "href": "LC-list.html#setup",
    "title": "List of learning checks",
    "section": "Setup",
    "text": "Setup\nThe math300 package will be needed for lessons 20 through 39.\n\nlibrary(math300)\nlibrary(moderndive)\nlibrary(NHANES)"
  },
  {
    "objectID": "LC-list.html#section",
    "href": "LC-list.html#section",
    "title": "List of learning checks",
    "section": "19.1",
    "text": "19.1\nConsider the moderndive::evals data that records students’ evaluations (score, on a 1-5 scale) of the professors in each of several courses (the course ID), as well as the age, “average beauty rating” (bty_avg) of the professor, enrollment in the course (cls_students) and the level o the course (cls_level). Each row in the data frame is an individual course section.\n\n\n\n\n\nID\nscore\nage\nbty_avg\ncls_students\ncls_level\n\n\n\n\n329\n2.7\n64\n2.333\n22\nupper\n\n\n313\n4.2\n42\n2.667\n86\nupper\n\n\n430\n4.5\n33\n5.833\n120\nlower\n\n\n95\n4.2\n48\n4.333\n33\nupper\n\n\n209\n4.8\n60\n3.667\n34\nupper\n\n\n442\n3.6\n61\n3.333\n39\nlower\n\n\n351\n4.6\n50\n3.333\n26\nlower\n\n\n317\n3.7\n52\n6.500\n44\nupper\n\n\n444\n4.1\n52\n4.500\n111\nlower\n\n\n315\n3.8\n52\n6.000\n88\nupper\n\n\n\n\n\nThe following commands model score versus age and plots the data as a point plot.\n\nlm(score ~ age, data = moderndive::evals) %>% coefficients()\n\n (Intercept)          age \n 4.461932354 -0.005938225 \n\nopenintro::evals %>% gf_point(score ~ age, alpha=0.2 )\n\n\n\n\n\nExplain why some of the dots are darker than others?\n\n\n\n\n\n\n\nSolution\n\n\n\nAll the ages have integer values—e.g., 43, 44, 45—so the dots line up in vertical lines.\nSimilarly, the scores have values only to one decimal place—e.g., 3.1, 3.2, 3.3—so the dots line up in horizontal lines. If there are two or more rows in evals that have the same age and score, the dots will be plotted over one another. Since transparency (alpha = 0.2) is being used, points where there is a lot of overplotting will appear darker.\n\n\n\nRemake the plot, but using gf_jitter() instead of gf_point(). Explain what’s different about the jittered plot. (Hint: Almost all of the dots are the same lightness.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ age, alpha=0.2 )\n\n\n\n\n“Jittering” means to shift each dot by a small random amount. This reduces the number of instances where dots are overplotted.\n\n\n\nNow make a jitter plot of score versus class level (cls_level).\n\nWhat do the tick-mark labels on the horizontal axis describe? Are they numerical?\nTo judge from the plot, are their more lower-level than upper-level courses? Explain briefly what graphical feature lets you answer this question at a glance.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nopenintro::evals %>% gf_jitter(score ~ cls_level)\n\n\n\n\n\nThe tick-mark labels are the levels of the categorical variable cls_level. The are words, not numbers.\nThere are many more dots in the right column than in the left. Since lower level class are shown in the left column, there are fewer lower-level courses than upper-level courses.\n\n\n\n\nThe two columns of points in the plot you made in (3) are not separated by very much empty space. You can fix this by giving gf_jitter() an argument width=0.2. Try different numerical values for width and report which one you find most effective at making the two columns clearly separated while avoiding overplotting.\nAre the scores, on average, different for the lower- vs upper-level classes? It’s hard to get more than a rough idea of the distribution of scores by looking at the “density” of points. The reason is that the number of points differs in the two columns. But there is an easy fix: add a layer to the graphic that shows the distribution (more or less like a histogram displays a distribution of values). You can do this by piping the jitter plot layer into a geom called a “violin,” like this:\n\n\nopenintro::evals %>% \n  gf_jitter(score ~ cls_level) %>%\n  gf_violin(fill=\"blue\", alpha=0.2, color=NA)\n\n\n\n\nExplain how to read the violins."
  },
  {
    "objectID": "LC-list.html#section-1",
    "href": "LC-list.html#section-1",
    "title": "List of learning checks",
    "section": "19.2",
    "text": "19.2\nThe openintro::promotions data comes the the 1970s and records the gender of 38 people along with the result of a decision to promote (or not) the person. =\nChapter 2 of ModernDive suggests graphically depicting decision versus gender by using a bar plot. There are two ways to make the bar plot, depending on which variable you assign to the horizontal axis and which to the fill color.\n\npromotions %>% gf_bar(~ decision, fill=~ gender)\npromotions %>% gf_bar(~ gender, fill=~decision)\n\n\n\n\nFigure 1: Two different ways to plot promotion outcome and gender\n\n\n\n\n\n\n\nFigure 2: Two different ways to plot promotion outcome and gender\n\n\n\n\nPlots like those in ?@fig-promotion-bars might be attractive or not, depending on your taste. What they don’t accomplish is to make sure which is the response variable and which the explanatory variable.\nThe choice of response and explanatory variables depends, of course, on what you are trying to display. But everyday English gives a big hint. For instance, you might describe the question at hand as, “Does gender affect promotion decisions.” Here, the variable doing the affecting is gender, and the outcome is the decision.\nModeling decision as a function of gender is easy once you convert the response variable to a zero-one variable. Like this:\n\nmod <- lm(zero_one(decision, one=\"promoted\") ~ gender, data = promotions)\ncoefficients(mod)\n\n (Intercept) genderfemale \n   0.8750000   -0.2916667 \n\nmosaicModel::mod_eval(mod)\n\n  gender model_output\n1   male    0.8750000\n2 female    0.5833333\n\n\n\nExplain what is the relationship between the model coefficients and the model outputs.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe coefficients tell how to calculate the model output. These coefficients say that the model output will be 0.875, but subtract 0.292 if the person is female.\nThe model outputs give the probability of being promoted for each of the two genders.\n\n\n\nMake this plot and explain what the red lines show. (We don’t expect you to be able to write the command to generate such plots on your own, but we do expect you to be able to interpret them.)\n\n\npromotions %>% \n  gf_jitter(zero_one(decision) ~ gender, height=0.2, width=0.2) %>%\n  gf_errorbar(model_output + model_output ~ gender, data=mod_eval(mod), \n              color=\"red\", inherit=FALSE) %>%\n  label_zero_one()\n\n\n\n\n\n\n\nSolution\n\n\n\nThe red lines show the proportion of the people in each gender group who were promoted. The y-axis scale on the left refers to the zero-one encoding of decision, while the y-axis labels on the right make it easier to read off the numerical value of the proportion."
  },
  {
    "objectID": "LC-list.html#section-2",
    "href": "LC-list.html#section-2",
    "title": "List of learning checks",
    "section": "19.3",
    "text": "19.3\nThe mosaicData::Whickham data from comes from a survey of a thousand or so nurses in the UK in the 1970s. The data record the age of each nurse along with whether the nurse was still alive in a follow-up survey 20 years later (outcome).\nMake this graph from the Whickham data:\n\ngf_jitter(zero_one(outcome) ~ age, data = Whickham, alpha=0.3, height=0.1) %>% \n  label_zero_one() \n\n\n\n\n\nExplain in everyday language what the graph shows about the lives of humans.\nMake the graph again, but leave out the %>% label_zero_one(). Then explain what label_zero_one() does.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nThe graph shows that young nurses tended to be alive at the 20-year follow-up, older nurses not so much.\n%>% label_zero_one() adds an axis on the left of the graph showing that in the zero-one tranform of outcome, “Alive” is assigned the value 1 and “Dead” the value 0.\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#section-3",
    "href": "LC-list.html#section-3",
    "title": "List of learning checks",
    "section": "19.4",
    "text": "19.4\nAbout the summarization of models. Pipe the model fit into any of four functions:\n\n%>% coefficients()\n%>% broom::tidy()\n%>% rsquared()\n%>% confint()\n\nREDO confint() so that the columns are named lower, middle, upper\n\nSolution"
  },
  {
    "objectID": "LC-list.html#obj.-19.3",
    "href": "LC-list.html#obj.-19.3",
    "title": "List of learning checks",
    "section": "19.5 (Obj. 19.3)",
    "text": "19.5 (Obj. 19.3)\nCalculation of a 95% coverage interval (or any other percent level interval) is straightforward with the right software. To illustrate, consider the efficiency of cars and light trucks in terms of CO_2 emissions per mile driven. We’ll use the CO2city variable in the math300::MPG data frame. The basic calculation using the mosaic package is:\n\ndf_stats( ~ CO2city, data = math300::MPG, coverage(0.95))\n\n  response   lower   upper\n1  CO2city 276.475 684.525\n\n\nThe following figure shows a violin plot of CO2city which has been annotated with various coverage intervals. Use the calculation above to identify which of the intervals corresponds to which coverage level.\n\n50% coverage interval -A- (c)\n75% coverage interval -A- (e)\n90% coverage interval -A- (g)\n100% coverage interval -A- (i). This extends from the min to the max, so you could have figured this out just from the figure."
  },
  {
    "objectID": "LC-list.html#obj-19.3",
    "href": "LC-list.html#obj-19.3",
    "title": "List of learning checks",
    "section": "19.6 (Obj 19.3)",
    "text": "19.6 (Obj 19.3)\nThe two jitter + violin graphs below show the distribution of two different variables, X and Y. Which variable has more variability?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is about the same level of variability in variable A and variable B. This surprises some people. Remember, the amount of variability has to do with the spread of values of the variable. In variable B, those values are have a 95% prediction interval of about 30 to 65, about the same as for variable A. There are two things about plot (b) that suggest to many people that there is more variability in variable B.\n\nThe larger horizontal spread of the dots. Note that variable B is shown along the vertical axis. The horizontal spread imposed by jittering is completely arbitrary: the only values that count are on the y axis.\n\nThe scalloped, irregular edges of the violin plot.\n\nOn the other hand, some people look at the clustering of the data points in graph (b) into several discrete values, creating empty spaces in between. To them, this clustering implies less variability. And, in a way, it does. But the statistical meaning of variability has to do with the overall spread of the points, not whether they are restricted to discrete values."
  },
  {
    "objectID": "LC-list.html#objs.-19.3-19.4",
    "href": "LC-list.html#objs.-19.3-19.4",
    "title": "List of learning checks",
    "section": "19.7 (Objs. 19.3 & 19.4)",
    "text": "19.7 (Objs. 19.3 & 19.4)\nThe graphs below show a violin plot of body mass index (BMI) for adults and children. One of the graphs shows a correct 95% coverage interval on BMI, the other does not.\nIdentify the incorrect graph and say what feature of the graph led to your answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nGraph (b) is correct. In graph (a), you can see that the interval fails to include a lot of the low BMI children and extends too high. For adults, the graph (a) interval extends too far low and doesn’t go high enough."
  },
  {
    "objectID": "LC-list.html#e",
    "href": "LC-list.html#e",
    "title": "List of learning checks",
    "section": "19.E",
    "text": "19.E\nThere are two equivalent formats describing an interval numerically that are widely used:\n\nSpecify the lower and upper endpoints of the interval, e.g. 7 to 13.\nSpecify the center and half-width of the interval, e.g. 10 ± 3, which is just the same as 7 to 13.\n\nComplete the following table to show the equivalences between the two notations.\n\n\n\n\n\nInterval\nbottom-to-top\nplus-or-minus\n\n\n\n\n(a)\n3 to 11\n\n\n\n(b)\n\n108 ± 10\n\n\n(c)\n\n30 ± 1\n\n\n(d)\n97 to 100\n\n\n\n(e)\n-4 to 16\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n7 ± 4\n98 to 118\n29 to 31\n98.5 ± 1.5\n6 ± 10\n\nIt’s a matter of judgement which format to use. The bottom-to-top notation highlights the range of the interval while the plus-or-minus notation emphasizes the center of the interval. As a rule of thumb, I suggest this:\n\nIf the first two digits are different between the top and bottom of the interval, use the bottom-to-top notation. So, write 387 to 393. If the first two digits are the same, use plus-or-minus. For instancer, the ratio of the mass of the Earth to that of the Moon is 81.3005678 ± 0.0000027. This is easier to take in at a glance than the equivalent 81.3005651 - 81.3005708"
  },
  {
    "objectID": "LC-list.html#f",
    "href": "LC-list.html#f",
    "title": "List of learning checks",
    "section": "19.F",
    "text": "19.F"
  },
  {
    "objectID": "LC-list.html#lesson-20",
    "href": "LC-list.html#lesson-20",
    "title": "List of learning checks",
    "section": "Lesson 20",
    "text": "Lesson 20\n\n\n\n\n\n\nOnly for use in drafting questions\n\n\n\n20.1. Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\n20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples."
  },
  {
    "objectID": "LC-list.html#a",
    "href": "LC-list.html#a",
    "title": "List of learning checks",
    "section": "20.A",
    "text": "20.A\nYou’re having a conversation about your statistics course with your engineer aunt at Thanksgiving. You tell her about how the course uses gaming (e.g. simulated data from DAGs) to develop an understanding of statistical methodology. She says (sensibly), “What? Simulated data? Isn’t statistics supposed to be about real data? It’s not a game.”\n\nWrite a paragraph-long response to your aunt explaining the point of gaming in learning statistics. Your paragraph should make a compelling case for gaming.\nWrite another paragraph that expresses any concerns you have about using games in a course that’s supposed to be about methods for extracting information about the world from real data."
  },
  {
    "objectID": "LC-list.html#b",
    "href": "LC-list.html#b",
    "title": "List of learning checks",
    "section": "20.B",
    "text": "20.B\nWrite statements using the computer commands covered in the first half of the course to calculate the size of the variability of these quantities:\n\nThe age variable from the Cherry_race_longitudinal data frame.\nThe Height variable from the NHANES data frame.\nCompute the body-mass index from the formula \\(\\text{BMI} = \\frac{w^2}/h\\) where \\(w\\) is the person’s weight and \\(h\\) is the person’s height. Then calculate the size of the variability of the BMI you calculate from the NHANES data."
  },
  {
    "objectID": "LC-list.html#c",
    "href": "LC-list.html#c",
    "title": "List of learning checks",
    "section": "20.C",
    "text": "20.C\n\n\n\n\n\n\nIn draft\n\n\n\nShow pictures of data together with violins. Ask students to estimate the “size” of the variation and to mark on the graph an annotation reflecting the “size.”\nIs “size” a single number or a pair of numbers?\nContrast “size” with the coverage interval: why is one of them two numbers and the other just a single number."
  },
  {
    "objectID": "LC-list.html#section-4",
    "href": "LC-list.html#section-4",
    "title": "List of learning checks",
    "section": "20.1",
    "text": "20.1\n?@sec-size-of-variable (in the reading for this variable) describes two very closely related summary quantities used to measures of the “size” of a variable: a) the variance and b) the “standard deviation” (which is the square root of the variance).\n\nUsing software, what is the variance of the XXX variable in the YYY data frame? Make sure to include the units.\nWhat is the “standard deviation” of the XXX variable? Calculate this in two different ways: i. “by-hand” taking of the square root of the variance; ii. using the sd() software directly.\n\n[Repeat for a number of variables from different data frames.]\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#d",
    "href": "LC-list.html#d",
    "title": "List of learning checks",
    "section": "20.D",
    "text": "20.D\n\n\n\n\n\n\nIn draft\n\n\n\nIn arithmetic notation, parentheses are used to group operations. So, \\(4*(3+2)\\) is different from \\(4*3 + 2\\). With do(), a similar logic applies, but we use curly braces—not parentheses—for the grouping. Another difference is that with do() and summarization, some configurations may not work at all because using braces or not will produce different data frames with different names.\n\nTry the following two commands, which differ in how curly braces are used.\n\ndo(5) * {sample(Galton, size=50)} %>% summarize(m = mean(height))\ndo(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))}\nOne command produces an error message and the other doesn’t. Explain what’s wrong with the erroneous command.\n\nTry these two statements. Again, one will work and the other won’t. Diagnose the\n\n\n{ do(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))} } %>% summarize(sz=sd(m))\n\n         sz\n1 0.3015075\n\ndo(5) * {sample(Galton, size=50) %>% summarize(m = mean(height))} %>% summarize(sz = sd(m))\n\n  sz\n1 NA\n2 NA\n3 NA\n4 NA\n5 NA"
  },
  {
    "objectID": "LC-list.html#e-1",
    "href": "LC-list.html#e-1",
    "title": "List of learning checks",
    "section": "20.E",
    "text": "20.E\nWrite single-line computer statements that will do the following (related) tasks:\n\nDraw a sample of size 5 from the mosaicData::TenMileRace data\nDraw a sample (but now of size 100) and compute the “size” of variation of the net variable (which gives the net time to complete the race, start line to finish line).\nCarry out 300 trials of (2) using the do() operator. (The result should be a data frame with entries that vary.) Note: Surround the statement for a single trial with curly braces: { and }. Also, arrange for the name of the column in the overall result to be sz. This will appear in the summarize() command.\nAdd on to (3) the computations needed to calculate the mean and the standard deviation of the trials? Note: Surround the statement from (3) with a pair of curly braces so that the summarize() command will look at all 300 trials as a single data frame.\n\nIs the mean thus calculated a single number comprising all trials or a number for each trial? Briefly justify your answer in terms of what the mean of the trials should be about.\nEexplain what the standard deviation of the trials captures and why it’s different from the standard deviation on one trial (as from (2)).\n\nRepeat (4), but now with a sample size of 400 instead of 100. With the larger sample size, how do the mean and the standard deviation compare to what you got with a sample size of 100.\nRepeat (4) but this time use 1200 trials instead of 300. With more trials, how do the mean and standard deviation compare to what you got with 300 trials?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n# sample of size 100 from TenMileRace\nsample(TenMileRace, size=5)\n\n     state time  net age sex orig.id\n7127    VA 4820 4796  37   F    7127\n1870    VA 5928 5518  36   M    1870\n4531    MD 5563 5563  23   F    4531\n1356    MD 5250 5090  33   M    1356\n2003    MD 5875 5435  37   M    2003\n\nsample(TenMileRace, size=100) %>% summarize(sz = sd(net))\n\n        sz\n1 951.0684\n\ndo(300)* {sample(TenMileRace, size=100) %>% summarize(sz = sd(net))}\n\n           sz\n1    972.5812\n2    824.8592\n3    821.5586\n4    938.5612\n5    939.4491\n6    928.3224\n7    965.7778\n8    992.9269\n9    992.3919\n10  1080.1513\n11   943.7595\n12  1037.2753\n13  1012.8912\n14   996.0352\n15  1079.2897\n16   899.4723\n17   906.8983\n18   862.4734\n19   924.8717\n20  1039.7077\n21   964.8859\n22   912.1430\n23   955.7692\n24  1079.1801\n25   945.2650\n26  1001.7177\n27   955.8785\n28   934.3646\n29   990.0511\n30   962.5995\n31   967.1691\n32  1097.1731\n33   907.0343\n34   993.1500\n35   937.3869\n36   939.2032\n37   907.1917\n38   936.3380\n39   936.3048\n40  1053.9427\n41   903.9515\n42  1122.2634\n43  1029.8397\n44   926.2964\n45  1103.9172\n46   975.8339\n47  1083.2056\n48   938.3758\n49   899.2549\n50   992.7765\n51   990.9264\n52   813.2843\n53  1102.1025\n54   755.8189\n55  1008.5489\n56   913.3470\n57  1055.8151\n58  1006.8449\n59   826.4333\n60  1007.9796\n61   988.5770\n62  1016.8350\n63   967.5634\n64   932.8218\n65   949.6178\n66   935.0267\n67   899.5244\n68   815.5198\n69   933.8537\n70  1017.0440\n71   779.8278\n72   867.1942\n73  1004.3558\n74  1005.0899\n75  1118.5752\n76   866.4319\n77  1040.7873\n78  1025.7857\n79   967.3428\n80   971.8905\n81  1029.6817\n82  1001.2666\n83   858.4597\n84   910.2976\n85   950.5084\n86   963.3022\n87   967.6966\n88   956.6273\n89   843.8254\n90  1029.6041\n91   951.6543\n92  1042.9482\n93   874.5089\n94   909.2968\n95   962.3898\n96  1074.9379\n97   881.3408\n98   936.6853\n99   895.3177\n100  975.7541\n101  972.2259\n102 1010.0560\n103  789.2095\n104 1132.4296\n105  969.5747\n106  957.4232\n107 1014.8825\n108  857.2816\n109 1103.3705\n110  851.8963\n111 1002.6471\n112  819.0540\n113  982.2953\n114  915.8515\n115  996.1651\n116  855.4621\n117  878.6094\n118  912.6942\n119 1030.0310\n120  825.6659\n121 1005.7683\n122 1031.5542\n123 1126.4620\n124 1100.2412\n125  915.7616\n126  921.1658\n127  999.6651\n128  978.5067\n129  844.5894\n130  955.6868\n131  947.4918\n132  963.0174\n133  952.6687\n134 1074.9521\n135  950.0944\n136  844.6555\n137 1066.3125\n138  980.3699\n139 1119.9149\n140 1037.1454\n141  949.1217\n142 1094.5156\n143  975.7513\n144  813.0390\n145  814.3828\n146  949.0838\n147  861.1970\n148  955.8846\n149  975.3213\n150  882.5769\n151  933.1666\n152  806.5156\n153  827.7887\n154  930.9380\n155  944.4835\n156 1026.5458\n157 1112.7747\n158 1038.9871\n159  835.5357\n160  921.8688\n161  927.8762\n162  928.4328\n163  771.5887\n164 1093.4424\n165 1125.0387\n166  864.7831\n167  857.8454\n168  903.1473\n169  957.3963\n170  860.0105\n171  886.2518\n172  914.7956\n173  993.6665\n174  857.3370\n175  830.1747\n176 1003.2101\n177 1096.0726\n178  971.3859\n179  970.9167\n180  907.7088\n181  982.6391\n182  930.2694\n183  950.4383\n184  956.4709\n185  957.9360\n186  938.9992\n187  902.7469\n188  997.7660\n189  952.0713\n190  850.9500\n191  900.8205\n192  996.3816\n193  978.8046\n194  955.1345\n195 1019.0840\n196  907.4728\n197 1107.4039\n198  956.8556\n199 1005.5707\n200  954.7452\n201 1051.8599\n202  927.9475\n203  835.2828\n204  983.7366\n205  935.3973\n206  943.8083\n207  921.2009\n208  859.0696\n209  950.8036\n210 1013.8412\n211 1012.3216\n212  934.2267\n213  856.8701\n214  939.1951\n215 1050.0359\n216  946.4714\n217 1043.4746\n218  934.7167\n219  958.0577\n220  938.1260\n221  959.8289\n222 1023.2610\n223  944.8401\n224 1126.1726\n225 1036.0903\n226  888.6611\n227  874.7365\n228  798.1713\n229 1143.1490\n230  969.9378\n231  957.8012\n232 1073.4201\n233  917.0050\n234  891.6083\n235 1028.5489\n236  971.0936\n237 1045.8578\n238 1068.3065\n239  973.8031\n240 1103.5208\n241  978.3619\n242 1074.7413\n243 1011.4880\n244  963.8257\n245  891.8563\n246  988.9697\n247 1073.4325\n248 1183.1512\n249  945.7524\n250 1086.4373\n251  793.2965\n252  972.1942\n253 1032.7620\n254  926.8040\n255  944.1331\n256  940.1985\n257 1089.3612\n258  921.3067\n259 1104.1693\n260  863.2042\n261 1023.1090\n262  889.9317\n263 1134.5078\n264  971.4643\n265 1038.0073\n266  969.7236\n267  982.1654\n268 1022.1162\n269  910.0159\n270 1019.7098\n271  910.3500\n272  953.3862\n273  961.2607\n274  854.7824\n275  908.5864\n276  877.5796\n277  904.0877\n278  980.0922\n279  942.7869\n280 1079.1974\n281  947.2121\n282  896.4217\n283  953.9140\n284  786.3968\n285 1111.9730\n286 1189.9003\n287  909.4212\n288 1013.4597\n289  835.7166\n290  985.0277\n291 1030.3573\n292 1005.7200\n293  932.0799\n294  910.6702\n295  965.9356\n296  812.0218\n297  910.2485\n298 1055.7511\n299  988.8919\n300  891.1537\n\n{do(300)* {sample(TenMileRace, size=100) %>% summarize(sz = sd(net))}} %>% summarize(m = mean(sz), s=sd(sz))\n\n         m       s\n1 970.0165 85.8338\n\n{do(300)* {sample(TenMileRace, size=400) %>% summarize(sz = sd(net))}} %>% summarize(m = mean(sz), s=sd(sz))\n\n         m        s\n1 965.6244 36.77361\n\n\n\nUsing a sample size that’s four times larger doesn’t affect the mean, but it reduces the standard deviation by a factor of two.\nIncreasing the number of trials does have any noticeable effect on either the mean or standard deviation.\n\n\n\n20.E\nThroughout this course, you’re going to be using lm() to build models. Often, to demonstrate “sampling variation.” you will use sample() on a dataset or a DAG to generate a random sample and then send the result as the data= argument to lm(),\nHere are three computer commands that use the data= argument in different ways. One of them doesn’t work at all. Which one?\nsample(mtcars, size=10) %>% lm(mpg ~ wt + hp, data=.)\nsample(mtcars, size=10) %>% lm(mpg ~ wt + hp)\nlm(mpg ~ wt + hp, data=sample(mtcars, size=10))\n\n\n\n\n\n\nIn draft\n\n\n\nThis problem would be come irrelevant if the fitmodel() command described in the [Lesson 19 NTI]{../NTI/NTI-Lesson19.html} is being used.\n\nLesson 21\n\n\n21.1\nThe following command will generate a data frame with 1000 rows from dag00 and calculate the variance of the x and y variables:\n\nsample(dag00, size=1000) %>%\n  summarize(vx = var(x), vy = var(y))\n\n# A tibble: 1 × 2\n     vx    vy\n  <dbl> <dbl>\n1  3.78  1.04\n\n\nCompare this result to the DAG tilde expressions\n\ndag00\n\n[[1]]\nx ~ eps(2) + 5\n\n[[2]]\ny ~ eps(1) - 7\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nIn the tilde expressions, eps(2) means to generate noise of magnitude 2.0.\n\nIs the argument to eps() specified in terms of the variance or the standard deviation?\nThe tilde expression for x specifies that the constant 5 is to be added to eps(2). Similarly, the constant -7 is added to y. How do these constants relate to the calculated magnitudes of x and y?\n\n\nSolution\n\nThe standard deviation. For instance, x has noise of magnitude 2. The variance of x is 4, the square of 2.\nThe standard deviation (and therefore the variance) ignore such added constants.\n\n\n\n\n\n21.2\n?@sec-signal-and-noise introduces the idea that variables consist of components. A simple breakdown is into two components: i. the part of the variable that is determined by other variables in the system (“signal”) and ii. the random part of the variable (“noise”). The section uses dag01 as an illustration of how a variable can be partly determined and partly random noise.\n\nWrite and execute a command that will generate 500 rows of simulated data from dag01 and will calculate the standard deviation of x and of y.\nWhat’s the magnitude of x in the simulated data? What’s the magnitude of y?\nDoes this change if you use data with 1000 or 20000 rows?\n\n\nSolution\n\nsample(dag01, size=500) %>% summarize(sx = sd(x), sy=sd(y))\nThe standard deviation of x is about 1, the standard deviation of y is about 1.8.\nNo, the values are roughly the same regardless of the size of the sample.\n\n\n\n\n\n21.3\n[DRAW several DAG-like graphs, one of which should be undirected in all edges, one should be undirected on one or two edges (but not all), and one should be cyclic and another acyclic.]\nReferring to the graphs in the figure, say which ones are DAGs. If a graph is not a DAG, say whether that’s because it’s not directed or because it’s not cyclic.\n\nSolution\n\n\n\n21.A\nA DAG (directed acyclic graph) is a mathematical object used to state hypothetical causal relationships between variables. Explain briefly (e.g. a few sentences overall) what each of the words “directed,” “acyclic,” and “graph” mean in the context of a DAG.\n\n\n\n\n\n\nSolution\n\n\n\n\nA graph is a relationship between discrete elements (called “nodes” abstraction) each of which for us represents a hypothetical quantity, that is, a variable. In addition to the nodes, the graph contains edges which represent the connections between variables.\nA directed graph is one whose arrows have a direction. For instance \\(A \\rightarrow B \\leftarrow C\\) means that \\(A\\) and \\(C\\) together cause \\(B\\), but \\(B\\) has no influence on \\(A\\) and \\(C\\).\nAn acyclic graph is one where it is impossible to start on any given node and, by following the directed edges, return back to that node.\n\n\n\n\n\n\n21.B\nDraw these DAGs:\n\n“April showers bring May flowers.”\n“Price of a rug is determined by the size and the quality of materials.”\n“The weight of an automobile is reflected in the MPG fuel economy, as is the speed of the car, and inflation level of the tires.”\n“Plants tend to grow in the direction of the sunlight.”\n“An ice-cream shop owner needs to plan staffing based on the season, day of the week, and holidays.”\n\n\n\n\n21.C\n\n\n\n21.4\nGenerate simulated data from dag01 with 1000 rows. Fit the regression model y ~ x to the data and examine the coefficients.\n\nHow do the coefficients relate to the tilde expressions that define dag01?\nInstead of using the regression model y ~ x, where y is the response variable, try the regression model x ~ y. Do the coefficients from x ~ y correspond in any simple way to the tilde expressions that define dag01?\n\n\nSolution\n\nsample(dag01, size=1000) %>%\n  lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      4.018        1.507  \n\n\nThe intercept corresponds to the additive constant (4) in the y tilde expression. The x coefficient corresponds to the multiplier on x in the tilde expression.\nThe formula for x isn’t reflected by the coefficients.\nUsing x as the response variable:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .)\n\n\nCall:\nlm(formula = x ~ y, data = .)\n\nCoefficients:\n(Intercept)            y  \n     -1.850        0.462  \n\n\nThese coefficients do not appear in the dag01 tilde expressions.\n\n\n\n\n21.5 (Objective 21.2)\nYou are trying to understand why automobile fuel economy varies from model to model. Using the mtcars data frame (documentation at help(\"mtcars\")) …\n\nWhat’s an appropriate choice of a response variable?\nPick two explanatory variables of interest to you. Build an appropriate model from the data, extracting the coefficients() of the model. Explain what the coefficients mean in everyday terms that your cylinder-head uncle would approve of.\nWhich of the other variables are covariates? iv.Pick a covariate that your intuition suggests would be important. Include that covariate in the model from (ii) and say whether the covariate shows up as important in the model coefficients.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nAll of the other variables are covariates. A covariate is merely a potential explanatory variable that you are not directly interested in. Of course, not all covariates play an important role in the system.\n\n\n\n\n\n\n21.6 (Obj 21.3)\nGenerate a sample of size \\(n=100\\) from dag03. Use the data to construct a model of y versus x. But instead of using coefficients() to look at the model coefficients, use confint(). While coefficients() reports a single value for each coefficient, confint() reports a plausible interval for coefficients that is consistent with the data.\n\nFor \\(n=100\\), how wide is the interval reported by confint() on the x coefficient.\nRepeat the process of sampling and modeling, but this time use \\(n=400\\). How wide is the interval reported by confint() on the x coefficient.\nAgain repeat the process of sampling and modeling, this time using \\(n=1600\\). iv. Does the interval reported depend systematically on the size \\(n\\) of the sample? Describe what pattern you see.\n\n\n\n\n21.7\nA short report from the British Broadcasting Company (BBC) was headlined “Millennials’ pay ‘scarred’ by the 2008 banking crisis.”\n\nPay for workers in their 30s is still 7% below the level at which it peaked before the 2008 banking crisis, research has suggested. The Resolution Foundation think tank said people who were in their 20s at the height of the recession a decade ago were worst hit by the pay squeeze. It suggested the crisis had a lasting “scarring” effect on their earnings.\n\n\nThe foundation said people in their 30s who wanted to earn more should move to a different employer. The research found those who stayed in the same job in 2018 had real wage growth of 0.5%, whereas those who found a different employer saw an average increase of 4.5%.\n\nThe phrase “should move” in the second paragraph of the report suggests causation. A corresponding DAG would look like this:\n\nmove_to_new_employer \\(\\rightarrow\\) higher_wage\n\nFor the sake of discussion, let’s add two more variables to the DAG:\n\neffectiveness—standing for how productive the employee is.\nqualifications—standing for whether the employee is a good candidate attractive to potential new employers.\n\nConstruct a DAG with all four variables that represent plausible causal connections between them. The DAG should not contain a direct causal connection between move_to_new_employer and higher_wage.\nIf the world worked this way, would it necessarily be good advice to switch to a new employer with the aim of earning a higher wage?\n\n\nLesson 22\n\n\n22.1 (Obj 20.3)\nConsider these three data frames:\n\nOne <- sample(dag01, size=25)\nTwo <- do(10) * {\n  lm(y ~ x, data = sample(dag01, size=25)) %>%\n    coefficients()\n  }\nThree <- Two %>% \n  summarize(mx = mean(x), sx = sd(x))\n\n\nBoth One and Two have columns called x, but they stand for different things. Explain what the unit of observation is and what the values in x represent..\nThree does not have a column named x, but it is a summary of the x column from Two. What kind of summary.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nIn One, the x column contains the simulated of the x variable from dag01. The unit of observation is a single case, for instance a person for whom observations were made of x and y. The simulation involves generating 25 rows of data: one row for each of 25 people.\nIn Two, the x column is the regression coefficient on x from the simulation. Each row of Two corresponds to one trial in which regression is being performed on a sample of size 25 of simulated data from dag01.\nThree is a summary of the 10 trials in Two. The columns, named mx and sx, tell about the distribution of x across all the trials.\n\n\n\n\n\n\n22.2 (Obj 21.3)\nPart 1\nYou are going to write a procedure that automates the following process:\n\nsampling from a DAG, specifically dag01, using sample() with a size of 25.\nfitting a model y ~ x using lm()\nreporting the coefficient on x using coefficients().\n\nCall the procedure proc1().\nTo do this fill in the following template in your Rmd document:\nproc1 <- function() {\n  # your statements go here\n}\nOnce you have proc1() ready, you can carry out the procedure by giving a simple command:\nproc1()\nPart 2\nNow that you have proc1() ready and have tried it out, you are going to run the procedure 100 times repeatedly and look at the distribution in the x coefficient.\nOf course, you could laboriously give the command proc1() 5 times, and write down the x coefficient each time. Far better, though, to automate the process of repeating and collecting the x coefficient.\nYou can do this easily by using do(5) in conjunction with proc1().\n\nWhat’s the form in which the coefficients are collected when using do()?\nIs the x coefficient the same from trial to trial? Explain why or why not.\nChange your statement to run 100 trials rather than just 5, and to store the collected results in a data frame called Trials. Use appropriate graphics to display the distribution of the x coefficient. Summarize the distribution in a sentence or two.\nCreate a consise summary of the x column of Trials using summarize() with sd(x) to calculate the standard deviation. Compare the size of the standard deviation to the graphical display in (3).\n\n\nSolution\nPart 1\n\nproc1 <- function() {\n  Dat <- sample(dag01, size=25)\n  Mod <- lm(y ~ x, data = Dat)\n  coefficients(Mod)\n}\n\nor, more concisely ::: {.cell}\nproc1 <- function() {\n  sample(dag01, size=25) %>%\n    lm(y ~ x, data = .) %>%\n    coefficients()\n}\n\n\n\n\nPart 2\n\ndo(5) * proc1()\n\n  Intercept        x\n1  4.048530 1.770314\n2  3.693512 1.475927\n3  4.105278 1.297085\n4  4.250074 1.639121\n5  3.934672 1.201477\n\n\n\nThe results of the five trials are collected into a data frame.\nThe x coefficients varies from trial to trial.\n\nCollect 100 trials\n\nTrials <- do(100) * proc1()\n\n\nAn appropriate graphical display of the trials:\n\n\nggplot(Trials, aes(x)) + geom_density(fill=\"blue\", alpha=0.3)\n\n\n\n\nThe x coefficient varies from near 0.5 to near 2.5 in a bell-shaped form.\n\nSummarize the trials by the standard deviation.\n\n\nTrials %>% summarize(s = sd(x))\n\n          s\n1 0.2084765\n\n\nThe standard deviation is about 1/4 the width of the distribution.\n\n\nLesson 23\n\n\n23.1\nVocabulary: Sampling distribution, standard error, sampling variability, sample size\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n23.2\nIn LC 22.2, using do(100), you displayed the sampling distribution on the x coefficient of the model y ~ x applied to data simulated from dag01. Among other things, you calculated the standard deviation of the sampling distribution. Copy over the proc1() you wrote for LC 22.2 into your Rmd document for this lesson.\nCalculate the standard deviation of the sampling distribution in each of these situations.\n\n\n\nNumber of trials\nSample size\n\n\n\n\ndo(100)\nsize=25\n\n\ndo(100)\nsize=100\n\n\ndo(100)\nsize=400\n\n\ndo(500)\nsize=25\n\n\ndo(500)\nsize=100\n\n\ndo(500)\nsize=400\n\n\n\nIn each case, the standard deviation is somewhat random, since new simulated data is collected from dag01 each time. Nonetheless, there is a systematic pattern to how the standard deviation varies with the number of trials and with the sample size.\n\nDescribe how the standard deviation of the sampling distribution of the x coefficient varies with sample size. The general trend should be easy to see.\nDoes the standard deviation of the sampling distribution depend on the number of trials?\nGoing back to your results from (1), try to find a simple quantitative relationship that describes how the standard deviation depends on sample size. State that relationship in words.\n\n\n\n\n23.3\nWe’re going to build models of prices of books based on the moderndive::amazon_books data frame. For each model, you will calculate the confidence interval of one or more coefficients in two ways:\n\nDirectly, using confint().\nIndirectly, using broom::tidy()\n\nModel 1.\n\nModel list_price versus amazon_price. Calculate the confidence intervals on the intercept and on the amazon_price coefficient.\nInterpret the amazon_price coefficient in everyday words.\n\n\n\n\n\n\n\nSolution to part 1.\n\n\n\n\nlm(list_price ~ amazon_price, data = amazon_books) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept)  3.716532 5.122737\namazon_price 1.049308 1.127471\n\nlm(list_price ~ amazon_price, data = amazon_books) %>% broom::tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      4.42    0.357       12.4 5.22e- 29\n2 amazon_price     1.09    0.0199      54.8 2.82e-165\n\n\n\nYou can read the confidence interval directly from the confint() report. For the regression report, calculate the confidence interval as the estimate \\(\\pm 2\\) times the “standard error.”\nJust to look at the amazon_price() coefficient, the list price is about 8% higher than the Amazon price. Here, “about” means 5% to 13%. But don’t forget the intercept. The list price is, on average, about $4.50 higher than the 1.08 multiplier on the Amazon price.\n\n\n\nModel 2.\n\nModel list_price versus amazon_price, including hard_paper as a covariate.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept)  3.028400 4.466996\namazon_price 1.042536 1.117826\nhard_paperH  1.786977 3.882884\n\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% broom::tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      3.75    0.366      10.3  1.62e- 21\n2 amazon_price     1.08    0.0191     56.5  9.65e-169\n3 hard_paperH      2.83    0.533       5.32 1.93e-  7\n\n\nA hardcover costs about $2 to $4 more than a paperback.\n\n\n\n\n\n\n\n\nNote in draft\n\n\n\nReturn to this example in the prediction lesson, to show how the confidence interval and the prediction interval are different.\nMaybe also use it in one of the side-exercises on interaction terms. (The plan is not to strongly emphasize interaction terms but to refer to them in the occasional exercise.)\n\n\n\n\n\n23.4 (Objective 23.1)\nWe’re going to work with a very short dataset so that you can see directly what resampling a data frame does. (Ordinarily, you use resampling on an entire dataset, but here we are trying to make a point about the mechanism of resampling.)\n\nCreate a data frame Five that consists of the first five rows of moderndive::mythbusters_yawn. (Hint: Use head().) Put the code for doing this into your Rmd homework paper. Note that Five contains the data from subjects 1 through 5.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nFive <- mythbusters_yawn %>% head(5)\n\n\n\n\nUse resample() to generate a new data frame from Five. At this point, you are just going to look at the result, processing it “by eye.” How many distinct human subjects are reported in the resampled data? (Your answer will likely differ from your classmates’, since resampling is done at random.)\nRepeat (2) ten times. Each time, count the number of distinct human subjects.\n\nReport those ten numbers on your write-up.\nThere will usually be one or more subjects repeated in the output. Look at these repeats carefully to check whether the variables have the same value for all the repeats or whether sometimes a repeated subject has different values for group or yawn.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nMost of the time there will be 2, 3, or 4 distinct subject. The balance of the five rows will be repeats of other subjects. When a subject is repeated, the entire row is identical for all instances of that subject.\n\n\nGoing further (optional). It’s pretty easy to automate the process of generating the resample and counting the number of distinct human subjects. Like this:\n\n{resample(Five) %>% unique() %>% nrow()}\n\n[1] 3\n\n\nUsing do(1000), carry out 1000 trials of this process, saving the overall results in a data frame named Trials. What is the mean number of unique human subjects across the 1000 trials? What fraction is this of the five subjects.\nDo the same again, but instead of using Five, use the whole mythbusters_yawn data frame (which has 50 rows). What fraction of the 50 human subjects, on average, shows up in the resamples?\n\n\n\n\n\n\nSolution\n\n\n\n\nTrials <- do(1000) * {resample(mythbusters_yawn) %>% unique() %>% nrow()}\nTrials %>% summarize(mn = mean(result)/nrow(mythbusters_yawn))\n\n       mn\n1 0.63526\n\n\n\n\n\n\n\n23.5 (Objective 23.1)\nReturn to the amazon_books data frame and the model list_price ~ amazon_price. In Exercise 23.3 you used the regression report to calculate the confidence intervals on the intercept and on the amazon_price coefficient. Now you are going to repeat the calculation in a different way, using randomization, a process called “bootstrapping”.\nThe basic process is to train a model using resampled data, like this:\n\nlm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()\n\n (Intercept) amazon_price \n    3.748436     1.129395 \n\n\nThen, using do(500), carry out 500 trials, saving the result in a data frame named Trials.\nProcess Trials to calculate both the mean and the standard deviation of the intercept and amazon_price columns. How do those results compare to the “standard error” results from the same model (without resampling) as you found in LC 23.3?\n\n\n\n\n\n\nSolution\n\n\n\nSOMETHING IS WRONG HERE. The means are about the same as from the regression report (as they should be) but the standard deviations are 3-4 times larger. WHAT GIVES?\nTHE PROBLEM IS a handful of books where the Amazon price is very different from the list price, because the book itself is very expensive (e.g. $100). Remedy\n\nSwitch to another data example, maybe doing both the regression report and the bootstrapping in one exercise.\nThis is an object lesson in outliers. Since the dollar discount is presumably proportional to the price, we should have used log transforms.\n\n\nTrials <- do(500) * {lm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()}\nTrials %>% summarize(m1 = mean(Intercept), \n                     m2 = mean(amazon_price), \n                     sintercept = sd(Intercept), \n                     samazon_price = sd(amazon_price))\n\n        m1       m2 sintercept samazon_price\n1 4.154913 1.109362  0.8972471      0.078366\n\n\n\n\n\n\n\nLesson 24\n\n\n\n\n\n\nJust to help when writing problems\n\n\n\n24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation.\n\n\n\n\n24.1 (Objective 24.1)\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\nSolution\n\nPrediction and (2) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price.\n\n\n\n\n\n24.2 (Objective 24.1)\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\nSolution\n\nPrediction\nRelationship\nRelationship\n\n\nSOME IDEAS FOR EXERCISE MODES\n\nUse mod_plot() and look at the slope of lines and offsets. Compare to the model coefficients.\nGenerate data from a DAG and look at the confidence interval on the effect size. Then make new samples and see if the effect size in those samples is consistent with the confidence interval.\nIn text, maybe look at the confidence intervals across new samples and show that they tend to overlap. Only a few of them don’t touch a common line. This is basically just a review of confidence intervals, but why not?\nInteraction. Show that when there is an interaction term, the effect size (as calculated by mod_effect()) is not constant, as it is for models with purely linear terms.\n\n\n\n\nLC 24.1\nThe Computational Probability and Statistics text describes an early study on human-to-human heart transplantation:\n\n“The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. [[Not in data set: Another variable called [MISSING] was used to indicate whether or not the patient was alive at the end of the study.]]”\n\nThe data frame is called Transplants. [NEED TO MOVE TO PACKAGE]\n\n\n\nYou’re going to build a model of outcome vs group based on the data in Transplants. The outcome variable has levels \"Dead\" and \"Alive\", that is, it is a two-level categorical variable. Consequently, the model output will be the probability that the transplant candidate was alive at the end of the study.\n\nBuild a model outcome == \"Alive\" ~ group from the Transplants data. Pay close attention to the left-hand side of the tilde expression: it is a calculation that produces a 1 if outcome is \"Alive\" and zero otherwise. Notice the double equal signs and the quotes around \"Alive\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod <- lm(outcome == \"Alive\" ~ group, data = Transplants)\n\n\n\n\nThe sole explanatory variable here, group also is categorical. It has levels \"Control\" and \"Treatment\".\n\nUsing eval_mod(), find the probability of being alive at the end of the study for the Control group and for the treatment group.\nThe two probabilities in (ii) do not add up to zero. Explain why.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(mod, group=\"Treatment\")\n\n      group model_output\n1 Treatment    0.3478261\n\nmod_eval(mod, group=\"Control\")\n\n    group model_output\n1 Control    0.1176471\n\n\n\n\n\nFind the effect size of the treatment. All you need is your results from (2)?\nUse mod_effect(modelname, ~ group) to calculate the effect size.\n\nIs the result consistent with what you found in (3).\nExplain in everyday language what this effect size means.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_effect(mod, ~ group)\n\n     change     group to_group\n1 -0.230179 Treatment  Control\n\n\n\n\n\n\n\n24.2\nEffect sizes generally come with units and you have to take into account the units in order to know if the effect is important or not.\n\n\n\n\n\n\nIn draft\n\n\n\nMove the Loans data into the math300 package. But for now …\n\nLoans <- readr::read_csv(\"data/loans.csv\")\n\n\n\nA case in point is provided by the Loans data frame, which records dozens of variables on each of 10,000 loans made through the Lending Club. The interest rate at which the loans are made varies substantially from loan to loan. Presumably, higher interest rates reflect a higher perception of risk of default (which would lead to the lender losing his or her money).\nHere’s a model of the interest rate. (This is for the borrowers who have a low debt-to-income percent; we won’t worry about the few very high debt-to-income cases.) We’re only interested in this problem with the effect size, which is the same as the coefficients on the model.\n\nmod <- lm(interest_rate ~ homeownership + debt_to_income + \n            account_never_delinq_percent + verified_income, \n          data = Loans %>% filter(debt_to_income<50))\nmod %>% confint()\n\n                                     2.5 %      97.5 %\n(Intercept)                    15.31467948 17.26502804\nhomeownershipOWN                0.16056412  0.73009641\nhomeownershipRENT               1.01571903  1.41695574\ndebt_to_income                  0.09565833  0.11523598\naccount_never_delinq_percent   -0.09149666 -0.07122369\nverified_incomeSource Verified  1.37798985  1.79909554\nverified_incomeVerified         2.88724295  3.39001806\n\n\nThere are two quantitative explanatory variables—debt_to_income and account_never_delinq_percent—both of which are measured in percent.\nThere are two categorical explanatory variables: homeownership and verified_income. The levels for homeownership are “MORTGAGE” (meaning money is still owed on the house), “OWN” (without a mortgage), and “RENT” (meaning the borrower rents rather than owning a home). The levels for verified_income are “Not Verified”, “Verified”, “Source Verified”.\nFor the categorical explanatory variables (and the intercept) the effect-size units are “percent interest.” For the quantitative explanatory variables, the effect-size units are “percent interest per percent,” so that when multiplied by the debt_to_income percent or the account_never_delinq_percent the result will be in “percent interest.”\n\nAccording to the model, who pays the higher interest rate (on average): people who OWN their home, people who RENT, or people who have a mortgage on their home? How much higher than the lowest-interest rate category.\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople who rent pay the highest interest rate, a little more than 1 percentage point higher than people who have mortgages. It’s interesting that people who own their homes outright pay (on average) pay about 0.45 percentage points more than people who own outright. This might be because having a mortgage means you also have a credit history.\n\n\n\nAccording to the model, who pays the higher interest rate (on average): people whose income is “not verified,” people whose income is “verified,” or people who have the source of income verified (level: “source verified”)?\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople whose income is verified pay about 3 percentage points higher interest than people whose income is “not verified.” This seems surprising, but it may be that people who have higher perceived default risk are also the people who are asked to verify their income. Things get complicated when explanatory variables are linked to each other.\n\n\n\nThe coefficients on debt_to_income and account_never_delinq_percent are the smallest numerically. Does this mean that the effects of debt_to_income and account_never_delinq_percent are smaller than the other two explanatory variables in the model? Explain why or why not. (Hint: Look at the distribution of debt_to_income and account_never_delinq_percent to get an idea for the range of values these variables take on.)\n\n\n\nSolution\ndebt_to_income varies over about 25 percentage points. The variation in account_never_delinq_percent is about the same, varying from about 80 to 100 percentage points. The effect of the variables (in percent interest) is determined by multiplying the coefficients by the amount of variation in the variables. So, from one extreme to the other, the effect of debt_to_income is about 2 perentage points of interest, and roughly the same for debt_to_income.\n\n\n\n24.4\nThe logic of effect size is to investigate the change in output of a model when one input variable is changed, holding all other things constant. This problem is about the extent to which we mean “all”.\nThe figure shows yearly CO2 production of individual gasoline-fueled passenger vehicles stratified by the number of engine 4 or 6 cylinders. ::: {.cell} ::: {.cell-output-display} \n\n\n\n:::\n\nThe effect size is the difference between the output variable when a change is made to an input. Consider the effect size of changing from a six-cylinder engine to a four-cylinder engine. Here’s a subtly incorrect way of calculating something like an effect size: Pick a dot from the six-cylinder group and another dot from the four-cylinder group. Subtract the 4-cylinder point’s CO2 value from the six-cylinder point’s CO2 value, and divide by the change in the input, that is, -2 cylinders.\n\nFollow this procedure for the top-most dot in each cloud and calculate the effect size. -A- (5100 kg - 4100 kg) / (-2 cylinders) = -500 kg/cylinder.\nFollow the procedure for the bottom-most dot in each cloud and calculate the effect size. -A- (3400 kg - 2500 kg) / (-2 cylinders) = -450 kg/cylinder.\nFollow the procedure for the bottom-most dot in the four-cylinder cloud and the top-most dot in the six-cylinder cloud. -A- (5100 kg - 3400 kg) / (-2 cylinders) = -850 kg/cylinder.\nDo the same as in (c) but use the bottom-most six-cylinder dot and the top-most four-cylinder dot. -A- (3400 kg - 4100 kg) / (-2 cylinders) = +350 kg/cylinder. In other words, switching from the six\n\nYou can imagine the (tedious) process of repeating the calculation for every possible pair of dots and getting a distribution of effect sizes. What would be the range of this distribution: from the smallest effect size to the biggest? (Hint: You can figure it out from your answers to (1).) -A- -850 kg/cylinder to +350 kg/cylinder\n\nFrom your result in (2), you might be tempted to conclude that the effect size is highly uncertain: it might be negative or it might be positive. But there is a problem, the procedure used in (1) and (2) fails to incorporate the notion of all other things being equal. This notion applies not just to known variables, but to all the other unknown factors that shape the data.\nFor the purpose of envisioning the concept, imagine that we actually had a measurement of all the factors that shaped CO2 emissions from a vehicle. We’ll call this imaginary measurement “all other things”. The figure below shows a conceptualization of what CO2 emissions as a function of the number of cylinders and “all other things” would look like.\n\n\n\n\n\n\nUsing the graph of CO2_year versus “all other things”, calculate the effect size of a change from six to four cylinders. Remember to hold “all other things” constant in your calculations. So do the effect size calculation at each of several values of “all other things”. What is the effect size and about how much does it vary from one value of “all other things” to another? -A- At “all other things being 0.25, the CO2 emissions for the 6- and 4-cylinder cars is 2800 kg and 3800 kg. The effect size is therefore (3800 kg - 2800 kg) / -2 cylinders = -500 kg/cylinder. The value of the effect size for other levels of”all other things” will be about the same, since the position of the six-cylinder dots is more-or-less constant compared to the corresponding four-cylinder dot.\n\nThis exercise is intended to give you a way of thinking about effect size and “all other things”. In reality, of course, we do not have a way to measure “all other things”. Instead, we calculate the effect size not directly from the data but from the model output (indicated by the statistic layer in the first graphic). As we saw in Lesson 22, there is actually some uncertainty about the model output stemming from sampling variability that can be summarized by a “confidence interval”. For the relationship between the number of cylinders and CO2 emissions, the extent of that uncertainty is indicated by the interval layer in the first graphic. A fair depiction for the corresponding uncertainty in the effect size can be had by repeating the process of (2), but only for points falling into the confidence interval.\n\n\n\n\n\n\nDo we need this?\n\n\n\nNote to readers who know something about car engines: As you know, switching out a six-cylinder engine for a four-cylinder engine is likely to change many other things: the weight of the vehicle, the displacement of the engine, the power available, etc. So it’s not useful to insist that everything other than the number of cylinders be held constant. Instead, we’ll be looking at the effect of that whole constellation of changes associated with a change in the number of cylinders."
  },
  {
    "objectID": "LC-list.html#e-2",
    "href": "LC-list.html#e-2",
    "title": "List of learning checks",
    "section": "20.E",
    "text": "20.E\nThroughout this course, you’re going to be using lm() to build models. Often, to demonstrate “sampling variation.” you will use sample() on a dataset or a DAG to generate a random sample and then send the result as the data= argument to lm(),\nHere are three computer commands that use the data= argument in different ways. One of them doesn’t work at all. Which one?\nsample(mtcars, size=10) %>% lm(mpg ~ wt + hp, data=.)\nsample(mtcars, size=10) %>% lm(mpg ~ wt + hp)\nlm(mpg ~ wt + hp, data=sample(mtcars, size=10))\n\n\n\n\n\n\nIn draft\n\n\n\nThis problem would be come irrelevant if the fitmodel() command described in the [Lesson 19 NTI]{../NTI/NTI-Lesson19.html} is being used.\n\nLesson 21\n\n\n21.1\nThe following command will generate a data frame with 1000 rows from dag00 and calculate the variance of the x and y variables:\n\nsample(dag00, size=1000) %>%\n  summarize(vx = var(x), vy = var(y))\n\n# A tibble: 1 × 2\n     vx    vy\n  <dbl> <dbl>\n1  3.78  1.04\n\n\nCompare this result to the DAG tilde expressions\n\ndag00\n\n[[1]]\nx ~ eps(2) + 5\n\n[[2]]\ny ~ eps(1) - 7\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nIn the tilde expressions, eps(2) means to generate noise of magnitude 2.0.\n\nIs the argument to eps() specified in terms of the variance or the standard deviation?\nThe tilde expression for x specifies that the constant 5 is to be added to eps(2). Similarly, the constant -7 is added to y. How do these constants relate to the calculated magnitudes of x and y?\n\n\nSolution\n\nThe standard deviation. For instance, x has noise of magnitude 2. The variance of x is 4, the square of 2.\nThe standard deviation (and therefore the variance) ignore such added constants.\n\n\n\n\n\n21.2\n?@sec-signal-and-noise introduces the idea that variables consist of components. A simple breakdown is into two components: i. the part of the variable that is determined by other variables in the system (“signal”) and ii. the random part of the variable (“noise”). The section uses dag01 as an illustration of how a variable can be partly determined and partly random noise.\n\nWrite and execute a command that will generate 500 rows of simulated data from dag01 and will calculate the standard deviation of x and of y.\nWhat’s the magnitude of x in the simulated data? What’s the magnitude of y?\nDoes this change if you use data with 1000 or 20000 rows?\n\n\nSolution\n\nsample(dag01, size=500) %>% summarize(sx = sd(x), sy=sd(y))\nThe standard deviation of x is about 1, the standard deviation of y is about 1.8.\nNo, the values are roughly the same regardless of the size of the sample.\n\n\n\n\n\n21.3\n[DRAW several DAG-like graphs, one of which should be undirected in all edges, one should be undirected on one or two edges (but not all), and one should be cyclic and another acyclic.]\nReferring to the graphs in the figure, say which ones are DAGs. If a graph is not a DAG, say whether that’s because it’s not directed or because it’s not cyclic.\n\nSolution\n\n\n\n21.A\nA DAG (directed acyclic graph) is a mathematical object used to state hypothetical causal relationships between variables. Explain briefly (e.g. a few sentences overall) what each of the words “directed,” “acyclic,” and “graph” mean in the context of a DAG.\n\n\n\n\n\n\nSolution\n\n\n\n\nA graph is a relationship between discrete elements (called “nodes” abstraction) each of which for us represents a hypothetical quantity, that is, a variable. In addition to the nodes, the graph contains edges which represent the connections between variables.\nA directed graph is one whose arrows have a direction. For instance \\(A \\rightarrow B \\leftarrow C\\) means that \\(A\\) and \\(C\\) together cause \\(B\\), but \\(B\\) has no influence on \\(A\\) and \\(C\\).\nAn acyclic graph is one where it is impossible to start on any given node and, by following the directed edges, return back to that node.\n\n\n\n\n\n\n21.B\nDraw these DAGs:\n\n“April showers bring May flowers.”\n“Price of a rug is determined by the size and the quality of materials.”\n“The weight of an automobile is reflected in the MPG fuel economy, as is the speed of the car, and inflation level of the tires.”\n“Plants tend to grow in the direction of the sunlight.”\n“An ice-cream shop owner needs to plan staffing based on the season, day of the week, and holidays.”\n\n\n\n\n21.C\n\n\n\n21.4\nGenerate simulated data from dag01 with 1000 rows. Fit the regression model y ~ x to the data and examine the coefficients.\n\nHow do the coefficients relate to the tilde expressions that define dag01?\nInstead of using the regression model y ~ x, where y is the response variable, try the regression model x ~ y. Do the coefficients from x ~ y correspond in any simple way to the tilde expressions that define dag01?\n\n\nSolution\n\nsample(dag01, size=1000) %>%\n  lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      4.018        1.507  \n\n\nThe intercept corresponds to the additive constant (4) in the y tilde expression. The x coefficient corresponds to the multiplier on x in the tilde expression.\nThe formula for x isn’t reflected by the coefficients.\nUsing x as the response variable:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .)\n\n\nCall:\nlm(formula = x ~ y, data = .)\n\nCoefficients:\n(Intercept)            y  \n     -1.850        0.462  \n\n\nThese coefficients do not appear in the dag01 tilde expressions.\n\n\n\n\n21.5 (Objective 21.2)\nYou are trying to understand why automobile fuel economy varies from model to model. Using the mtcars data frame (documentation at help(\"mtcars\")) …\n\nWhat’s an appropriate choice of a response variable?\nPick two explanatory variables of interest to you. Build an appropriate model from the data, extracting the coefficients() of the model. Explain what the coefficients mean in everyday terms that your cylinder-head uncle would approve of.\nWhich of the other variables are covariates? iv.Pick a covariate that your intuition suggests would be important. Include that covariate in the model from (ii) and say whether the covariate shows up as important in the model coefficients.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nAll of the other variables are covariates. A covariate is merely a potential explanatory variable that you are not directly interested in. Of course, not all covariates play an important role in the system.\n\n\n\n\n\n\n21.6 (Obj 21.3)\nGenerate a sample of size \\(n=100\\) from dag03. Use the data to construct a model of y versus x. But instead of using coefficients() to look at the model coefficients, use confint(). While coefficients() reports a single value for each coefficient, confint() reports a plausible interval for coefficients that is consistent with the data.\n\nFor \\(n=100\\), how wide is the interval reported by confint() on the x coefficient.\nRepeat the process of sampling and modeling, but this time use \\(n=400\\). How wide is the interval reported by confint() on the x coefficient.\nAgain repeat the process of sampling and modeling, this time using \\(n=1600\\). iv. Does the interval reported depend systematically on the size \\(n\\) of the sample? Describe what pattern you see.\n\n\n\n\n21.7\nA short report from the British Broadcasting Company (BBC) was headlined “Millennials’ pay ‘scarred’ by the 2008 banking crisis.”\n\nPay for workers in their 30s is still 7% below the level at which it peaked before the 2008 banking crisis, research has suggested. The Resolution Foundation think tank said people who were in their 20s at the height of the recession a decade ago were worst hit by the pay squeeze. It suggested the crisis had a lasting “scarring” effect on their earnings.\n\n\nThe foundation said people in their 30s who wanted to earn more should move to a different employer. The research found those who stayed in the same job in 2018 had real wage growth of 0.5%, whereas those who found a different employer saw an average increase of 4.5%.\n\nThe phrase “should move” in the second paragraph of the report suggests causation. A corresponding DAG would look like this:\n\nmove_to_new_employer \\(\\rightarrow\\) higher_wage\n\nFor the sake of discussion, let’s add two more variables to the DAG:\n\neffectiveness—standing for how productive the employee is.\nqualifications—standing for whether the employee is a good candidate attractive to potential new employers.\n\nConstruct a DAG with all four variables that represent plausible causal connections between them. The DAG should not contain a direct causal connection between move_to_new_employer and higher_wage.\nIf the world worked this way, would it necessarily be good advice to switch to a new employer with the aim of earning a higher wage?\n\n\nLesson 22\n\n\n22.1 (Obj 20.3)\nConsider these three data frames:\n\nOne <- sample(dag01, size=25)\nTwo <- do(10) * {\n  lm(y ~ x, data = sample(dag01, size=25)) %>%\n    coefficients()\n  }\nThree <- Two %>% \n  summarize(mx = mean(x), sx = sd(x))\n\n\nBoth One and Two have columns called x, but they stand for different things. Explain what the unit of observation is and what the values in x represent..\nThree does not have a column named x, but it is a summary of the x column from Two. What kind of summary.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nIn One, the x column contains the simulated of the x variable from dag01. The unit of observation is a single case, for instance a person for whom observations were made of x and y. The simulation involves generating 25 rows of data: one row for each of 25 people.\nIn Two, the x column is the regression coefficient on x from the simulation. Each row of Two corresponds to one trial in which regression is being performed on a sample of size 25 of simulated data from dag01.\nThree is a summary of the 10 trials in Two. The columns, named mx and sx, tell about the distribution of x across all the trials.\n\n\n\n\n\n\n22.2 (Obj 21.3)\nPart 1\nYou are going to write a procedure that automates the following process:\n\nsampling from a DAG, specifically dag01, using sample() with a size of 25.\nfitting a model y ~ x using lm()\nreporting the coefficient on x using coefficients().\n\nCall the procedure proc1().\nTo do this fill in the following template in your Rmd document:\nproc1 <- function() {\n  # your statements go here\n}\nOnce you have proc1() ready, you can carry out the procedure by giving a simple command:\nproc1()\nPart 2\nNow that you have proc1() ready and have tried it out, you are going to run the procedure 100 times repeatedly and look at the distribution in the x coefficient.\nOf course, you could laboriously give the command proc1() 5 times, and write down the x coefficient each time. Far better, though, to automate the process of repeating and collecting the x coefficient.\nYou can do this easily by using do(5) in conjunction with proc1().\n\nWhat’s the form in which the coefficients are collected when using do()?\nIs the x coefficient the same from trial to trial? Explain why or why not.\nChange your statement to run 100 trials rather than just 5, and to store the collected results in a data frame called Trials. Use appropriate graphics to display the distribution of the x coefficient. Summarize the distribution in a sentence or two.\nCreate a consise summary of the x column of Trials using summarize() with sd(x) to calculate the standard deviation. Compare the size of the standard deviation to the graphical display in (3).\n\n\nSolution\nPart 1\n\nproc1 <- function() {\n  Dat <- sample(dag01, size=25)\n  Mod <- lm(y ~ x, data = Dat)\n  coefficients(Mod)\n}\n\nor, more concisely ::: {.cell}\nproc1 <- function() {\n  sample(dag01, size=25) %>%\n    lm(y ~ x, data = .) %>%\n    coefficients()\n}\n\n\n\n\nPart 2\n\ndo(5) * proc1()\n\n  Intercept        x\n1  4.048530 1.770314\n2  3.693512 1.475927\n3  4.105278 1.297085\n4  4.250074 1.639121\n5  3.934672 1.201477\n\n\n\nThe results of the five trials are collected into a data frame.\nThe x coefficients varies from trial to trial.\n\nCollect 100 trials\n\nTrials <- do(100) * proc1()\n\n\nAn appropriate graphical display of the trials:\n\n\nggplot(Trials, aes(x)) + geom_density(fill=\"blue\", alpha=0.3)\n\n\n\n\nThe x coefficient varies from near 0.5 to near 2.5 in a bell-shaped form.\n\nSummarize the trials by the standard deviation.\n\n\nTrials %>% summarize(s = sd(x))\n\n          s\n1 0.2084765\n\n\nThe standard deviation is about 1/4 the width of the distribution."
  },
  {
    "objectID": "LC-list.html#lesson-21",
    "href": "LC-list.html#lesson-21",
    "title": "List of learning checks",
    "section": "Lesson 21",
    "text": "Lesson 21"
  },
  {
    "objectID": "LC-list.html#section-5",
    "href": "LC-list.html#section-5",
    "title": "List of learning checks",
    "section": "21.1",
    "text": "21.1\nThe following command will generate a data frame with 1000 rows from dag00 and calculate the variance of the x and y variables:\n\nsample(dag00, size=1000) %>%\n  summarize(vx = var(x), vy = var(y))\n\n# A tibble: 1 × 2\n     vx    vy\n  <dbl> <dbl>\n1  3.78  1.04\n\n\nCompare this result to the DAG tilde expressions\n\ndag00\n\n[[1]]\nx ~ eps(2) + 5\n\n[[2]]\ny ~ eps(1) - 7\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nIn the tilde expressions, eps(2) means to generate noise of magnitude 2.0.\n\nIs the argument to eps() specified in terms of the variance or the standard deviation?\nThe tilde expression for x specifies that the constant 5 is to be added to eps(2). Similarly, the constant -7 is added to y. How do these constants relate to the calculated magnitudes of x and y?\n\n\nSolution\n\nThe standard deviation. For instance, x has noise of magnitude 2. The variance of x is 4, the square of 2.\nThe standard deviation (and therefore the variance) ignore such added constants."
  },
  {
    "objectID": "LC-list.html#section-6",
    "href": "LC-list.html#section-6",
    "title": "List of learning checks",
    "section": "21.2",
    "text": "21.2\n?@sec-signal-and-noise introduces the idea that variables consist of components. A simple breakdown is into two components: i. the part of the variable that is determined by other variables in the system (“signal”) and ii. the random part of the variable (“noise”). The section uses dag01 as an illustration of how a variable can be partly determined and partly random noise.\n\nWrite and execute a command that will generate 500 rows of simulated data from dag01 and will calculate the standard deviation of x and of y.\nWhat’s the magnitude of x in the simulated data? What’s the magnitude of y?\nDoes this change if you use data with 1000 or 20000 rows?\n\n\nSolution\n\nsample(dag01, size=500) %>% summarize(sx = sd(x), sy=sd(y))\nThe standard deviation of x is about 1, the standard deviation of y is about 1.8.\nNo, the values are roughly the same regardless of the size of the sample."
  },
  {
    "objectID": "LC-list.html#section-7",
    "href": "LC-list.html#section-7",
    "title": "List of learning checks",
    "section": "21.3",
    "text": "21.3\n[DRAW several DAG-like graphs, one of which should be undirected in all edges, one should be undirected on one or two edges (but not all), and one should be cyclic and another acyclic.]\nReferring to the graphs in the figure, say which ones are DAGs. If a graph is not a DAG, say whether that’s because it’s not directed or because it’s not cyclic.\n\nSolution"
  },
  {
    "objectID": "LC-list.html#a-1",
    "href": "LC-list.html#a-1",
    "title": "List of learning checks",
    "section": "21.A",
    "text": "21.A\nA DAG (directed acyclic graph) is a mathematical object used to state hypothetical causal relationships between variables. Explain briefly (e.g. a few sentences overall) what each of the words “directed,” “acyclic,” and “graph” mean in the context of a DAG.\n\n\n\n\n\n\nSolution\n\n\n\n\nA graph is a relationship between discrete elements (called “nodes” abstraction) each of which for us represents a hypothetical quantity, that is, a variable. In addition to the nodes, the graph contains edges which represent the connections between variables.\nA directed graph is one whose arrows have a direction. For instance \\(A \\rightarrow B \\leftarrow C\\) means that \\(A\\) and \\(C\\) together cause \\(B\\), but \\(B\\) has no influence on \\(A\\) and \\(C\\).\nAn acyclic graph is one where it is impossible to start on any given node and, by following the directed edges, return back to that node."
  },
  {
    "objectID": "LC-list.html#b-1",
    "href": "LC-list.html#b-1",
    "title": "List of learning checks",
    "section": "21.B",
    "text": "21.B\nDraw these DAGs:\n\n“April showers bring May flowers.”\n“Price of a rug is determined by the size and the quality of materials.”\n“The weight of an automobile is reflected in the MPG fuel economy, as is the speed of the car, and inflation level of the tires.”\n“Plants tend to grow in the direction of the sunlight.”\n“An ice-cream shop owner needs to plan staffing based on the season, day of the week, and holidays.”"
  },
  {
    "objectID": "LC-list.html#c-1",
    "href": "LC-list.html#c-1",
    "title": "List of learning checks",
    "section": "21.C",
    "text": "21.C"
  },
  {
    "objectID": "LC-list.html#section-8",
    "href": "LC-list.html#section-8",
    "title": "List of learning checks",
    "section": "21.4",
    "text": "21.4\nGenerate simulated data from dag01 with 1000 rows. Fit the regression model y ~ x to the data and examine the coefficients.\n\nHow do the coefficients relate to the tilde expressions that define dag01?\nInstead of using the regression model y ~ x, where y is the response variable, try the regression model x ~ y. Do the coefficients from x ~ y correspond in any simple way to the tilde expressions that define dag01?\n\n\nSolution\n\nsample(dag01, size=1000) %>%\n  lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      4.018        1.507  \n\n\nThe intercept corresponds to the additive constant (4) in the y tilde expression. The x coefficient corresponds to the multiplier on x in the tilde expression.\nThe formula for x isn’t reflected by the coefficients.\nUsing x as the response variable:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .)\n\n\nCall:\nlm(formula = x ~ y, data = .)\n\nCoefficients:\n(Intercept)            y  \n     -1.850        0.462  \n\n\nThese coefficients do not appear in the dag01 tilde expressions."
  },
  {
    "objectID": "LC-list.html#objective-21.2",
    "href": "LC-list.html#objective-21.2",
    "title": "List of learning checks",
    "section": "21.5 (Objective 21.2)",
    "text": "21.5 (Objective 21.2)\nYou are trying to understand why automobile fuel economy varies from model to model. Using the mtcars data frame (documentation at help(\"mtcars\")) …\n\nWhat’s an appropriate choice of a response variable?\nPick two explanatory variables of interest to you. Build an appropriate model from the data, extracting the coefficients() of the model. Explain what the coefficients mean in everyday terms that your cylinder-head uncle would approve of.\nWhich of the other variables are covariates? iv.Pick a covariate that your intuition suggests would be important. Include that covariate in the model from (ii) and say whether the covariate shows up as important in the model coefficients.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nAll of the other variables are covariates. A covariate is merely a potential explanatory variable that you are not directly interested in. Of course, not all covariates play an important role in the system."
  },
  {
    "objectID": "LC-list.html#obj-21.3",
    "href": "LC-list.html#obj-21.3",
    "title": "List of learning checks",
    "section": "21.6 (Obj 21.3)",
    "text": "21.6 (Obj 21.3)\nGenerate a sample of size \\(n=100\\) from dag03. Use the data to construct a model of y versus x. But instead of using coefficients() to look at the model coefficients, use confint(). While coefficients() reports a single value for each coefficient, confint() reports a plausible interval for coefficients that is consistent with the data.\n\nFor \\(n=100\\), how wide is the interval reported by confint() on the x coefficient.\nRepeat the process of sampling and modeling, but this time use \\(n=400\\). How wide is the interval reported by confint() on the x coefficient.\nAgain repeat the process of sampling and modeling, this time using \\(n=1600\\). iv. Does the interval reported depend systematically on the size \\(n\\) of the sample? Describe what pattern you see."
  },
  {
    "objectID": "LC-list.html#section-9",
    "href": "LC-list.html#section-9",
    "title": "List of learning checks",
    "section": "21.7",
    "text": "21.7\nA short report from the British Broadcasting Company (BBC) was headlined “Millennials’ pay ‘scarred’ by the 2008 banking crisis.”\n\nPay for workers in their 30s is still 7% below the level at which it peaked before the 2008 banking crisis, research has suggested. The Resolution Foundation think tank said people who were in their 20s at the height of the recession a decade ago were worst hit by the pay squeeze. It suggested the crisis had a lasting “scarring” effect on their earnings.\n\n\nThe foundation said people in their 30s who wanted to earn more should move to a different employer. The research found those who stayed in the same job in 2018 had real wage growth of 0.5%, whereas those who found a different employer saw an average increase of 4.5%.\n\nThe phrase “should move” in the second paragraph of the report suggests causation. A corresponding DAG would look like this:\n\nmove_to_new_employer \\(\\rightarrow\\) higher_wage\n\nFor the sake of discussion, let’s add two more variables to the DAG:\n\neffectiveness—standing for how productive the employee is.\nqualifications—standing for whether the employee is a good candidate attractive to potential new employers.\n\nConstruct a DAG with all four variables that represent plausible causal connections between them. The DAG should not contain a direct causal connection between move_to_new_employer and higher_wage.\nIf the world worked this way, would it necessarily be good advice to switch to a new employer with the aim of earning a higher wage?"
  },
  {
    "objectID": "LC-list.html#lesson-22",
    "href": "LC-list.html#lesson-22",
    "title": "List of learning checks",
    "section": "Lesson 22",
    "text": "Lesson 22"
  },
  {
    "objectID": "LC-list.html#obj-20.3",
    "href": "LC-list.html#obj-20.3",
    "title": "List of learning checks",
    "section": "22.1 (Obj 20.3)",
    "text": "22.1 (Obj 20.3)\nConsider these three data frames:\n\nOne <- sample(dag01, size=25)\nTwo <- do(10) * {\n  lm(y ~ x, data = sample(dag01, size=25)) %>%\n    coefficients()\n  }\nThree <- Two %>% \n  summarize(mx = mean(x), sx = sd(x))\n\n\nBoth One and Two have columns called x, but they stand for different things. Explain what the unit of observation is and what the values in x represent..\nThree does not have a column named x, but it is a summary of the x column from Two. What kind of summary.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nIn One, the x column contains the simulated of the x variable from dag01. The unit of observation is a single case, for instance a person for whom observations were made of x and y. The simulation involves generating 25 rows of data: one row for each of 25 people.\nIn Two, the x column is the regression coefficient on x from the simulation. Each row of Two corresponds to one trial in which regression is being performed on a sample of size 25 of simulated data from dag01.\nThree is a summary of the 10 trials in Two. The columns, named mx and sx, tell about the distribution of x across all the trials."
  },
  {
    "objectID": "LC-list.html#obj-21.3-1",
    "href": "LC-list.html#obj-21.3-1",
    "title": "List of learning checks",
    "section": "22.2 (Obj 21.3)",
    "text": "22.2 (Obj 21.3)\nPart 1\nYou are going to write a procedure that automates the following process:\n\nsampling from a DAG, specifically dag01, using sample() with a size of 25.\nfitting a model y ~ x using lm()\nreporting the coefficient on x using coefficients().\n\nCall the procedure proc1().\nTo do this fill in the following template in your Rmd document:\nproc1 <- function() {\n  # your statements go here\n}\nOnce you have proc1() ready, you can carry out the procedure by giving a simple command:\nproc1()\nPart 2\nNow that you have proc1() ready and have tried it out, you are going to run the procedure 100 times repeatedly and look at the distribution in the x coefficient.\nOf course, you could laboriously give the command proc1() 5 times, and write down the x coefficient each time. Far better, though, to automate the process of repeating and collecting the x coefficient.\nYou can do this easily by using do(5) in conjunction with proc1().\n\nWhat’s the form in which the coefficients are collected when using do()?\nIs the x coefficient the same from trial to trial? Explain why or why not.\nChange your statement to run 100 trials rather than just 5, and to store the collected results in a data frame called Trials. Use appropriate graphics to display the distribution of the x coefficient. Summarize the distribution in a sentence or two.\nCreate a consise summary of the x column of Trials using summarize() with sd(x) to calculate the standard deviation. Compare the size of the standard deviation to the graphical display in (3).\n\n\nSolution\nPart 1\n\nproc1 <- function() {\n  Dat <- sample(dag01, size=25)\n  Mod <- lm(y ~ x, data = Dat)\n  coefficients(Mod)\n}\n\nor, more concisely ::: {.cell}\nproc1 <- function() {\n  sample(dag01, size=25) %>%\n    lm(y ~ x, data = .) %>%\n    coefficients()\n}"
  },
  {
    "objectID": "LC-list.html#lesson-23",
    "href": "LC-list.html#lesson-23",
    "title": "List of learning checks",
    "section": "Lesson 23",
    "text": "Lesson 23"
  },
  {
    "objectID": "LC-list.html#section-10",
    "href": "LC-list.html#section-10",
    "title": "List of learning checks",
    "section": "23.1",
    "text": "23.1\nVocabulary: Sampling distribution, standard error, sampling variability, sample size\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#section-11",
    "href": "LC-list.html#section-11",
    "title": "List of learning checks",
    "section": "23.2",
    "text": "23.2\nIn LC 22.2, using do(100), you displayed the sampling distribution on the x coefficient of the model y ~ x applied to data simulated from dag01. Among other things, you calculated the standard deviation of the sampling distribution. Copy over the proc1() you wrote for LC 22.2 into your Rmd document for this lesson.\nCalculate the standard deviation of the sampling distribution in each of these situations.\n\n\n\nNumber of trials\nSample size\n\n\n\n\ndo(100)\nsize=25\n\n\ndo(100)\nsize=100\n\n\ndo(100)\nsize=400\n\n\ndo(500)\nsize=25\n\n\ndo(500)\nsize=100\n\n\ndo(500)\nsize=400\n\n\n\nIn each case, the standard deviation is somewhat random, since new simulated data is collected from dag01 each time. Nonetheless, there is a systematic pattern to how the standard deviation varies with the number of trials and with the sample size.\n\nDescribe how the standard deviation of the sampling distribution of the x coefficient varies with sample size. The general trend should be easy to see.\nDoes the standard deviation of the sampling distribution depend on the number of trials?\nGoing back to your results from (1), try to find a simple quantitative relationship that describes how the standard deviation depends on sample size. State that relationship in words."
  },
  {
    "objectID": "LC-list.html#section-12",
    "href": "LC-list.html#section-12",
    "title": "List of learning checks",
    "section": "23.3",
    "text": "23.3\nWe’re going to build models of prices of books based on the moderndive::amazon_books data frame. For each model, you will calculate the confidence interval of one or more coefficients in two ways:\n\nDirectly, using confint().\nIndirectly, using broom::tidy()\n\nModel 1.\n\nModel list_price versus amazon_price. Calculate the confidence intervals on the intercept and on the amazon_price coefficient.\nInterpret the amazon_price coefficient in everyday words.\n\n\n\n\n\n\n\nSolution to part 1.\n\n\n\n\nlm(list_price ~ amazon_price, data = amazon_books) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept)  3.716532 5.122737\namazon_price 1.049308 1.127471\n\nlm(list_price ~ amazon_price, data = amazon_books) %>% broom::tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      4.42    0.357       12.4 5.22e- 29\n2 amazon_price     1.09    0.0199      54.8 2.82e-165\n\n\n\nYou can read the confidence interval directly from the confint() report. For the regression report, calculate the confidence interval as the estimate \\(\\pm 2\\) times the “standard error.”\nJust to look at the amazon_price() coefficient, the list price is about 8% higher than the Amazon price. Here, “about” means 5% to 13%. But don’t forget the intercept. The list price is, on average, about $4.50 higher than the 1.08 multiplier on the Amazon price.\n\n\n\nModel 2.\n\nModel list_price versus amazon_price, including hard_paper as a covariate.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept)  3.028400 4.466996\namazon_price 1.042536 1.117826\nhard_paperH  1.786977 3.882884\n\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% broom::tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      3.75    0.366      10.3  1.62e- 21\n2 amazon_price     1.08    0.0191     56.5  9.65e-169\n3 hard_paperH      2.83    0.533       5.32 1.93e-  7\n\n\nA hardcover costs about $2 to $4 more than a paperback.\n\n\n\n\n\n\n\n\nNote in draft\n\n\n\nReturn to this example in the prediction lesson, to show how the confidence interval and the prediction interval are different.\nMaybe also use it in one of the side-exercises on interaction terms. (The plan is not to strongly emphasize interaction terms but to refer to them in the occasional exercise.)"
  },
  {
    "objectID": "LC-list.html#objective-23.1",
    "href": "LC-list.html#objective-23.1",
    "title": "List of learning checks",
    "section": "23.4 (Objective 23.1)",
    "text": "23.4 (Objective 23.1)\nWe’re going to work with a very short dataset so that you can see directly what resampling a data frame does. (Ordinarily, you use resampling on an entire dataset, but here we are trying to make a point about the mechanism of resampling.)\n\nCreate a data frame Five that consists of the first five rows of moderndive::mythbusters_yawn. (Hint: Use head().) Put the code for doing this into your Rmd homework paper. Note that Five contains the data from subjects 1 through 5.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nFive <- mythbusters_yawn %>% head(5)\n\n\n\n\nUse resample() to generate a new data frame from Five. At this point, you are just going to look at the result, processing it “by eye.” How many distinct human subjects are reported in the resampled data? (Your answer will likely differ from your classmates’, since resampling is done at random.)\nRepeat (2) ten times. Each time, count the number of distinct human subjects.\n\nReport those ten numbers on your write-up.\nThere will usually be one or more subjects repeated in the output. Look at these repeats carefully to check whether the variables have the same value for all the repeats or whether sometimes a repeated subject has different values for group or yawn.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nMost of the time there will be 2, 3, or 4 distinct subject. The balance of the five rows will be repeats of other subjects. When a subject is repeated, the entire row is identical for all instances of that subject.\n\n\nGoing further (optional). It’s pretty easy to automate the process of generating the resample and counting the number of distinct human subjects. Like this:\n\n{resample(Five) %>% unique() %>% nrow()}\n\n[1] 3\n\n\nUsing do(1000), carry out 1000 trials of this process, saving the overall results in a data frame named Trials. What is the mean number of unique human subjects across the 1000 trials? What fraction is this of the five subjects.\nDo the same again, but instead of using Five, use the whole mythbusters_yawn data frame (which has 50 rows). What fraction of the 50 human subjects, on average, shows up in the resamples?\n\n\n\n\n\n\nSolution\n\n\n\n\nTrials <- do(1000) * {resample(mythbusters_yawn) %>% unique() %>% nrow()}\nTrials %>% summarize(mn = mean(result)/nrow(mythbusters_yawn))\n\n       mn\n1 0.63526"
  },
  {
    "objectID": "LC-list.html#objective-23.1-1",
    "href": "LC-list.html#objective-23.1-1",
    "title": "List of learning checks",
    "section": "23.5 (Objective 23.1)",
    "text": "23.5 (Objective 23.1)\nReturn to the amazon_books data frame and the model list_price ~ amazon_price. In Exercise 23.3 you used the regression report to calculate the confidence intervals on the intercept and on the amazon_price coefficient. Now you are going to repeat the calculation in a different way, using randomization, a process called “bootstrapping”.\nThe basic process is to train a model using resampled data, like this:\n\nlm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()\n\n (Intercept) amazon_price \n    3.748436     1.129395 \n\n\nThen, using do(500), carry out 500 trials, saving the result in a data frame named Trials.\nProcess Trials to calculate both the mean and the standard deviation of the intercept and amazon_price columns. How do those results compare to the “standard error” results from the same model (without resampling) as you found in LC 23.3?\n\n\n\n\n\n\nSolution\n\n\n\nSOMETHING IS WRONG HERE. The means are about the same as from the regression report (as they should be) but the standard deviations are 3-4 times larger. WHAT GIVES?\nTHE PROBLEM IS a handful of books where the Amazon price is very different from the list price, because the book itself is very expensive (e.g. $100). Remedy\n\nSwitch to another data example, maybe doing both the regression report and the bootstrapping in one exercise.\nThis is an object lesson in outliers. Since the dollar discount is presumably proportional to the price, we should have used log transforms.\n\n\nTrials <- do(500) * {lm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()}\nTrials %>% summarize(m1 = mean(Intercept), \n                     m2 = mean(amazon_price), \n                     sintercept = sd(Intercept), \n                     samazon_price = sd(amazon_price))\n\n        m1       m2 sintercept samazon_price\n1 4.154913 1.109362  0.8972471      0.078366"
  },
  {
    "objectID": "LC-list.html#lesson-24",
    "href": "LC-list.html#lesson-24",
    "title": "List of learning checks",
    "section": "Lesson 24",
    "text": "Lesson 24\n\n\n\n\n\n\nJust to help when writing problems\n\n\n\n24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation."
  },
  {
    "objectID": "LC-list.html#objective-24.1",
    "href": "LC-list.html#objective-24.1",
    "title": "List of learning checks",
    "section": "24.1 (Objective 24.1)",
    "text": "24.1 (Objective 24.1)\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\nSolution\n\nPrediction and (2) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price."
  },
  {
    "objectID": "LC-list.html#objective-24.1-1",
    "href": "LC-list.html#objective-24.1-1",
    "title": "List of learning checks",
    "section": "24.2 (Objective 24.1)",
    "text": "24.2 (Objective 24.1)\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\nSolution\n\nPrediction\nRelationship\nRelationship\n\n\nSOME IDEAS FOR EXERCISE MODES\n\nUse mod_plot() and look at the slope of lines and offsets. Compare to the model coefficients.\nGenerate data from a DAG and look at the confidence interval on the effect size. Then make new samples and see if the effect size in those samples is consistent with the confidence interval.\nIn text, maybe look at the confidence intervals across new samples and show that they tend to overlap. Only a few of them don’t touch a common line. This is basically just a review of confidence intervals, but why not?\nInteraction. Show that when there is an interaction term, the effect size (as calculated by mod_effect()) is not constant, as it is for models with purely linear terms."
  },
  {
    "objectID": "LC-list.html#lc-24.1",
    "href": "LC-list.html#lc-24.1",
    "title": "List of learning checks",
    "section": "LC 24.1",
    "text": "LC 24.1\nThe Computational Probability and Statistics text describes an early study on human-to-human heart transplantation:\n\n“The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. [[Not in data set: Another variable called [MISSING] was used to indicate whether or not the patient was alive at the end of the study.]]”\n\nThe data frame is called Transplants. [NEED TO MOVE TO PACKAGE]\n\n\n\nYou’re going to build a model of outcome vs group based on the data in Transplants. The outcome variable has levels \"Dead\" and \"Alive\", that is, it is a two-level categorical variable. Consequently, the model output will be the probability that the transplant candidate was alive at the end of the study.\n\nBuild a model outcome == \"Alive\" ~ group from the Transplants data. Pay close attention to the left-hand side of the tilde expression: it is a calculation that produces a 1 if outcome is \"Alive\" and zero otherwise. Notice the double equal signs and the quotes around \"Alive\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod <- lm(outcome == \"Alive\" ~ group, data = Transplants)\n\n\n\n\nThe sole explanatory variable here, group also is categorical. It has levels \"Control\" and \"Treatment\".\n\nUsing eval_mod(), find the probability of being alive at the end of the study for the Control group and for the treatment group.\nThe two probabilities in (ii) do not add up to zero. Explain why.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(mod, group=\"Treatment\")\n\n      group model_output\n1 Treatment    0.3478261\n\nmod_eval(mod, group=\"Control\")\n\n    group model_output\n1 Control    0.1176471\n\n\n\n\n\nFind the effect size of the treatment. All you need is your results from (2)?\nUse mod_effect(modelname, ~ group) to calculate the effect size.\n\nIs the result consistent with what you found in (3).\nExplain in everyday language what this effect size means.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_effect(mod, ~ group)\n\n     change     group to_group\n1 -0.230179 Treatment  Control"
  },
  {
    "objectID": "LC-list.html#section-13",
    "href": "LC-list.html#section-13",
    "title": "List of learning checks",
    "section": "24.2",
    "text": "24.2\nEffect sizes generally come with units and you have to take into account the units in order to know if the effect is important or not.\n\n\n\n\n\n\nIn draft\n\n\n\nMove the Loans data into the math300 package. But for now …\n\nLoans <- readr::read_csv(\"data/loans.csv\")\n\n\n\nA case in point is provided by the Loans data frame, which records dozens of variables on each of 10,000 loans made through the Lending Club. The interest rate at which the loans are made varies substantially from loan to loan. Presumably, higher interest rates reflect a higher perception of risk of default (which would lead to the lender losing his or her money).\nHere’s a model of the interest rate. (This is for the borrowers who have a low debt-to-income percent; we won’t worry about the few very high debt-to-income cases.) We’re only interested in this problem with the effect size, which is the same as the coefficients on the model.\n\nmod <- lm(interest_rate ~ homeownership + debt_to_income + \n            account_never_delinq_percent + verified_income, \n          data = Loans %>% filter(debt_to_income<50))\nmod %>% confint()\n\n                                     2.5 %      97.5 %\n(Intercept)                    15.31467948 17.26502804\nhomeownershipOWN                0.16056412  0.73009641\nhomeownershipRENT               1.01571903  1.41695574\ndebt_to_income                  0.09565833  0.11523598\naccount_never_delinq_percent   -0.09149666 -0.07122369\nverified_incomeSource Verified  1.37798985  1.79909554\nverified_incomeVerified         2.88724295  3.39001806\n\n\nThere are two quantitative explanatory variables—debt_to_income and account_never_delinq_percent—both of which are measured in percent.\nThere are two categorical explanatory variables: homeownership and verified_income. The levels for homeownership are “MORTGAGE” (meaning money is still owed on the house), “OWN” (without a mortgage), and “RENT” (meaning the borrower rents rather than owning a home). The levels for verified_income are “Not Verified”, “Verified”, “Source Verified”.\nFor the categorical explanatory variables (and the intercept) the effect-size units are “percent interest.” For the quantitative explanatory variables, the effect-size units are “percent interest per percent,” so that when multiplied by the debt_to_income percent or the account_never_delinq_percent the result will be in “percent interest.”\n\nAccording to the model, who pays the higher interest rate (on average): people who OWN their home, people who RENT, or people who have a mortgage on their home? How much higher than the lowest-interest rate category.\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople who rent pay the highest interest rate, a little more than 1 percentage point higher than people who have mortgages. It’s interesting that people who own their homes outright pay (on average) pay about 0.45 percentage points more than people who own outright. This might be because having a mortgage means you also have a credit history.\n\n\n\nAccording to the model, who pays the higher interest rate (on average): people whose income is “not verified,” people whose income is “verified,” or people who have the source of income verified (level: “source verified”)?\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople whose income is verified pay about 3 percentage points higher interest than people whose income is “not verified.” This seems surprising, but it may be that people who have higher perceived default risk are also the people who are asked to verify their income. Things get complicated when explanatory variables are linked to each other.\n\n\n\nThe coefficients on debt_to_income and account_never_delinq_percent are the smallest numerically. Does this mean that the effects of debt_to_income and account_never_delinq_percent are smaller than the other two explanatory variables in the model? Explain why or why not. (Hint: Look at the distribution of debt_to_income and account_never_delinq_percent to get an idea for the range of values these variables take on.)"
  },
  {
    "objectID": "LC-list.html#solution-34",
    "href": "LC-list.html#solution-34",
    "title": "List of learning checks",
    "section": "Solution",
    "text": "Solution\ndebt_to_income varies over about 25 percentage points. The variation in account_never_delinq_percent is about the same, varying from about 80 to 100 percentage points. The effect of the variables (in percent interest) is determined by multiplying the coefficients by the amount of variation in the variables. So, from one extreme to the other, the effect of debt_to_income is about 2 perentage points of interest, and roughly the same for debt_to_income."
  },
  {
    "objectID": "LC-list.html#section-14",
    "href": "LC-list.html#section-14",
    "title": "List of learning checks",
    "section": "24.4",
    "text": "24.4\nThe logic of effect size is to investigate the change in output of a model when one input variable is changed, holding all other things constant. This problem is about the extent to which we mean “all”.\nThe figure shows yearly CO2 production of individual gasoline-fueled passenger vehicles stratified by the number of engine 4 or 6 cylinders. ::: {.cell} ::: {.cell-output-display}"
  },
  {
    "objectID": "LC-list.html#x",
    "href": "LC-list.html#x",
    "title": "List of learning checks",
    "section": "24.X",
    "text": "24.X\nIn very simple settings, you don’t need access to the original data: a simple summary will do.\n\n\n\n\n\n\nIn draft\n\n\n\nUse the data/bloodthinner.csv data from CPS chapter 19. Create the table of counts and calculate the effect size/probabilities from that."
  },
  {
    "objectID": "LC-list.html#z",
    "href": "LC-list.html#z",
    "title": "List of learning checks",
    "section": "24.Z",
    "text": "24.Z\nThe National Cancer Institute publishes an online, interactive “Breast Cancer Risk Assessment Tool”, also called the “Gail model.” The output of the model is a probability that the woman whose characteristics are used as input will develop breast cancer in the next five years. The inputs include:\n\nage\nrace/ethnicity\nage at first menstrual period\nage at birth of first child (if any)\nhow many of the woman’s close relatives (mother, sisters, or children) have had breast cancer\n\nAs a baseline, consider a 55-year-old, African-American woman who has never had a breast biopsy or any history of breast cancer, who doesn’t know her BRCA status, and whose close relatives have no history of breast cancer, whose first menstrual period was at age 13 and first child at age 23. No “subrace” or “place of birth” is specified.\n\nFollow the link above to the cancer risk assessment tool, and enter the baseline values into the assessment tool using the link above. According to the assessment tool, what is the probability (“risk”) of developing breast cancer in the next five years? What is the lifetime risk of developing breast cancer? -A- 1.4% risk in the next five years, 7.8% lifetime risk.\nWhat is the effect size on our baseline subject of finding out that:\n\n\none of her close relatives has developed breast cancer? -A- Using the baseline values for the inputs, but changing the number of close relatives who have developed breast cancer to 1, the new five year risk is 2.2% and the lifetime risk is 12.3%. This is a change of five year risk of 0.8 percentage points and 4.5 percentage points for lifetime risk. (In Lesson 33, you’ll see that expressing effect size of a probability is often done using “log-odds”.)\nmore than one of her close relatives have developed breast cancer? -A- This has a dramatic effect, with five-year risk increasing to 4.8 percentage points and lifetime risk increasing to 29.4%.\n\n\nWhat is the effect size associated with comparing the baseline conditions to a woman with the same conditions but a race of white? -A- Five year risk goes down by 0.3 percentage points, lifetime risk goes down by 0.4 percentage points.\nWhat is the effect size of age? Compare the baseline 55-year old woman to herself when she is 65, without the other inputs changing. Since age is quantitative, Report the effect size as percentage-points-per-year. -A- Five year risk goes up to 1.5%, an increase of 0.1 percentage points per year. Lifetime risk goes down to 5.6%, a rate of -0.67 percentage points per year.\n\n\nPossibly Stat2Data::ArcheryData and ask about effect size."
  },
  {
    "objectID": "LC-list.html#zz",
    "href": "LC-list.html#zz",
    "title": "List of learning checks",
    "section": "24.ZZ",
    "text": "24.ZZ\nThe graphs shows world record times in the 100m and 200m butterfly swim race as a function of year, sex, and race length. (Data frame: math300::Butterfly)\n\n\n\n\n\n\n\n\n\nIn 1980, what is the effect size of race distance? (Make sure to give the units.)\nIn 1980, what is the effect size of sex? (Make sure to give the units.)\nIn 2000, what is the effect size of race distance? (Be sure to give the units.)"
  },
  {
    "objectID": "LC-list.html#ww",
    "href": "LC-list.html#ww",
    "title": "List of learning checks",
    "section": "24WW",
    "text": "24WW\nFigure 3 shows a model of the running time of winners of Scottish hill races as a function of climb, distance, and sex.\n\n\n\n\n\nFigure 3: A model of running time in Scottish hill racing versus climb, distance, and sex of the runner\n\n\n\n\nUsing as a baseline a distance of 10 km, a climb of 500m, and sex female, calculate each of these effect sizes. Remember for quantitative inputs to format the effect size as a rate, for instance seconds-per-meter-of-climb.\n\nThe effect size of climb. Consider an increase of climb by 500m. - For females running a 10 km course, an increase in climb from 500m (the specified baseline for comparison) to 1000m is roughly 2000 seconds. Since climb is quantitative, this should be reported as a rate, the change of output (2000 s) divided by the change in input (500 m). This comes to 4 seconds per meter of climb.\nThe effect size of distance. Consider an increase in distance of 10 km. -A- The model output at baseline is about 3500 seconds. The model output for a distance of 20 km (an additional 10 km above baseline) is about 6000 s. The effect size is a rate: (6000 - 3500) s / 10 km, or 250 s/km. For comparison, the fastest 20 km road run by a woman (at the time this is being written) is 1 hour 1 minute 25 seconds, that is, 3685 seconds. This amounts to an average of about 180 s/km. A walker would cover 1 km in about 720 seconds. So the racers are quite fast.\nIs there any evidence of an interaction between sex and climb? -A- Yes. Note that the lines for different climb amounts are vertically spaced more widely for females than for males. This means that the effect size of climb differs between the sexes. That’s an interaction between climb and sex."
  },
  {
    "objectID": "LC-list.html#r",
    "href": "LC-list.html#r",
    "title": "List of learning checks",
    "section": "24.R",
    "text": "24.R\nOver the past hundred years, attitudes toward sex have changed. Let’s examine one way that this change might show up in data: the age at which people first had sex. The National Health and Nutrition Evaluation Survey includes a question about the age at which a person first had sex. For the moment, let’s focus just on that subset of people who have had sex at least once.\n\n\n\n\n\n\nWhat is the effect size of year born on age at first sex? Use units of years-over-years. -A- The model output is 17 years for those born round about 1940, and slightly more than 16 years for those born around 1990. The effect size is therefore about negative 1 year per 50 years\nIs there evidence for an interaction between gender and year born? -A- No, the slopes of the lines are essentially identical for males and females.\nTranslate the effect size in (1) to the units weeks-over-years. -A- 1 year per 50 years is the same as 52 weeks per 50 years, or about one week per year."
  },
  {
    "objectID": "LC-list.html#lesson-25",
    "href": "LC-list.html#lesson-25",
    "title": "List of learning checks",
    "section": "Lesson 25",
    "text": "Lesson 25\n\n\n\n\n\n\nJust while in draft\n\n\n\n25.1 Given a data frame, construct a predictor function for a specified response variable.\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error. Relate this to a predition interval.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "LC-list.html#section-15",
    "href": "LC-list.html#section-15",
    "title": "List of learning checks",
    "section": "25.2",
    "text": "25.2\nThe data frame moderndive::house_prices lists the sales prices of 21,613 houses in King County, Washington (which includes Seattle) sold from May 2014 and May 2015. Often, with price or income data, economists work with the logarithm of the price or income or income-related quantity such as house living area. We are going to do here, but this problem is not about logarithms, so once you create the “logged” data frame, you’ll just be modeling the data using the usual methods.\nTo create the “logged” data, add these two new variables to the data frame, which we will call Seattle.\n\nSeattle <- moderndive::house_prices %>% \n  mutate(log_price = log10(price),\n         log_area = log10(sqft_living))\n\n\nBuild a model of log_price ~ log_area using the Seattle data. Store the model under the name pmod.\n\n\n\n\n\n\n\nSolution\n\n\n\n\npmod <- lm(log_price ~ log_area, data = Seattle)\n\n\n\n\nImagine that you are moving to Seattle in August 2014. Housing is expensive in the Seattle area, so you might decide to live in a small house, say 750 square feet. The log_area of such a house is 2.87. Using mod_eval(), predict the log_price of such a house. (Note that mod_eval() puts the model output in a column named model_output, not log_price.) In your Rmd file, give the command and show the output. If you’re curious about what the predicted price is in dollars (rather than log-dollars), simply raise 10 to the log-dollar amount. For instance, if the model_output were 5, the dollar amount will be \\(10^5 = \\$100,000\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 2.87)\n\n  log_area model_output\n1     2.87     5.324298\n\n\nIn dollar terms, the predicted price is 105.324 = 2.1086281^{5}.\n\n\n\nRepeat (2), but for a house with lots of space: 1500 square feet. The log_area of such a house is 3.18. As in (2), give the command and show the output in your Rmd file.\nYour budget is $200,000. In log dollars this budget is log10(200000) = 5.3. The predicted price of a 750 square-foot house is somewhat beyond your budget. But you figure that some 750-square foot house will be within your budget. To see if this is likely, look at the prediction interval of the house price. You can do this by adding the interval=\"prediction\" to the mod_eval() command. Is your budget (5.3 log dollars) within the prediction interval? Show your command and the result in your Rmd file and also give a sentence stating your conclusion.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 2.87, interval=\"prediction\")\n\n  log_area model_output    lower   upper\n1     2.87     5.324298 4.993416 5.65518\n\n\nYeah! Your budget of 5.3 log dollars is near the center of the prediction interval.\n\n\n\nOn a hunch, you decide to see whether you might find a 1500 square foot (log_area = 3.18) might also fall within the prediction interval. Will it? Show your command, the result, and a sentence interpreting the result.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(pmod, log_area = 3.18, interval=\"prediction\")\n\n  log_area model_output    lower    upper\n1     3.18     5.583697 5.252851 5.914543\n\n\nStrictly speaking, your budget (5.3 log dollars) is within the prediction interval. But it is very close to the lower bound of 5.25 log dollars. So there will likely be few houses of 1500 square feet within your budget. So plan that the house you will end up purchasing will be somewhere in the range 750-1500 square feet."
  },
  {
    "objectID": "LC-list.html#xx",
    "href": "LC-list.html#xx",
    "title": "List of learning checks",
    "section": "25.XX",
    "text": "25.XX\nYou are a bus dispatcher in New York City. The Department of Education bus logistics office has called to say that a school bus has broken down and the students need to be offloaded onto a functioning bus to take them to school. Unfortunately, the DOE officer didn’t tell you how many students are on the bus. You need to make a quick prediction in order to decide what kind and how many busses you will need for the pickup.\nYou go to the NYC OpenData site bus breakdown page to get the historical data on how many students are on the bus. There are more than 200,000 bus events listed, each one of them including the number of students. You make a jitter/violin plot of the number of students on each of the 200,000 busses.\n\n\n\n\n\n\nThe violin plot looks like an upside-down T. Explain what’s going on. (Hint: How many students fit on a school bus?) -A- As very often happens, the data file contains data-entry or other mistakes producing outliers. Almost all of the 200,000 bus incidents fall into the horizontal line near zero. There are only 164 with a number above 100 students. In the US, the legal maximum capacity for a school bus is 72 students.\n\nOne of the ways of handling outliers is to delete them from the data. A softer way is to trim the outliers, giving them a value that is distinct but not so far from the mass of values. The figure below shows a violin plot where any record where the number of students is greater than 20 is trimmed to 21. ::: {.cell} ::: {.cell-output-display}  ::: :::\n\nIf you sent a small school bus (capacity 14), what fraction of the time would you be able to handle all the students on the school bus? -A- Only about 5% of the area of the violin plot is above 14.\nIf you sent one 14-passenger school bus with another on stand-by (just in case the first bus doesn’t have sufficient capacity), what fraction of the time could you handle all the students?\n\n-A- It’s tempting to say that the 2 x 14 = 28 passenger capacity could handle all the cases, but remember, the cases at 21 stand for “21 or more passengers.” We can’t tell from the violin plot how many of those have more than 28 students on board.\n\nNotice that the violin plot is jagged. Explain why. -A- The number of passengers is an integer, e.g. 1, 2, 3, …. It can’t be a number like 4.5."
  },
  {
    "objectID": "LC-list.html#yyy",
    "href": "LC-list.html#yyy",
    "title": "List of learning checks",
    "section": "25.YYY",
    "text": "25.YYY\nAt a very large ballroom dance class, you are to be teamed up with a randomly selected partner. There are 200 potential partners. The figure below shows their heights.\nFrom the data plotted, calculate a 95% prediction interval on the height of your eventual partner. (Hint: You can do this by counting.)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n59 to 74 inches.\nSince there are 200 points, a 95% interval should exclude the top five cases and the bottom five cases. So draw the bottom boundary of the interval just above the bottom five points, and the top boundary just below the top five points."
  },
  {
    "objectID": "LC-list.html#lesson-26",
    "href": "LC-list.html#lesson-26",
    "title": "List of learning checks",
    "section": "Lesson 26",
    "text": "Lesson 26\nIdeas\n\nConstruct prediction interval when evaluating a model function.\nPlot a prediction band.\nCheck the consistency of the prediction band with the DAG mechanism for large \\(n\\).\n\nIs the width right?\nIs the slope right?\n\nFor small \\(n\\) (say, \\(n=5\\)), how is the prediction band different than for large \\(n\\)?"
  },
  {
    "objectID": "LC-list.html#section-16",
    "href": "LC-list.html#section-16",
    "title": "List of learning checks",
    "section": "26.1",
    "text": "26.1\nThe openintro::bac data frame records an experiment with sixteen student volunteers at Ohio State University who each drank a randomly assigned number of cans of beer (beers). These students were evenly divided between men and women, and they differed in weight and drinking habits. Thirty minutes later, a police officer measured their blood alcohol content (bac) in grams of alcohol per deciliter of blood.\nConstruct a model of bac ~ beers using the openintro::bac data.\n\nmod <- lm(bac ~ beers, data = openintro::bac)\n\n\nFederal and state laws typically specify a legal upper limit for blood alcohol content of a driver of 0.08%. According to the model function, how many beers corresponds to this upper limit?"
  },
  {
    "objectID": "LC-list.html#solution-40",
    "href": "LC-list.html#solution-40",
    "title": "List of learning checks",
    "section": "Solution",
    "text": "Solution\nOne way to calculate this is to guess at the number of beers, then modify your guess according to whether it’s high or low.\n\nmod_eval(mod, beers=4) # too low\n\n  beers model_output\n1     4   0.05915444\n\nmod_eval(mod, beers=6) # too high\n\n  beers model_output\n1     6   0.09508197\n\n# next guess should be around 5\n\nAnother way, for the avid calculus student, is to turn the model into a function, then use Zeros() to find where the output is 0.08.\n\nf <- makeFun(mod)\nmosaicCalc::Zeros(f(beers) - 0.08 ~ beers, mosaicCalc::bounds(beers=0:10))\n\n# A tibble: 1 × 2\n  beers .output.\n  <dbl>    <dbl>\n1  5.16        0\n\n\nSimplest of all, just graph the model function and read backwards from the vertical axis."
  },
  {
    "objectID": "LC-list.html#gg",
    "href": "LC-list.html#gg",
    "title": "List of learning checks",
    "section": "26.GG",
    "text": "26.GG\nThe town where you live has just gone through a so-called 100-year rain storm, which caused flooding of the town’s sewage treatment plant and consequent general ickiness. The city council is holding a meeting to discuss install flood barriers around the sewage treatment plant. The are trying to decide how urgent it is to undertake this expensive project. When will the next 100-year storm occur.\nTo address the question, the city council has enlisted you, the town’s most famous data scientist, to do some research to find the soonest that a 100-year flood can re-occcur.\nYou look at the historical weather records for towns that had a 100-year flood at least 20 years ago. The records start in 1900 and you found 1243 towns with a 100-year flood that happened 20 or more years ago. The plot shows, for all the towns that had a 100-year flood at least 20 years ago, how long it was until the next flood occurred. Those town for which no second flood occurred are shown in a different color.\nYou explain to the city council what a 95% prediction interval is and that you will put your prediction in the form of a probability of 2.5% that the flood will occur sooner than the date you give. You show them how to count dots on a jitter plot to find the 2.5% level.\n\n\nWarning: Removed 1110 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\nSince the town council is thinking of making the wall-building investment in the next 10 years, you also have provided a zoomed-in plot showing just the floods where the interval to the next flood was less than ten years.\n\nYou have n = 1243 floods in your database. How many is 2.5% of 1243? -A- 31\nUsing the zoomed-in plot, starting at the bottom count the number of floods you calculated in part (a). A line drawn where the counting stops is the location of the bottom of the 95% coverage interval. Where is the bottom of the 95% interval.-A- About 2.5 years.\nA council member proposes that the town act soon enough so that there is a 99% chance that the next 100-year flood will not occur before the work is finished. It will take 1 year to finish the work, once it is started. According to your data, when should the town start work? -A- Find the bottom limit that excludes 1% of the 1243 floods in your data. This will be between the 12th and 13th flood, counting up from the bottom. This will be at about 1.25 years, that is 15 months. So the town has 3 months before work must begin. That answer will be a big surprise to those who think the next 100-year flood won’t come for about 100 years.\nA council member has a question. “Judging from the graph on the left, are you saying that the next 100-year flood must come sometime within the next 120 years?” No, that’s not how the graph shold be read. Explain why. -a- Since the records only start in 1900, the longest possible interval can be 120 years, that is, from about 2020 to 1900. About half of the dots in the plot reflect towns that haven’t yet had a recurrence 100-year flood. Those could happen at any time, and presumably many of them will happen after an interval of, say, 150 years or even longer."
  },
  {
    "objectID": "LC-list.html#ww-1",
    "href": "LC-list.html#ww-1",
    "title": "List of learning checks",
    "section": "26.WW",
    "text": "26.WW\n\n\n\n\n\n\nIn draft\n\n\n\nThis exercise will be about the mean square error when modeling BMI versus weight. Weight is not the same thing as BMI, though it is closely related. You can see from the data the amount of variation there is in BMI at any given weight.\nAdd in Height as an explanatory variable. Mean square error gets smaller. (R^2 goes from .85 to .95)\nFOR EXTRA CREDIT? Model log BMI against log Weight + log Height. RMS residual is zero."
  },
  {
    "objectID": "LC-list.html#nn",
    "href": "LC-list.html#nn",
    "title": "List of learning checks",
    "section": "26.NN",
    "text": "26.NN\nThe violin plot shows gasoline consumption (in miles-per-gallon for city driving) stratified by the number of passenger doors for the SDSdata::MPG data frame. (For many vehicles, the values are missing. These are marked “NA”.)\n\n\n\n\n\n\nSketch in 95% prediction intervals for gasoline consumption stratified by the number of passenger doors.\n\n::: {.callout-note} ## Solution ::: {.cell} ::: {.cell-output-display}  ::: ::: :::\n\nWhich category – 2 doors, 4 doors, or NA – has the largest number of vehicles?\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is no way to tell from the violin graph. Each violin shows the distribution of values within its category. There’s no information about how many data rows are comprised by an individual violin.\n\n\nThe unit of observation in the MPG data frame is “a model of car”. There are 1154 different models included in MPG, thus 1154 rows. Imagine that you had another data frame with the same variables as MPG, but where the unit of observation were “a registered vehicle”. There are roughly 290 million vehicles registered in the US, so the data frame would have 290 million rows. Also imagine (probably contrary to reality at present) that car models with relatively high miles per gallon are much more popular than cars with low miles per gallon.\n\nSketch out what the MPG ~ doors violins would look like for the 290 million-row table.\n\n\n\n\n\n\n\nSolution\n\n\n\nCompared to the 1154 data frame, the violins would be fatter at the higher miles-per-gallon and correspondingly thinner at low miles-per-gallon. If they are fatter in one place, they must be thinner in another so that the overall area of each violin stays the same.\nThe graph shows an imagined scenario where cars with a 10 mile-per-gallon increase in fuel efficiency triples the popularity of a car model."
  },
  {
    "objectID": "LC-list.html#r-1",
    "href": "LC-list.html#r-1",
    "title": "List of learning checks",
    "section": "26.R",
    "text": "26.R\nYou’ve been told that Jenny is in an elementary school that covers grade K through 6. Predict how old is Jenny.\n\nPut your prediction in the format of assigning a probability to each of the possible outcomes, as listed below. Remember that the sum of your probabilities should be 1. (You don’t have to give too much thought to the details. Anything reasonable will do.)\n\nAge         | 3 or under | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12  | 13 | 14 | 15+\n------------|------------|---|---|---|---|---|---|----|----|-----|----|----|-----\nprobability |            |   |   |   |   |   |   |    |    |     |    |    |\n\n\n\n\n\n\nSolution\n\n\n\nPerhaps something like the following, where the probabilities are given in percentage points.\nAge         | 3 or under  | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15+\n------------|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|------\nprobability |      0      | 2.5 | 12  | 12  | 12  | 12  | 12  | 12  | 12  | 11  | 2   | 0.5 | 0\nAges 5 through 12 are equally likely, with a small possibility of 4-year olds or 14 year olds.\n\n\n\nTranslate your set of probabilities to a 95% prediction interval.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe 95% prediction interval 5 to 12 years old.\nA 95% interval should leave out 2.5% of the total probability on either end. Below age 5 there is 2.5% and above age 12 there is 2.5%.\nIf you wrote your own probabilities so that there’s no cut-off that gives exactly 2.5%, then set the interval to come as close as possible to 2.5%."
  },
  {
    "objectID": "LC-list.html#lesson-27",
    "href": "LC-list.html#lesson-27",
    "title": "List of learning checks",
    "section": "Lesson 27",
    "text": "Lesson 27\nThis is a QR day."
  },
  {
    "objectID": "LC-list.html#lesson-28",
    "href": "LC-list.html#lesson-28",
    "title": "List of learning checks",
    "section": "Lesson 28",
    "text": "Lesson 28\n\n\n\n\n\n\nRemove these hard-coded objectives after drafting problems\n\n\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "LC-list.html#section-17",
    "href": "LC-list.html#section-17",
    "title": "List of learning checks",
    "section": "28.1",
    "text": "28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution\n:::"
  },
  {
    "objectID": "LC-list.html#b-2",
    "href": "LC-list.html#b-2",
    "title": "List of learning checks",
    "section": "28.B",
    "text": "28.B\nWhenever you seek to study a partial relationship, there must be at least three variables involves: a response variable, an explanatory variable that is of direct interest, and one or more other explanatory variables that will be held constant: the covariates. Unfortunately, it’s hard to graph out models involving three variables on paper: the usual graph of a model just shows one variable as a function of a second.\nOne way to display the relationship between a response variable and two quantitative explanatory variables is to use a contour plot. The two explanatory variables are plotted on the axes and the fitted model values are shown by the contours. Figure 4 shows such a display of the fitted model of used car prices as a function of mileage and age.\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\nThe dots are the mileage and age of the individual cars — the model Price is indicated by the contours.\nThe total relationship between Price and mileage involves how the price changes for typical cars of different mileage.\n\nPick a dot that is a typical car with about 10,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price (in dollars)?\n\n18000, 21000, 25000, 30000\n\nPick another dot that is a typical car with about 70,000 miles. Using the contours, find the model price of this car. Which of the following is closest to the model price?\n\n18000 21000, 25000, 30000\nThe total relationship between Price and mileage is reflected by this ratio: change in model price divided by change in mileage. What is that ratio (roughly)?\n\n\\(\\frac{30000 - 21000}{70000-10000}=0.15\\) dollars/mile\n\\(\\frac{70000-10000}{25000-21000}=15.0\\) dollars/mile\n\\(\\frac{25000 - 18000}{70000-10000}=0.12\\) dollars/mile\n\nIn contrast, the partial relationship between Price and mileage holding age constant is found in a different way, by comparing two points with different mileage but exactly the same age.\n\nMark a point on the graph where age is 3 years and mileage is\nKeep in mind that this point doesn’t need to be an actual car, that is, a data point in the graph typical car. There might be no actual car with an age of 3 years and mileage 10000. But using the contour model, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000 28000, 30000\n\nFind another point, one where the age is exactly the same (3 years) but the mileage is different. Again there might not be an actual car there. Let’s pick mileage as 80000. Using the contours, find the model price at this point. Which of these is closest?\n\n22000, 24000, 26000, 28000, 30000\n\nThe partial relationship between price and mileage (holding age constant) is reflected again reflected by the ratio of the change in model price divided by the change in mileage. What is that ratio (roughly)?\n\n\n\\(\\frac{80000-10000}{25000-21000} = 17.50\\) dollars/mile\n\\(\\frac{28000 - 22000}{80000-10000}=0.09\\) dollars/mile\n\\(\\frac{26000 - 24000}{80000-10000}=0.03\\) dollars/mile\n\n\nBoth the total relationship and the partial relationship are indicated by the slope of the model price function given by the contours. The total relationship involves the slope between two points that are typical cars, as indicated by the dots. The partial relationship involves a slope along a different direction. When holding age constant, that direction is the one where mileage changes but age does not (vertical in the graph).\nThere’s also a partial relationship between price and age holding mileage constant. That partial relationship involves the slope along the direction where age changes but mileage is held constant. Estimate that slope by finding the model price at a point where age is 2 years and another point where age is 5 years. You can pick whatever mileage you like, but it’s key that your two points be at exactly the same mileage.\n\nEstimate the slope of the price function along a direction where age changes but mileage is held constant (horizontally on the graph).\n\n\n100 dollars per year\n500 dollars per year\n1000 dollars per year\n2000 dollars per year\n\nThe contour plot in Figure 4 depicts a model in which both mileage and age are explanatory variables. By choosing the direction in which to measure the slope, one determines whether the slope reflects a total relationship (a direction between typical cars), or a partial relationship holding age constant (a direction where age does not change, which might not be typical for cars), or a partial relationship holding mileage constant (a direction where mileage does not change, which also might not be typical for cars).\nIn calculus, the partial derivative of price with respect to mileage refers to an infinitesimal change in a direction where age is held constant. Similarly, the partial derivative of price with respect to age refers to an infinitesimal change in a direction where mileage is held constant.\nOf course, in order for the directional derivatives to make sense, the price function needs to have both age and mileage as explanatory variables. Figure 5 shows a model in which only age has been used as an explanatory variable: there is no dependence of the function on mileage.\n\n\n\n\n\nFigure 5: ?(caption)\n\n\n\n\nSuch a model is incapable of distinguishing between a partial relationship and a total relationship. Both the partial and the total relationship involve a ratio of the change in price and change in age between two points. For the total relationship, those two points would be typical cars of different ages. For the partial relationship, those two points would be different ages at exactly the same mileage. But, because the model depend on mileage, the two ratios will be exactly the same."
  },
  {
    "objectID": "LC-list.html#c-2",
    "href": "LC-list.html#c-2",
    "title": "List of learning checks",
    "section": "28.C",
    "text": "28.C\nIn each of the following, a situation is described and a question is asked that is to be answered by modeling. Several variables are listed. Imagine an appropriate model and identify each variable as either the response variable, an explanatory variable, a covariate, or a variable to be ignored.\n\nEXAMPLE: Some people have claimed that police foot patrols are more effective at reducing the crime rate than patrols done in automobiles. Data from several different cities is available; each city has its own fraction of patrols done by foot, its own crime rate, etc. The mayor of your town has asked for your advice on whether it would be worthwhile to shift to more foot patrols in order to reduce crime. She asks, “Is there evidence that a larger fraction of foot patrols reduces the crime rate?”\n\nVariables:\n\nCrime rate (e.g., robberies per 100000 population)\nFraction of foot patrols\nNumber of policemen per 1000 population\nDemographics (e.g., poverty rate)\n\nAnswer: The question focuses on how the fraction of foot patrols might influence crime rate, so crime rate is the response variable and fraction of foot patrols is an explanatory variable.\nBut, the crime rate might also depend on the overall level of policing (as indicated by the number of policemen), or on the social conditions that are associated with crime (e.g., demographics). Since the mayor has no power to change the demographics of your town, and probably little power to change the overall level number of policemen, in modeling the data from the different cities, you would want to hold constant number of policemen and the demographics. You can do this by treating number of policemen and demographics as covariates and including them in your model.\n\n\nAlcohol and Road Safety\nFifteen years ago, your state legislature raised the legal drinking age from 18 to 21 years. An important motivation was to reduce the number of car accident deaths due to drunk or impaired drivers. Now, some people are arguing that the 21-year age limit encourages binge drinking among 18 to 20 year olds and that such binge drinking actually increases car accident deaths. But the evidence is that the number of car accident deaths has gone down since the 21-year age restriction was introduced.\nYou are asked to examine the issue: Does the reduction in the number of car-accident deaths per year point to the effectiveness of the 21-year drinking age?\nVariables:\n\nDrinking age limit. Levels: 18 or 21.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car-accident deaths per year.\n\nWhich is it? response explanatory covariate ignore\n\nPrevalence of seat-belt use.\n\nWhich is it? response explanatory covariate ignore\n\nFraction of cars with air bags.\n\nWhich is it? response explanatory covariate ignore\n\nNumber of car accidents (with or without death).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nOf direct interest is how the drinking age limit accounts for the number of deaths, so these are, respectively, explanatory and response variables. But a lower death rate might also be explained by increased use of seat belts and of air bags; these can prevent deaths in an accident and they have been increasing over the same period in which the 21-year age limit was introduced.\nIn examining how the drinking age limit might affect the number of deaths, it might be important to hold these other factors constant. So, seat belts and air bags should be covariates included in the model.\nThe number of accidents is different. It seems plausible that the mechanism by which drunk driving causes deaths is by causing accidents. If the number of accidents were included as a covariate, then the model would be examining how the death rate changes with drinking age when {} even though the point is that the higher drinking age might reduce the number of accidents. So, number of accidents ought to be left out of the model.\n\n\n\n\nRating Surgeons\nYour state government wants to guide citizens in choosing physicians. As part of this effort, they are going to rank all the surgeons in your state. You have been asked to build the rating system and you have a set of variables available for your use. These variables have been measured for each of the 342,861 people who underwent surgery in your state last year: one person being treated by one doctor. How should you construct a rating system that will help citizens to choose the most effective surgeon for their own treatment?\nVariables:\n\nOutcome score. (A high score means that the operation did whatit was supposed to. A low score reflects failure, e.g. death. Death is a very bad outcome, post-operative infection a somewhat bad outcome.)\n\nWhich is it? response explanatory covariate ignore\n\nSurgeon. One level for each of the operating surgeons.\n\nWhich is it? response explanatory covariate ignore\n\nExperience of the surgeon.\n\nWhich is it? response explanatory covariate ignore\n\nDifficulty of the case.\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe patient has a choice of doctors and wants to have the best possible outcome. So the model needs to include surgeon as an explanatory variable and outcome score as the response.\nA simple model might be misleading for informing a patient’s choice. The best doctors might take on the most difficult cases and therefore have worse outcomes than doctors who are not as good. But the patient’s condition doesn’t change depending on what doctor is selected. This means that the difficulty of the case ought to be included as a covariate. The model would thus tell what is the typical outcome for each surgeon adjusting for the difficulty of the case, that is, given the patient’s condition.\nAnother variable that might explain the outcome is the experience of the surgeon; possibly more experienced surgeons produce better outcomes. However, experience of the surgeon\nshould not be included in the model used to inform a patient’s choice. The reason is that the patient’s choice of a doctor already reflects the experience of that doctor. From the patient’s point of view, it doesn’t matter whether the doctor’s outcomes reflect a high level of talent, a lot of experience, or superior training.\nThe choice of variables and covariates depends on the purpose of the model. If the purpose of the model were to decide how much experience to require of doctors before they are licensed, then an appropriate model would have outcome as the response, experience as the explanatory variable, and difficulty of the case and surgeon as covariates.\n\n\n\n\nSchool testing\nLast year, your school district hired a new superintendent to ``shake things up.’’ He did so, introducing several controversial new policies. At the end of the year, test scores were higher than last year. A representative of the teachers’ union has asked you to examine the score data and answer this question: Is there reason to think that the higher scores were the result of the superintendent’s new policies?\nVariables:\n\nSuperintendent (levels: New or Former superintendent)\n\nWhich is it? response explanatory covariate ignore\n\nExam difficulty\n\nWhich is it? response explanatory covariate ignore\n\nTest scores\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nThe issue of direct interest is whether the policies of the new superintendent might have influenced the test scores, so the model should be test scores as the response and superintendent as an explanatory variable. Of course, one possible mechanism that might have improved the scores, outside of the influence of the superintendent’s policies, is the test itself. If it were easier this year than last year, then it wouldn’t be surprising that the test scores improved this year even if the superintendent’s policies had no effect. So exam difficulty should be a covariate to be included in the model.\n\n\n\n\nGravity\nIn a bizarre twist of time, you find yourself as Galileo’s research assistant in Pisa in 1605. Galileo is studying gravity: Does gravity accelerate all materials in the same way, whether they be made of metal, wood, stone, etc.? Galileo hired you as his assistant because you have brought with you, from the 21st century, a stop-watch with which to measure time intervals, a computer, and your skill in statistical modeling. All of these seem miraculous to him.\nHe drops objects off the top of the Leaning Tower of Pisa and you measure the following:\nVariables\n\nThe size of the object (measured by its diameter).\n\nWhich is it? response explanatory covariate ignore\n\nTime of fall of the object.\n\nWhich is it? response explanatory covariate ignore\n\nThe material from which the object is made (brass, lead, wood, stone).\n\nWhich is it? response explanatory covariate ignore\n\n\n\n\n\n\nExplanation\n\n\n\nGalileo wants to know how the material affects the time of fall of the object. These are the explanatory and response variables respectively. But the size of the object also has an influence, due to air resistance. For instance, a tiny ball will fall more slowly than a large ball. So the size of the object should be a covariate.\n\n\n\n##28.D\nPolling organizations often report their results in tabular form, as in Figure 6. The basic question in the poll summarized in Figure 6 asked whether the respondant agrees with the statement, “The US was a better place to live in the 1990s and will continue to decline.”\n\n\n\n\n\nFigure 6: Results from a poll conducted by Time magazine. (Source: Time, July 28, 2008, p. 41)\n\n\n\n\nThe response variable here is “pessimism.” In the report, there are three explanatory variables: race/ethnicity, income, and age. The report’s breakdown is one explanatory variable at a time, meaning that it considers “total change” rather than “change holding other factors constant.” This can be misleading when there are connections among the explanatory variables. For instance, relatively few people in the 18 to 29 age group have high incomes.\nPollsters rarely make available the raw data they collected. This is unfortunate because it prevents others from looking at the data in different ways. For the purpose of this exercise, you’ll use simulated data in the frame math300::Econ_outlook_poll. Of course, the simulation doesn’t necessarily describe people’s attitudes directly, but it does let you see how the conclusions drawn from the poll might have been different if the results for each explanatory variable had been presented in a way that adjusts for the other explanatory variables.\n\nConstruct the model pessimism ~ age - 1. Look at the coefficients and choose the statement that best reflects the results. (In case you’re wondering: The -1 is convenient when the explanatory variable is categorical. It ensures that a coefficient is reported for each level of the age variable. You’ll have to compare coefficients for different age groups to see a trend.)\nMiddle aged people have lower pessimism than young or old people.\nYoung people have the least pessimism.\nThere is no relationship between age and pessimism.\nNow construct the model pessimism ~ income - 1. Look at the coefficients and choose the statement that best reflects the results:\nHigher income people are more pessimistic than low-income people.\nHigher income people are less pessimistic than low-income people.\nThere is no relationship between income and pessimism.\nConstruct a model in which you can look at the relationship between pessimism and age while adjusting for income. That is, include income as a covariate in your model. Look at the coefficients from your model and choose the statement that best reflects the results:\nHolding income constant, older people tend to have higher levels of pessimism than young people.\nHolding income constant, young people tend to have higher levels of pessimism than old people.\nHolding income constant, there is no relationship between age and pessimism.\nYou can also interpret that same model to see the relationship between pessimism and income while adjusting for age. Which of the following statements best reflects the results? (Hint: make sure to pay attention to the sign of the coefficients.)\nHolding age constant, higher income people are more pessimistic than low-income people.\nHolding age constant, higher income people are less pessimistic than low-income people.\nHolding age constant, there is no relationship between income and pessimism."
  },
  {
    "objectID": "LC-list.html#e-3",
    "href": "LC-list.html#e-3",
    "title": "List of learning checks",
    "section": "28.E",
    "text": "28.E\nA study1 on drug D indicates that patients who were given the drug were less likely to recover from their condition C. Here is a table showing the overall results:\n\n\n\nDrug\n# recovered\n# died\nRecovery Rate\n\n\n\n\nGiven\n1600\n2400\n40%\n\n\nNot given\n2000\n2000\n50%\n\n\n\nStrangely, when investigators looked at the situation separately for males and females, they found that the drug improves recovery for each group:\nSex | Drug | num recovered | # died | Recovery Rate :—-::———|—–:|—–:|——: Sex | Drug | num recovered | # died | Recovery Rate Females| Given | 900 | 2100 | 30% | Not given | 200 | 800 | 20% e | | | |\nMales | Given | 700 | 300 | 70% | Not given | 1800 | 1200 | 60%\n\nAre the two tables consistent with one another in terms of the numbers reported?\nDoes the drug improve recovery or hinder recovery?\nWhat advice would you give to a physician about whether or not to prescribe the drug to her patients? Give enough of an explanation that the physician can judge whether your advice is reasonable."
  },
  {
    "objectID": "LC-list.html#f-1",
    "href": "LC-list.html#f-1",
    "title": "List of learning checks",
    "section": "28.F",
    "text": "28.F\nEconomists measure the inflation rate as a percent change in price per year. Unemployment is measured as the fraction (percentage) of those who want to work who are seeking jobs.\nAccording to economists, in the short run — say, from one year to another — there is a relationship between inflation and unemployment: all other things being equal, as unemployment goes up, inflation should go down. (The relationship is called the “Phillips curve,” but you don’t need to know that or anything technical about economics to answer this question.)\n\n\nIf the Phillips-curve relationship is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative\n\n\n\nBut despite the short term relationship, economists claim that In the long run — over decades — unemployment and inflation should be unrelated.\n\n\nIf the long-run theory is true, in the model\n\nInflation ~ Unemployment, what should be the sign of the coefficient on Unemployment? positive, zero, negative}\n\n\n\n\nThe point of this exercise is to figure out how to arrange a model so that you can study the short-term behavior of the relationship, or so that you can study the long term relationship.\nFor your reference, Figure 7 shows inflation and unemployment rates over about 30 years in the US. Each point shows the inflation and unemployment rates during one quarter of a year. The plotting symbol indicates which of three decade-long periods the point falls into.\n\n\n\n\n\nFigure 7: ?(caption)\n\n\n\n\nThe relationship between inflation and unemployment seems to be different from one decade to another — that’s the short term.\n\nWhich decade seems to violate the economists’ Phillips Curve short-term relationship? A, B, C, none, all\n\nUsing the modeling language, express these different possible relationships between the variables Inflation, Unemployment, and Decade, where the variable Decade is a categorical variable with the three different levels shown in the legend for the graph.\n\nInflation depends on Unemployment in a way that doesn’t change over time.\nInflation ~ Decade\n** ~ Inflation ~ Unemployment**\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation changes with the decade, but doesn’t depend on Unemployment.\n** ~ Inflation ~ Decade**\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in the same way every decade, but each decade introduces a new background inflation rate independent of Unemployment.\nInflation ~ Decade\nInflation ~ Unemployment\n** ~ Inflation ~ Unemployment + Decade**\nInflation ~ Unemployment * Decade\nInflation depends on Unemployment in a way that differs from decade to decade.\nInflation ~ Decade\nInflation ~ Unemployment\nInflation ~ Unemployment + Decade\n** ~ Inflation ~ Unemployment * Decade**\n\n\nWhether a model examines the short-term or the long-term behavior is analogous to whether a partial change or a total change is being considered.\n\nSuppose you wanted to study the long-term relationship between inflation and unemployment. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)\nNow suppose you want to study the short-term relationship. Which of these is appropriate?\nHold Decade constant. (Partial change)\nLet Decade vary as it will. (Total change)"
  },
  {
    "objectID": "LC-list.html#sec-28-G",
    "href": "LC-list.html#sec-28-G",
    "title": "List of learning checks",
    "section": "28.G",
    "text": "28.G\nConsider dag03, involving three variables: x, y, g\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nThe nature of 95% confidence intervals means that even when the true coefficient is zero, 5% of the time the confidence interval will not include zero. In a class with 100 students, around 5 will see this failure to include zero. In order to avoid those students from being fooled by such accidental effects, feel free to analyze 2 or more samples.\n\nRegretably, in the real world, when working with data that have already been collected, you can’t use such multiple samples to check your work. So there is always that 5% chance that a real effect of size zero will produce a confidence interval that doesn’t include zero. This is one reason why replication of results is useful."
  },
  {
    "objectID": "LC-list.html#h",
    "href": "LC-list.html#h",
    "title": "List of learning checks",
    "section": "28.H",
    "text": "28.H\nThis learning challenge is much like LC -@28-G, but uses dag11 instead of dag03.\n\nCompare the graphs of dag03 and dag11. (You can use dag_draw() to generate the graph.) Are the two DAGs equivalent or not? Describe what differences you see between the two graphs and explain whether those differences are sufficient to make the two DAGs causally different.\n\nLet’s take y as the response variable, x as the explanatory variable of interest, and g as a covariate that we might or might not want to include in a model. Consequently, there are two model structures that we can choose between: y ~ x versus y ~ x + g.\n\nIs there any causal path from x to y or vice versa?\n\n\n\n\n\n\n\nSolution\n\n\n\nNo. Even though x and y are connected to one another via g, the path from x to y (or vice versa) is not causal. There is no way to get from x to y (or vice versa) by starting on one of those two nodes and following the links in their causal direction.\n\n\n\nGenerate a sample of, say, size \\(n=1000\\) from dag03 and train each of the two models mentioned above on the sample. Examine the 95% confidence intervals on the coefficients and explain which model (if either) is suitable to show that x and y are connected, and which model (if either) shows that there is no causal path between x and y.\nNow look at the graph of dag12 and speculate whether the models x ~ y and x ~ y + g will give equivalent results. (Don’t include node h in the models.) Write down your speculation. (No penalty for being wrong, it’s just a speculation!) Then, generate a sample from dag12 and, looking at the 95% confidence intervals on the coefficients, explain whether your speculation was correct or not."
  },
  {
    "objectID": "LC-list.html#i",
    "href": "LC-list.html#i",
    "title": "List of learning checks",
    "section": "28.I",
    "text": "28.I"
  },
  {
    "objectID": "LC-list.html#lesson-29",
    "href": "LC-list.html#lesson-29",
    "title": "List of learning checks",
    "section": "Lesson 29",
    "text": "Lesson 29"
  },
  {
    "objectID": "LC-list.html#sec-LC-29-A",
    "href": "LC-list.html#sec-LC-29-A",
    "title": "List of learning checks",
    "section": "29.A",
    "text": "29.A\nThe `math300::Hill_racing” data frame records 2236 winning times (in seconds) in Scottish hill racing competitions. Consider this model of the winning time as a function of the race distance (km) and the total climb (meters):\n\nmod <- lm(time ~ distance + climb, data=Hill_racing)\n\nThe model_eval() function provides a convenient way to evaluate the model output (.output) for each of the rows in a data frame and, at the same time, calculates row-by-row residuals (.resid) and prediction errors (.lwr and .upr). Make sure to take not of the starting periods on the names.\n\nmodel_eval(mod) %>% head()\n\n  time distance climb  .output     .resid       .lwr     .upr\n1 1630        6   240 1679.215  -49.21475  -29.56279 3387.992\n2 1655        6   240 1679.215  -24.21475  -29.56279 3387.992\n3 2391        6   240 1679.215  711.78525  -29.56279 3387.992\n4 2351        6   240 1679.215  671.78525  -29.56279 3387.992\n5 4151       14   660 4805.779 -654.77947 3097.10184 6514.457\n6 3975       14   660 4805.779 -830.77947 3097.10184 6514.457\n\n\nThe RMS residual from the model can be calculated this way:\n\nmodel_eval(mod) %>%\n  summarize(rms = sqrt(mean(.resid^2)))\n\n       rms\n1 870.4631\n\n\n\nWhat are the units of the RMS residual?\nModify the calculation to compute the sum-of-square residuals. Report the result numerically. Be sure to say what are the units.\nWhat are the units of the effect size on time with respect to climb?\nWhat are the units of the effect size on time with respect to climb?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nRMS residual has the same units as the response variable. In this case, that’s the time to run the race, with units “seconds.”\nSS residual has units that are the square of the respond variable, in this case “square-seconds.”\nRecall that the effect size on the response with respect to an explanatory variable has the units of the response variable divided by the units of the explanatory variable. The climb variable has units of meters, so the effect size has units “seconds/meters.”\nseconds/km"
  },
  {
    "objectID": "LC-list.html#lc-29.b",
    "href": "LC-list.html#lc-29.b",
    "title": "List of learning checks",
    "section": "LC 29.B",
    "text": "LC 29.B\nWhich of the following models are not nested within time ~ distance + climb?\n\ntime ~ 1\ntime ~ distance + sex\ntime ~ distance\ntime ~ climb\n\n\n\n\n\n\n\nSolution\n\n\n\nThe model time ~ distance + sex is not nested in time ~ distance + climb.\nNote that time ~ 1 is indeed nested in time ~ distance + climb. The 1 corresponds to the intercept."
  },
  {
    "objectID": "LC-list.html#c-3",
    "href": "LC-list.html#c-3",
    "title": "List of learning checks",
    "section": "29.C",
    "text": "29.C\nIn LC -Section 75 you calculated the RMS residuals and the sum-of-square residuals by wrangling the results from mod_eval(). That’s a perfectly good way to do things, but the work becomes tedious when there are multiple models you want to compare.\nFor convenience, there is a compare_model_residuals() command, which can calculate the RMS residual or sum-of-square residual for each of a set of models. All the models must have the same response variable.\n\nHill_racing %>% \n  compare_model_residuals(time ~ 1, \n                          time ~ distance + climb, \n                          time ~ distance + climb + sex,\n                          time ~ distance,\n                          measure = \"RMS\"\n                          )\n\n[1] 3122.4821  870.4631  775.2962 1189.7148\n\n\nIt happens that all of the models in the command are a nested set. Re-order the models so that each model nests inside the following model, that is, from smaller model to bigger model.\n\nDo the RMS residuals for the nested models increase or decrease when moving from a smaller model to a larger model?\nYou can calculate the sum-of-square residual by using the argument measure=\"SS\". Do the sum-of-square residuals for the nested models increas or decrease when moving from a smaller model to a larger model.\nYou can calculate R2 by using the argument measure=\"R2\". Do the R2 for the nested models increase or decrease when moving from a smaller model to a larger model.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nDecrease\nDecrease\nIncrease. R2 tells you how big the model output is compared to the response variable. 1-R2 tells you how big the residuals are compared to the response variable."
  },
  {
    "objectID": "LC-list.html#lc-29.d",
    "href": "LC-list.html#lc-29.d",
    "title": "List of learning checks",
    "section": "LC 29.D",
    "text": "LC 29.D\n\n\n\n\n\n\nStill a draft\n\n\n\nLook at dag07. Notice that d is not connected to any of the other variables.\nGenerate a sample of size \\(n=6\\). Compare the sum of square residual (in sample) from the nested models c ~ 1, c ~ a c ~ a + b, and c ~ a + b + d. (Use the compare_model_residuals() using the argument method=\"SS\".\nWhich, if any, of the variables a, b, or d reduces the in-sample sum-of-squared residuals compared to the previous model.\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag07, c ~ 1, c ~ a, c ~ a + b, c ~ a + b + d, \n                        n=6, measure=\"R2\")\n\n[1] 0.0000000 0.6065123 0.5833351 0.9854472\n\n\n\n\nOut of sample, the useless covariate often increases the SS error.\n\n\nLC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50)\n\n[1] 1.0007343 0.8410465 0.7984146 0.7117050\n\n\nd seems to block effect of a and b on c.\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5)\n\n[1] 0.9550914 0.8577920 0.9246427 1.4332908\n\n\n\n\n\n\nLC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, \n                        testing = \"out-of-sample\")\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?\n\n\n\nLC 29.3\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::teacher. What’s the base pay difference between a teacher with an MA and a BA degree? What’s a confidence interval on this effect size? How does the confidence interval change if you include years as a covariate.\n\n\n\n\n\nLC 29.4\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::census Predict log personal income based on other variables. Eat variance using the total_family_income variable.\n\nmod <- lm(log10(total_personal_income) ~ log10(age) + sex + marital_status + log10(total_family_income), data = openintro::census %>% filter(total_personal_income > 0, total_family_income > 0))\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: log10(total_personal_income)\n                            Df Sum Sq Mean Sq  F value    Pr(>F)    \nlog10(age)                   1  5.938  5.9383  35.6102 6.660e-09 ***\nsex                          1  5.976  5.9758  35.8351 6.006e-09 ***\nmarital_status               5  4.302  0.8604   5.1596 0.0001464 ***\nlog10(total_family_income)   1 17.620 17.6198 105.6615 < 2.2e-16 ***\nResiduals                  306 51.028  0.1668                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ngf_jitter(total_personal_income ~ total_family_income | sex, \n         data =openintro::census %>% filter(total_personal_income > 3000),\n         color=~marital_status, alpha=0.3) %>% \n  gf_refine(scale_y_log10(), scale_x_log10())\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\n\n29.5\n\n\n\n\n\n\nStill in draft\n\n\n\nopenintro::starbucks where do the calories come from? Find effect size of, say, protein on calories. Then see what happens if you use carbohydrates as a covariate.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ fat + carb + fiber + protein, data = openintro::starbucks) %>% confint()\n\n                2.5 %    97.5 %\n(Intercept) -2.938079 13.605279\nfat          8.591250  9.315766\ncarb         3.686593  3.997527\nfiber       -1.418966  1.370022\nprotein      3.631695  4.364091\n\n\n\n\n\nIn draft\nMaybe come back to this in confounding lesson. Look for components that tend to go together\n\nwith(openintro::starbucks, cor(fat, protein))\n\n[1] 0.22347\n\nwith(openintro::starbucks, cor(fiber, protein))\n\n[1] 0.488564\n\nlm( calories ~ fiber , data = openintro::starbucks) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 276.119106 343.80739\nfiber         1.923476  24.07453\n\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ protein + fiber, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 247.891487 320.333514\nprotein       1.700777   7.996897\nfiber        -8.099007  15.978382\n\n\n\n\n\nLesson 30\n\n\n30.1\nDags with longer confounding pathways. Is there mixing when leaving out an element in the pathway. Mix up the directions of the arrows and show that the mixing occurs when the covariate is included in the model.\nRegression to the mean example.\nCollider?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nLesson 31\n\n\n31.1\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nLesson 32\n\n\n32.1\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nLesson 33\nSee medGPA from the Stat2Data package. Maybe come back to this in Lesson 34.\nMaybe Data2Stats::FlightResponse\nMaybe space shuttle Challenger o-ring data.\n\n\n33.1\n\nConvert probability to odds and log odds, and vice versa.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n33.XXX\nA major company in big trouble is planning to lay off 20% of its employees. You work in the personnel office and have constructed a multivariate model of the probability of an individual being laid off. The effect size of the different input factors in the model are as follows:\n\nage over 50 years: change in log-odds 1\nsoftware engineer: change in log-odds -0.5\npay level: change in log-odd rate 0.2 per $10,000 above the company average of $40K\n\n\nWhat are the odds of being laid off for an employee for whom you have no information about age, engineering capabilities, or pay level. -A- The risk of being laid off is 20% so the probability of not being laid off is 80%. The odds are therefore 20/80 = 0.25.\nWhat are the log odds of an employee in (1) being laid off? -A- Simply the logarithm of the odds, so \\(\\log(0.25) = -1.4\\).\nConsider a 30-year old software engineer making $50,000 per year.\n\nWhat are the log odds of her being laid off? -A- At baseline, her log odds is -1.4. Being a software engineer, her log odds are lower by 0.5. But she makes more than the average pay by $10,000, which adds 0.2 to her log odds. Altogether, -1.4 - 0.5 + 0.2 = -1.7.\nTranslate the log odds into an absolute risk of her being laid off. -A- Undoing the logarithm on -1.7 gives an odds of 0.18, which corresponds to a probability of 0.18/(1 + 0.18) or 15%.\n\n\n\nIn LC 24.2, we took as baseline inputs for [this online breast-cancer risk assessment model]((https://bcrisktool.cancer.gov/calculator.html), a 55-year-old, African-American woman who has never had a breast biopsy or any history of breast cancer, who doesn’t know her BRCA status, and whose close relatives have no history of breast cancer, whose first menstrual period was at age 13 and first child at age 23.\nFor this baseline case, the output of the model was a probability of 1.4% five-year risk of developing breast cancer. Finding out that one close relative has developed breast cancer elevates this risk to 2.2%.\nThese are absolute risks: probabilities. The effect size is 2.2 - 1.4 = 0.8 percentage points. When risks are specified as probabilities, differences in risks are in “percentage points.”\n\nWhat risk ratio corresponds to the 1.4 to 2.2 change of risk? -A- This is a simple ratio: 2.2% / 1.4% which is 1.57.\nSuppose (hypothetically) that the risks were ten times higher: 14% and 22%.\n\nWhat would be the risk ratio? -A- Still 1.57.\nWhat would be the percentage point change in absolute risk? -A- 8 percentage points.\n\nPut yourself in the place of the baseline woman who has just found out that a close relative developed breast cancer. Which way of reporting a change of risk is more pertinent to your personal life? -A- The change in absolute risk. Even though the risk ratios are the same in (1) and (2), the change in absolute risk is much greater in (2).\n\n\n\n\nLesson 34\nStat2Data::FaithfulFaces. Come back to it concerning prevalence.\n\n\n34.1\nTo illustrate how stratification is used to build a classifier, consider this very simple, unrealistically small, made-up data frame listing observations of animals:\n\n\n\n\n \n  \n    species \n    size \n    color \n  \n \n\n  \n    A \n    large \n    reddish \n  \n  \n    B \n    large \n    brownish \n  \n  \n    B \n    small \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n\n\n\n\n\nYou are going to build classifiers using the data. The output of the classifier will be the probability that the species is A. The classifier itself will be a simple table: each row lists the different levels of the explanatory variable(s) the the classifier output (as a probability that the species is A).\n\nUse just size as an explanatory variable. Since there are two levels for size, the classifier can take the form of a simple table, giving the proportion of rows for each of the two sizes. Fill in the table to reflect the data.\n\n\n\n\n\n \n  \n    size \n    prop_of_A \n  \n \n\n  \n    large \n     \n  \n  \n    small \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are three rows where the size is large, of which one is species A. The classifier output is thus 2/3 for large.\nSimilarly, there is only one row where the size is small, none of which are species A. The classifier output is 0/1 for small.\n\n\n\nRepeat (1), but instead of “size”, use just “color” as an explanatory variable. ::: {.cell} ::: {.cell-output-display}\n\n\n\n \n  \n    color \n    prop_of_A \n  \n \n\n  \n    reddish \n     \n  \n  \n    brownish \n     \n  \n\n\n\n\n\n\n:::\n\n\n\n\n\n\nSolution\n\n\n\nThere are three rows where the color is brownish, of which two are species A. The classifier output is thus 1/3 for brownish.\nThere is only one row where the color is reddish, and it is species A. The classifier output is 1/1 for reddish.\n\n\n\nAgain build a classifier, but use both color and size as explanatory variables.\n\n\n\n\n\n \n  \n    color \n    size \n    prop_of_A \n  \n \n\n  \n    reddish \n    large \n     \n  \n  \n    reddish \n    small \n     \n  \n  \n    brownish \n    large \n     \n  \n  \n    brownish \n    small \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere is just one row in which color is reddish and size is large, and it is species A. The classifier output is thus 1/1.\nThere are two rows in which color is brownish and size is large, one of which is species A. The classifier output is thus 1/2.\nThere is one row in which color is brownish and size is small. It is species B. The classifier output is 1/1.\nThere are no rows in which color is reddish and size is small. A classifier output of 0/0 is meaningless. So our classifier has nothing to say for these inputs.\n\n\n\nFinally, build the “null model”, a no-input classifier. This means there is just one group, which has all four rows. -A- Of the four rows, two are species A, so the classifier output is 2/4."
  },
  {
    "objectID": "LC-list.html#lc-29.1",
    "href": "LC-list.html#lc-29.1",
    "title": "List of learning checks",
    "section": "LC 29.1",
    "text": "LC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50)\n\n[1] 1.0007343 0.8410465 0.7984146 0.7117050\n\n\nd seems to block effect of a and b on c.\n\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5)\n\n[1] 0.9550914 0.8577920 0.9246427 1.4332908"
  },
  {
    "objectID": "LC-list.html#lc-29.2",
    "href": "LC-list.html#lc-29.2",
    "title": "List of learning checks",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, \n                        testing = \"out-of-sample\")\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "LC-list.html#lc-29.3",
    "href": "LC-list.html#lc-29.3",
    "title": "List of learning checks",
    "section": "LC 29.3",
    "text": "LC 29.3\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::teacher. What’s the base pay difference between a teacher with an MA and a BA degree? What’s a confidence interval on this effect size? How does the confidence interval change if you include years as a covariate."
  },
  {
    "objectID": "LC-list.html#lc-29.4",
    "href": "LC-list.html#lc-29.4",
    "title": "List of learning checks",
    "section": "LC 29.4",
    "text": "LC 29.4\n\n\n\n\n\n\nIn draft\n\n\n\nopenintro::census Predict log personal income based on other variables. Eat variance using the total_family_income variable.\n\nmod <- lm(log10(total_personal_income) ~ log10(age) + sex + marital_status + log10(total_family_income), data = openintro::census %>% filter(total_personal_income > 0, total_family_income > 0))\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: log10(total_personal_income)\n                            Df Sum Sq Mean Sq  F value    Pr(>F)    \nlog10(age)                   1  5.938  5.9383  35.6102 6.660e-09 ***\nsex                          1  5.976  5.9758  35.8351 6.006e-09 ***\nmarital_status               5  4.302  0.8604   5.1596 0.0001464 ***\nlog10(total_family_income)   1 17.620 17.6198 105.6615 < 2.2e-16 ***\nResiduals                  306 51.028  0.1668                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ngf_jitter(total_personal_income ~ total_family_income | sex, \n         data =openintro::census %>% filter(total_personal_income > 3000),\n         color=~marital_status, alpha=0.3) %>% \n  gf_refine(scale_y_log10(), scale_x_log10())\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning: Removed 20 rows containing missing values (geom_point)."
  },
  {
    "objectID": "LC-list.html#section-18",
    "href": "LC-list.html#section-18",
    "title": "List of learning checks",
    "section": "29.5",
    "text": "29.5\n\n\n\n\n\n\nStill in draft\n\n\n\nopenintro::starbucks where do the calories come from? Find effect size of, say, protein on calories. Then see what happens if you use carbohydrates as a covariate.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ fat + carb + fiber + protein, data = openintro::starbucks) %>% confint()\n\n                2.5 %    97.5 %\n(Intercept) -2.938079 13.605279\nfat          8.591250  9.315766\ncarb         3.686593  3.997527\nfiber       -1.418966  1.370022\nprotein      3.631695  4.364091"
  },
  {
    "objectID": "LC-list.html#in-draft-8",
    "href": "LC-list.html#in-draft-8",
    "title": "List of learning checks",
    "section": "In draft",
    "text": "In draft\nMaybe come back to this in confounding lesson. Look for components that tend to go together\n\nwith(openintro::starbucks, cor(fat, protein))\n\n[1] 0.22347\n\nwith(openintro::starbucks, cor(fiber, protein))\n\n[1] 0.488564\n\nlm( calories ~ fiber , data = openintro::starbucks) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 276.119106 343.80739\nfiber         1.923476  24.07453\n\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n\nlm( calories ~ protein + fiber, data = openintro::starbucks) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 247.891487 320.333514\nprotein       1.700777   7.996897\nfiber        -8.099007  15.978382"
  },
  {
    "objectID": "LC-list.html#lesson-30",
    "href": "LC-list.html#lesson-30",
    "title": "List of learning checks",
    "section": "Lesson 30",
    "text": "Lesson 30"
  },
  {
    "objectID": "LC-list.html#section-19",
    "href": "LC-list.html#section-19",
    "title": "List of learning checks",
    "section": "30.1",
    "text": "30.1\nDags with longer confounding pathways. Is there mixing when leaving out an element in the pathway. Mix up the directions of the arrows and show that the mixing occurs when the covariate is included in the model.\nRegression to the mean example.\nCollider?\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-31",
    "href": "LC-list.html#lesson-31",
    "title": "List of learning checks",
    "section": "Lesson 31",
    "text": "Lesson 31"
  },
  {
    "objectID": "LC-list.html#section-20",
    "href": "LC-list.html#section-20",
    "title": "List of learning checks",
    "section": "31.1",
    "text": "31.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-32",
    "href": "LC-list.html#lesson-32",
    "title": "List of learning checks",
    "section": "Lesson 32",
    "text": "Lesson 32"
  },
  {
    "objectID": "LC-list.html#section-21",
    "href": "LC-list.html#section-21",
    "title": "List of learning checks",
    "section": "32.1",
    "text": "32.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-33",
    "href": "LC-list.html#lesson-33",
    "title": "List of learning checks",
    "section": "Lesson 33",
    "text": "Lesson 33\nSee medGPA from the Stat2Data package. Maybe come back to this in Lesson 34.\nMaybe Data2Stats::FlightResponse\nMaybe space shuttle Challenger o-ring data."
  },
  {
    "objectID": "LC-list.html#section-22",
    "href": "LC-list.html#section-22",
    "title": "List of learning checks",
    "section": "33.1",
    "text": "33.1\n\nConvert probability to odds and log odds, and vice versa.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#xxx",
    "href": "LC-list.html#xxx",
    "title": "List of learning checks",
    "section": "33.XXX",
    "text": "33.XXX\nA major company in big trouble is planning to lay off 20% of its employees. You work in the personnel office and have constructed a multivariate model of the probability of an individual being laid off. The effect size of the different input factors in the model are as follows:\n\nage over 50 years: change in log-odds 1\nsoftware engineer: change in log-odds -0.5\npay level: change in log-odd rate 0.2 per $10,000 above the company average of $40K\n\n\nWhat are the odds of being laid off for an employee for whom you have no information about age, engineering capabilities, or pay level. -A- The risk of being laid off is 20% so the probability of not being laid off is 80%. The odds are therefore 20/80 = 0.25.\nWhat are the log odds of an employee in (1) being laid off? -A- Simply the logarithm of the odds, so \\(\\log(0.25) = -1.4\\).\nConsider a 30-year old software engineer making $50,000 per year.\n\nWhat are the log odds of her being laid off? -A- At baseline, her log odds is -1.4. Being a software engineer, her log odds are lower by 0.5. But she makes more than the average pay by $10,000, which adds 0.2 to her log odds. Altogether, -1.4 - 0.5 + 0.2 = -1.7.\nTranslate the log odds into an absolute risk of her being laid off. -A- Undoing the logarithm on -1.7 gives an odds of 0.18, which corresponds to a probability of 0.18/(1 + 0.18) or 15%.\n\n\n\nIn LC 24.2, we took as baseline inputs for [this online breast-cancer risk assessment model]((https://bcrisktool.cancer.gov/calculator.html), a 55-year-old, African-American woman who has never had a breast biopsy or any history of breast cancer, who doesn’t know her BRCA status, and whose close relatives have no history of breast cancer, whose first menstrual period was at age 13 and first child at age 23.\nFor this baseline case, the output of the model was a probability of 1.4% five-year risk of developing breast cancer. Finding out that one close relative has developed breast cancer elevates this risk to 2.2%.\nThese are absolute risks: probabilities. The effect size is 2.2 - 1.4 = 0.8 percentage points. When risks are specified as probabilities, differences in risks are in “percentage points.”\n\nWhat risk ratio corresponds to the 1.4 to 2.2 change of risk? -A- This is a simple ratio: 2.2% / 1.4% which is 1.57.\nSuppose (hypothetically) that the risks were ten times higher: 14% and 22%.\n\nWhat would be the risk ratio? -A- Still 1.57.\nWhat would be the percentage point change in absolute risk? -A- 8 percentage points.\n\nPut yourself in the place of the baseline woman who has just found out that a close relative developed breast cancer. Which way of reporting a change of risk is more pertinent to your personal life? -A- The change in absolute risk. Even though the risk ratios are the same in (1) and (2), the change in absolute risk is much greater in (2)."
  },
  {
    "objectID": "LC-list.html#lesson-34",
    "href": "LC-list.html#lesson-34",
    "title": "List of learning checks",
    "section": "Lesson 34",
    "text": "Lesson 34\nStat2Data::FaithfulFaces. Come back to it concerning prevalence."
  },
  {
    "objectID": "LC-list.html#section-23",
    "href": "LC-list.html#section-23",
    "title": "List of learning checks",
    "section": "34.1",
    "text": "34.1\nTo illustrate how stratification is used to build a classifier, consider this very simple, unrealistically small, made-up data frame listing observations of animals:\n\n\n\n\n \n  \n    species \n    size \n    color \n  \n \n\n  \n    A \n    large \n    reddish \n  \n  \n    B \n    large \n    brownish \n  \n  \n    B \n    small \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n\n\n\n\n\nYou are going to build classifiers using the data. The output of the classifier will be the probability that the species is A. The classifier itself will be a simple table: each row lists the different levels of the explanatory variable(s) the the classifier output (as a probability that the species is A).\n\nUse just size as an explanatory variable. Since there are two levels for size, the classifier can take the form of a simple table, giving the proportion of rows for each of the two sizes. Fill in the table to reflect the data.\n\n\n\n\n\n \n  \n    size \n    prop_of_A \n  \n \n\n  \n    large \n     \n  \n  \n    small \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nThere are three rows where the size is large, of which one is species A. The classifier output is thus 2/3 for large.\nSimilarly, there is only one row where the size is small, none of which are species A. The classifier output is 0/1 for small.\n\n\n\nRepeat (1), but instead of “size”, use just “color” as an explanatory variable. ::: {.cell} ::: {.cell-output-display}\n\n\n\n \n  \n    color \n    prop_of_A \n  \n \n\n  \n    reddish \n     \n  \n  \n    brownish"
  },
  {
    "objectID": "LC-list.html#section-24",
    "href": "LC-list.html#section-24",
    "title": "List of learning checks",
    "section": "34.2",
    "text": "34.2\nThe graph below shows data on marital status versus age from National Health and Nutrition Evaluation Survey data. You can see that the probability of the various possibilities are a function of age.\n\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:mosaic':\n\n    cross\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\n\n\n\n\nA very simple classifier can be constructed just by indicating at each age which marital status is the most likely, as seen in the figure below.\n\n\nmaximum number of iterations reached -0.001660052 -0.001675416\n\n\n\n\n\nA classifier output should be a probability, not a categorical level. On the blank graph below, sketch out a plausible form for probability vs age for each of three categorical levels shown in the above plot. (Hint: At an age where, say, “NeverMarried” is the categorical output, the probability for “NeverMarried” will be higher than the other categories.)\n\n\n\n\n\n\nPresumably the probability output for each category varies somewhat smoothly. There are two constraints:\n\nAt any age/sex, one probability will be the highest of the three. That one should correspond to the category shown in the first graph.\nThe probabilities should add up to 1.\n\nHere’s one possibility. Note that for females, the highest probability around age 80 is “widowed”.\n\n\n\n\n\n\n\nFrom CPS §32.6: openintro::possum. Let’s investigate the possum data set again. This time we want to model a binary outcome variable. As a reminder, the common brushtail possum of the Australia region is a bit cuter than its distant cousin, the American opossum. We consider 104 brushtail possums from two regions in Australia, where the possums may be considered a random sample from the population. The first region is Victoria, which is in the eastern half of Australia and traverses the southern coast. The second region consists of New South Wales and Queensland, which make up eastern and northeastern Australia.\nWe use logistic regression to differentiate between possums in these two regions. The outcome variable, called pop, takes value Vic when a possum is from Victoria and other when it is from New South Wales or Queensland. We consider five predictors: sex, head_l, skull_w, total_l, and tail_l.\nExplore the data by making histograms or boxplots of the quantitative variables, and bar charts of the discrete variables.\n\nAre there any outliers that are likely to have a very large influence on the logistic regression model?\nBuild a logistic regression model with all the variables. Report a summary of the model.\nUsing the p-values decide if you want to remove a variable(s) and if so build that model.\nFor any variable you decide to remove, build a 95% confidence interval for the parameter.\nExplain why the remaining parameter estimates change between the two models.\nWrite out the form of the model. Also identify which of the following variables are positively associated (when controlling for other variables) with a possum being from Victoria: head_l, skull_w, total_l, and tail_l.\nSuppose we see a brushtail possum at a zoo in the US, and a sign says the possum had been captured in the wild in Australia, but it doesn’t say which part of Australia. However, the sign does indicate that the possum is male, its skull is about 63 mm wide, its tail is 37 cm long, and its total length is 83 cm. What is the reduced model’s computed probability that this possum is from Victoria? How confident are you in the model’s accuracy of this probability calculation?"
  },
  {
    "objectID": "LC-list.html#section-25",
    "href": "LC-list.html#section-25",
    "title": "List of learning checks",
    "section": "34.3",
    "text": "34.3\nThe HELPrct date frame (in the mosaicData package) is about a clinical trial (that is, an experiment) conducted with adult inpatients recruited from a detoxification unit. The response variable of interest reflects the success or failure of the detox treatment, namely, did the patient continue use of the substance abused after the treatment.\nFigure @ref(fig:giraffe-fall-door-1) shows the output of a simple classifier (maybe too simple!) of the response given these inputs: the average number of alcoholic drinks consumed per day in the past 30 day (before treatment); and the patient’s self-perceived level of social support from friends. (The scale for social support is zero to fourteen, with a higher number meaning more support.)\n\n\n\n\n\nClassifier based on data from a clinical trial\n\n\n\n\n\nWhat’s the probability of treatment failure for a patient who has 25 alcoholic drinks per day? Does the probability depend on the level of social support? -A- Probability of failure is 75%, and doesn’t depend on the level of social support.\nFor a patient at 0 to 10 alcoholic drinks per day, what’s the probability of treatment failure? Does the probability depend on the level of social support? -A- The probability of failure ranges from about 72% for those with no social support to 82% for those with high social support?\nYou are thinking about a friend who has roughly five alcoholic drinks per day. You are concerned that he will go on to substance abuse. Do the data from the clinical trial give good reason for your concern? Explain why or why not.\n\n\n\n\n\n\n\nSolution\n\n\n\nIt’s always a good idea to be concerned for your friend, but the data reported here are not a basis for that concern. These data are from a population consisting of inpatients from a detoxification unit. These are people who have already shown strong substance abuse. The classifier is not generalizable to your friend, unless he is an inpatient from a detox unit.\n\n\n\nExplain what’s potentially misleading about the y-axis scale selected for the plot.\n\n\n\n\n\n\n\nSolution\n\n\n\nThe selected scale doesn’t include zero and so tends to over-emphasize what amount to small differences in the probability of failure."
  },
  {
    "objectID": "LC-list.html#lesson-35",
    "href": "LC-list.html#lesson-35",
    "title": "List of learning checks",
    "section": "Lesson 35",
    "text": "Lesson 35"
  },
  {
    "objectID": "LC-list.html#section-26",
    "href": "LC-list.html#section-26",
    "title": "List of learning checks",
    "section": "35.1",
    "text": "35.1\nGiven some classifier summaries, calculate the false-positive and false-negative rates as well as the sensitivity and specificity\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#q",
    "href": "LC-list.html#q",
    "title": "List of learning checks",
    "section": "35.Q",
    "text": "35.Q\nTITLE GOES HERE: Customize the classifiers in (ref:ant-take-room) for a population in which species A is three times as common as species B."
  },
  {
    "objectID": "LC-list.html#solution-66",
    "href": "LC-list.html#solution-66",
    "title": "List of learning checks",
    "section": "Solution",
    "text": "Solution\nFollow the same procedure as in (ref:ant-take-room), but duplicate each of the A rows two times, so that the data show a world in which A is three times as common as B. That is,\n\n\n\n\n \n  \n    species \n    size \n    color \n  \n \n\n  \n    A \n    large \n    reddish \n  \n  \n    A \n    large \n    reddish \n  \n  \n    A \n    large \n    reddish \n  \n  \n    B \n    large \n    brownish \n  \n  \n    B \n    small \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n  \n    A \n    large \n    brownish \n  \n\n\n\n\n\nFor the classifier using just size as an explanatory variable … There are 7 rows for which size is large. Of these six are species A so the classifier output is 6/7. For size small, there is just one row, which is B, so the classifier output is 0/1.\nFor the classifier using just color as an explanatory variable … There are five rows for which color is brownish. Of these, 2 are species A. So the classifier output is 2/5 for brownish. For reddish, the classifier output is 3/3."
  },
  {
    "objectID": "LC-list.html#r-2",
    "href": "LC-list.html#r-2",
    "title": "List of learning checks",
    "section": "35.R",
    "text": "35.R\nThe two tables below are different summaries of the Univ. of California Berkeley graduate admissions data from the 197e fall quarter. (Data frame: UCB_applicants) Each of the tables is about conditional probabilities about admission and sex.\n\n\n\n\nTable A\n \n  \n      \n    admitted \n    rejected \n  \n \n\n  \n    female \n    31.7 \n    46.1 \n  \n  \n    male \n    68.3 \n    53.9 \n  \n\n\n\n\n\n\n\nTable B\n \n  \n      \n    female \n    male \n  \n \n\n  \n    admitted \n    30.4 \n    44.5 \n  \n  \n    rejected \n    69.6 \n    55.5 \n  \n\n\n\n\n\n\nWhich table displays p(sex given admit)?\n\n\n\n\n\n\n\nSolution Table\n\n\n\nA\n\n\n\nWhich table displays p(admit given sex)?\n\n\n\n\n\n\n\nSolution\n\n\n\nTable B\n\n\n\nWhich of these statements is true?\n\nTable A shows that admitted students are more likely to be male.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nTrue\n\n\nb. Table A shows that rejected students are more likely to be male. \n\n\n\n\n\n\nSolution\n\n\n\nTrue\n\n\nc. Table A shows that females are less likely to be admitted than rejected. \n\n\n\n\n\n\nSolution False.\n\n\n\nThere’s nothing in Table A to tell us what fraction of applicants were admitted. Because table A stratifies by admitted/rejected, we don’t know how large those two groups are with respect to one another.\n\n\nd. Table B shows that admitted students are more likely to be male. \n\n\n\n\n\n\nSolution False.\n\n\n\nTable B is stratified on female/male. As a result, there’s no information in Table B about what fraction of applicants is male.\n\n\ne. Table B shows that females are less likely to be admitted than males. \n\nSuppose you are interested in the possibility of discrimination against women in graduate admissions (in Berkeley in 1973). Which of these questions is the right one to ask, and which table gives you the information needed to answer the question?\n\nWhat is the probability of an admitted student being a female compared to the probability of a rejected student being a female?\n\n\n\n\n\n\n\n\nSolution False.\n\n\n\nThe question is about the relative admissions probability of females and males.\n\n\nb. What is the probability of a female applicant being admitted compared to the probability that an admitted student is male? \n\n\n\n\n\n\nSolution False. You want to compare like with like. The group involved in “the probability of a female student being admitted” is females, whereas the group involved in “the probability that an admitted student is male” is admitted students. There’s little if any meaning in comparing a probability among the group of females to a probability among group of admitted students.\n\n\n\n\n\n\nc. What is the probability of  a female applicant being admitted compared to the probability of a male applicant being admitted? \n\n\n\n\n\n\nSolution True.\n\n\n\nWe want to compare two groups: females and males. Here, we’re comparing the probability of being admitted in each of the comparison groups. Table B is formatted to enable this comparison."
  },
  {
    "objectID": "LC-list.html#lesson-36",
    "href": "LC-list.html#lesson-36",
    "title": "List of learning checks",
    "section": "Lesson 36",
    "text": "Lesson 36"
  },
  {
    "objectID": "LC-list.html#section-27",
    "href": "LC-list.html#section-27",
    "title": "List of learning checks",
    "section": "36.1",
    "text": "36.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-37",
    "href": "LC-list.html#lesson-37",
    "title": "List of learning checks",
    "section": "Lesson 37",
    "text": "Lesson 37\nIdeas:\n\nUse the openintro::mlbbat10 baseball data to look at on-base performace as a function of hitting position. How many p-values are being reported from the regression report? Bonferroni adjustment. Compare to ANOVA format which looks at the term as a whole. Also look at the large number of indicators of batting performance. How many different tests could we perform?"
  },
  {
    "objectID": "LC-list.html#section-28",
    "href": "LC-list.html#section-28",
    "title": "List of learning checks",
    "section": "37.1",
    "text": "37.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-38",
    "href": "LC-list.html#lesson-38",
    "title": "List of learning checks",
    "section": "Lesson 38",
    "text": "Lesson 38"
  },
  {
    "objectID": "LC-list.html#section-29",
    "href": "LC-list.html#section-29",
    "title": "List of learning checks",
    "section": "38.1",
    "text": "38.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-19.html",
    "href": "Reading-notes/Reading-notes-lesson-19.html",
    "title": "Math 300R Lesson 19 Reading Notes",
    "section": "",
    "text": "Up to now, you’ve studied three main topics using the OpenIntro textbooks.\nChapters 7 through 10 of OpenIntro cover a topic known broadly as “statistical inference.” If you have studied statistics previously—say in an advanced high-school course—you have learned some of the term of statistical inference, such as “confidence interval,” “p-value,” “statistical significance”, and the “t-test.”\nStarting with this lesson, we are going to go in a different direction than OpenIntro Chapters 7-10. Instead of focusing exclusively on statistical inference, we are going to work with a broader idea called “statistical thinking.” Statistical inference is a small part of statistical thinking, and hardly the most important part. Indeed, many statisticians and statistically-savvy scientists believe that statistical inference can be harmful and misleading. We will discuss the good reasons behind this belief in Lesson 38. If you can’t wait, take a look at this article in the prestigious science journal Nature. Figure 1 reproduces a cartoon from that article that puts the shortcomings of “statistical significance” in a historical context."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-19.html#statistical-thinking",
    "href": "Reading-notes/Reading-notes-lesson-19.html#statistical-thinking",
    "title": "Math 300R Lesson 19 Reading Notes",
    "section": "Statistical thinking",
    "text": "Statistical thinking\nOver the next dozen lessons, you are going to be learning a way of thinking that is historically novel, unfamiliar to most otherwise well-educated people, and incredibly useful for making sense of the world and what data can tell us about the world. Learning a new way of thinking is genuinely hard. One reason is that you will have to suspend some of the familiar, go-to concepts that you’ve learned in school or through your reading.\nTo get you started with statistical thinking, it will help to have a concise definition of “statistical thinking.” Here’s one I like:\n\nStatistic thinking is the explanation or description of measured variation in the context of what remains unexplained or undescribed.\n\nImplicit in this definition is a pathway for learning to think statistically: first, you need to learn how to use data to describe variation; second, you need to know how to measure “what remains undescribed” and to use that as a context for interpretation; third, you’ll need to understand how “explanation” differs from “description.” The lessons that follow will take you down this path.\nThis lesson covers a few basic tools that you will be using throughout the remaining lessons.\n\nA standard, unified format for data graphics that simplifies both the construction and the interpretation of graphics and permits layers of descriptions to be laid on top of a data layer.\nThe presentation of descriptions using intervals rather than a number like the mean or proportion.\nA modern mode of displaying one type of description—the “density” (also called the “distribution”) of data—that is compatible with the unified data-graphics format.\nHow extend regression modeling, which in Chapters 11-17 of OpenIntro always required a quantitative response variable, to be useful for modeling categorical response variables.\n\nSince this is an introductory course, we will treat only categorical response variables that have two levels, for instance, Alive/Dead, Promoted/Not, Win/Loss, and so on. We will call these types of categorical response variables as “binomial” variables (that is, bi (two) nomial (names)) or “yes/no” variables, or zero/one variables. All of these terms refer to the same idea: a categorical variable with two levels.\nStatistical techniques for handling categorical response variables with three or many more levels require more book-keeping and more intricate computer programming. The models used by the machine-learning community are called “classifiers” rather than “regression models.” But limiting ourselves in this course to binomial response variables means that classifiers are indeed regression models."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-19.html#unified-format-for-data-graphics",
    "href": "Reading-notes/Reading-notes-lesson-19.html#unified-format-for-data-graphics",
    "title": "Math 300R Lesson 19 Reading Notes",
    "section": "Unified format for data graphics",
    "text": "Unified format for data graphics\nThe core descriptive technique we will be using is based on regression models. And, as you know, a key paradigm for building regression models is the choice of a response variable and the choice of one or more explanatory variables. (Actually, the previous sentence would be more complete if it said, “the choice of zero or more explanatory variables. You’ll see why a zero explanatory variable model is a useful concept as we move through the rest of the course. It is one of the main ways of”establishing context” for “what remains unexplained or undescribed. But we will cross that bridge when we come to it.)\nSince our descriptions will be grounded in regression models, and since we want to be able to generate graphics that show in different layers in the same graphics frame both the raw data and the description, it makes sense to structure data graphics so that there is a response variable displayed as well as one or more explanatory variables. Following convention, we will always display the response variable on the vertical (y) axis, and an explanatory variable on the horizontal (x) axis. If there are other explanatory variables to be displayed, we will use color and faceting.\nAnother aspect of our unified data graphic format is that it will always be a point plot or, closely related, a jitter plot.\nTo illustrate the construction of standard-format data graphics, consider the mosaicData::Gestation data frame. You can read about this data frame with the R command ?Gestation. Suppose we want to address the question, “Do experienced mothers have systematically different gestation periods than inexperienced mothers?” For this question, an appropriate response variable is the length of gestation. The explanatory variable needs to measure “experience,” which is a vague idea. We will make it concrete by taking it to mean the number of the mother’s pregnancies prior to the one reported in the data. This is the variable parity and ranges from zero to thirteen.\nNow that we know the response and explanatory variable, we can generate the data graphic simply enough:\n\nGestation %>% ggplot(aes(x=parity, y=gestation)) + geom_point() \n\nWarning: Removed 13 rows containing missing values (geom_point).\n\n\n\n\n\nThis graph tells you some things at a glance. A typical gestation period is about 275 days, that is, about 9 months. And you can see that it’s much more common to have a low parity than a high one. But perhaps there is some overplotting that’s hiding the number of low-parity cases. We can easily resolve this by using geom_jitter(), perhaps with some transparency. At the same time, noting that there are very few cases with, say, parity greater than 5, we will focus on the part of the data with parity of zero to five:\n\nGestation %>% \n  filter(parity <= 5) %>%\n  #mutate(parity = as.character(parity)) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) \n\n\n\n\nFigure 2: A jitter plot showing gestational period for pregnancies where the mother had five or fewer previous pregnancies. The width=0.2 controls the amount of horizontal jittering. We chose it to make the columns of data clear. Also, there’s no need to jitter in the vertical direction, so we set height=0"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-19.html#displaying-density",
    "href": "Reading-notes/Reading-notes-lesson-19.html#displaying-density",
    "title": "Math 300R Lesson 19 Reading Notes",
    "section": "Displaying density",
    "text": "Displaying density\nIt is easy to see a pattern in Figure 2: It looks like mothers with high parity tend to have gestation periods that are more reliably close to 280 days than for mothers with low parity. Or, maybe this pattern is an illusion. There are so few pregnancies with parity 3, 4, or 5 that we don’t expect to see as many uncommonly short or long gestational periods as for the parities with lots of cases.\nOne way to explore this idea is to plot the density of the dots as a function of gestation for each of the parity levels individually. A “violin” layer will make it easier to compare the distributions in the different columns, despite the unevenness in the case count. Figure 3 gives an example.\n\nGestation %>% \n  filter(parity <= 5) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) +\n  geom_violin(aes(group=parity), fill=\"blue\", alpha=0.2, color=NA)\n\nWarning: Removed 11 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 11 rows containing missing values (geom_point).\n\n\n\n\n\nFigure 3: A violin plot. The long axis of the violin-like shape is oriented along the response-variable axis (that is, the vertical axis in our standard format). The width of the violin for each possible value of the response variable is proportional to the density of data near that value.\n\n\n\n\nThe violin plot is a more flexible display of the distribution of gestation period that would be a histogram. The histogram has all those bars that clutter up the display. Even worse, one of the axes in the frame of a histogram plot is “count” or maybe “density.” Such a frame is not consistent with the unified response/explanatory format we will be using. The violin is drawn in the no-mans-land between the different levels of parity, just as the jittering moves data away from a single vertical line into that same no-mans-land.\nThis idea of using the graphical no-mans-land between levels of a categorical explanatory variable is not new. You encountered it earlier when you drew box plots. ?@fig-density-box adds a box-plot annotation layer on top of the violin-plot layer.\n\nGestation %>% \n  filter(parity <= 5) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) +\n  geom_violin(aes(group=parity), fill=\"blue\", alpha=0.2, color=NA) +\n  geom_boxplot(aes(group=parity), color=\"blue\", fill=NA, alpha=.5)\n\n\n\n\nFigure 4: A box and whisker plot uses the no-mans-land between levels of a categorical explanatory variable.\n\n\n\n\n:::: {.callout-note} ## Violins versus boxes\nAll of the graphical statistical annotations are human inventions. Each invention attempts to meet a need, but usually the invention is a compromise between the statistical objective and the computational and graphical resources available. The box plot format is a case in point. The statistical goal of a box plot is to display the distribution of values of a variable. It was invented in a time when graphics were mostly drawn by hand and computers were not widely available. The computations behind a box plot produce a five-number summary: min, first quartile, median, third quartile, max. It’s straightforward (but tedious!) to do these by hand since they are based on sorting and counting.The drawing itself uses only straight lines, which are easy to draw by hand with only a pencil and a straightedge.\nA violin plot requires hundreds or thousands of evaluations of the gaussian function along with post-processing. They are not feasible for a human; a computer is required. Similarly, drawing the detailed shape of the violin (Figure Figure 3) requires a computer.\nThe box plot has important deficiencies. It is appropriate only for uni-modal distributions and doesn’t give even a hint of possible bi-modality. The sharp boundaries of the box and endpoints of the whiskers suggest that even smooth density shapes have abrupt transitions. Points are marked as “outliers” in order to keep the whiskers from becoming absurdly long, but box-plots of data with a normal (gaussian) distribution will produce such “outliers” whenever the sample size is large.\nWhen it comes to computing power, we are today unimaginably rich compared to the generation that introduced box plots. In a sense, we are so rich we can use expensive, well made products such as a violin. The box-plot generation was living in computational poverty. Not having the (computational) funds to buy a violin, they had to make do with primitive instruments they had to make do with the materials at hand, just as early blues mucisians, coming out of poverty, often had to build instruments such as a cigar-box guitar.\n\n\n\n\n\nCigar box guitar\n\n\nFigure 5: A cigar-box guitar."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-19.html#describing-with-intervals",
    "href": "Reading-notes/Reading-notes-lesson-19.html#describing-with-intervals",
    "title": "Math 300R Lesson 19 Reading Notes",
    "section": "Describing with intervals",
    "text": "Describing with intervals\nStatistical thinking often involves quantifying uncertainty. One manifestation of this is moving away from single-number “point” summaries such as the mean or median to “interval” summaries. As you will see as we progress through the future lessons, there are many kinds of such intervals, each of which is designed to deal to address a specific question. So you’ll see prediction intervals, confidence intervals, confidence bands, and so on.\nFor this lesson, we’re concerned only with what interval summaries look like. So let’s generate some, without worrying yet about the computer commands and mathematical underpinnings involved.\n\n\n\n\n\n\nGround rules for “demonstrations”\n\n\n\nThere will be many occasions in these lessons where we want you illustrate a statistical technique or phenomenon, but we don’t expect the reader to master the commands involved. We will call these demonstrations: something we don’t expect you to do at home. A good way to think about these demonstrations is that you should focus on the outputs from the calculations, rather than the calculation steps themselves. We’ll show you the calculations since some readers might be interested, but focus your attention on the output.\n\n\n\n\n\n\n\n\nDemonstration: Food at Starbucks\n\n\n\nStarbucks is a famous coffee-shop franchise, with more than 30,000 branches (as of 2021) across the world. People go to Starbucks for the coffee, but they often buy something to eat as well. Let’s look at the calorie content of Starbucks’ food offerings. As always, when conducting a statistical analysis, it’s helpful to have in mind the purpose for the task. We’ll imagine, tongue in cheek, that we want to make food recommendations for the calorie conscious consumer.\nFirst, a point summary of the calories in the different types of food products available at Starbucks:\n\npoint_summary <- df_stats(calories ~ type, data = openintro::starbucks, mean)\npoint_summary\n\n  response          type     mean\n1 calories        bakery 368.7805\n2 calories    bistro box 377.5000\n3 calories hot breakfast 325.0000\n4 calories       parfait 300.0000\n5 calories        petite 177.7778\n6 calories         salad  80.0000\n7 calories      sandwich 395.7143\n\n\nThis summary supports the common-sense advise that to avoid calories, focus your choices on salads or on smaller portions (type “petite”). You might be tempted to go further, for example concluding that a sandwich is a bad choice (in terms of calorie content) so lean toward parfaits or hot breakfasts. You can even imagine someone concluding from this summary that a bistro box is a better calorie-conscious choice than a sandwich.\nA graphic showing both the point summary and the raw data can put things in a useful context.\n\nopenintro::starbucks %>% \n  ggplot(aes(x=type, y=calories)) +\n  geom_jitter(width=0.2, alpha=0.5) +\n  geom_errorbar(data=point_summary, aes(ymin=mean, ymax=mean), y=NA, color=\"blue\") +\n  geom_point(data=point_summary, aes(y=mean), color=\"red\")\n\n\n\n\nWe’ve shown the point summary as red dots, one for each food type. A somewhat stronger visual impression is given by drawing the point summary not as points, but as lines that extend into the no-mans-land between food types. These are drawn in blue and they make the red dots superfluous; you don’t need both.\nPlotting the point summary in the context of the raw data shows at a glance that the point summary is not of any use beyond the common sense advice to eat salads and small portions if you are trying to avoid calories. With the point summary on its own, we were tempted to conclude that, say, hot breakfasts are a better choice than sandwiches, but the data display suggests otherwise; there’s just one low-calorie breakfast. The others are much like sandwiches.\nA point summary is compact, but it fails to take into account the variation within each food type.\nAn interval summary does take into account this variation. This is an important aspect of statistical thinking. Recall the definition of statistical thinking given earlier:\n\nThe explanation or description of measured variation in the context of what remains unexplained or undescribed.\n\nThere are several kinds of interval summaries. You’re not yet in a position to know which kind is the appropriate one for the task at hand—giving advice about food choices based on food type—so we’ll tell you: a prediction interval.\n\ninterval_summary <- df_stats(calories ~ type, data = openintro::starbucks, coverage(.95))\ninterval_summary\n\n  response          type  lower  upper\n1 calories        bakery 130.00 490.00\n2 calories    bistro box 284.00 469.50\n3 calories hot breakfast 166.25 473.75\n4 calories       parfait 300.00 300.00\n5 calories        petite 162.00 190.00\n6 calories         salad  80.00  80.00\n7 calories      sandwich 351.50 454.00\n\n\n\n\nOr in graphical form:\n\nopenintro::starbucks %>% \n  ggplot(aes(x=type, y=calories)) +\n  geom_jitter(width=0.2, alpha=0.5, height=0) +\n  geom_errorbar(data=interval_summary, aes(ymin=lower, ymax=upper), y=NA, color=\"blue\") \n\n\n\n\nUnlike point summaries, interval summaries can overlap. Such overlap is an indication that the groups being summarized are not all that different. Here, an appropriate conclusion indicated by the interval summary is, “Don’t make your diet choices based on food type. Look at the calorie content of individual items before making your choice.”\nAdmittedly, in this simple setting the data themselves would lead to the conclusion. But as we move into more complicated settings, it will become infeasible to quickly see patterns straight from the data. In these complicated settings, summaries are an important tool for displaying and quantifying patterns. The statistical thinker knows to prefer interval summaries."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-19.html#categorical-response-variables.",
    "href": "Reading-notes/Reading-notes-lesson-19.html#categorical-response-variables.",
    "title": "Math 300R Lesson 19 Reading Notes",
    "section": "Categorical response variables.",
    "text": "Categorical response variables.\nOur last topic in this lesson is relatively simple: the zero-one transformation of categorical variables which allows regression and related techniques to be used for categorical response variables.\nTo illustrate, let’s use data collected in the 1970s to study the relationship between smoking and mortality. The data we’ll use, mosaicData::Whickham, recorded for one-thousand nurses whether or not they smoked at the time of the initial interview and whether or not they were still alive twenty years after the initial interview.\nHere’s a graph of the data in our standard response-vs-explanatory graphic frame:\n\nWhickham %>%\n  ggplot(aes(x = smoker, y = outcome)) +\n  geom_jitter(width=0.2, height=0.2, alpha=0.5)\n\n\n\n\nThe graph suggests that non-smokers were more likely than smokers to be dead at the follow-up interview. But it’s hard to calculate proportions from such a graph. It’s reasonable to argue that for the purpose of showing the fraction of smokers and of non-smokers who died, a bar chart would be better.\n\nWhickham %>%\n  ggplot(aes(x=smoker, fill=outcome)) +\n  geom_bar()\nWhickham %>%\n  ggplot(aes(x=smoker, fill=outcome)) +\n  geom_bar(position = \"fill\") +\n  ylab(\"Proportions\")\n\n\n\n\n\n\n\n(a) counts\n\n\n\n\n\n\n\n(b) proportions\n\n\n\n\nFigure 6: Barplots of the Whickham data.\n\n\n\nThe left barplot, showing counts, suggests that a higher proportion of non-smokers died than of smokers. But its easy to instruct the geom_bar() to graph proportions rather than counts, as done in the left plot. This makes it easy to conclude at a glance that a higher proportion of non-smokers have died.\nThe important question here, “Does smoking affect mortality?” translates well into the response/explanatory paradigm: outcome is the response variable while smoker is the explanatory variable. In the jitter-plot presentation of the data, these assignments are clearly indicated in the computer commands, which set x=smoker, y=outcome. In the barplot, a different notation is used: x=smoker, fill=outcome.\nUnfortunately, neither of the graphic styles—jitter or boxplot—answers the important question. At best they provide a description of the nurses in the Whickham data frame.\nTo answer the important question, we need to invoke statistical thinking. In particular, we need an interval summary of the proportion who died, not the point summary produced by the barplot.\nThis doesn’t mean that we can’t easily calculate the proportions from the categorical response variable: we just have to use the right commands. for instance:\n\nWhickham %>%\n  df_stats(outcome ~ smoker, prop, ci.prop)\n\n  response smoker prop_Alive     lower    center     upper\n1  outcome     No  0.6857923 0.6507824 0.6857923 0.7192969\n2  outcome    Yes  0.7611684 0.7243939 0.7611684 0.7952677\n\n\nThe point summary—the prop_Alive column—suggests an obvious difference between the smokers and non-smokers. The interval summary—columns lower and upper—tempers this conclusion a little: the intervals almost touch.\nAlthough regression is our go-to technique for modeling relationships between variables, we can’t use it directly on a categorical response variable. Here’s what happens if we try:\n\nlm(outcome ~ smoker, data = Whickham) %>% confint()\n\nWarning in model.response(mf, \"numeric\"): using type = \"numeric\" with a factor\nresponse will be ignored\n\n\nWarning in Ops.factor(y, z$residuals): '-' not meaningful for factors\n\n\nWarning in Ops.factor(r, 2): '^' not meaningful for factors\n\n\n            2.5 % 97.5 %\n(Intercept)    NA     NA\nsmokerYes      NA     NA\n\n\nThe computer’s warning message is a reminder that the response variable is categorical. (The message uses the phrase “factor response,” which is just computerese for “categorical response.”)\nWe can fix things with a simple trick: trasforming the response variable to a zero-one encoding. In the following, we’ll use 1 to represent \"Alive\" and 0 to represent \"Dead\", although we can equally well do things the other way around.\n\nlm(zero_one(outcome, one=\"Alive\") ~ smoker, data = Whickham) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 0.65329520 0.7182895\nsmokerYes   0.02654662 0.1242054\n\n\nYou don’t yet know enough to interpret this interval summary. That will have to wait until Lesson 24. The significant1 feature of the interval on smokerYes is that it does not include zero. In everyday terms, the interval means, “Smokers are 3 to 12 percentage points more likely to survive for 20 years than are non-smokers.”\nUsing interval summaries instead of point summaries is an important aspect of statistical thinking, but there are other aspects that need to be taken into account. A simple, but important, question is whether the nurses recorded in the Whickham data frame are good representatives of all smokers. (It turns out that the nurses in Whickham are all women interviewed in the 1970s. At that moment of history, women were very different than men when it comes to smoking, and the Whickham smokers were also very different from today’s female smokers. We’ll say more about this in the demonstration below.)\nStatistical thinking also leads one to ask another sort of question: What else might be going on other than smoking? In technical language, the other-goings-on are called “covariates,” the topic of Lessons 28 & 29.\nFor instance, you might wonder about the overall result from our brief examination of the Whickham data. Is it really the case that the smokers were more likely to survive than the non-smokers? The answer is “yes,” as we have demonstrated from the previous analysis. But this answer is completely misleading. Tobacco companies worked hard to mislead people into thinking that smoking was not dangerous. They knew full well the negative health consequence of smoking, but they used statistical-sounding claims to hide this knowledge from the public.\nIn the following demonstration, we’ll look at the Whickham data again using the power of regression models to incorporate covariates.\n\n\n\n\n\n\nDemonstration: Smoking with covariates\n\n\n\nRemember that you are not expected to master the calculations in these demonstrations. Focus your attention on the output from the calculations.\nIt goes without saying that smoking is not the only thing that kills people. There are other risky behaviors such as heavy drinking, there’s environmental exposure to pollutants, and there’s disease (other than the smoking induced ones of lung cancer, emphasema, and high blood pressure). But there’s one risk factor for death that everyone knows about but nobody is doing anything about: getting old.\nIn virtually every public health or clinical study, the participants’ age is taken into account. Not doing so can produce a completely misleading view of the situation. This is also the case with smoking and mortality in the Whickham study.\nRegression techniques enable us to take multiple explanatory variables into account. In this demonstration, we’ll use regression to study outcome as a function of smoker and, importantly, age.\nTo get started, we need to remember to convert the categorical outcome variable into a zero-one encoding. After that, building the model is not so hard.\n\nsurvival_model <- Whickham %>% \n  mutate(survived = zero_one(outcome, one=\"Alive\")) %>%\n  model_train(survived ~ age + smoker, data=.)\n\nFrom this model, we can read off an interval summary of the effect of smoking on survival:\n\nsurvival_model %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept)  6.7686824  8.5002535\nage         -0.1382922 -0.1101260\nsmokerYes   -0.5369777  0.1238805\n\n\nA full understanding of this interval summary will need to wait until Lessons 22 through 24. For the present, we’ll simply point out that the summary interval on smokerYes includes zero, so Whickham provides no support for the mistaken conclusion that smoking improves survival. But seeing this requires taking into account age. A graphic may help explain why:\n\nModel_output <- model_eval(survival_model, interval=\"confidence\")\nModel_output %>%\n  ggplot(aes(y = survived, ymin=.lwr, ymax=.upr, x=age, color=smoker, fill=smoker)) +\n  geom_jitter(height=.1, width=0, alpha=0.2) +\n  geom_ribbon(alpha=0.2) \n\n\n\n\nThe interval summary in the graph shows how the probability of survival changes for different ages. The intervals for non-smokers and smokers entirely overlap. For both groups, 20-year survival goes down with greater initial age. So why did the model outcome ~ smoker suggest that smokers have a higher survival? The reason relates to the proportion of smokers with initial age 70+. In the 1970s, life expectancy was such that people 70+ were unlikely to survive 20 years. This pulls down the survival rate at that age. Notice that the 70+ nurses were unlikely to have been smokers compared to younger nurses. The 70+ nurses grew up in an era when social conventions caused smoking to be uncommon for women (even though it was very common for men)."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html#sec-size-of-variable",
    "href": "Reading-notes/Reading-notes-lesson-20.html#sec-size-of-variable",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "Measuring variation",
    "text": "Measuring variation\n\nVariation itself is nature’s only irreducible essence. Variation is the hard reality, not a set of imperfect measures for a central tendency. Means and medians are the abstractions. —– Stephen Jay Gould (1941- 2002), paleontologist and historian of science\n\nA common task in statistical modeling is to break down a variable into components. For instance, a person’s height or intelligence or charm is presumably a combination of genetics and environment. In doing this breaking down, it’s convenient to be able to characterize the size of each component.\nThere are many possible ways to measure “size.” In this course, we will emphasize two, intimately related measures:\n\nvariance\nstandard deviation, which is simply the square root of variance.\n\nThe OpenIntro text introduced the standard deviation in Chapter 3 where it was described as a measure of “spread.” In Chapter 6, OpenIntro introduced the variance as the square of the variance. All this is right, so far as it goes, but it dramatically understates the importance of the two measures. These measures are as important to statistical thinking as the Pythagorean Theorem is to geometry.\nYou remember the Pythagorean Theorem: \\(A^2 + B^2 = C^2\\), where \\(C\\) is the length of the hypothenuse of a right triangle, and \\(A\\) and \\(B\\) are the lengths of the other two sides of the triangle. Surprisingly, the Pythagorean Theorem is highly relevant to statistical models.\nRecall from OpenIntro chapters 5 & 6 that the linear modeling technique produces two columns of numbers: the fitted values and the residuals. These columns have the same number of rows as the data frame used for training. The fitted values are the output from the model when the explanatory variables from the training data are given as inputs to the model. The residuals are the row-by-row numerical difference betwen the response variable and the fitted values.\nThese three columns of numbers—the response variable, the fitted model values, and the residuals—are exactly analogous to the three sides of a right triangle. (This is not an obvious fact, but it is an important one to keep in mind.) In particular, the following numerical relationship is as true for linear models as it is for triangles:\n\\[\\text{sd(fitted)}^2 + \\text{sd(residuals)}^2 = \\text{sd(response)}^2\\] where sd() refers to the standard deviation. Consequently, sd()^2 is the variance.\nThe variance and the standard deviation are defined mathematically in a special way that makes the Pythagorean relationship always describe models constructed the the lm() technique, that is “least squares” models.\nThe antique name “standard deviation” hardly hints at this, largely because it was invented before the least squares technique was invented. For thinking about data, it can be helpful to have in mind a modernization of “standard deviation.”\nStep 1 in the modernization is to make clear what “standard” means: $$ =   \n\n $$\nStep 2 in the modernization replaces the archaic word “deviation” with something more descriptive:\n\\[\\text{standard deviation} = \\text{accepted}\\ \\ \\textit{measure of} \\\n\\ \\text{variation in the variable}\\]\nWe won’t explain here why the standard deviation became the go-to, accepted, standard measure of variation, but it did and for excellent reasons.\n:::\nBoth variance and standard deviation are quantities, that is, a single number with associated units. The standard deviation of any variable has units that are exactly the same as the variable itself. For instance, the measured heights of a group of people is often measured in cm. So the units of the standard deviation of height will also be in cm.\nIn contrast, the variance, being the square of standard deviation, has units of the square of the units of the variable. The variance of height, for instance, will be measured in cm2. This will seem odd at first glance, but you have to get used to it: the variance of a variable has units that are the square of the units of the variable itself.\nKeep in mind also that variance and standard deviation are summaries of a variable. A variable in a data frame consists of multiple values, one for each row of the data frame. The variance or standard deviation of that variable will be just a single number (and units), summarizing all of the values in the variable.\n::: {.callout-note} ## Calculating variance\nAlmost always, people use software to do the calculations. The relevant R functions are sd() and var(). You can use these functions in a summarize() statement, for instance\n\nmtcars %>%\n  summarize(v = var(hp))\n\n         v\n1 4700.867\n\nmtcars %>% \n  summarize(s = sd(hp))\n\n         s\n1 68.56287\n\n\nRegrettably, the software does not indicate the units of the quantity. For that, you need to determine the units of the variable itself, typically by reading the documentation for the data frame.\nTo understand what is being calculated by var(), we will describe an algorithm. There are more efficient algorithms than the one described here, but this one is easy to understand.\nStarting material:\n\nA single column of numbers creating by pulling out from the data frame the variable whose variance is to be calculated.\nA long roll of paper on which you can write numbers, one after the other.\n\nBasic calculation: You are going to repeat a calculation for each and every row in the column of numbers. To illustrate, suppose you are doing the calculation for the kth row. Take the data value from the kth row, and call it the “reference value.” Then subtract the reference value from each and every other value in the column and square the results. Write those numbers, all of them, on the roll.\nUsing the same roll of paper for all, carry out the basic calculation starting at each of the rows in the single column of data. Now your roll of paper has many numbers on each, each of which is the square difference between the values from two rows of the table. If you are mathematically inclined, you might like to know that there will be exactly \\(n(n-1)\\) numbers written on the roll. If you are a statistical instructor, your ears might perk up when you notice the \\(n-1\\) in that count.\nThe final result—the variance—will be the mean of the numbers on the roll. Since each of the numbers on the roll is the square difference between two values of the variable, the mean will be the average square difference.\n\n\n\n\n\n\nFor the statistically experienced reader …\n\n\n\nWarning! This box contains mathematical formulas that are not needed for the course. The formulas might be interested to mathematically inclined statistics instructors. If that’s not you, skip this material.\nI realize that the algorithm described above is probably not used by any statistical software package. It’s really inefficient numerically.\nYou can see this by comparing the traditional formula for the variance to the formula version of the above algorithm:\n\\[\\underbrace{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}_\\text{traditional} \\ \\ \\text{versus} \\ \\ \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\left[\\frac{1}{n-1}\\sum_{j\\neq i} (x_i - x_j)^2\\right]}_\\text{our algorithm}\\]\nThe inefficiency of the algorithm stems from the double sum. The advantage of the algorithm is conceptual and two-fold:\n\nYou can see where the \\(n-1\\) in the formula for the variance comes from: the inner sum involves \\((n-1)\\) numbers. No hand waving needed to explain the \\(n-1\\). (What might need explaining is the \\(j \\neq i\\) in the inner sum. Why not \\(\\sum_j=1^n\\)? Because that would put \\(n\\) zeros on the roll and bias the result downward. We want to average the square distance between each value and every other value.)\nThere is no need to introduce the mean \\(\\bar{x}\\) of the values. Of course, \\(\\bar{x}\\) is easy and fast to calculate so there is no numerical reason to avoid using it in the calculation. There is, however, a philosophical reason based on Stephen J. Gould’s observation, quoted at the start of this lesson: “Variation is the hard reality. Means … are the abstractions.”\n\nHere’s a traditional-minded definition: “Variance is equal to the average squared deviations from the mean.” This definition makes it seem like the mean has some special status and that variations from it are “deviations.”"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html#causality",
    "href": "Reading-notes/Reading-notes-lesson-20.html#causality",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "Causality",
    "text": "Causality\nThe introduction to this chapter contained a very brief mention of a causal relationship: changing the dose of a medication to, say, lower a patient’s blood pressure. Assuming that the drug is effective, it’s common sense that the change in dose had a causal influence on the patients’ condition. A natural belief that one thing can cause another is the entire basis for medical and other interventions.\nThe historical statisticians who insisted that data alone cannot establish a causal connection would also, nonetheless, go to the doctor for treatment. Without an experiment, professional pride would lead them to stipulate that data can only establish “correlation” or “association” and that it’s impossible to say from data what causes what. But in their everyday lives they believed that medication has a causal influence. How could they justify this belief given their professional attitudes toward causality? Because they have common sense and know something about how the world works.\nThis section is about formal ways to say “something about how the world works” that can be used, along with data, to make responsible conclusions about causal relationships."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html#directed-acyclic-graphs",
    "href": "Reading-notes/Reading-notes-lesson-20.html#directed-acyclic-graphs",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "Directed acyclic graphs",
    "text": "Directed acyclic graphs\nThe title of this section is a mouthful, but the mathematical structure of a “directed acyclic graph” (DAG, for short) is one of the most popular ways for statistical thinkers to express their ideas about what might be going on in the real world. Despite the long name, DAGs are very accessible to a broad audience. You may even have constructed one without knowing the formal name.\nStatistical graphics are so common and so often used in this course, that you may think that the “graph” in DAG refers to these. Not really, even though statistical thinkers often draw pictures of DAGs. The “graph” in DAG is a mathematical term of art; a good synonym is “network.” Mathematical graphs consist of a set of “nodes” and a set of “edges” connecting the nodes. ?@fig-graphs shows three different graphs, each one having five nodes labelled A, B, C, D, and E.\n\n\n\n\n\n\nFigure 1: undirected graph\n\n\n\n\n\n\n\nFigure 2: directed but cyclic\n\n\n\n\n\n\n\nFigure 3: directed acyclic graph\n\n\n\n\n\nThe nodes are the same in all three graphs of ?@fig-graphs, but each of the graphs is different from the others. That’s because it is not just the nodes that define a graph; the edges (drawn as lines) are part of the definition as well.\nThe left-most graph in ?@fig-graphs is an “undirected” graph; there is no suggestion that the edges run one way or another. In contrast, the middle graph as the same nodes and edges, but the edges are directed. A nice way to think about a directed graph is that each node is a pool of water and each directed edge shows how the water flows between pools. This analogy is also helpful in thinking about causality: the causal influence flow like water.\nLook more carefully at the middle graph. There is a couple of loops; the graph is cyclic. In one loop, water is flowing from E to C to D and back again to E. The other loop runs B, C, D, E, and back to B. Such a flow pattern could not exist unless some of the edges involved pumps to push the water back uphill.\nIn the right-most graph, some of the edges have had their direction reversed. This graph has no cycles; it is acyclic. To use the flowing and pumped water analogy, in an acyclic graph no pumps are needed. You could arrange the pools of water at different heights to create the flow through gravity. The node-D pool would be the highest, E and C a little lower. But C has to be lower than E in order for gravity to pull water along the edge from E to C. The node-B pool has to be the lowest of all so that water can flow to it from E, C, and A.\nDirected acyclic graphs are used to represent causal influences; think of “A causes B” as meaning that causal “water” flows naturally from A to B.\nIn a DAG, a node can have multiple outputs, like D and E, and it might have multiple inputs, like B and C. In terms of causality, when a node—like B—has multiple inputs it means merely that more than one factor is responsible for the value of that node. A real-world example: the rising sun causes a rooster to crow, but so can another intruder to the coop.\nOften, nodes do not have any inputs. These are called “exogenous factors”—at least by economists. The “genous” beens “originates from,” the “exo” means “outside.” The value of an exogenous node is determined by something, just not something that we are interested in (or perhaps capable of) modeling. But we can be sure that the exogenous node is not determined by any of the other nodes in the DAG. Otherwise there would need to be an arrow drawn into the node. But then it wouldn’t be exogenous."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html#using-dags",
    "href": "Reading-notes/Reading-notes-lesson-20.html#using-dags",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "Using DAGs",
    "text": "Using DAGs\nThe point of a DAG is to make a clear statement of a hypothesis about causation. Drawing a DAG doesn’t mean that the hypothesis is right, just that we believe the hypothesis is in some sense a possibility. Different people might have different beliefs about what causes what in real world systems. Comparing their different DAGs can help, sometimes, to discuss and resolve the disagreement.\nWe are going to use DAGs for two distinctive purposes. One purpose is to inform responsible conclusions from data about what causes what. The data on its own is not sufficient to establish what are the causal connections. But data combined with a DAG can tell us something. Sometimes a DAG includes a causal connection that should create an association between variables. Not seeing that association in the data is evidence that the hypothesis behind the DAG is wrong. DAGs are also useful for building models; they can tell us which variables to include and which to exclude from a model in order to capture the hypothetical causal connections.\nThe second purpose is for learning modeling technique. We are going to generate simulated data from various DAGs. You can see exactly what is going on the the DAG and then see whether your statistical analysis of data is capturing the known mechanism of the DAG. This is useful for learning what can go right or wrong in building a model, just as a aircraft simulator is useful for training pilots to handle real-world situations in real aircraft. The DAGs that we will use for this second purpose consist of formulas that relate the value of each variable to the values of its inputs. The value of exogenous nodes is usually set randomly, without any input from the other nodes in the DAG.\nHere’s a simple example: a DAG with two exogenous nodes (a and b) and one another node, c, that gets input from both a and b.\n\ndag_draw(dag09)\n\n\n\nprint(dag09)\n\n[[1]]\na ~ eps()\n\n[[2]]\nb ~ eps()\n\n[[3]]\nc ~ binom(2 * a + 3 * b)\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nThe dag_draw() command draws a picture of the graph. Printing the value of the dag gives the formulas that set the values of the nodes. In dag09, the nodes a and b are both set at random and independently of one another. That’s what the eps() function does: set the value at random. In contrast, the formula for node c says that the value of c will be a linear combination of the values of a and b, translated into a zero-one format.\nYou will be generating simulated data from such dags using the sample() function. For instance,\n\nsample(dag09, size=5)\n\n# A tibble: 5 × 3\n       a      b     c\n   <dbl>  <dbl> <dbl>\n1 -0.326  1.17      1\n2  0.552  0.619     0\n3 -0.675 -0.113     0\n4  0.214  0.917     1\n5  0.311 -0.223     0\n\n\nEach of the rows in the sample is one trial in which the values of the nodes have been assigned, either by random numbers for exogenous nodes, or by a formula for nodes that receive inputs.\n\n\n\n\n\n\nWARNING!! DAGs are not reality\n\n\n\nSometimes students get so used to working with DAGs that they forget they are simulations. They start to think that they can generate data from a DAG that will help to understand some aspect of the real world.\nWRONG! If you want to know about the real world, you need to collect data from the real world, not a computer simulation. Where DAGs can be useful in practice is to guide model building from real data when you have a hypothesis about the causal connections in the world.\nIn this course, we also sample from DAGs to help learn about statistical technique. But never to make claims about real-world phenomena.\n\n\nAs the name suggests, sample() collects a sample of data. Typically, you will then summarize the data, often by fitting a model to the data and then summarizing the model. To illustrate, here we generate a sample of size \\(n=10,000\\), then fit the model c ~ a + b, and summarize by taking the coefficients.\n\nsample(dag09, size=10000) %>% \n  lm(c ~ a + b, data = .) %>%\n  coefficients()\n\n(Intercept)           a           b \n  0.5064368   0.1994570   0.3013939 \n\n\nYou might notice that the coefficients on a and b are not the same as the coefficients in the dag09 formulas. That’s because the lm() technique is not adequate to reveal the coefficients.\nThe simulated data from DAGs, along with a knowledge of the actual formulas used in the simulation, will help you learn about model building.\n\n\n\n\n\n\nDemonstration: Modeling binomial variables\n\n\n\nKeep in mind that this is just a demonstration. You’re not expected to master (or even understand) the calculations done in this box.\nRecall from the printed version of dag09 that the value of node c was set by a linear combination of a and b converted into a zero-one, binomial value. The linear modeling, lm(), technique is not well tuned to work with binomial data. Instead, another technique called “generalized linear modeling” (implemented by glm) is appropriate. When we use the right technique we can, in this case, recover the coefficients in the DAG formula: 2 for a and 3 for b.\n\nsample(dag09, size=10000) %>% \n  glm(c ~ a + b, data = ., family=\"binomial\") %>%\n  coefficients()\n\n(Intercept)           a           b \n 0.03286538  1.96356232  3.10024718"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html#samples-summaries-and-samples-of-summaries",
    "href": "Reading-notes/Reading-notes-lesson-20.html#samples-summaries-and-samples-of-summaries",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "Samples, summaries, and samples of summaries",
    "text": "Samples, summaries, and samples of summaries\nA “sample” (for the purposes of this course) is a set of rows in a data frame. The “sample size” is the number or rows. “Sampling” is the process of collecting the data to be put into the data frame.\nA “summary of a sample” is exactly that: a summary, not the sample itself. In Chapter 3 of the OpenIntro textbook, you were introduced to a data wrangling operator, summarize() and used it to construct some summaries of data frames, that is, “summaries of a sample.” For instance, you might decide to summarize the mtcars data frame by finding the mean and standard deviation of the mpg variable. In the following command, mtcars is the sample and the summary is produced by summarize().\n\nmtcars %>% \n  summarize(m = mean(mpg), s = sd(mpg))\n\n         m        s\n1 20.09062 6.026948\n\n\nIt can be very good style for the summary to be contained within a single row. The dplyr package for data wrangling is popular because it makes this happen automatically.\nWhen we use DAGs and sometimes even with real data (see Lesson 22), we may want to see whether the summary is always the same or whether it varies from trial to trial.\nThe following command is one trial of sampling and summarizing data from dag09.\n\nsample(dag09, size=10000) %>% \n  glm(c ~ a + b, data = ., family=\"binomial\") %>%\n  coefficients()\n\n(Intercept)           a           b \n-0.01222425  1.99487924  2.93153502 \n\n\nTo generate a sample of summaries, run many trials of the summary. The do() function is handy for this. The following command runs five trials of the dag09 summary. (Note that the command for the trial is placed inside curly braces.)\n\ndo(5) * {\n  sample(dag09, size=10000) %>% \n    glm(c ~ a + b, data = ., family=\"binomial\") %>%\n    coefficients()\n}\n\n    Intercept        a        b\n1 -0.01315189 2.041331 3.049575\n2 -0.01027258 1.987569 3.074509\n3  0.02787421 1.934953 3.044001\n4 -0.03509540 1.973983 2.969853\n5 -0.09887196 1.978861 2.971170\n\n\nEach trial produces one row of the data frame. The five trials are collected together by do() into the five rows of a single data frame. Such a data frame can be considered a “sample of summaries.”\nOne of the things we will do with a “sample of summaries” is to … wait for it … summarize it. For instance, in the following code chunk, a sample of 40 summaries is stored under the name Trials. Then we will summarize Trials, in this case to see how much the the values of the a and b coefficients vary from trial to trial.\n\nTrials <- do(40) * {\n  sample(dag09, size=10000) %>% \n    glm(c ~ a + b, data = ., family=\"binomial\") %>%\n    coefficients()\n} \nTrials %>% \n  summarize(mean_a = mean(a), spread_a = sd(a), \n            mean_b = mean(b), spread_b = sd(b))\n\n    mean_a   spread_a   mean_b   spread_b\n1 2.004514 0.04364563 3.012044 0.06410626\n\n\nThe result of summarizing the trials is a “summary of a sample of summaries.” This phrase is admittedly awkward, but we will be using this technique often: summarizing trials, where each trial is a summary of a sample. Often the clue will be the use of do(), which repeats trials as many times as you ask."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-21.html#old-stuff",
    "href": "Reading-notes/Reading-notes-lesson-21.html#old-stuff",
    "title": "Math 300R Lesson 21 Reading Notes",
    "section": "Old stuff",
    "text": "Old stuff\nThe theme of this second half of the course is one word: “inference.” To illustrate how inference is distinct from the data science methods—wrangling, visualization, modeling—you have been learning, consider Figure 1, copied from a well-regarded data-science text.\n\n\n\n\n\nFigure 1: Schematic diagram illustrating a path for data exploration. From Wickham & Grolemund, R for Data Science\n\n\n\n\nYou’ve been to all the stepping stones on this path (with “transform” meaning “wrangling”), except the one on the far right: “Communicate.” I’d prefer if “communicate” were replaced with “inform.” One of the main uses of data science is to extract from data information that is useful for decision-making. The best information is that which reflects the true state of the world.\nStatistical inference is the body of concepts and techniques that help us be careful and responsible that our statistical results do actually reflect the true state of the world insofar as the data can tell us about it.\nExample: Imagine a salesperson bargaining with a customer. Successful salespeople want to have some idea about how much money the customer would be willing to spend. They try to predict this based on observations they can make about the customer. These might include the kind of car the customer drives, how the customer is dressed, age of the customer, etc. You might think that the prediction takes the form of a quantity: the amount of money. But it’s better if the prediction is in the form of a range. After all, the relationship between willingness to pay and the observed customer attributes is weak and uncertain. It’s not good if the prediction suggests more precision than is actually warranted.\nExample: Nutritionists are interested in helping people to make diet choices that will increase health. Often they do this by collecting data on what people eat and their health outcomes. A regression model might indicate, for instance, that “organic” food is associated with a reduction in risk of an illness by, say, 20%. Skeptics could point out that “correlation is not causation,” and that other factors, say family income, account for the association. To inform the consumers decision about spending on food, it’s helpful if the data modeling is structured in a way that can provide confidence that the link between food and health is causal.\nSince inference involves the relationship between the resulted gleaned from models and the state of the world, it’s helpful to have situations where the state of the world is well known. We are going to provide such situations in a simple, sure way: by simulation.\nTo emphasize that the simulation is only for developing an understanding of inference, and not for depicting the real world, we’ll use the word “gaming” to refer to using such simulations. The full process of gaming has four stages.\n\n\n\n\n\n\nThe Four Stages\n\n\n\n\nbuilding the deck: Instructors provide a simulation of a mechanism that generates rows of a data frame.\nthe deal: Some of these rows will be dealt to you, constituting the data you have to work with. [real-world]\nthe play: Build models and extract results. [real-world]\nthe reveal: Compare your results from (iii) either to the mechanism given in (i) or to more data generated by the simulation.\n\n\n\nWE’RE GOING TO USE SIMPLE NAMES like X, Y, A, B, C just to remind you that this is a simulation. Real-world data will usually have descriptive variable names, like height, age, score."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-21.html#measuring-variation",
    "href": "Reading-notes/Reading-notes-lesson-21.html#measuring-variation",
    "title": "Math 300R Lesson 21 Reading Notes",
    "section": "Measuring variation",
    "text": "Measuring variation\nWe already know the standard way to measure variation in a single variable: the variance or, equivalently, the standard deviation, which is simply the square root of the variance.\nPerhaps you are wondering why there are two standard ways to measure variation, when each can be calculated from the other? The variance can be found by squaring the standard deviation, the standard deviation by taking the square root of the variance. Either will do, so why both?\nThe answer to this question is illustrated by a bit of geometry: the mathematics of right triangles and the corresponding Pythagorean relationship among the sides of the triangle: \\(A^2 + B^2 = C^2\\). The quantities \\(A\\), \\(B\\), and \\(C\\) are the lengths of the edges of the right triangle. The quantities \\(A^2\\), \\(B^2\\) and \\(C^2\\) are the lengths-squared of those edges. In order to calculate the length of one edge given the lengths of the others, we need first to square the lengths. Having squared them, we can easily do the calculation of the length-squared of the unknown edge. Then, we take the square root of the length-squared to find the length of the edge.\nLengths are like standard deviations, lengths-squared are like the variance. Where does the right triangle fit in? The overall variation in the response variable is like the hypothenuse of a right triangle. One of the other two edges represents the noise in the relationship. The other edge represents the signal: the relationship itself. It’s easy to measure the overall variation in the response variable. We can also measure the noise, but indirectly. First, we fit a model connecting the response variable to the explanatory variable(s). Then the variation of the residuals for that model are the estimate for the noise.\nOur first illustration will use data from dag01. We will arbitrarily set the sample size to \\(n=10,000\\). (In Lesson 22, we will look at the impact of sample size on the results.)\n\nDag_data <- sample(dag01, size=10000)\n\nNow measure the variation in x and y in the standard way:\n\nDag_data %>%\n  summarize(sx = sd(x), sy = sd(y), vx = var(x), vy = var(y))\n\n# A tibble: 1 × 4\n     sx    sy    vx    vy\n  <dbl> <dbl> <dbl> <dbl>\n1 0.998  1.81 0.995  3.29\n\n\nThe size of the x variation is about 1. The size of the y variation is about 1.7. (We’re using the standard deviation to measure the size of the variation.\nLook again at the formulas that compose dag01:\n\nprint(dag01)\n\n[[1]]\nx ~ eps()\n\n[[2]]\ny ~ 1.5 * x + 4 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nFrom the formula for x we can see that x comes from a random number generator, eps(). The eps() generator is designed to generate noise of size 1 by default.\nAs for y, the formula includes two sources of variation:\n\nThe part of y determined by x, that is \\(y = \\mathbf{1.5 x} + \\color{gray}{4 + \\text{eps()}}\\)\nThe noise added directly into y, that is \\(y = \\color{gray}{\\mathbf{1.5 x} + 4} + \\color{blank}{\\mathbf{eps(\\,)}}\\)\n\nThe 4 in the formula doesn’t add any variation to y; it’s just a number.\nLet’s measure variation using the standard deviation: We already know that eps() generates variation of size 1. So the amount of variation contributed by the + eps() term in the DAG formula is 1. The remaining variation is contributed by 1.5 * x. The amount of variation in x is 1, coming from the eps() in the formula for x. A reasonable guess is that 1.5 * x will have 1.5 times the variation in x. So, the variation contributed by the 1.5 * x component is 1.5. The overall variation in y is the sum of the variations contributed by the individual components. This suggests that the variation in y should be \\[\\underbrace{1}_\\text{from eps()} + \\underbrace{1.5}_\\text{from 1.5 x} = \\underbrace{2.5}_\\text{overall variation in y}.\\] Simple addition! Unfortunately, the result is wrong. In the previous summarize() of the Dag_data, we measured the overall variation in y as about 1.72.\nLet’s try again, this time using the variance as our measure of variation.\nSince eps() generates variation whose standard deviation is 1, the variance is simply \\(1^2 = 1\\). The variance of x is therefore 1, as is the variance of the eps() component of y.\nWhat’s the variance of 1.5 * x? It turns out to be \\(1.5^2\\, \\text{var(}\\mathit{x}\\text{)} = 2.25\\). Adding up the variances from the two components gives\n\\[\\text{var(}\\mathit{y}\\text{)} = \\underbrace{2.25}_\\text{from 1.5 eps()} + \\underbrace{1}_\\text{from eps()} = 3.25\\]\nThis result, that the variance of y is 3.25, is a close match to what we found in summarizing the y data generated by the DAG. And, of course, \\(\\sqrt{3.25} = 1.80\\), which is what we found by calculating the standard deviation of the y directly from the data.\nThe lesson here: When adding two sources of variation, the variances of the individual sources add to produce the overall variance of the sum. Just like \\(A^2 + B^2 = C^2\\)."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-21.html#dags-from-data",
    "href": "Reading-notes/Reading-notes-lesson-21.html#dags-from-data",
    "title": "Math 300R Lesson 21 Reading Notes",
    "section": "DAGs from data",
    "text": "DAGs from data\nIn modeling data from dag01 we could recover the DAGs formula for y.\n\nsample(dag01, size=10000) %>%\n  lm(y ~ x, data = .) %>%\n  coefficients()\n\n(Intercept)           x \n   3.996846    1.494939 \n\n\nIt is wrong to think that from data we can determine the DAG that generated the data. It’s only if we know the structure of the data-generation DAG that we can recover the mechanism inside that DAG. But another statistical thinker might claim that what’s behind the data is y causing x. Based on this assumption, she also can find the mechanism inside her hypothesized DAG:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .) %>%\n  coefficients()\n\n(Intercept)           y \n -1.8485902   0.4635813 \n\n\nA DAG is a hypothesis, a statement that might or might not be true. DAGs are part of the statistical apparatus for thinking responsibly about causality. You use a DAG—or, potentially, multiple DAGs—when the issue of what causes what is relevant to your work.\nWhen there are only two variables involved in the system under consideration—we’ll call them X and Y for simplicity—there are only two possible DAGs:\n\\[X \\rightarrow Y\\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\ X \\leftarrow Y\\]\nOften, but not always, our understanding of the world allows us to focus on one of these and not the other. Example: Does the rooster crowing cause the sun to rise, or does the rising sun cause the rooster to crow? That’s a pretty easy question if you know how things work.\nBut there are additional DAG possibilities that can account for the relationship between x and y. For instance, if we introduce another quantity, c in between x and y, four other DAGs need to be considered:\n\\[X \\rightarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\leftarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\rightarrow C \\leftarrow Y\\]\nActually, there are many other configurations of DAGs involving three variables. To keep things simple, we’ll restrict things to DAGs where X might or might not cause Y, but Y never causes X. (We don’t lose anything from this restriction because you get to make the choice of what real-world variable correspond to X and which one to Y.) Figure 2 shows the 10 configurations of 3-variable DAGs where Y doesn’t cause X.\n\n\n\n\n\nFigure 2: Ten DAG configurations involving three variables X, Y, and C and where there is no causal path going from Y to X.\n\n\n\n\nThe statistical thinker, with the conceptual tool of DAGs at, can consider multiple possibilities for what might cause what. Sometimes she will be able to discard some of the possibilities based on common sense. (Think: roosters and the sun.) But other times there may be possibilities that she doesn’t favor but which nonetheless might be plausible to other people. In Lesson 28 we will explore how each configuration of DAG has implications for which model formulas can or cannot reveal the hypothesized causal mechanism."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-22.html",
    "href": "Reading-notes/Reading-notes-lesson-22.html",
    "title": "Math 300R Lesson 22 Reading Notes",
    "section": "",
    "text": "There are many sources of noise in data; every variable has its own story, part of which is noise from measurement error, recording blunders, etc. Economists use national statistics, like GDP, even though the definition is arbitrary (a Hurricane can raise GDP!) and early reports are invariably corrected a few months later. Historians go back to original documents, but inevitably many of the documents have been lost or destroyed: a source of noise. Even in elections, where you would think counting is straightforward, the voters’ intentions are measured imperfectly due to “hanging chads,” “butterfly ballots,” broken voting machines, spoiled ballots, and so on.\nThe statistical thinker is well advised to know about the sources of noise in the system she is studying. Your analysis of data will be better the more you know about how measurements are made and data collected."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-22.html#sampling-variation",
    "href": "Reading-notes/Reading-notes-lesson-22.html#sampling-variation",
    "title": "Math 300R Lesson 22 Reading Notes",
    "section": "Sampling variation",
    "text": "Sampling variation\nThere is one source of noise that is so common across many settings that every statistical thinker needs to be intimately familiar with. This is called “sampling variation.”\nWe’ve been using “sample” as a near synonym for “data frame.” But that’s not completely fair. Often, data frames contain a row for each and every “item” of relevance. For instance, the Department of Labor never suggested that the contractor in the previous example had left out some of the applicants. Such a complete enumeration—the inventory records of a merchant, the records kept of student grades by the school registrar—has a technical name: a “census.” Famously, many countries have a regular census of the population—every 10 years in the US or the UK or China—in which (they try to) reach out to every resident.\nBut there are many settings where it is unfeasible to collect data in the form of a census. The records will be incomplete and therefore constitute a “sample.” Sampling is called for when we want to find out about a large group but don’t have the resources—time, energy, money—to contact every member of the group. France, in order to collect up-to-date data while staying within a budget, runs a “rolling census” where samples are made at short time intervals. It’s estimated that the French rolling census ultimately reaches 70% of the population.\nSometimes, as in quality control in manufacturing, the measurement process is destructive: the item is consumed in the process of measurement. Then, of course, it would be pointless to make a measurement of every single item. A sample will have to do.\nCollecting a reliable sample is usually a lot of work. One idealization is called a “simple random sample” (SRS) where all of the items are available, but only some are selected, at random, to be recorded as data. The work comes from having to assemble all of the items. Making a SRS calls for first assembling a “sampling frame,” which is essentially a census. If a census is unfeasible, the construction of a perfect sampling frame is hardly less so.\nProfessional work, such as the collection of unemployment data, often requires government-level resources and draws on specialized statistical techniques such as stratified sampling and weighting of the results. We won’t cover the specialized techniques in this introductory course, even though they are very important in creating representative samples. If you’re interested in seeing what’s involved, you can get an idea by scrolling through the table of contents of a classic text, William Cochran’s Sampling techniques\nAll statistical thinkers, whether expert in sampling techniques or not, should be aware of factors that can bias a sample away from being representative. Non-response bias can be significant, even overwhelming, in surveys. In political polls, many (most?) people will not respond to the questions. If this non-response stems from, for example, an expectation that the response will be unpopular, then the poll sample won’t adequately reflect unpopular opinions.\nSurvival bias is an important consideration in many settings. An example is given by the mosaicData::TenMileRace data. This records the running times of 8636 participants in a 10-mile road race held in 2005 and includes information about the runner, such as his or her age. You might think that such data could tell you about changes in running performance as people age: the data frame includes runners from age 10 to 87. But a model of running time as a function of age from this data frame is seriously biased. The reason? As people age, casual runners tend to drop out of such races. So the older runners are skewed toward higher performance. (We can see this by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.\n\n\n\n\n\n\nExamples: Returned to base\n\n\n\nAn inspiring story about dealing with survival bias comes from a World War II study of the damage sustained by bombers due to enemy guns. The sample, by necessity, included only those bombers that survived the mission and returned to base. The shell holes in those surviving bombers were not representative of where shells hit the planes, they were only representative of shell hits that did not prevent the plane from returning. The study report, available here, is a tribute the the work and ingenuity needed to deal with issues such as survival bias. The report itself doesn’t contain any diagram showing where shells it the bombers, but a hypothetical diagram on Wikipedia conveys the idea.\n\n\n\nThe shell holes on the surviving planes were clustered in certain areas. The clustering stems from survivor bias. Planes that were it in the areas, such as the middle of the wings, the cockpit, the engines, and the back of the fusalage did not return to base. Consequently, those shell hits were never recorded."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-22.html#measuring-sampling-variation",
    "href": "Reading-notes/Reading-notes-lesson-22.html#measuring-sampling-variation",
    "title": "Math 300R Lesson 22 Reading Notes",
    "section": "Measuring sampling variation",
    "text": "Measuring sampling variation\nWe start the process of learning about sampling variation on the training ground. That is, we’ll use simulations from DAGs even though our ultimate goal is to work with real data. DAGs are a convenient training tool because the data generated is always a simple random sample and we can generate any number of samples of any size we wish. In the spirit of starting simply, we’ll return to dag01 which, you may remember, has the structure \\(\\mathtt{x}\\longrightarrow\\mathtt{y}\\) and the causal formula y ~ 4 + 1.5 * x + eps(x).\nIt’s crucial to remember that sampling variation is not about the row-to-row variation in a single sample, it is about the variation in the summary from one sample to another. So our initial process for exploring sampling variation will be to carry out many trials, each of which is a summary of a sample.\nTo illustrate, here is one trial using a sample of size \\(n=25\\). There are many ways to summarize a sample, here we will use y ~ 1.\n\nSample <- sample(dag01, size=25) \nSample %>% \n  lm(y ~ 1, data = .) %>%\n  coefficients()\n\n(Intercept) \n   4.622099 \n\n\nWe can’t see sampling variation directly in the above result because there is only one trial. To see sampling variation directly, we need to run many trials. In each trial, a new sample (of size \\(n=25\\) is taken and summarized.)\n\nTrials <- do(100) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coefficients()\n}\nTrials\n\n    Intercept\n1    4.325743\n2    4.259373\n3    3.774027\n4    4.260662\n5    3.774720\n6    4.504612\n7    4.324866\n8    3.886647\n9    4.221058\n10   4.181181\n11   4.164594\n12   4.009576\n13   3.777807\n14   4.236963\n15   3.972496\n16   3.804015\n17   4.336199\n18   4.157989\n19   3.845247\n20   4.551283\n21   4.201726\n22   4.279147\n23   4.197916\n24   4.403323\n25   4.205382\n26   4.683501\n27   4.122489\n28   3.973316\n29   3.858270\n30   4.298748\n31   3.584820\n32   4.416405\n33   4.233296\n34   4.135118\n35   3.638739\n36   4.271793\n37   4.012241\n38   3.694062\n39   4.177496\n40   2.712570\n41   3.156607\n42   4.145464\n43   4.275481\n44   4.108306\n45   3.653740\n46   3.953338\n47   3.786200\n48   3.362691\n49   3.390494\n50   4.063216\n51   4.254318\n52   3.524185\n53   3.869243\n54   4.496233\n55   4.153982\n56   4.731352\n57   4.097335\n58   4.321772\n59   4.961461\n60   3.964094\n61   4.147353\n62   3.390652\n63   4.523000\n64   4.577773\n65   4.310574\n66   4.622263\n67   4.676236\n68   3.428335\n69   3.836534\n70   4.139999\n71   3.822753\n72   3.569538\n73   3.241943\n74   3.906785\n75   4.093538\n76   4.178026\n77   4.176137\n78   3.298370\n79   4.842166\n80   3.863946\n81   3.878656\n82   3.981178\n83   3.719393\n84   4.099604\n85   4.239947\n86   3.684081\n87   3.880939\n88   4.138508\n89   4.229457\n90   4.296386\n91   4.014592\n92   4.053822\n93   4.283775\n94   4.234725\n95   4.531647\n96   4.090854\n97   4.102844\n98   3.440418\n99   4.196119\n100  3.755595\n\n\nTrials is a sample of summaries. In Trials, the sampling variation can indeed be seen in the row-to-row variation in the data frame, but only because the data frame is a summary of samples. Since it is hard to read columns of numbers, we will summarize the variation in the sample of summaries.\nAs always, our standard measure of variation is the standard deviation (or, equivalently, variance):\n\nTrials %>%\n  summarize(sIntercept = sd(Intercept))\n\n  sIntercept\n1  0.3778659\n\n\nThis quantity, which is the standard deviation of a sample of summaries, has a technical name in statistics: the standard error. The words standard error should properly be followed by a description of the summary and the size of the individual samples involved. Here it would be, “0.348 is the standard error of the Intercept coefficient from a sample of size 25.\nThe standard error is an ordinary standard deviation, but in a particular context: the standard deviation of a sample of summaries. This can be confusing, since “error” and “deviation” are somewhat synonomous in everyday language. It can be hard to remember when to use “error” and when to use “deviation.” Fortunately, it’s more common to use another way to present the information about sampling variation."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-22.html#the-confidence-interval",
    "href": "Reading-notes/Reading-notes-lesson-22.html#the-confidence-interval",
    "title": "Math 300R Lesson 22 Reading Notes",
    "section": "The confidence interval",
    "text": "The confidence interval\nThe “confidence interval” is a more user-friendly format for describing the amount of sampling variation. As an interval, it commonly written either as [lower, upper] or center\\(\\pm\\)half-width. These styles are completely equivalent and either style can be used. The preferred style can depend on the field or the journal in which a report is being published. Some journals like a different style, center (half-width).\n\n\n\n\n\n\nTechnical vocabulary\n\n\n\nThere is a technical name for the half-width: the “margin of error.” We will leave the confidence interval calculation to software, so we won’t have much need to refer to the margin of error, but it is a term commonly used by statisticians and scientists.\n\n\nThe margin of error is defined to be twice the SE. A lot of early statistical theory was given over to defining “twice.” For our purposes, twice means “multiply by 2.” Some people prefer the theoretically more precise “multiply by 1.96” which is appropriate for very large sample sizes. For small sample sizes “twice” is larger than 1.96 and depends on how many model coefficients there are. For instance, consider the simplest model y ~ 1. There is one coefficient and for a sample size of \\(n=20\\) twice would be 2.09 while a for a sample size of \\(n=5\\) “twice” would be 2.8.\n\n\n\n\n\n\nDemonstration: Confidence interval on the Intercept coefficient for \\(n=25\\)\n\n\n\nThe following command computes the confidence interval (that is, the two numbers [lower,upper]) for the trials we ran on samples of size \\(n=25\\) from dag01 and summarized by the intercept coefficient from y ~ 1. We show this just to make clear what that the margin of error is twice the standard error.\n\nTrials %>%\n  summarize(m=mean(Intercept), se=sd(Intercept)) %>%\n  mutate(lower = m - 2*se, upper = m + 2*se)\n\n         m        se    lower    upper\n1 4.052374 0.3778659 3.296642 4.808106\n\n\nNotice that we have used 2 for “twice.” But best to leave the detailed calculations to the software.\n\n\nStatistical software is written to use the correct value of “twice” for any given sample size and number of coefficients. But for everyday purposes, and samples larger than, say, \\(n=10\\), “twice” is roughly 2.\nIn calculations, finding the half-width of the confidence interval requires first finding the standard error, then multiplying by “twice.” In practice, it’s far easier to use software. In R, the confint() function reports the confidence interval for model coefficients:\n\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  confint()\n\n                  2.5 %      97.5 %\n(Intercept) -533.432471 -406.521402\ndistance     246.387096  261.229494\nclimb          2.493307    2.726209\n\n\n\n\n\n\n\n\nDemonstration: How many digits?\n\n\n\nIn the calculation of confidence intervals on the model time ~ distance + climb, the results were reported to many digits. Such a report is appropriate for whatever further calculations might need to be done on the results, but it is usually not appropriate for a human reader.\nTo know how many digits are worth reporting to humans, you can look at the standard error. The standard error is a part of a different kind of summary of a model: the “regression report.” We won’t need to look at regression reports until the end of the course. We show one here just to make the point about how many digits are worth reporting to humans.\nHere’s the regression report on the Hill_racing model\n\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  broom::tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -470.     32.4        -14.5 9.92e- 46\n2 distance      254.      3.78        67.1 0        \n3 climb           2.61    0.0594      43.9 4.08e-304\n\n\nThe “standard error” for each coefficient is reported in the column labelled std.error.\nFor the human reader, only the first two significant digits of the standard error are worth reporting. In this case, that is 32 for the Intercept, 3.8 for the distance coefficient, and 0.059 for the climb coefficient. The confidence interval will be the coefficient itself (column labelled estimate) plus-or-minus “twice” the std.error. The report of the confidence interval (for a human reader) should be rounded to the place of the first two significant digits of the standard error.\nFor example, the confidence interval on the distance coefficient will be \\(253.808295 \\pm 2 \\times 3.78433220\\). Keep only the digits that come before the first two significant digits of the SE, so the reported interval can be \\(253.8 \\pm 3.8\\)."
  },
  {
    "objectID": "NTI/NTI-Lesson22.html",
    "href": "NTI/NTI-Lesson22.html",
    "title": "Math 300R NTI Lesson 22",
    "section": "",
    "text": "22.1 Implement on the computer a procedure to generate a sample, calculate a regression model, and produce a summary.\n22.2 Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\n22.3 Graphically display the distribution of summaries and generate a compact numerical description of the sampling distribution."
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#reading",
    "href": "NTI/NTI-Lesson22.html#reading",
    "title": "Math 300R NTI Lesson 22",
    "section": "Reading",
    "text": "Reading\nReading notes for Lesson 22"
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#lesson",
    "href": "NTI/NTI-Lesson22.html#lesson",
    "title": "Math 300R NTI Lesson 22",
    "section": "Lesson",
    "text": "Lesson\nIt’s important for the students to realize that sampling variation is about the summary of a sample. There are many ways to summarize a sample. To bring this point home, ask the students to suggest summaries of the sort of data generated by dag01. The textbook uses the coefficient on the model y ~ 1, but that is just because I want to emphasize the use of models. (The coefficient from y ~ 1 will be the mean of y, but that’s not the point to make here.) When you have a handful of measure types to use, create a “summarize a sample” command. For instance, here is a command to summarize in three ways: the mean, the median, and the variance:\n\nSample <- sample(dag01, size=10)\nSample %>%\n  summarize(m = mean(y), med = median(y), s = sd(y))\n\n# A tibble: 1 × 3\n      m   med     s\n  <dbl> <dbl> <dbl>\n1  3.07  3.77  1.89\n\n\nI’ve used the variable y in the above, but maybe someone will want to do the mean of x or the variance of x*y.\nThe above command could be written in one line, but I like to make clear that we are summarizing a sample, so I use the intermediate object Sample to hold the sample before summarizing.\nNow construct an set of trials:\n\nTrials <- do(100) * {\n  Sample <- sample(dag01, size=10)\n  Sample %>%\n    summarize(m = mean(y), med = median(y), s = sd(y))\n}\n\nShow the whole set of trials scrolling through the data frame Trials.\n\nView(Trials)\n\nAfter they have seen the whole set of trials, summarize it. Since we are interested in sampling variation, the summary of the trials will be the standard deviation of the measures we used to summarize each individual sample.\n\nTrials %>%\n  summarize(sd(m), sd(med), sd(s))\n\n# A tibble: 1 × 3\n  `sd(m)` `sd(med)` `sd(s)`\n    <dbl>     <dbl>   <dbl>\n1   0.620     0.698   0.401\n\n\nThe remainder of the session should examine how the measure of sampling variation depends on each of two aspects of the simulation.\n\nHow many trials are used. (The results don’t depend on this, but let the students find this out for themselves.)\nThe sample size. (Famously, the sampling variation will go as \\(1/\\sqrt{\\strut n}\\). )"
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#the-standard-error",
    "href": "NTI/NTI-Lesson22.html#the-standard-error",
    "title": "Math 300R NTI Lesson 22",
    "section": "The “standard error”",
    "text": "The “standard error”"
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#the-confidence-interval",
    "href": "NTI/NTI-Lesson22.html#the-confidence-interval",
    "title": "Math 300R NTI Lesson 22",
    "section": "The confidence interval",
    "text": "The confidence interval"
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#a-possible-cliff-hanger",
    "href": "NTI/NTI-Lesson22.html#a-possible-cliff-hanger",
    "title": "Math 300R NTI Lesson 22",
    "section": "A possible cliff-hanger",
    "text": "A possible cliff-hanger\nToward the end of the class, you might want to bring things back to real data. But let’s be a little silly here. Pick some data frame of interest to the students and summarize it with model coefficients. I doubt that the Hill_racing data will be of much interest to students, but I’ll write my illustration using it.\n\nsample(Hill_racing) %>%\n  lm(time ~ distance, data = .) %>%\n  coefficients()\n\n(Intercept)    distance \n  -210.9137    381.0230 \n\n\nAttentive students will howl at using sample() on a data frame rather than on a DAG. You can show them that the result of sample() is just to shuffle the order of the rows, and that the model coefficients don’t depend on the order of the rows.\nThen construct the trials and the summary of the trials:\n\nTrials <- do(10) * {\n  sample(Hill_racing) %>%\n    lm(time ~ distance, data = .) %>%\n    coefficients()\n}\nTrials %>%\n  summarize(sIntercept = sd(Intercept), sdistance = sd(distance))\n\n    sIntercept    sdistance\n1 2.280113e-11 1.536876e-12\n\n\nBoth the standard deviations are zero. In Lesson 23 we’ll see ways to measure the amount of inherited variability that makes it way to the summaries of a data frame."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html",
    "href": "NTI/NTI-Lesson24.html",
    "title": "Math 300R NTI Lesson 24",
    "section": "",
    "text": "In draft: a good illustration of interactions\n\n\n\nThis can be an example, perhaps in the reading notes.\n\nlibrary(splines)\nmod <- lm(time ~ ns(distance,4)*climb*sex, data = Hill_racing)\nmodel_plot(mod,  x=distance, color=sex, facet=climb)\n\nWarning: Excluding 2 rows due to missing data [df_stats()].\n\n\nWarning: Removed 10 rows containing missing values (geom_point).\n\n\n\n\n\nAt higher climbs, the effect size of distance depends on sex, but not at lower climbs.\nAsk them to calculate the effect size of sex at specific distances and climbs."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#objectives",
    "href": "NTI/NTI-Lesson24.html#objectives",
    "title": "Math 300R NTI Lesson 24",
    "section": "Objectives",
    "text": "Objectives\n24.1 Distinguish between the two settings for decision-making:\na. **Prediction**: predict an outcome for an individual\nb. **Relationship**: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\nGiven a research question, identify whether it corresponds to a prediction setting or a relationship setting.\n24.2 Estimate an effect size from a regression model of one and two variables.\n24.3 Construct a confidence interval on the effect size and evaluate whether a confidence interval indicates that estimated effect size is consistent with a specified value."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#reading",
    "href": "NTI/NTI-Lesson24.html#reading",
    "title": "Math 300R NTI Lesson 24",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#lesson",
    "href": "NTI/NTI-Lesson24.html#lesson",
    "title": "Math 300R NTI Lesson 24",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nIn-class example\n\n\n\n\n3 + 2\n\n[1] 5\n\n\n\n\n\n\n\n\n\n\nIn-class activity\n\n\n\n\ndate()\n\n[1] \"Wed Nov  9 08:41:18 2022\"\n\n\n\n\nThis is just a template."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#learning-checks",
    "href": "NTI/NTI-Lesson24.html#learning-checks",
    "title": "Math 300R NTI Lesson 24",
    "section": "Learning Checks",
    "text": "Learning Checks\n\n\n\n\n\n\nJust to help when writing problems\n\n\n\n24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#objective-24.1",
    "href": "NTI/NTI-Lesson24.html#objective-24.1",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.1 (Objective 24.1)",
    "text": "24.1 (Objective 24.1)\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\nSolution\n\nPrediction and (2) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#objective-24.1-1",
    "href": "NTI/NTI-Lesson24.html#objective-24.1-1",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.2 (Objective 24.1)",
    "text": "24.2 (Objective 24.1)\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\nSolution\n\nPrediction\nRelationship\nRelationship\n\n\nSOME IDEAS FOR EXERCISE MODES\n\nUse mod_plot() and look at the slope of lines and offsets. Compare to the model coefficients.\nGenerate data from a DAG and look at the confidence interval on the effect size. Then make new samples and see if the effect size in those samples is consistent with the confidence interval.\nIn text, maybe look at the confidence intervals across new samples and show that they tend to overlap. Only a few of them don’t touch a common line. This is basically just a review of confidence intervals, but why not?\nInteraction. Show that when there is an interaction term, the effect size (as calculated by mod_effect()) is not constant, as it is for models with purely linear terms."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#lc-24.1",
    "href": "NTI/NTI-Lesson24.html#lc-24.1",
    "title": "Math 300R NTI Lesson 24",
    "section": "LC 24.1",
    "text": "LC 24.1\nThe Computational Probability and Statistics text describes an early study on human-to-human heart transplantation:\n\n“The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. [[Not in data set: Another variable called [MISSING] was used to indicate whether or not the patient was alive at the end of the study.]]”\n\nThe data frame is called Transplants. [NEED TO MOVE TO PACKAGE]\n\n\n\nYou’re going to build a model of outcome vs group based on the data in Transplants. The outcome variable has levels \"Dead\" and \"Alive\", that is, it is a two-level categorical variable. Consequently, the model output will be the probability that the transplant candidate was alive at the end of the study.\n\nBuild a model outcome == \"Alive\" ~ group from the Transplants data. Pay close attention to the left-hand side of the tilde expression: it is a calculation that produces a 1 if outcome is \"Alive\" and zero otherwise. Notice the double equal signs and the quotes around \"Alive\".\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod <- lm(outcome == \"Alive\" ~ group, data = Transplants)\n\n\n\n\nThe sole explanatory variable here, group also is categorical. It has levels \"Control\" and \"Treatment\".\n\nUsing eval_mod(), find the probability of being alive at the end of the study for the Control group and for the treatment group.\nThe two probabilities in (ii) do not add up to zero. Explain why.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_eval(mod, group=\"Treatment\")\n\n      group model_output\n1 Treatment    0.3478261\n\nmod_eval(mod, group=\"Control\")\n\n    group model_output\n1 Control    0.1176471\n\n\n\n\n\nFind the effect size of the treatment. All you need is your results from (2)?\nUse mod_effect(modelname, ~ group) to calculate the effect size.\n\nIs the result consistent with what you found in (3).\nExplain in everyday language what this effect size means.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmod_effect(mod, ~ group)\n\n     change     group to_group\n1 -0.230179 Treatment  Control"
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#section",
    "href": "NTI/NTI-Lesson24.html#section",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.2",
    "text": "24.2\nEffect sizes generally come with units and you have to take into account the units in order to know if the effect is important or not.\n\n\n\n\n\n\nIn draft\n\n\n\nMove the Loans data into the math300 package. But for now …\n\nLoans <- readr::read_csv(\"data/loans.csv\")\n\n\n\nA case in point is provided by the Loans data frame, which records dozens of variables on each of 10,000 loans made through the Lending Club. The interest rate at which the loans are made varies substantially from loan to loan. Presumably, higher interest rates reflect a higher perception of risk of default (which would lead to the lender losing his or her money).\nHere’s a model of the interest rate. (This is for the borrowers who have a low debt-to-income percent; we won’t worry about the few very high debt-to-income cases.) We’re only interested in this problem with the effect size, which is the same as the coefficients on the model.\n\nmod <- lm(interest_rate ~ homeownership + debt_to_income + \n            account_never_delinq_percent + verified_income, \n          data = Loans %>% filter(debt_to_income<50))\nmod %>% confint()\n\n                                     2.5 %      97.5 %\n(Intercept)                    15.31467948 17.26502804\nhomeownershipOWN                0.16056412  0.73009641\nhomeownershipRENT               1.01571903  1.41695574\ndebt_to_income                  0.09565833  0.11523598\naccount_never_delinq_percent   -0.09149666 -0.07122369\nverified_incomeSource Verified  1.37798985  1.79909554\nverified_incomeVerified         2.88724295  3.39001806\n\n\nThere are two quantitative explanatory variables—debt_to_income and account_never_delinq_percent—both of which are measured in percent.\nThere are two categorical explanatory variables: homeownership and verified_income. The levels for homeownership are “MORTGAGE” (meaning money is still owed on the house), “OWN” (without a mortgage), and “RENT” (meaning the borrower rents rather than owning a home). The levels for verified_income are “Not Verified”, “Verified”, “Source Verified”.\nFor the categorical explanatory variables (and the intercept) the effect-size units are “percent interest.” For the quantitative explanatory variables, the effect-size units are “percent interest per percent,” so that when multiplied by the debt_to_income percent or the account_never_delinq_percent the result will be in “percent interest.”\n\nAccording to the model, who pays the higher interest rate (on average): people who OWN their home, people who RENT, or people who have a mortgage on their home? How much higher than the lowest-interest rate category.\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople who rent pay the highest interest rate, a little more than 1 percentage point higher than people who have mortgages. It’s interesting that people who own their homes outright pay (on average) pay about 0.45 percentage points more than people who own outright. This might be because having a mortgage means you also have a credit history.\n\n\n\nAccording to the model, who pays the higher interest rate (on average): people whose income is “not verified,” people whose income is “verified,” or people who have the source of income verified (level: “source verified”)?\n\n\n\n\n\n\n\nSolution\n\n\n\nPeople whose income is verified pay about 3 percentage points higher interest than people whose income is “not verified.” This seems surprising, but it may be that people who have higher perceived default risk are also the people who are asked to verify their income. Things get complicated when explanatory variables are linked to each other.\n\n\n\nThe coefficients on debt_to_income and account_never_delinq_percent are the smallest numerically. Does this mean that the effects of debt_to_income and account_never_delinq_percent are smaller than the other two explanatory variables in the model? Explain why or why not. (Hint: Look at the distribution of debt_to_income and account_never_delinq_percent to get an idea for the range of values these variables take on.)"
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#solution-7",
    "href": "NTI/NTI-Lesson24.html#solution-7",
    "title": "Math 300R NTI Lesson 24",
    "section": "Solution",
    "text": "Solution\ndebt_to_income varies over about 25 percentage points. The variation in account_never_delinq_percent is about the same, varying from about 80 to 100 percentage points. The effect of the variables (in percent interest) is determined by multiplying the coefficients by the amount of variation in the variables. So, from one extreme to the other, the effect of debt_to_income is about 2 perentage points of interest, and roughly the same for debt_to_income."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#section-1",
    "href": "NTI/NTI-Lesson24.html#section-1",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.4",
    "text": "24.4\nThe logic of effect size is to investigate the change in output of a model when one input variable is changed, holding all other things constant. This problem is about the extent to which we mean “all”.\nThe figure shows yearly CO2 production of individual gasoline-fueled passenger vehicles stratified by the number of engine 4 or 6 cylinders. ::: {.cell} ::: {.cell-output-display}  ::: :::\n\nThe effect size is the difference between the output variable when a change is made to an input. Consider the effect size of changing from a six-cylinder engine to a four-cylinder engine. Here’s a subtly incorrect way of calculating something like an effect size: Pick a dot from the six-cylinder group and another dot from the four-cylinder group. Subtract the 4-cylinder point’s CO2 value from the six-cylinder point’s CO2 value, and divide by the change in the input, that is, -2 cylinders.\n\nFollow this procedure for the top-most dot in each cloud and calculate the effect size. -A- (5100 kg - 4100 kg) / (-2 cylinders) = -500 kg/cylinder.\nFollow the procedure for the bottom-most dot in each cloud and calculate the effect size. -A- (3400 kg - 2500 kg) / (-2 cylinders) = -450 kg/cylinder.\nFollow the procedure for the bottom-most dot in the four-cylinder cloud and the top-most dot in the six-cylinder cloud. -A- (5100 kg - 3400 kg) / (-2 cylinders) = -850 kg/cylinder.\nDo the same as in (c) but use the bottom-most six-cylinder dot and the top-most four-cylinder dot. -A- (3400 kg - 4100 kg) / (-2 cylinders) = +350 kg/cylinder. In other words, switching from the six\n\nYou can imagine the (tedious) process of repeating the calculation for every possible pair of dots and getting a distribution of effect sizes. What would be the range of this distribution: from the smallest effect size to the biggest? (Hint: You can figure it out from your answers to (1).) -A- -850 kg/cylinder to +350 kg/cylinder\n\nFrom your result in (2), you might be tempted to conclude that the effect size is highly uncertain: it might be negative or it might be positive. But there is a problem, the procedure used in (1) and (2) fails to incorporate the notion of all other things being equal. This notion applies not just to known variables, but to all the other unknown factors that shape the data.\nFor the purpose of envisioning the concept, imagine that we actually had a measurement of all the factors that shaped CO2 emissions from a vehicle. We’ll call this imaginary measurement “all other things”. The figure below shows a conceptualization of what CO2 emissions as a function of the number of cylinders and “all other things” would look like.\n\n\n\n\n\n\nUsing the graph of CO2_year versus “all other things”, calculate the effect size of a change from six to four cylinders. Remember to hold “all other things” constant in your calculations. So do the effect size calculation at each of several values of “all other things”. What is the effect size and about how much does it vary from one value of “all other things” to another? -A- At “all other things being 0.25, the CO2 emissions for the 6- and 4-cylinder cars is 2800 kg and 3800 kg. The effect size is therefore (3800 kg - 2800 kg) / -2 cylinders = -500 kg/cylinder. The value of the effect size for other levels of”all other things” will be about the same, since the position of the six-cylinder dots is more-or-less constant compared to the corresponding four-cylinder dot.\n\nThis exercise is intended to give you a way of thinking about effect size and “all other things”. In reality, of course, we do not have a way to measure “all other things”. Instead, we calculate the effect size not directly from the data but from the model output (indicated by the statistic layer in the first graphic). As we saw in Lesson 22, there is actually some uncertainty about the model output stemming from sampling variability that can be summarized by a “confidence interval”. For the relationship between the number of cylinders and CO2 emissions, the extent of that uncertainty is indicated by the interval layer in the first graphic. A fair depiction for the corresponding uncertainty in the effect size can be had by repeating the process of (2), but only for points falling into the confidence interval.\n\n\n\n\n\n\nDo we need this?\n\n\n\nNote to readers who know something about car engines: As you know, switching out a six-cylinder engine for a four-cylinder engine is likely to change many other things: the weight of the vehicle, the displacement of the engine, the power available, etc. So it’s not useful to insist that everything other than the number of cylinders be held constant. Instead, we’ll be looking at the effect of that whole constellation of changes associated with a change in the number of cylinders."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#x",
    "href": "NTI/NTI-Lesson24.html#x",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.X",
    "text": "24.X\nIn very simple settings, you don’t need access to the original data: a simple summary will do.\n\n\n\n\n\n\nIn draft\n\n\n\nUse the data/bloodthinner.csv data from CPS chapter 19. Create the table of counts and calculate the effect size/probabilities from that."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#z",
    "href": "NTI/NTI-Lesson24.html#z",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.Z",
    "text": "24.Z\nThe National Cancer Institute publishes an online, interactive “Breast Cancer Risk Assessment Tool”, also called the “Gail model.” The output of the model is a probability that the woman whose characteristics are used as input will develop breast cancer in the next five years. The inputs include:\n\nage\nrace/ethnicity\nage at first menstrual period\nage at birth of first child (if any)\nhow many of the woman’s close relatives (mother, sisters, or children) have had breast cancer\n\nAs a baseline, consider a 55-year-old, African-American woman who has never had a breast biopsy or any history of breast cancer, who doesn’t know her BRCA status, and whose close relatives have no history of breast cancer, whose first menstrual period was at age 13 and first child at age 23. No “subrace” or “place of birth” is specified.\n\nFollow the link above to the cancer risk assessment tool, and enter the baseline values into the assessment tool using the link above. According to the assessment tool, what is the probability (“risk”) of developing breast cancer in the next five years? What is the lifetime risk of developing breast cancer? -A- 1.4% risk in the next five years, 7.8% lifetime risk.\nWhat is the effect size on our baseline subject of finding out that:\n\n\none of her close relatives has developed breast cancer? -A- Using the baseline values for the inputs, but changing the number of close relatives who have developed breast cancer to 1, the new five year risk is 2.2% and the lifetime risk is 12.3%. This is a change of five year risk of 0.8 percentage points and 4.5 percentage points for lifetime risk. (In Lesson 33, you’ll see that expressing effect size of a probability is often done using “log-odds”.)\nmore than one of her close relatives have developed breast cancer? -A- This has a dramatic effect, with five-year risk increasing to 4.8 percentage points and lifetime risk increasing to 29.4%.\n\n\nWhat is the effect size associated with comparing the baseline conditions to a woman with the same conditions but a race of white? -A- Five year risk goes down by 0.3 percentage points, lifetime risk goes down by 0.4 percentage points.\nWhat is the effect size of age? Compare the baseline 55-year old woman to herself when she is 65, without the other inputs changing. Since age is quantitative, Report the effect size as percentage-points-per-year. -A- Five year risk goes up to 1.5%, an increase of 0.1 percentage points per year. Lifetime risk goes down to 5.6%, a rate of -0.67 percentage points per year.\n\n\nPossibly Stat2Data::ArcheryData and ask about effect size."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#zz",
    "href": "NTI/NTI-Lesson24.html#zz",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.ZZ",
    "text": "24.ZZ\nThe graphs shows world record times in the 100m and 200m butterfly swim race as a function of year, sex, and race length. (Data frame: math300::Butterfly)\n\n\n\n\n\n\n\n\n\nIn 1980, what is the effect size of race distance? (Make sure to give the units.)\nIn 1980, what is the effect size of sex? (Make sure to give the units.)\nIn 2000, what is the effect size of race distance? (Be sure to give the units.)"
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#ww",
    "href": "NTI/NTI-Lesson24.html#ww",
    "title": "Math 300R NTI Lesson 24",
    "section": "24WW",
    "text": "24WW\nFigure 1 shows a model of the running time of winners of Scottish hill races as a function of climb, distance, and sex.\n\n\n\n\n\nFigure 1: A model of running time in Scottish hill racing versus climb, distance, and sex of the runner\n\n\n\n\nUsing as a baseline a distance of 10 km, a climb of 500m, and sex female, calculate each of these effect sizes. Remember for quantitative inputs to format the effect size as a rate, for instance seconds-per-meter-of-climb.\n\nThe effect size of climb. Consider an increase of climb by 500m. - For females running a 10 km course, an increase in climb from 500m (the specified baseline for comparison) to 1000m is roughly 2000 seconds. Since climb is quantitative, this should be reported as a rate, the change of output (2000 s) divided by the change in input (500 m). This comes to 4 seconds per meter of climb.\nThe effect size of distance. Consider an increase in distance of 10 km. -A- The model output at baseline is about 3500 seconds. The model output for a distance of 20 km (an additional 10 km above baseline) is about 6000 s. The effect size is a rate: (6000 - 3500) s / 10 km, or 250 s/km. For comparison, the fastest 20 km road run by a woman (at the time this is being written) is 1 hour 1 minute 25 seconds, that is, 3685 seconds. This amounts to an average of about 180 s/km. A walker would cover 1 km in about 720 seconds. So the racers are quite fast.\nIs there any evidence of an interaction between sex and climb? -A- Yes. Note that the lines for different climb amounts are vertically spaced more widely for females than for males. This means that the effect size of climb differs between the sexes. That’s an interaction between climb and sex."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#r",
    "href": "NTI/NTI-Lesson24.html#r",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.R",
    "text": "24.R\nOver the past hundred years, attitudes toward sex have changed. Let’s examine one way that this change might show up in data: the age at which people first had sex. The National Health and Nutrition Evaluation Survey includes a question about the age at which a person first had sex. For the moment, let’s focus just on that subset of people who have had sex at least once.\n\n\n\n\n\n\nWhat is the effect size of year born on age at first sex? Use units of years-over-years. -A- The model output is 17 years for those born round about 1940, and slightly more than 16 years for those born around 1990. The effect size is therefore about negative 1 year per 50 years\nIs there evidence for an interaction between gender and year born? -A- No, the slopes of the lines are essentially identical for males and females.\nTranslate the effect size in (1) to the units weeks-over-years. -A- 1 year per 50 years is the same as 52 weeks per 50 years, or about one week per year."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#documenting-software",
    "href": "NTI/NTI-Lesson24.html#documenting-software",
    "title": "Math 300R NTI Lesson 24",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-11-09\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-19.html",
    "href": "Student-notes/Student-notes-lesson-19.html",
    "title": "Math 300 Lesson 19 Notes",
    "section": "",
    "text": "Starting with this lesson, the course will be about ways to extract actionable information from data. “Actionable information” is in a form to guide decision making. Core techniques for extracting information from data are graphics and models.\nIn this lesson …\n\nYou will start to construct data graphics in a standard format suited for modeling: a response variable on the vertical axis and an explanatory variable on the horizontal axis. (Other explanatory variables can be mapped to color or facets. We’ll get to that in later lessons.)\nYou will be introduced to statistical annotations that display an interval over the response variable. (In later lessons, we’ll introduce specific types of intervals.)\nYou will extend regression models to be able to handle a two-level categorical response variable.\n\n\n\n\n\n\n\n\nIn draft\n\n\n\nMake a jitter plot of gestation period versus smoking status. Then find the mean for each smoking status show this table. Then the ci.mean() as a table and graphed as an interval.\n\nvalues <- lm(gestation ~ smoke, data = Gestation) %>% \n  model_eval(interval=\"confidence\", skeleton=TRUE)\nGestation %>% ggplot(aes(x = smoke, y = gestation)) +\n  geom_jitter(width=0.2, alpha=0.1) +\n  geom_errorbar(data = values, aes(x=smoke, ymin=.lwr, ymax=.upr), y=NA)\n\nWarning: Removed 13 rows containing missing values (geom_point)."
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-19.html#objectives",
    "href": "Student-notes/Student-notes-lesson-19.html#objectives",
    "title": "Math 300 Lesson 19 Notes",
    "section": "Objectives",
    "text": "Objectives\nWhile in draft, see Objectives/Obj-lesson-19.qmd. Those will be copied over here."
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-19.html#a-standard-format-for-data-graphics",
    "href": "Student-notes/Student-notes-lesson-19.html#a-standard-format-for-data-graphics",
    "title": "Math 300 Lesson 19 Notes",
    "section": "A standard format for data graphics",
    "text": "A standard format for data graphics\nA “data graphic” is one that displays each of the rows of a data frame. Graphics are fundamentally two-dimensional, so the data graphic has a frame that maps one variable to the vertical axis (“the y aesthetic”) and another to the horizontal axis (“the x aesthetic”).\nModels are important to extracting information from data. Models always have a response variable and one or more explanatory variables. So our standard format for data graphics will put the response variable on the vertical axis and the one of the explanatory variables on the horizontal axis.\n\nExample: Scottish hill racing\nA popular competitive sport in Scotland is hill racing. This is a running race that involves ascending a hill rather than running on the flat. The math300::Hill_racing data frames records about 2000 winning performances in hill races. (See ?Hill_racing for the documentation.)\n\nA simple model of a hill race performance is time ~ distance.\n\nWhich is the response variable and which is the explanatory variable in the model time ~ distance?\n\nANSWER:\n\nMake a data graphic from Hill_racing that is consistent with this choice of explanatory and response variables.\n\n\n\n# complete the code\n# Hill_racing %>%\n#  ggplot(aes(x=_____, y=______)) +\n#  geom_point()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n# complete the code\nHill_racing %>%\n ggplot(aes(x=distance, y=time)) +\n geom_point() \n\nWarning: Removed 10 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\nThe “Null” model\nThe basis of the techniques you’ll learn in this second half of the course is building a model that uses explanatory variables to account for the response variable.\nWe’ll extract various quantities from the models we build and, in particular, use those quantities to compare one model to another, the point being to see SAY WHAT.\nSurprisingly, an important model for starting the chain of comparisons has no explanatory variable. OR RATHER, WE MAKE UP AN EXPLANATORY variable. SHOW MEAN is a coefficient from such a model. Model formula: y ~ 1\nLet’s use the standard data-graphic format to display to display y ~ 1,\n\nWhickham %>% mutate(group = \"all\") %>% ggplot(aes(y=age, x=group)) + geom_jitter()\n\n\n\n\n\nA somewhat more complex model is time ~ distance + climb. In this model there are two explanatory variables. Only one of them can be mapped to the horizontal axis, the other will need to be mapped to some other aesthetic or to faceting.\n\n\n# complete the code\n# Hill_racing %>%\n#  ggplot(aes(x=_____, y=______)) +\n#  geom_point()"
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-21.html",
    "href": "Student-notes/Student-notes-lesson-21.html",
    "title": "Math 300 Lesson 21 Notes",
    "section": "",
    "text": "Note in draft\n\n\n\nOne activity can be to determine whether the parameter to eps() is in terms of standard deviation or of variance."
  },
  {
    "objectID": "Student-notes/Student-notes-lesson-21.html#key-ideas",
    "href": "Student-notes/Student-notes-lesson-21.html#key-ideas",
    "title": "Math 300 Lesson 21 Notes",
    "section": "Key ideas",
    "text": "Key ideas"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#from-sm2",
    "href": "Reading-notes/Reading-notes-lesson-28.html#from-sm2",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "From SM2",
    "text": "From SM2\nOne of the most important ideas in science is “experiment”. In a simple, ideal form of an experiment, you cause one explanatory factor to vary, hold all the other conditions constant, and observe the response. A famous story of such an experiment involves Galileo Galilei (1564-1642) dropping balls of different masses but equal diameter from the Leaning Tower of Pisa.1 Would a heavy ball fall faster than a light ball, as theorized by Aristotle 2000 years previously? The quantity that Galileo varied was the weight of the ball, the quantity he observed was how fast the balls fell, the conditions he held constant were the height of the fall and the diameter of the balls. The experimental method of dropping balls side by side also holds constant the atmospheric conditions: temperature, humidity, wind, air density, etc.\nOf course, Galileo had no control over the atmospheric conditions. By carrying out the experiment in a short period, while atmospheric conditions were steady, he effectively held them constant.\nToday, Galileo’s experiment seems obvious. But not at the time. In the history of science, Galileo’s work was a landmark: he put observation at the fore, rather than the beliefs passed down from authority. Aristotle’s ancient theory, still considered authoritative in Galileo’s time, was that heavier objects fall faster.\nThe ideal of “holding all other conditions constant” is not always so simple as with dropping balls from a tower in steady weather. Consider an experiment to test the effect of a blood-pressure drug. Take two groups of people, give the people in one group the drug and give nothing to the other group. Observe how blood pressure changes in the two groups. The factor being caused to vary is whether or not a person gets the drug. But what is being held constant? Presumably the researcher took care to make the two groups as similar as possible: similar medical conditions and histories, similar weights, similar ages. But “similar” is not “constant.”\nFor non-experimentalists – people who study data collected through observation, without doing an experiment – a central question is whether there is a way to mimic “holding all other conditions constant.” For example, suppose you observe the academic performance of students, some taught in large classes and some in small classes, some taught by well-paid teachers and some taught by poorly-paid teachers, some coming from families with positive parental involvement and some not, and so on. Is there a way to analyze data so that you can separate the influences of these different factors, examining one factor while, through analysis if not through experiment, holding the others constant?\nIn this chapter you’ll see how models can be used to examine data as if some variables were being held constant. Perhaps the most important message of the chapter is that there is no point hiding your head in the sand; simply ignoring a variable is not at all the same thing as holding that variable constant. By including multiple variables in a model you make it possible to interpret that model in terms of holding the variables constant. But there is no methodological magic at work here. The results of modeling can be misleading if the model does not reflect the reality of what is happening in the system under study. Understanding how and when models can be used effectively, and when they can be misleading, will be a major theme of the remainder of the book."
  }
]
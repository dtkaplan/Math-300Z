[
  {
    "objectID": "LC-list.html",
    "href": "LC-list.html",
    "title": "List of learning checks",
    "section": "",
    "text": "This list is assembled from the individual-lesson learning check files in the LC/ directory. Make any changes in that directory."
  },
  {
    "objectID": "LC-list.html#lesson-19",
    "href": "LC-list.html#lesson-19",
    "title": "List of learning checks",
    "section": "Lesson 19",
    "text": "Lesson 19"
  },
  {
    "objectID": "LC-list.html#setup",
    "href": "LC-list.html#setup",
    "title": "List of learning checks",
    "section": "Setup",
    "text": "Setup\nThe math300 package will be needed for lessons 20 through 39.\n\nlibrary(math300)\n\nLoading required package: mosaic\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum"
  },
  {
    "objectID": "LC-list.html#objective-2",
    "href": "LC-list.html#objective-2",
    "title": "List of learning checks",
    "section": "19.1 (Objective 2)",
    "text": "19.1 (Objective 2)\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\nSolution\n\nPrediction and (2) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price."
  },
  {
    "objectID": "LC-list.html#objective-2-1",
    "href": "LC-list.html#objective-2-1",
    "title": "List of learning checks",
    "section": "19.2 (Objective 2)",
    "text": "19.2 (Objective 2)\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\nSolution\n\nPrediction\nRelationship\nRelationship"
  },
  {
    "objectID": "LC-list.html#section",
    "href": "LC-list.html#section",
    "title": "List of learning checks",
    "section": "19.3",
    "text": "19.3\nFit a model to some data. Write down the function implied by the coefficients.\nEvaluate the function for:\n\na=7, b=9\nand so on.\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#section-1",
    "href": "LC-list.html#section-1",
    "title": "List of learning checks",
    "section": "19.4",
    "text": "19.4\nAbout the summarization of models. Pipe the model fit into any of four functions:\n\n%>% coefficients()\n%>% broom::tidy()\n%>% rsquared()\n%>% confint()\n\nREDO confint() so that the columns are named lower, middle, upper\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-20",
    "href": "LC-list.html#lesson-20",
    "title": "List of learning checks",
    "section": "Lesson 20",
    "text": "Lesson 20"
  },
  {
    "objectID": "LC-list.html#section-2",
    "href": "LC-list.html#section-2",
    "title": "List of learning checks",
    "section": "20.1",
    "text": "20.1\n?@sec-size-of-variable describes two very closely related summary quantities used to measures of the “size” of a variable: i. the variance and ii. the “standard deviation” (which is the square root of the variance.\n\nUsing software, what is the variance of the XXX variable in the YYY data frame? Make sure to include the units.\nWhat is the “standard deviation” of the XXX variable? Calculate this in two different ways: i. “by-hand” taking of the square root of the variance; ii. using the sd() software directly.\n\n[Repeat for a number of variables from different data frames.]\n\nSolution"
  },
  {
    "objectID": "LC-list.html#section-3",
    "href": "LC-list.html#section-3",
    "title": "List of learning checks",
    "section": "20.2",
    "text": "20.2\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-21",
    "href": "LC-list.html#lesson-21",
    "title": "List of learning checks",
    "section": "Lesson 21",
    "text": "Lesson 21"
  },
  {
    "objectID": "LC-list.html#section-4",
    "href": "LC-list.html#section-4",
    "title": "List of learning checks",
    "section": "21.1",
    "text": "21.1\nThe following command will generate a data frame with 1000 rows from dag00 and calculate the variance of the x and y variables:\n\nsample(dag00, size=1000) %>%\n  summarize(vx = var(x), vy = var(y))\n\n# A tibble: 1 × 2\n     vx    vy\n  <dbl> <dbl>\n1  3.97  1.03\n\n\nCompare this result to the DAG tilde expressions\n\ndag00\n\n[[1]]\nx ~ eps(2) + 5\n\n[[2]]\ny ~ eps(1) - 7\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nIn the tilde expressions, eps(2) means to generate noise of magnitude 2.0.\n\nIs the argument to eps() specified in terms of the variance or the standard deviation?\nThe tilde expression for x specifies that the constant 5 is to be added to eps(2). Similarly, the constant -7 is added to y. How do these constants relate to the calculated magnitudes of x and y?\n\n\nSolution\n\nThe standard deviation. For instance, x has noise of magnitude 2. The variance of x is 4, the square of 2.\nThe standard deviation (and therefore the variance) ignore such added constants."
  },
  {
    "objectID": "LC-list.html#section-5",
    "href": "LC-list.html#section-5",
    "title": "List of learning checks",
    "section": "21.2",
    "text": "21.2\n?@sec-signal-and-noise introduces the idea that variables consist of components. A simple breakdown is into two components: i. the part of the variable that is determined by other variables in the system (“signal”) and ii. the random part of the variable (“noise”). The section uses dag01 as an illustration of how a variable can be partly determined and partly random noise.\n\nWrite and execute a command that will generate 500 rows of simulated data from dag01 and will calculate the standard deviation of x and of y.\nWhat’s the magnitude of x in the simulated data? What’s the magnitude of y?\nDoes this change if you use data with 1000 or 20000 rows?\n\n\nSolution\n\nsample(dag01, size=500) %>% summarize(sx = sd(x), sy=sd(y))\nThe standard deviation of x is about 1, the standard deviation of y is about 1.8.\nNo, the values are roughly the same regardless of the size of the sample."
  },
  {
    "objectID": "LC-list.html#section-6",
    "href": "LC-list.html#section-6",
    "title": "List of learning checks",
    "section": "21.3",
    "text": "21.3\n[DRAW several DAG-like graphs, one of which should be undirected in all edges, one should be undirected on one or two edges (but not all), and one should be cyclic and another acyclic.]\nReferring to the graphs in the figure, say which ones are DAGs. If a graph is not a DAG, say whether that’s because it’s not directed or because it’s not cyclic.\n\nSolution"
  },
  {
    "objectID": "LC-list.html#section-7",
    "href": "LC-list.html#section-7",
    "title": "List of learning checks",
    "section": "21.4",
    "text": "21.4\nGenerate simulated data from dag01 with 1000 rows. Fit the regression model y ~ x to the data and examine the coefficients.\n\nHow do the coefficients relate to the tilde expressions that define dag01?\nInstead of using the regression model y ~ x, where y is the response variable, try the regression model x ~ y. Do the coefficients from x ~ y correspond in any simple way to the tilde expressions that define dag01?\n\n\nSolution\n\nsample(dag01, size=1000) %>%\n  lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      4.004        1.502  \n\n\nThe intercept corresponds to the additive constant (4) in the y tilde expression. The x coefficient corresponds to the multiplier on x in the tilde expression.\nThe formula for x isn’t reflected by the coefficients.\nUsing x as the response variable:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .)\n\n\nCall:\nlm(formula = x ~ y, data = .)\n\nCoefficients:\n(Intercept)            y  \n    -1.8508       0.4638  \n\n\nThese coefficients do not appear in the dag01 tilde expressions."
  },
  {
    "objectID": "LC-list.html#lesson-22",
    "href": "LC-list.html#lesson-22",
    "title": "List of learning checks",
    "section": "Lesson 22",
    "text": "Lesson 22"
  },
  {
    "objectID": "LC-list.html#obj-20.3",
    "href": "LC-list.html#obj-20.3",
    "title": "List of learning checks",
    "section": "22.1 (Obj 20.3)",
    "text": "22.1 (Obj 20.3)\nConsider these three data frames:\n\nOne <- sample(dag01, size=25)\nTwo <- do(10) * {\n  lm(y ~ x, data = sample(dag01, size=25)) %>%\n    coefficients()\n  }\nThree <- Two %>% \n  summarize(mx = mean(x), sx = sd(x))\n\n\nBoth One and Two have columns called x, but they stand for different things. Explain what the unit of observation is and what the values in x represent..\nThree does not have a column named x, but it is a summary of the x column from Two. What kind of summary.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nIn One, the x column contains the simulated of the x variable from dag01. The unit of observation is a single case, for instance a person for whom observations were made of x and y. The simulation involves generating 25 rows of data: one row for each of 25 people.\nIn Two, the x column is the regression coefficient on x from the simulation. Each row of Two corresponds to one trial in which regression is being performed on a sample of size 25 of simulated data from dag01.\nThree is a summary of the 10 trials in Two. The columns, named mx and sx, tell about the distribution of x across all the trials."
  },
  {
    "objectID": "LC-list.html#lesson-23",
    "href": "LC-list.html#lesson-23",
    "title": "List of learning checks",
    "section": "Lesson 23",
    "text": "Lesson 23"
  },
  {
    "objectID": "LC-list.html#section-8",
    "href": "LC-list.html#section-8",
    "title": "List of learning checks",
    "section": "23.1",
    "text": "23.1\nVocabulary: Sampling distribution, standard error, sampling variability, sample size\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-24",
    "href": "LC-list.html#lesson-24",
    "title": "List of learning checks",
    "section": "Lesson 24",
    "text": "Lesson 24"
  },
  {
    "objectID": "LC-list.html#section-9",
    "href": "LC-list.html#section-9",
    "title": "List of learning checks",
    "section": "24.1",
    "text": "24.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-25",
    "href": "LC-list.html#lesson-25",
    "title": "List of learning checks",
    "section": "Lesson 25",
    "text": "Lesson 25"
  },
  {
    "objectID": "LC-list.html#section-10",
    "href": "LC-list.html#section-10",
    "title": "List of learning checks",
    "section": "25.1",
    "text": "25.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-26",
    "href": "LC-list.html#lesson-26",
    "title": "List of learning checks",
    "section": "Lesson 26",
    "text": "Lesson 26\nIdeas\n\nConstruct prediction interval when evaluating a model function.\nPlot a prediction band.\nCheck the consistency of the prediction band with the DAG mechanism for large \\(n\\).\n\nIs the width right?\nIs the slope right?\n\nFor small \\(n\\) (say, \\(n=5\\)), how is the prediction band different than for large \\(n\\)?"
  },
  {
    "objectID": "LC-list.html#section-11",
    "href": "LC-list.html#section-11",
    "title": "List of learning checks",
    "section": "26.1",
    "text": "26.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-27",
    "href": "LC-list.html#lesson-27",
    "title": "List of learning checks",
    "section": "Lesson 27",
    "text": "Lesson 27\nThis is a QR day."
  },
  {
    "objectID": "LC-list.html#section-12",
    "href": "LC-list.html#section-12",
    "title": "List of learning checks",
    "section": "27.1",
    "text": "27.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-28",
    "href": "LC-list.html#lesson-28",
    "title": "List of learning checks",
    "section": "Lesson 28",
    "text": "Lesson 28"
  },
  {
    "objectID": "LC-list.html#section-13",
    "href": "LC-list.html#section-13",
    "title": "List of learning checks",
    "section": "28.1",
    "text": "28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution\n:::"
  },
  {
    "objectID": "LC-list.html#lesson-29",
    "href": "LC-list.html#lesson-29",
    "title": "List of learning checks",
    "section": "Lesson 29",
    "text": "Lesson 29"
  },
  {
    "objectID": "LC-list.html#lc-29.1",
    "href": "LC-list.html#lc-29.1",
    "title": "List of learning checks",
    "section": "LC 29.1",
    "text": "LC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_rms_error(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50, in_sample = TRUE)\n\n[1] 0.8470443 0.6628943 0.6408581 0.5688379\n\n\nd seems to block effect of a and b on c.\n\ncompare_rms_error(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5, in_sample = TRUE)\n\n[1] 1.4806412 0.5652314 0.5373352 0.4960967"
  },
  {
    "objectID": "LC-list.html#lc-29.2",
    "href": "LC-list.html#lc-29.2",
    "title": "List of learning checks",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = TRUE)\n\n[1] 4.689275e-01 4.188891e-01 3.603896e-01 1.416962e-16\n\nset.seed(101)\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = FALSE)\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "LC-list.html#lesson-30",
    "href": "LC-list.html#lesson-30",
    "title": "List of learning checks",
    "section": "Lesson 30",
    "text": "Lesson 30"
  },
  {
    "objectID": "LC-list.html#section-14",
    "href": "LC-list.html#section-14",
    "title": "List of learning checks",
    "section": "30.1",
    "text": "30.1\nDags with longer confounding pathways. Is there mixing when leaving out an element in the pathway. Mix up the directions of the arrows and show that the mixing occurs when the covariate is included in the model.\nRegression to the mean example.\nCollider?\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-31",
    "href": "LC-list.html#lesson-31",
    "title": "List of learning checks",
    "section": "Lesson 31",
    "text": "Lesson 31"
  },
  {
    "objectID": "LC-list.html#section-15",
    "href": "LC-list.html#section-15",
    "title": "List of learning checks",
    "section": "31.1",
    "text": "31.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-32",
    "href": "LC-list.html#lesson-32",
    "title": "List of learning checks",
    "section": "Lesson 32",
    "text": "Lesson 32"
  },
  {
    "objectID": "LC-list.html#section-16",
    "href": "LC-list.html#section-16",
    "title": "List of learning checks",
    "section": "32.1",
    "text": "32.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-33",
    "href": "LC-list.html#lesson-33",
    "title": "List of learning checks",
    "section": "Lesson 33",
    "text": "Lesson 33"
  },
  {
    "objectID": "LC-list.html#section-17",
    "href": "LC-list.html#section-17",
    "title": "List of learning checks",
    "section": "33.1",
    "text": "33.1\n\nConvert probability to odds and log odds, and vice versa.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-34",
    "href": "LC-list.html#lesson-34",
    "title": "List of learning checks",
    "section": "Lesson 34",
    "text": "Lesson 34"
  },
  {
    "objectID": "LC-list.html#section-18",
    "href": "LC-list.html#section-18",
    "title": "List of learning checks",
    "section": "34.1",
    "text": "34.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-35",
    "href": "LC-list.html#lesson-35",
    "title": "List of learning checks",
    "section": "Lesson 35",
    "text": "Lesson 35"
  },
  {
    "objectID": "LC-list.html#section-19",
    "href": "LC-list.html#section-19",
    "title": "List of learning checks",
    "section": "35.1",
    "text": "35.1\nGiven some classifier summaries, calculate the false-positive and false-negative rates as well as the sensitivity and specificity\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-36",
    "href": "LC-list.html#lesson-36",
    "title": "List of learning checks",
    "section": "Lesson 36",
    "text": "Lesson 36"
  },
  {
    "objectID": "LC-list.html#section-20",
    "href": "LC-list.html#section-20",
    "title": "List of learning checks",
    "section": "36.1",
    "text": "36.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-37",
    "href": "LC-list.html#lesson-37",
    "title": "List of learning checks",
    "section": "Lesson 37",
    "text": "Lesson 37"
  },
  {
    "objectID": "LC-list.html#section-21",
    "href": "LC-list.html#section-21",
    "title": "List of learning checks",
    "section": "37.1",
    "text": "37.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC-list.html#lesson-38",
    "href": "LC-list.html#lesson-38",
    "title": "List of learning checks",
    "section": "Lesson 38",
    "text": "Lesson 38"
  },
  {
    "objectID": "LC-list.html#section-22",
    "href": "LC-list.html#section-22",
    "title": "List of learning checks",
    "section": "38.1",
    "text": "38.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson35.html",
    "href": "NTI/NTI-Lesson35.html",
    "title": "Math 300R NTI Lesson 35",
    "section": "",
    "text": "35.1 Explain why case-control data may not give an proper measure of “prevalence.”\n35.2 Understand sensitivity and specificity as conditional probabilities.\n35.3 Calculate false-positive and false-negative rates for a given prevalence."
  },
  {
    "objectID": "NTI/NTI-Lesson35.html#reading",
    "href": "NTI/NTI-Lesson35.html#reading",
    "title": "Math 300R NTI Lesson 35",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson35.html#lesson",
    "href": "NTI/NTI-Lesson35.html#lesson",
    "title": "Math 300R NTI Lesson 35",
    "section": "Lesson",
    "text": "Lesson\nIn Lesson 33 we built a classifier based on simulated data from dag10.\n\nA classifier is a function of the variables deemed relevant by the designer and that produces a yes/no output.\nDiscovering a suitable function requires insight and usually trial and error.\n\nHere’s a classifier relevant to dag10:\n\nC1 <- make_classifier(c1 ~ b < 0)\n\nC1 is a function taking a data frame as input. The calculation is whether b is less than 0. If so, the function output is a 1. The output will be added as column c1 to the input data\nTo evaluate it, create a dataframe from dag10 and run the classifier on it.\n\nSample <- sample(dag10, size=1000) %>% C1()\nwith(Sample, table(y, c1))\n\n   c1\ny     -   +\n  0 368 125\n  1 131 376\n\n\nThe false-positive rate is 125/1000, the false-negative rate is 131/1000. The “accuracy” is (368+376)/1000, about 74%.\nWe designed classifier C1() using where the disease state is 1 about half the time, as appropriate for a case-control design.\n\nSample %>% summarize(yesses = mean(y))\n\n# A tibble: 1 × 1\n  yesses\n   <dbl>\n1  0.507\n\n\nNow we want to deploy the classifier in a realistic setting, where the base rate is only, say, 10%. To simulate this, we’ll keep all of the 0s from the simulation, but only 11% of the 1s.\n\nField_results <- sample(dag10, \n                        size=1000, \n                        survive=~ifelse(y==1, unif() < 0.11, TRUE)) %>% C1()\nwith(Field_results, table(y, c1))\n\n   c1\ny     -   +\n  0 683 225\n  1  23  69\n\n\nThe false negative rate is now close to zero: 19/1000. But the false positive rate is 31%, much higher than with the case-control data used to design the classifier.\nWhat’s gone wrong? Why did the false positive rate change?\nThe reason is that the “base rate” (or prevalence) for the disease is 10.1% in dag10b compared to the 50% in dag10. Same classifier, but a different false positive rate.\nWe would like to have a way to characterize a classifier that is independent of the base rate.\nThe false positive rate is a probability: p(0 & +). Similarly, the false negative rate is p(1 & -). We can read these off the table.\nThe quantities of interest to the patient and doctor are different probabilities:\n\np(0 | -) — probability you don’t have the disease given a negative test result\np(1 | +) — probability you have the disease given a positive test result\n\nCalculate these probabilities from the classifer test results for the two base rates we’ve looked at: 50% for dag10 and 21% for dag10b.\n\nCase/control — p(0 | -) is 368/(368 + 131) = 73%\nField – p(0 | -) is 759/(746 + 29) = 96%\nCase/control — p(1 | +) is 376/(376 + 125) = 75%\nField – p(1 | +) is 88/(248+88) = 26%\n\nThese patient-centered probabilities change as the base rate changes. It turns out that the proper way to characterize the classifier is with two different probabilities:\n\nSensitivity: Probability of a + test if you have the disease: p(+ | 1)\nSpecificity: Probability of a - test if you do not have the disease: p(- | 1)\n\nNote that these are not probabilities of direct interest to the patient or doctor. But they do come out the same regardless of the base rate of the disease.\n\nCase/control — p(+ | 1) is 376/(376+131) = 74%\nField – p(+ | 1) is 88/(29+88) = 75%\nCase/control — p(- | 0) is 368/(368 + 125) = 75%\nField – p(- | 0) is 746/(746+248)= 75%\n\nGiven the sensitivity/specificity and the base rate, we can calculate the probability of interest to the patient\np(1 | +) = p(+ | 1) p(1) / (p(+ | 1)p(1) + p(-|0)p(0))\nIn statistics, the sensitivity/specificity is called the “likelihood function”, the probability of the observed data under the actual values in the world. THIS IS WEAK."
  },
  {
    "objectID": "NTI/NTI-Lesson35.html#learning-checks",
    "href": "NTI/NTI-Lesson35.html#learning-checks",
    "title": "Math 300R NTI Lesson 35",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson35.html#section",
    "href": "NTI/NTI-Lesson35.html#section",
    "title": "Math 300R NTI Lesson 35",
    "section": "35.1",
    "text": "35.1\nGiven some classifier summaries, calculate the false-positive and false-negative rates as well as the sensitivity and specificity\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson35.html#documenting-software",
    "href": "NTI/NTI-Lesson35.html#documenting-software",
    "title": "Math 300R NTI Lesson 35",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson21.html",
    "href": "NTI/NTI-Lesson21.html",
    "title": "Math 300R NTI Lesson 21",
    "section": "",
    "text": "21.1 Determine whether a proposed graph is directed and acyclic.\n21.2 Having selected a response and one or more explanatory variables, identify other DAG notes as covariates.\n21.3 Generate data from simulations and use the data to model the relationships."
  },
  {
    "objectID": "NTI/NTI-Lesson21.html#reading",
    "href": "NTI/NTI-Lesson21.html#reading",
    "title": "Math 300R NTI Lesson 21",
    "section": "Reading",
    "text": "Reading\nTBD\nReading note sketches"
  },
  {
    "objectID": "NTI/NTI-Lesson21.html#lesson",
    "href": "NTI/NTI-Lesson21.html#lesson",
    "title": "Math 300R NTI Lesson 21",
    "section": "Lesson",
    "text": "Lesson\nRemember that you will be running this more like a lab than a lecture. You want them using R and answering questions. Have them open the notes rmd and work through it together.\n\n\n\n\n\n\nSetup\n\n\n\n\nlibrary(math300)\nlibrary(mosaic)\n\n\n\nIntroduce DAGs with multiple variables.\nGenerate data from them.\nUse the data to summarize the relationships.\n\n\n\n\n\n\nActivity\n\n\n\n\nWhat happens when you use the model y ~ x + a? Interpret those coefficients. Are they consistent with the DAG?\nAre x and a related to one another? What does the DAG say about this? What does the model a ~ x say.\n\n\n\n\n\n\n\n\n\nInference example: Regression to the mean\n\n\n\nConstruct the Galton height DAG, simulate, and show that Galton’s finding is replicated."
  },
  {
    "objectID": "NTI/NTI-Lesson21.html#learning-checks",
    "href": "NTI/NTI-Lesson21.html#learning-checks",
    "title": "Math 300R NTI Lesson 21",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson21.html#section",
    "href": "NTI/NTI-Lesson21.html#section",
    "title": "Math 300R NTI Lesson 21",
    "section": "21.1",
    "text": "21.1\nThe following command will generate a data frame with 1000 rows from dag00 and calculate the variance of the x and y variables:\n\nsample(dag00, size=1000) %>%\n  summarize(vx = var(x), vy = var(y))\n\n# A tibble: 1 × 2\n     vx    vy\n  <dbl> <dbl>\n1  4.03  1.01\n\n\nCompare this result to the DAG tilde expressions\n\ndag00\n\n[[1]]\nx ~ eps(2) + 5\n\n[[2]]\ny ~ eps(1) - 7\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nIn the tilde expressions, eps(2) means to generate noise of magnitude 2.0.\n\nIs the argument to eps() specified in terms of the variance or the standard deviation?\nThe tilde expression for x specifies that the constant 5 is to be added to eps(2). Similarly, the constant -7 is added to y. How do these constants relate to the calculated magnitudes of x and y?\n\n\nSolution\n\nThe standard deviation. For instance, x has noise of magnitude 2. The variance of x is 4, the square of 2.\nThe standard deviation (and therefore the variance) ignore such added constants."
  },
  {
    "objectID": "NTI/NTI-Lesson21.html#section-1",
    "href": "NTI/NTI-Lesson21.html#section-1",
    "title": "Math 300R NTI Lesson 21",
    "section": "21.2",
    "text": "21.2\n?@sec-signal-and-noise introduces the idea that variables consist of components. A simple breakdown is into two components: i. the part of the variable that is determined by other variables in the system (“signal”) and ii. the random part of the variable (“noise”). The section uses dag01 as an illustration of how a variable can be partly determined and partly random noise.\n\nWrite and execute a command that will generate 500 rows of simulated data from dag01 and will calculate the standard deviation of x and of y.\nWhat’s the magnitude of x in the simulated data? What’s the magnitude of y?\nDoes this change if you use data with 1000 or 20000 rows?\n\n\nSolution\n\nsample(dag01, size=500) %>% summarize(sx = sd(x), sy=sd(y))\nThe standard deviation of x is about 1, the standard deviation of y is about 1.8.\nNo, the values are roughly the same regardless of the size of the sample."
  },
  {
    "objectID": "NTI/NTI-Lesson21.html#section-2",
    "href": "NTI/NTI-Lesson21.html#section-2",
    "title": "Math 300R NTI Lesson 21",
    "section": "21.3",
    "text": "21.3\n[DRAW several DAG-like graphs, one of which should be undirected in all edges, one should be undirected on one or two edges (but not all), and one should be cyclic and another acyclic.]\nReferring to the graphs in the figure, say which ones are DAGs. If a graph is not a DAG, say whether that’s because it’s not directed or because it’s not cyclic.\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson21.html#section-3",
    "href": "NTI/NTI-Lesson21.html#section-3",
    "title": "Math 300R NTI Lesson 21",
    "section": "21.4",
    "text": "21.4\nGenerate simulated data from dag01 with 1000 rows. Fit the regression model y ~ x to the data and examine the coefficients.\n\nHow do the coefficients relate to the tilde expressions that define dag01?\nInstead of using the regression model y ~ x, where y is the response variable, try the regression model x ~ y. Do the coefficients from x ~ y correspond in any simple way to the tilde expressions that define dag01?\n\n\nSolution\n\nsample(dag01, size=1000) %>%\n  lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      4.044        1.554  \n\n\nThe intercept corresponds to the additive constant (4) in the y tilde expression. The x coefficient corresponds to the multiplier on x in the tilde expression.\nThe formula for x isn’t reflected by the coefficients.\nUsing x as the response variable:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .)\n\n\nCall:\nlm(formula = x ~ y, data = .)\n\nCoefficients:\n(Intercept)            y  \n    -1.8566       0.4643  \n\n\nThese coefficients do not appear in the dag01 tilde expressions."
  },
  {
    "objectID": "NTI/NTI-Lesson21.html#documenting-software",
    "href": "NTI/NTI-Lesson21.html#documenting-software",
    "title": "Math 300R NTI Lesson 21",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-10-14\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2\nmosaic package version: 1.8.4\nmath300 package version: 0.1.0.9000"
  },
  {
    "objectID": "NTI/NTI-Lesson20.html",
    "href": "NTI/NTI-Lesson20.html",
    "title": "Math 300R NTI Lesson 20",
    "section": "",
    "text": "20.1. Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\n20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples."
  },
  {
    "objectID": "NTI/NTI-Lesson20.html#reading",
    "href": "NTI/NTI-Lesson20.html#reading",
    "title": "Math 300R NTI Lesson 20",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson20.html#lesson",
    "href": "NTI/NTI-Lesson20.html#lesson",
    "title": "Math 300R NTI Lesson 20",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nSetup\n\n\n\nThe software for generating the simulations, and which contains a library of different scenarios, is in the package {math300}. The {mosaic} package contains functions for calculating statistics and iteration.\n\nlibrary(mosaic)\nlibrary(math300)\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\nRoll a die 1000 times. How often does each possible result appear?\n\n\nSim <- dag_make(x ~ each(sample(1:6)))\nSamp <- sample(Sim, size=100)\nSamp %>% group_by(x) %>% summarize(count = n())\n\n# A tibble: 6 × 2\n      x count\n  <int> <int>\n1     1    17\n2     2    17\n3     3    17\n4     4    17\n5     5    16\n6     6    16\n\n\n\nGenerate 1000 rolls of the sum of two dice. How often does each possible result appear?\n\n\nSim <- dag_make(x ~ each(sum(sample(1:6, 2, replace=TRUE))))\nSample <- sample(Sim, size=1000)\nSample %>%  \n  dplyr::group_by(x) %>%\n  summarize(frac = n()/nrow(.)) %>% \n  print.data.frame()\n\n    x  frac\n1   2 0.023\n2   3 0.066\n3   4 0.085\n4   5 0.116\n5   6 0.136\n6   7 0.144\n7   8 0.159\n8   9 0.115\n9  10 0.073\n10 11 0.056\n11 12 0.027\n\n\nQuestion: What do your results tell you about the possible results when summing 2 dice?\n\nDo the same but for summing 6 dice.\n\nQuestion: What do your results tell you about the possible results when summing 6 dice? Do the data show you that it is possible to get a result of 36?\nThis illustrates something about “inference”: the data do not always point you directly to the truth.\nQuestion: How would you modify the simulation to get a better handle on the truth.\n\n\n\n\n\n\n\n\nMeasuring noise\n\n\n\nCreate a DAG where the X variable is simply noise: eps()\n\nSim <- dag_make(x ~ eps())\nSample <- sample(Sim, size=100)\nSample %>% summarize(sd(x), mean(x))\n\n# A tibble: 1 × 2\n  `sd(x)` `mean(x)`\n    <dbl>     <dbl>\n1   0.939    0.0259\n\n\n\nThe argument to eps() sets the size of the noise. Confirm that this claim is true.\n\n\n\nSometimes we will want to repeat a simulation over and over again, collecting the results. Use do() for this:\n\n# one trial\nSim <- dag_make(x ~ eps())\nsample(Sim, size=100) %>% \n  summarize(sd(x), mean(x))\n\n# A tibble: 1 × 2\n  `sd(x)` `mean(x)`\n    <dbl>     <dbl>\n1   0.916    0.0519\n\n# five trials\ndo(5) *\n  {sample(Sim, size=100) %>% \n  summarize(s = sd(x), m = mean(x))}\n\n# A tibble: 5 × 4\n      s       m  .row .index\n  <dbl>   <dbl> <int>  <dbl>\n1 0.951  0.0872     1      1\n2 0.923  0.0318     1      2\n3 0.996  0.293      1      3\n4 1.03  -0.0484     1      4\n5 1.03  -0.0852     1      5\n\n\n\nNot all the rows are the same. Why not?\nNone of the standard deviations are exactly 1. Does this mean the simulation is not working?\n\nA typical problem in statistical inference is to determine, from just one trial, whether the results are consistent or not with a proposed mechanism."
  },
  {
    "objectID": "NTI/NTI-Lesson20.html#learning-checks",
    "href": "NTI/NTI-Lesson20.html#learning-checks",
    "title": "Math 300R NTI Lesson 20",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson20.html#section",
    "href": "NTI/NTI-Lesson20.html#section",
    "title": "Math 300R NTI Lesson 20",
    "section": "20.1",
    "text": "20.1\n?@sec-size-of-variable describes two very closely related summary quantities used to measures of the “size” of a variable: i. the variance and ii. the “standard deviation” (which is the square root of the variance.\n\nUsing software, what is the variance of the XXX variable in the YYY data frame? Make sure to include the units.\nWhat is the “standard deviation” of the XXX variable? Calculate this in two different ways: i. “by-hand” taking of the square root of the variance; ii. using the sd() software directly.\n\n[Repeat for a number of variables from different data frames.]\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson20.html#section-1",
    "href": "NTI/NTI-Lesson20.html#section-1",
    "title": "Math 300R NTI Lesson 20",
    "section": "20.2",
    "text": "20.2\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson20.html#documenting-software",
    "href": "NTI/NTI-Lesson20.html#documenting-software",
    "title": "Math 300R NTI Lesson 20",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-10-14\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2\nmosaic package version: 1.8.4\nmath300 package version: 0.1.0.9000"
  },
  {
    "objectID": "NTI/NTI-Lesson34.html",
    "href": "NTI/NTI-Lesson34.html",
    "title": "Math 300R NTI Lesson 34",
    "section": "",
    "text": "34.1. Build a classifier from case-control data.\n34.2. Cross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\n34.3. Calculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome."
  },
  {
    "objectID": "NTI/NTI-Lesson34.html#reading",
    "href": "NTI/NTI-Lesson34.html#reading",
    "title": "Math 300R NTI Lesson 34",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson34.html#lesson",
    "href": "NTI/NTI-Lesson34.html#lesson",
    "title": "Math 300R NTI Lesson 34",
    "section": "Lesson",
    "text": "Lesson\nReview the credit-card fraud example.\n\n\n\n\n\n\nModel performance\n\n\n\nSuppose the performance were as presented in the reading\n\n\n\n.\n+\n-\n\n\n\n\nyes\n1900\n100\n\n\nno\n50\n3950\n\n\n\nCalculate false-positive and false-negative rates from the testing data.\nWhat fraction of fraud was detected? What fraction of legitimate charges were denied? What’s your intuition about whether its worthwhile to have 50 false positives in order to avoid 1900 fraudulent uses?\n\n\nThe testing data was assembled to have a very large fraction of fraudulent events, more than is realistic. What’s the prevalence of fraud in the testing data?\nSuppose that rather than there being 1 fraud for every 2 legitimate charges, there is 1 fraud for every 200 charges. What would be the false-positive and false-negative rates then.\n\n\n\n\n\n\nClass activity\n\n\n\nAsk the class to figure this out in small group discussions."
  },
  {
    "objectID": "NTI/NTI-Lesson34.html#learning-checks",
    "href": "NTI/NTI-Lesson34.html#learning-checks",
    "title": "Math 300R NTI Lesson 34",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson34.html#section",
    "href": "NTI/NTI-Lesson34.html#section",
    "title": "Math 300R NTI Lesson 34",
    "section": "34.1",
    "text": "34.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson34.html#documenting-software",
    "href": "NTI/NTI-Lesson34.html#documenting-software",
    "title": "Math 300R NTI Lesson 34",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson22.html",
    "href": "NTI/NTI-Lesson22.html",
    "title": "Math 300R NTI Lesson 22",
    "section": "",
    "text": "22.1 Implement on the computer a procedure to generate a sample, calculate a regression model, and produce a summary.\n22.2 Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\n22.3 Graphically display the distribution of summaries and generate a compact numerical description (“confidence interval”) of the sampling distribution."
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#reading",
    "href": "NTI/NTI-Lesson22.html#reading",
    "title": "Math 300R NTI Lesson 22",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#lesson",
    "href": "NTI/NTI-Lesson22.html#lesson",
    "title": "Math 300R NTI Lesson 22",
    "section": "Lesson",
    "text": "Lesson\nIn the lesson, establish\n\n\n\n\n\n\nSetup\n\n\n\n\nlibrary(mosaic)\nlibrary(math300)\n\n\n\n\n\n\n\n\n\nExample: A simple DAG\n\n\n\nWe’ll work with a simple example: dag01\n\nGive dag01 as a command to view the DAG. Draw it as a diagram.\nBased on dag01, what should be the coefficients on the regression model y ~ x.\nGenerate simulated data with nrow=25 and find the regression coefficients.\n\n\nSample <- sample(dag01, size=25)\nlm(y ~ x, data = Sample) %>% coefficients()\n\n(Intercept)           x \n   4.010625    1.643239 \n\n\nInference task: Given a report like the one above, say what the coefficient on x is likely to be in the underlying data generation mechanism.\n\n\n\n\n\n\nActivity\n\n\n\n\nHow would you represent “is likely to be”?\nTake your best guess and report your representation.\n\n\n\n\n\n::: {.callout-note icon=false} ## Automating the procedure\nIn the previous example, we\n\ngenerated (simulated) data,\nused it to build a regression model,\nsummarized it\n\nTo automate the procedure, put all the steps in one block of code, surrounded by curly braces.\n\nproc01 <- function() {\n  lm(y ~ x, data = sample(dag01, size=25)) %>% coefficients()\n}\nproc01()\n\n(Intercept)           x \n   3.972924    1.567417 \n\n\nNow that the procedure is automated, we can see what happens if we carry out many trials:\n\nTrials <- do(100) * proc01()\n\n\n\n\n\n\n\nActivity: Simulating the sampling distribution\n\n\n\n\nGenerate the 100 trials. each involving a simple of size \\(n=25\\) from dag01. The x column gives the regression coefficient on the simulation’s x variable x for each of the 100 trials.\nSummarize the x coefficient in the trials, graphically and numerically.\nCompare the distribution of x in Trials to the corresponding coefficient in dag01.\n\n\n\nQuestion: How does the sampling distribution change if the sample size in each trial is changed to \\(n=100\\) from \\(n=25\\)?"
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#learning-checks",
    "href": "NTI/NTI-Lesson22.html#learning-checks",
    "title": "Math 300R NTI Lesson 22",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#obj-20.3",
    "href": "NTI/NTI-Lesson22.html#obj-20.3",
    "title": "Math 300R NTI Lesson 22",
    "section": "22.1 (Obj 20.3)",
    "text": "22.1 (Obj 20.3)\nConsider these three data frames:\n\nOne <- sample(dag01, size=25)\nTwo <- do(10) * {\n  lm(y ~ x, data = sample(dag01, size=25)) %>%\n    coefficients()\n  }\nThree <- Two %>% \n  summarize(mx = mean(x), sx = sd(x))\n\n\nBoth One and Two have columns called x, but they stand for different things. Explain what the unit of observation is and what the values in x represent..\nThree does not have a column named x, but it is a summary of the x column from Two. What kind of summary.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nIn One, the x column contains the simulated of the x variable from dag01. The unit of observation is a single case, for instance a person for whom observations were made of x and y. The simulation involves generating 25 rows of data: one row for each of 25 people.\nIn Two, the x column is the regression coefficient on x from the simulation. Each row of Two corresponds to one trial in which regression is being performed on a sample of size 25 of simulated data from dag01.\nThree is a summary of the 10 trials in Two. The columns, named mx and sx, tell about the distribution of x across all the trials."
  },
  {
    "objectID": "NTI/NTI-Lesson22.html#documenting-software",
    "href": "NTI/NTI-Lesson22.html#documenting-software",
    "title": "Math 300R NTI Lesson 22",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-10-14\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/NTI-Lesson36.html",
    "href": "NTI/NTI-Lesson36.html",
    "title": "Math 300R NTI Lesson 35",
    "section": "",
    "text": "35.1 Explain why case-control data may not give an proper measure of “prevalence.”\n35.2 Understand sensitivity and specificity as conditional probabilities.\n35.3 Calculate false-positive and false-negative rates for a given prevalence."
  },
  {
    "objectID": "NTI/NTI-Lesson36.html#reading",
    "href": "NTI/NTI-Lesson36.html#reading",
    "title": "Math 300R NTI Lesson 35",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson36.html#lesson",
    "href": "NTI/NTI-Lesson36.html#lesson",
    "title": "Math 300R NTI Lesson 35",
    "section": "Lesson",
    "text": "Lesson\nPicture of F under Null in Compact Inference 12.2"
  },
  {
    "objectID": "NTI/NTI-Lesson36.html#learning-checks",
    "href": "NTI/NTI-Lesson36.html#learning-checks",
    "title": "Math 300R NTI Lesson 35",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson36.html#section",
    "href": "NTI/NTI-Lesson36.html#section",
    "title": "Math 300R NTI Lesson 35",
    "section": "35.1",
    "text": "35.1\nGiven some classifier summaries, calculate the false-positive and false-negative rates as well as the sensitivity and specificity\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson36.html#documenting-software",
    "href": "NTI/NTI-Lesson36.html#documenting-software",
    "title": "Math 300R NTI Lesson 35",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson37.html",
    "href": "NTI/NTI-Lesson37.html",
    "title": "Math 300R NTI Lesson 37",
    "section": "",
    "text": "37.1 The permutation test\n37.2 Interpret correctly from regression/ANOVA reports\n37.3 Traditional names for hypothesis tests in different “textbook” settings.\n37.4. Distinguish between p-value and effect size, that is, “significance” and “substance.”"
  },
  {
    "objectID": "NTI/NTI-Lesson37.html#reading",
    "href": "NTI/NTI-Lesson37.html#reading",
    "title": "Math 300R NTI Lesson 37",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson37.html#lesson",
    "href": "NTI/NTI-Lesson37.html#lesson",
    "title": "Math 300R NTI Lesson 37",
    "section": "Lesson",
    "text": "Lesson\nMany planets picture"
  },
  {
    "objectID": "NTI/NTI-Lesson37.html#learning-checks",
    "href": "NTI/NTI-Lesson37.html#learning-checks",
    "title": "Math 300R NTI Lesson 37",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson37.html#section",
    "href": "NTI/NTI-Lesson37.html#section",
    "title": "Math 300R NTI Lesson 37",
    "section": "37.1",
    "text": "37.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson37.html#documenting-software",
    "href": "NTI/NTI-Lesson37.html#documenting-software",
    "title": "Math 300R NTI Lesson 37",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson23.html",
    "href": "NTI/NTI-Lesson23.html",
    "title": "Math 300R NTI Lesson 23",
    "section": "",
    "text": "23.1 Use bootstrapping to estimate sampling variation.\n23.2 Infer sampling variation from a regression table: “standard error” of a model coefficient.\n23.3 Construct and interpret confidence intervals on a model coefficient.\n23.4. Understand and use scaling of confidence interval length as a function of \\(n\\)."
  },
  {
    "objectID": "NTI/NTI-Lesson23.html#reading",
    "href": "NTI/NTI-Lesson23.html#reading",
    "title": "Math 300R NTI Lesson 23",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson23.html#lesson",
    "href": "NTI/NTI-Lesson23.html#lesson",
    "title": "Math 300R NTI Lesson 23",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nSetup\n\n\n\n\nsource(\"../_startup.R\")\n\n\n\nThe point of today’s lesson is to show how some properties of the sampling distribution can be estimated from a single sample, rather than the many trials of sample-then-summarize that we used in Lesson 22."
  },
  {
    "objectID": "NTI/NTI-Lesson23.html#bootstrapping",
    "href": "NTI/NTI-Lesson23.html#bootstrapping",
    "title": "Math 300R NTI Lesson 23",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nOne approach is to mimic what we did when we had a DAG available: run many trials. But all we have is a sample, not the DAG. Can we run many trials from the sample itself?\nLet’s use the moderndive::amazon_books data set and the model tilde list_price ~ num_pages\nHere’s the results on the actual data:\n\nlm(list_price ~ num_pages, data = moderndive::amazon_books) %>% coefficients()\n\n(Intercept)   num_pages \n 11.8438357   0.0198815 \n\n\nInterpretation: books cost about $12 plus 2 cents per page.\nHere is our first stab at sampling from the data. Note that when sampling from a data set, sample() sets the sample size to be the same as the number of rows in the data set.\n\ntrial <- function() {\n  lm(list_price ~ num_pages, data = sample(moderndive::amazon_books)) %>% \n    coefficients()\n}\n{do(100) * trial()} %>% summarize(sd(Intercept), sd(num_pages))\n\n  sd(Intercept) sd(num_pages)\n1  1.248821e-14  2.476454e-17\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nHow to interpret these numbers? What are they telling us about the sampling distribution on the regression coefficients?\nWhat’s wrong?\nHow to fix it?\n\n\n\nIntroduce the replace=TRUE argument for sample(). Explain what it does and show the result. Is the result correct? We can’t know, because we don’t have access to the data-generating DAG in order to run many trials. But we can test the method on data generated from a DAG and confirm that it gives a reasonable result.\n\n\n\n\n\n\nActivity\n\n\n\nEach student should construct one from a dag of his or her choice.\n\n\n\nRegression table\nIn the readings, we mentioned that there are formulas for the standard deviation of the sampling distribution. [NEEDED TO GIVE THIS A NAME EARLIER: “SD of sampling variability”, “standard error”] All you need to know about the formulas for the standard error is that i) they exist for linear regression and ii) the standard error is inversely proportional to \\(\\sqrt{n}\\).\nShow how to construct the regression table and where to find the relevant standard error\n\ndo(3) * {lm(list_price ~ num_pages, data = moderndive::amazon_books) %>% broom::tidy()}\n\n# A tibble: 6 × 7\n  term        estimate std.error statistic  p.value  .row .index\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl> <int>  <dbl>\n1 (Intercept)  11.8      1.79         6.63 1.40e-10     1      1\n2 num_pages     0.0199   0.00479      4.15 4.24e- 5     2      1\n3 (Intercept)  11.8      1.79         6.63 1.40e-10     1      2\n4 num_pages     0.0199   0.00479      4.15 4.24e- 5     2      2\n5 (Intercept)  11.8      1.79         6.63 1.40e-10     1      3\n6 num_pages     0.0199   0.00479      4.15 4.24e- 5     2      3\n\n\n\nlm(list_price ~ num_pages, data = moderndive::amazon_books) %>% broom::glance()\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0511  0.0481  13.9    17.2 4.24e-5     1 -1304. 2614. 2625.  61999.     320\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual"
  },
  {
    "objectID": "NTI/NTI-Lesson23.html#margin-of-error",
    "href": "NTI/NTI-Lesson23.html#margin-of-error",
    "title": "Math 300R NTI Lesson 23",
    "section": "Margin of error",
    "text": "Margin of error\nConfidence interval as estimate \\(\\pm 2\\times\\) standard error.\nGraphics for confidence interval. They need to know how to read them, not to make the graphics."
  },
  {
    "objectID": "NTI/NTI-Lesson23.html#learning-checks",
    "href": "NTI/NTI-Lesson23.html#learning-checks",
    "title": "Math 300R NTI Lesson 23",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson23.html#section",
    "href": "NTI/NTI-Lesson23.html#section",
    "title": "Math 300R NTI Lesson 23",
    "section": "23.1",
    "text": "23.1\nVocabulary: Sampling distribution, standard error, sampling variability, sample size\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson23.html#documenting-software",
    "href": "NTI/NTI-Lesson23.html#documenting-software",
    "title": "Math 300R NTI Lesson 23",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-10-14\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2\nmosaic package version: 1.8.4\nmath300 package version: 0.1.0.9000"
  },
  {
    "objectID": "NTI/NTI-Lesson33.html",
    "href": "NTI/NTI-Lesson33.html",
    "title": "Math 300R NTI Lesson 33",
    "section": "",
    "text": "33.1. Distinguish between absolute and relative risk and identify when a change in risk is being presented as absolute or relative.\n33.2. Calculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\n33.3. Interpret effect size as stated in log odds."
  },
  {
    "objectID": "NTI/NTI-Lesson33.html#reading",
    "href": "NTI/NTI-Lesson33.html#reading",
    "title": "Math 300R NTI Lesson 33",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson33.html#lesson",
    "href": "NTI/NTI-Lesson33.html#lesson",
    "title": "Math 300R NTI Lesson 33",
    "section": "Lesson",
    "text": "Lesson\n\nRisk\nIntroduce words related to “risk” and try to convert them into seeing a continuum of “risk” to “opportunity,” that is, uncertain outcomes that could be positive or negative. (“Risk reduction,” “volatility,” “gain,” “gambling”).\nEach risky outcome is quantified by two numbers: a probability, and a value. It’s easiest to talk about the value measured in dollars, but there of course other scales: “lives (lost or saved),” “years of life,” “happiness,” “satisfaction,” etc. Unfortunately, we don’t have standard numerical scales for these.\n(Epidemiologists have a scale for quantifying health outcome: Quality adjusted life years (QALYs) which would take a class session on its own to explain, but which is highly instructive.)\nA “loss function” takes as input a possible outcome of an uncertain event and produces as output a value, which we might measure in dollars or QALYs or lives, etc. For the sake of definiteness, denote a loss function as \\(L(\\cal O)\\), where \\(\\cal O\\) is an outcome of the uncertain event. In quantifying loss, each possible outcome \\(\\cal O\\) is associated with a probability. Let’s call this \\(p(\\cal O)\\). Mathematical philosophers have been arguing since the mid 1600s about what exactly is meant by probability. Don’t expect a resolution to this quandry. For our purposes, it suffices to think about a probability of an outcome in the context of a prediction model, using the tools from Lesson 26.\n\n\n\n\n\n\nExample: Ice cream sales\n\n\n\nTo illustrate, imagine the situation of an ice-cream pushcart vendor. If it is cold or rainy, sales will be slight. If it’s beautiful and hot, sales will be high. Help the students construct a plausible “loss” function: sales versus temperature.1 Then generalize that into a function with two inputs, say, raininess and temperature. Or even three inputs: day of the week/holiday, raininess and temperature. Whatever the inputs, the output is sales of ice cream.\n\n\n\n\nSimplifying loss\nWe have two functions that, together, quantify risk. A loss function \\(L(\\cal O\\) and a probability function \\(p(\\cal O)\\) where \\(\\cal O\\) is the uncertain outcome of the event. It’s very common to compose these two functions into a single function: \\(p(L(\\cal O))\\). Imagine \\(L(\\cal O)\\) has an output denominated in, say, dollars. The composed function then looks like \\(p(\\$)\\). There may be many different outcomes that lead to the same $ loss, but lumping them together as a single $ outcome gives a function \\(p(\\$)\\). Presented this way, risk measurement is reduced to a single function with a single input, $, and an output in the form of a probability.\n\n\nProbability, odds, log odds\nThe scale generally used for uncertainty is “probability,” a number between 0 and 1. But there are other scales in use and sometimes these have advantages for doing calculations. The two most important are:\n\nOdds\nLog odds\n\nAny probability can be converted to an odds and vice versa. Similarly, log odds can be converted into odds by using exponentiation (or vice versa, by using the logarithm function).\n\n\n\n\n\n\nOdds and gambling\n\n\n\nThe bookie arranges things so that the money staked by the gamblers is simply redistributed. The winners get back their stake plus the money staked by the losers. The odds given reflect the balance between the winners’ stakes and the losers’ stakes. If $50 is bet on A and $25 on B, then the odds on A will be 1:2. That is, the winners, if A is the outcome, will get $25 plus their stake back. Since their stake was $50, the odds will be 1:2. On the other hand, if B is the outcome, the winners will get $50 plus their stake. This corresponds to odds of 2:1.\n\n\n\n\nAccumulating loss\nProvide a situation where there are many “risk factors” for an outcome. Suppose the outcome is disease D and there are risk factors \\(R_1\\), \\(R_2\\), \\(R_3\\), …, say “smoking,” “drinking,” “sun exposure,” “speeding,” and so on. With disease, age is almost always a factor.\nImagine that the risk of D in the next 10 years is p_0 = 20% for a 20-year old who doesn’t smoke, drink, sunbathe, or drive too fast. For the sake of illustration, let’s imagine that smoking triples the risk, drinking doubles it, sunbathing increases it by 50%, speeding doubles it, and each year of age increases it by 2%. The 50-year old who has all the risk factors therefore faces a risk of D of\n$             = 6.52 $\nThis is ridiculous: you can’t have a probability of 6.52. In general, it doesn’t work to multiply the effects of risk factors.\nInstead, the standard statistical/mathematical approach is to represent risk using log odds.\nBase risk: 20% which corresponds to odds of 2/8 or log odds of -1.39.\nSmoking: Triples the risk to 60%, that is odds of 6/4 or log odds of 0.405. This is an increase of \\((0.405 - -1.39) = 1.75\\) above the base risk.\nDrinking: Doubles the risk to 40%, that is odds of 4/6 or log odds of -0.405, an increase above the base rate of \\(-0.405 - -1.39 = 0.985\\).\nTo accumulate risk, rather than multiplying, we add in the log-odds increment due to each risk factor.\nIf multiplying, smoking and drinking for a 20-year old would produce a risk of \\(0.2 \\times 3 \\times 2 = 1.2\\), which cannot be a probability.\nAdding the log-odds increments gives the log-odds for a 20-year smoker/drinker as \\(-1.39 + 1.75 + 0.985 = 1.345\\). The odds for this person are therefore \\(e^{1.345} = 3.83\\). Converting odds back to probability gives \\(\\frac{3.83}{1+3.83} = 79\\%\\).\n\n\n\n\n\n\nThe sigmoidal pattern\n\n\n\nShow the risk as a function of age in the multiplying style: \\(0.2 \\times 1.02^t\\).\nNow show risk as a function of age in log odds format. Base is \\(-1.39\\). One year’s increment gives \\(0.2 \\times 1.02 = 0.204\\) which converts to \\(\\ln(\\frac{.204/.796}) = -1.36\\), an increment of 0.03 in log-odds units.\nIn this format, risk as a function of age will be \\(-1.39 + 0.03t\\), where \\(t\\) is the number of years past age 20. We can convert this to a probability as a function of age:\n\\(p(t) = \\frac{\\exp(-1.39 + 0.03t)}{1 + \\exp(-1.39 + 0.03t)}\\). To emphasize the pattern, let’s follow the person’s risk over the next 200 years (even though nobody lives to age 220).\n\nslice_plot(exp(-1.39+0.03*t)/ (1+exp(-1.39+0.03*t)) ~ t, bounds(t=0:200))\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Human perception of risk\n\n\n\nPeople tend to perceive risk in a way that doesn’t correspond to a probability or a value scale (such as ice-cream sales). First, outcomes with a high probability are seen as certain. Outcomes with a very low probability are either seen as impossible or rounded up. So the perception of a one-in-a-million risk is seen as a risk on the order of 1% or 10%.\nSecond, there are “dread factors” that can dramatically increase the perceived risk. Examples are: “out of my control,” “impact on the unborn,” “hidden impact.” The dread-factor theory provides an explanation for why many people see commercial aviation as riskier than driving, or the huge aversion to low levels of radiation exposure."
  },
  {
    "objectID": "NTI/NTI-Lesson33.html#learning-checks",
    "href": "NTI/NTI-Lesson33.html#learning-checks",
    "title": "Math 300R NTI Lesson 33",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson33.html#section",
    "href": "NTI/NTI-Lesson33.html#section",
    "title": "Math 300R NTI Lesson 33",
    "section": "33.1",
    "text": "33.1\n\nConvert probability to odds and log odds, and vice versa.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson33.html#documenting-software",
    "href": "NTI/NTI-Lesson33.html#documenting-software",
    "title": "Math 300R NTI Lesson 33",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson32.html",
    "href": "NTI/NTI-Lesson32.html",
    "title": "Math 300R NTI Lesson 32",
    "section": "",
    "text": "32.1 Properly use nomenclature of experiment.\n32.2 Correctly re-draw DAG for an ideal experimental intervention.\n32.3 Use blocking to set assignment to treatment or control."
  },
  {
    "objectID": "NTI/NTI-Lesson32.html#reading",
    "href": "NTI/NTI-Lesson32.html#reading",
    "title": "Math 300R NTI Lesson 32",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson32.html#lesson",
    "href": "NTI/NTI-Lesson32.html#lesson",
    "title": "Math 300R NTI Lesson 32",
    "section": "Lesson",
    "text": "Lesson\nShort discussion about the words “hypothesis” and “theory.” Try to establish a simple, concrete definition of hypothesis: “A statement about the world that might or might not be correct.”\n\n\n\n\n\n\nDefining “hypothesis”\n\n\n\n\nA supposition or proposed explanation made on the basis of limited evidence as a starting point for further investigation: professional astronomers attacked him for popularizing an unconfirmed hypothesis.\nPhilosophy a proposition made as a basis for reasoning, without any assumption of its truth: [with clause] : the hypothesis that every event has a cause.\n\nORIGIN: late 16th century: via late Latin from Greek hupothesis ‘foundation’, from hupo ‘under’ + thesis ‘placing’.\nSource: New Oxford American Dictionary\n\n\nA DAG is merely a hypothesis about how mechanisms function. One of the prime objectives of “science,” be it a natural science like chemistry or a social science like economics, is to provide evidence for an against relevant hypotheses.\nIt’s a common view of science, at least in sciences such as chemistry or physics or molecular genetics, that the theories (say, \\(F = ma\\) or the translation of DNA triples into amino acids) are a complete and truthful description of the world. The word “theory” itself aids in this perception and often one studies “theory” in order to be able to calculate the outcomes in various situations, for instance the speed over time of a rocket accelerating upward as it burns fuel. We don’t talk about the “hypothesis of relativity” or “quantum hypothesis.” The word “hypothesis” suggests doubt. There was certainly substantial doubt in the early days of relativity or quantum mechanics or even Newton’s theory of universal gravitation. But evidence has been adduced to such an extent that the doubt is practically dispelled and what were once hypotheses are now considered theories. (There’s also a pattern of using “theory” to promote hypotheses into being regarded as more substantial. An example is “string theory,” which is not a generally accepted way of describing nature. The “theory of dark matter” is similar.)\n\nEvidence for or against hypotheses\n\nPrediction and observation.\n\nParticularly noteworthy are predictions that go against other hypotheses. For example, Newtonian gravitation isn’t a wave phenomenon, but general relativity (Einstein’s theory of gravity) does predict wave-like behavior. In (Lesson 26)[Math300R-Lesson26.html] we discussed techniques for making predictions that incorporate uncertainty. This provides a means to refine judgements about the extent to which a prediction is supported by observation.\nFor instance, General Relativity supports a prediction that gravity propagates like a wave. This prediction can be quantified: a wave that might be observable by an incredibly delicate apparatus would be generated by, say, a supernova. Apparatus is built, the predicted phenomena is observed, and confidence in the theory increases accordingly. Often, predictions are not followed by observation. This unhappy situation is often followed by attempts to tweak the theory to make improved predictions that correspond to the (lack of) observation.\n\nExperiment, which is closely associated with the “scientific method.”\n\nThe hallmark of experiment is intervention. For instance, an important theory of plant growth involves a chemical called “auxin.” The theory holds that exposing plant tissue to light inhibits the generation of auxin. The auxin in turn, facilitates tissue growth and expansion. The result of the differential growth in on the lit and shaded side of a plant is growth toward the source of light. Illustration.\nMuch of the experimentalist’s intervention involves standardizing the conditions of the experiment. For instance, a plant growth study will provide a standard soil mixture, temperature, and watering amount for all the plants being studied.\nBut experiment always involves another aspect: one or more of the conditions is made to vary from one experimental unit to another. For instance, some plants might be lit from the North and others from the South. Seeing the same pattern of light-associated plant growth from both directions provides evidence that the Earth’s magnetic field is not a factor in determining plant orientation.\n\n\nSimulating experiment with DAGs\nExperiment is hard work and good experimental technique is paramount. Our goal here is to enhance the understanding of how experiment works generally. We’re going to do this by simulation/gaming.\nWe’ll start with a DAG that provides the rules of the game for how different factors are connected. We’ll generate samples from the DAG and do some data analysis to illuminate the conditions in which data analysis can reveal—or not—the DAG mechanism. Then we’ll simulate the intervention of experiment and see whether this can improve our ability to reveal the mechanism. (Of course, in real science, we don’t have this ability to look immediately at the mechanism and see how it’s structured. Our goal, however, is not to do science but to illustrate how scientific technique works.)\nConsider this DAG:\n\ndag08\n\n[[1]]\nc ~ eps()\n\n[[2]]\nx ~ c + eps()\n\n[[3]]\ny ~ x + c + 3 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\ndag_draw(dag08)\n\n\n\n\nYou can see from the DAG formulas that the value of y increases or decreases exactly with the value of x. (The “coefficient” on x is 1.0)\nAs we’ve done before, we can simulate data from dag08 and use regression to observe the relationship between x and y.\n\nSample <- sample(dag08, size=1000) # big, so that the noise is not a strong issue\nlm(y ~ x, data = Sample) %>% coefficients()\n\n(Intercept)           x \n   3.039498    1.450582 \n\nlm(y ~ x, data = Sample) %>% confint()\n\n               2.5 %   97.5 %\n(Intercept) 2.962382 3.116615\nx           1.395967 1.505197\n\n\nThe coefficient we observe is 1.5, not 1.0. So this approach to data analysis is not revealing the true mechanism. (It’s legitimate to wonder whether 1.5 is really that different from 1.0, given the random factors involved in the mechanism. The confidence interval guides us here. By using size=1000, we narrowed the confidence interval to the extent that the interval on the observed coefficient has no overlap with the DAG specified coefficient of 1.0).\nIn Lesson 27 we discussed using covariates in a model in order to get a more accurate view of the true mechanism. This is a valuable technique, but using it requires that we know what are the covariates and can record their values in data. In this lesson, we’ll imagine that we know how to do an experiment but don’t have any reliable knowledge about covariates.\n\n\nIntervening in a DAG\nCarrying out an experiment means intervening in the workings of the system. The intervention we’re going to make in this example is to experimentally “control” or “set” the values of x.\nIn practice, this means we are creating a new DAG that represents our intervention in the system.\nThe dag_intervene() function is convenient for constructing the new DAG. The function works by letting us create a new formula for any variable in the DAG. For an ideal experiment, the new formula will just involve setting the values of x as we will.\n\ndag08\n\n[[1]]\nc ~ eps()\n\n[[2]]\nx ~ c + eps()\n\n[[3]]\ny ~ x + c + 3 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\nexp08 <- dag_intervene(dag08, x ~ binom())\ndag_draw(exp08)\n\n\n\n\nNotice that the experimental intervention has disconnected all the inputs from x.\nSample data from exp08 and do an appropriate data analysis:\n\nExp_sample <- sample(exp08, size=1000)\nlm(y ~ x, data = Exp_sample) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept) 2.9378476 3.187039\nx           0.8344393 1.185796\n\n\nThe confidence interval on the coefficient on x is 0.88 to 1.21, completely consistent with the (known) mechanism linking x and y. In other words, the experimental intervention correctly reveals the true mechanism.\n\n\n\n\n\n\nCovariates eat variance\n\n\n\nIf we have access to covariates, it’s perfectly legitimate to include them in our models as explanatory variables. The advantage of doing so is that the amount of noise in y is reduced, because we are accounting for it by using the covariate.\n\nlm(y ~ x + c, data = Exp_sample) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept) 2.9637715 3.141354\nx           0.8511857 1.101605\nc           0.9590015 1.088100\n\n\nThe confidence interval on the x coefficient is smaller: 0.92 to 1.17.\n\n\n\n\nImperfect experiment\nAn imperfect experiment, in the sense we mean here, is an experiment where the intervention attenuates the inputs to the variable of interest, but doesn’t utterly eliminate them. Here’s an example:\n\nexp08b <- dag_intervene(dag08, x ~ c/2 + binom())\ndag_draw(exp08b)\n\n\n\n\nImperfect experiments are the rule in, say, clinical drug trials. For instance, people who are still sick may drop out of the trial or stop taking the assigned medicine.\nLet’s look at the impact of the imperfection in exp08b:\n\nExpb_sample <- sample(exp08b, size=1000)\nlm(y ~ x, data = Expb_sample) %>% confint()\n\n               2.5 %   97.5 %\n(Intercept) 2.327218 2.503296\nx           1.919945 2.119241\n\n\nThe confidence interval on the coefficient on x shows that we have not correctly revealed the mechanism of x -> y of the DAG.\nThere is a technique to improve results in experimental trial. We’ll make yet another DAG, which will be the same as exp08b but where we record the experimenter’s intended setting for the x variable.\n\nexp08c <- dag_make(\n  c ~ eps(),\n  z ~ binom(),\n  x ~ c/2 + z,\n  y ~ x + c + 3 + eps()\n)\ndag_draw(exp08c)\n\n\n\n\nLet’s see what data analysis reveals for this DAG.\n\nExpc_sample <- sample(exp08c, size=1000)\nlm(y ~ x, data = Expc_sample) %>% confint()\n\n               2.5 %   97.5 %\n(Intercept) 2.460788 2.649098\nx           1.808017 2.031968\n\n\nStill a wrong result. The improvement comes when we ignore the actual value of x and look at the relationship between the experimentally perfect z and y:\n\nlm(y ~ z, data = Expc_sample) %>% confint()\n\n                2.5 %   97.5 %\n(Intercept) 2.9116824 3.218985\nz           0.6171827 1.057989\n\n\nBack on target! This method is called “intent to treat” because we are looking at how our experimental intentions play out to y."
  },
  {
    "objectID": "NTI/NTI-Lesson32.html#learning-checks",
    "href": "NTI/NTI-Lesson32.html#learning-checks",
    "title": "Math 300R NTI Lesson 32",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson32.html#section",
    "href": "NTI/NTI-Lesson32.html#section",
    "title": "Math 300R NTI Lesson 32",
    "section": "32.1",
    "text": "32.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson32.html#documenting-software",
    "href": "NTI/NTI-Lesson32.html#documenting-software",
    "title": "Math 300R NTI Lesson 32",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson26.html",
    "href": "NTI/NTI-Lesson26.html",
    "title": "Math 300R NTI Lesson 26",
    "section": "",
    "text": "26.1 In evaluating a model function, generate a prediction interval.\n26.2 Interpret prediction bands as a series of intervals, one for each value of the model input.\n26.3 Identify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t.\nHere’s a demo which\nA data scientist makes many prediction models over their career. Sometimes, the prediction interval will be much wider than the actual error. People will accuse you of being overly humble, suggesting that your skills are better than you take credit for. Other times, the prediction interval will be too small. Then you will be accused of over-confidence. But both these outcomes are a matter of luck: whether your sample happened to be a good representation of the actual mechanism or not.\nWhen the prediction bands are double-trumpet shaped, you are at risk of such “over-confidence” or “under-confidence.”\n::: {.callout-note icon=false} ## Optional extension\nThis construction of prediction intervals is based on the idea that we measure the input variables exactly, but the output can involve some noise.\nIt can also happen that our measurement of the input quantities involves noise. This goes by the obscure name “error in variables.” dag05 simulates the error-in-noise situation. Note that the estimated function is systematically flatter than the actual function (which you can read from the DAG).\nTo demonstrate this, insert dag04 in place of dag05 in the simulation, and repeat many times."
  },
  {
    "objectID": "NTI/NTI-Lesson26.html#reading",
    "href": "NTI/NTI-Lesson26.html#reading",
    "title": "Math 300R NTI Lesson 26",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson26.html#lesson",
    "href": "NTI/NTI-Lesson26.html#lesson",
    "title": "Math 300R NTI Lesson 26",
    "section": "Lesson",
    "text": "Lesson"
  },
  {
    "objectID": "NTI/NTI-Lesson26.html#setup",
    "href": "NTI/NTI-Lesson26.html#setup",
    "title": "Math 300R NTI Lesson 26",
    "section": "Setup",
    "text": "Setup\n\n# Simple correlation\ndag04 <- dag_make(x ~ eps(), y ~ 2*x + eps(0.5))\n# \"Error in variables\", that is, we don't measure x exactly.\ndag05 <- dag_make(.latent ~ eps(), x ~ .latent + eps(0.5), y ~ 2*.latent + eps(0.5))"
  },
  {
    "objectID": "NTI/NTI-Lesson26.html#learning-checks",
    "href": "NTI/NTI-Lesson26.html#learning-checks",
    "title": "Math 300R NTI Lesson 26",
    "section": "Learning Checks",
    "text": "Learning Checks\nIdeas\n\nConstruct prediction interval when evaluating a model function.\nPlot a prediction band.\nCheck the consistency of the prediction band with the DAG mechanism for large \\(n\\).\n\nIs the width right?\nIs the slope right?\n\nFor small \\(n\\) (say, \\(n=5\\)), how is the prediction band different than for large \\(n\\)?"
  },
  {
    "objectID": "NTI/NTI-Lesson26.html#section",
    "href": "NTI/NTI-Lesson26.html#section",
    "title": "Math 300R NTI Lesson 26",
    "section": "26.1",
    "text": "26.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson26.html#documenting-software",
    "href": "NTI/NTI-Lesson26.html#documenting-software",
    "title": "Math 300R NTI Lesson 26",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson30.html",
    "href": "NTI/NTI-Lesson30.html",
    "title": "Math 300R NTI Lesson 30",
    "section": "",
    "text": "30.1 Identify confounding in a DAG\n30.2 Choose whether to include covariate depending on form of DAG"
  },
  {
    "objectID": "NTI/NTI-Lesson30.html#reading",
    "href": "NTI/NTI-Lesson30.html#reading",
    "title": "Math 300R NTI Lesson 30",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson30.html#lesson",
    "href": "NTI/NTI-Lesson30.html#lesson",
    "title": "Math 300R NTI Lesson 30",
    "section": "Lesson",
    "text": "Lesson\nEarlier we described two different types of statistical task:\n\nPrediction. Say something about what the outcome will be for a new case.\nRelationship. Describe how two or more variables are inter-related.\n\nIn a prediction task, we use training data to build a model based on data from the system of interest. The model is then used as a function to calculate the output (and its precision). The goal is to minimize out-of-sample prediction error. Including covariates in the model can sometimes improve the precision, but sometimes not.\nIn a relationship task, we also use training data to build a model. But rather than looking at the output from the model directly, we look at the partial derivative of the model function with respect to an input of interest. This quantifies the strength of the relationship between selected input variable and the response variable.\nSince we’re using first order polynomial models (e.g. y ~ a + b + c), there is no technical difficulty finding the partial derivative. It is simply the model coefficient on the variable of interest.\nIn today’s lesson, we’re going to use gaming so that we know exactly what the causal connections are. Remember, it’s just a game! Our objective in playing the game is to learn in what circumstances we can capture the true causal mechanism behind the data.\n\n\n\n\n\n\nExample: Sorting out multiple causes\n\n\n\ndag02 simulates a situation where two variables are connected causally to y. Questions: Can a fitted model capture the true relationships? Must we use both variables?\n\ndag02\n\n[[1]]\nx ~ eps()\n\n[[2]]\na ~ eps()\n\n[[3]]\ny ~ 3 * x - 1.5 * a + 5 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\nSample <- sample(dag02, size=500)\nlm(y ~ x, data = Sample) %>% coefficients()\n\n(Intercept)           x \n   5.040952    2.942947 \n\nlm(y ~ a, data = Sample) %>% coefficients()\n\n(Intercept)           a \n   5.270944   -1.413438 \n\nlm(y ~ a + x, data = Sample) %>% coefficients()\n\n(Intercept)           a           x \n   4.989721   -1.463364    2.962672 \n\n\nConclusion: Ignoring one of the explanatory variables prevents us (of course!) from seeing that variable’s relationship with y. But the other variable’s connection shows up correctly. Using both explanatory variables let’s us capture the correct mechanism behind y.\n\n\nBut things aren’t always as simple as in the previous example. Consider dag08:\n\ndag_draw(dag08)\n\n\n\n\nHere both c and x have a causal relationship with y. They often happen to have a causal relationship with each other. Will this interfere with finding the direct relationships between x and y and between c and y?\n\ndag08\n\n[[1]]\nc ~ eps()\n\n[[2]]\nx ~ c + eps()\n\n[[3]]\ny ~ x + c + 3 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\nSample <- sample(dag08, size=500)\nlm(y ~ x, data = Sample) %>% coefficients()\n\n(Intercept)           x \n   3.057572    1.481704 \n\nlm(y ~ c, data = Sample) %>% coefficients()\n\n(Intercept)           c \n   2.997044    2.037481 \n\nlm(y ~ c + x, data = Sample) %>% coefficients()\n\n(Intercept)           c           x \n  3.0316396   0.8898113   1.0446811 \n\n\nHere we need to include both c and x in the model to see the correct causal relationships. If we leave out either x or c the other variable will “inherit” some of the causal connection between the left-out variable and the response.\nThis is called confounding. The dictionary has two different definitions of “confound”:\n\nTo surprise or confuse someone.\nTo mix up (something) with something else so that the individual elements become difficult to distinguish.\n\nIt’s the second definition that is relevant to us.\nAs an example of the “mixing up,” look at the coefficients for the model y ~ c. The DAG shows c having an effect size of 1 directly on y. But c also has an effect on y that is mediated by x. If we leave x out of the model, that indirect effect is mixed in with the direct effect.\nThe mixing of the direct and indirect causal routes from c to y is actually correctly captured by the coefficient 2 from the model. It’s important to be aware of this mixing. If you weren’t, you might be confused (confounded in the first definition!) because the results from y ~ c and y ~ c + x appear to conflict with one another."
  },
  {
    "objectID": "NTI/NTI-Lesson30.html#learning-checks",
    "href": "NTI/NTI-Lesson30.html#learning-checks",
    "title": "Math 300R NTI Lesson 30",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson30.html#section",
    "href": "NTI/NTI-Lesson30.html#section",
    "title": "Math 300R NTI Lesson 30",
    "section": "30.1",
    "text": "30.1\nDags with longer confounding pathways. Is there mixing when leaving out an element in the pathway. Mix up the directions of the arrows and show that the mixing occurs when the covariate is included in the model.\nRegression to the mean example.\nCollider?\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson30.html#documenting-software",
    "href": "NTI/NTI-Lesson30.html#documenting-software",
    "title": "Math 300R NTI Lesson 30",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson24.html",
    "href": "NTI/NTI-Lesson24.html",
    "title": "Math 300R NTI Lesson 24",
    "section": "",
    "text": "24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#reading",
    "href": "NTI/NTI-Lesson24.html#reading",
    "title": "Math 300R NTI Lesson 24",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#lesson",
    "href": "NTI/NTI-Lesson24.html#lesson",
    "title": "Math 300R NTI Lesson 24",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nIn-class example\n\n\n\n\n3 + 2\n\n[1] 5\n\n\n\n\n\n\n\n\n\n\nIn-class activity\n\n\n\n\ndate()\n\n[1] \"Fri Oct 14 13:26:01 2022\"\n\n\n\n\nThis is just a template."
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#learning-checks",
    "href": "NTI/NTI-Lesson24.html#learning-checks",
    "title": "Math 300R NTI Lesson 24",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#section",
    "href": "NTI/NTI-Lesson24.html#section",
    "title": "Math 300R NTI Lesson 24",
    "section": "24.1",
    "text": "24.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson24.html#documenting-software",
    "href": "NTI/NTI-Lesson24.html#documenting-software",
    "title": "Math 300R NTI Lesson 24",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-10-14\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/NTI-Lesson25.html",
    "href": "NTI/NTI-Lesson25.html",
    "title": "Math 300R NTI Lesson 25",
    "section": "",
    "text": "25.1 Given a sample from a DAG simulation, construct a predictor function for a specified response variable.\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "NTI/NTI-Lesson25.html#reading",
    "href": "NTI/NTI-Lesson25.html#reading",
    "title": "Math 300R NTI Lesson 25",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson25.html#lesson",
    "href": "NTI/NTI-Lesson25.html#lesson",
    "title": "Math 300R NTI Lesson 25",
    "section": "Lesson",
    "text": "Lesson\nReview mathematical/computational notation for functions.\n\n\\(f(x,y,z)\\) has three inputs (“arguments”) that are separated by commas inside the function parentheses.\nYou can create such a function using makeFun() (as in Math 141Z/142Z).\n\n\nset.seed(358549)\n\n\nSamp <- sample(dag03, size=10)\nMod <- lm(y ~ x, data = Samp)\n\nShow that Mod is not yet in the form of a function.\n\nMod(x=1)\n\nError in Mod(x = 1): supplied argument name 'x' does not match 'z'\n\n\nBut we can turn it into one with makeFun():\n\nf <- makeFun(Mod)\nf(x=1)\n\n       1 \n1.368519 \n\n\nThe function notation was invented long before statistics was a field. It turns out not to be very convenient for working in statistics. The reason: We have many variables stored in data frames. We’d like the input to our model functions to be in the form of a data frame. Even better, we’d like the output also to be in that form.\nThe mod_eval() function let’s us do this.\n\nOutput <- mod_eval(Mod, data = Samp)\n\n\nNote that we built the model with g ~ x + y, so the model outputs a g-like thing.\nThe inputs are x and y which are drawn from the data= frame.\nThe output calculated from the model is called model_output.\n\nThis output is often called a prediction, what the model tells us to expect for the response variable when the inputs are given.\n\nPrediction error\nThe prediction made by the model is not perfect. We can calculate the error, that is the difference between model output and the actual output for the given set of inputs.\n\nOutput <- Output %>% \n  mutate(error = y - model_output)\ngf_density(~ error, data = Output)\n\n\n\ngf_point(error ~ x, data = Output)\n\n\n\n\n\nThe errors have a bell-shaped distribution.\nNote that the error is centered on zero; sometimes the model is high and sometimes low. Only occasionally is it right on target.\nI’ve plotted the error versus the actual value. In this case, there seems to be no systematic deviation from being centered on zero.\n\nWe can quantify the average size of the error with the root mean square error:\n\nOutput %>% summarize(rms_error = sqrt(mean(error^2)))\n\n# A tibble: 1 × 1\n  rms_error\n      <dbl>\n1     0.960\n\n\nThis number is intended to be used to quantify the uncertainty in predictions from the model.\nBUT THERE IS A CATCH. Notice that something funny is going on in this pair of commands:\n\nMod <- lm(y ~ x, data = Samp)\nOutput <- mod_eval(Mod, data = Samp)\n\nThe prediction being made here is called an in-sample prediction; the same data are used to construct the model and to calculate the prediction error.\nIn contrast, here is an out-of-sample prediction, where “new” data is used for the data input to mod_eval(). The new data is called testing data, while the data used to construct the model is called training data.\n\nOOS <- mod_eval(Mod, data = sample(dag03, size=10000))\nOOS %>% \n  mutate(error = y - model_output) %>%\n  summarize(rms = sqrt(mean((error^2))))\n\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.47"
  },
  {
    "objectID": "NTI/NTI-Lesson25.html#activity",
    "href": "NTI/NTI-Lesson25.html#activity",
    "title": "Math 300R NTI Lesson 25",
    "section": "Activity",
    "text": "Activity\nWhich of these is in-sample and which out-of-sample prediction error."
  },
  {
    "objectID": "NTI/NTI-Lesson25.html#learning-checks",
    "href": "NTI/NTI-Lesson25.html#learning-checks",
    "title": "Math 300R NTI Lesson 25",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson25.html#section",
    "href": "NTI/NTI-Lesson25.html#section",
    "title": "Math 300R NTI Lesson 25",
    "section": "25.1",
    "text": "25.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson25.html#documenting-software",
    "href": "NTI/NTI-Lesson25.html#documenting-software",
    "title": "Math 300R NTI Lesson 25",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson31.html",
    "href": "NTI/NTI-Lesson31.html",
    "title": "Math 300R NTI Lesson 31",
    "section": "",
    "text": "By which I mean correlations that are not just statistical noise but do not represent a causal path between two variables."
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#objectives",
    "href": "NTI/NTI-Lesson31.html#objectives",
    "title": "Math 300R NTI Lesson 31",
    "section": "Objectives",
    "text": "Objectives\n31.1 Distinguish “common cause” and “collider” forms of DAG.\n31.2 Construct appropriate DAG to match a narrative hypothesis."
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#reading",
    "href": "NTI/NTI-Lesson31.html#reading",
    "title": "Math 300R NTI Lesson 31",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#lesson",
    "href": "NTI/NTI-Lesson31.html#lesson",
    "title": "Math 300R NTI Lesson 31",
    "section": "Lesson",
    "text": "Lesson\nStatisticians are often careful in their use of language about relationships. They will say two variables are “correlated” or “associated” rather than casually using words like “connected” or “related.” There is even a proverb, “Correlation is not causation.”\n\n\n\n\n\nFigure 1: XKCD’s take on correlation and causation.\n\n\n\n\nTraditionally, statistics courses have emphasized disputing any causal interpretation of correlations found in data. The response “Well, maybe,” in the last panel of the cartoon shows a student correctly having assimilated this lesson.\nAnother formulation might be more enlightening for statistics students: “Correlation is the sign in data of causal connections in mechanisms.” If two variables a and b are correlated, then either a causes b (perhaps indirectly), b causes a (also perhaps indirectly), or both a and b are caused by other factors. There’s another possibility as well, most easily seen with a corresponding DAG in hand. It’s important to consider all of these possibilities, rather than jumping to a conclusion of what causes what.\nIn this lesson, we’re going to look at only those correlations that would be supported by out-of-sample testing. Later, we’ll also need to deal with correlations that are an illusion of sampling fluctuation or due to a systematic hunt.\nWe’ll use the term “spurious correlation” to refer to those correlations created by mechanisms other than a causal path between two variables in a DAG.\n\n\n\n\n\n\nPaths in DAGS\n\n\n\nExamples and exercises classifying causal and correlating paths in DAGs.\nSelection on a collider or using it as a covariate.\n\ndag_draw(dag09)\n\n\n\nSample <- sample(dag09, size=500)\nlm(a ~ b, data = Sample)\n\n\nCall:\nlm(formula = a ~ b, data = Sample)\n\nCoefficients:\n(Intercept)            b  \n    0.00996     -0.01017  \n\nlm(a ~ b, data = Sample %>% filter(c==1))\n\n\nCall:\nlm(formula = a ~ b, data = Sample %>% filter(c == 1))\n\nCoefficients:\n(Intercept)            b  \n     0.6576      -0.4080  \n\nlm(a ~ b, data = Sample %>% filter(c==0))\n\n\nCall:\nlm(formula = a ~ b, data = Sample %>% filter(c == 0))\n\nCoefficients:\n(Intercept)            b  \n    -0.5680      -0.3422  \n\nlm(a ~ b, data = Sample) %>% summary()\n\n\nCall:\nlm(formula = a ~ b, data = Sample)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0229 -0.7115  0.0057  0.6907  3.2052 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.00996    0.04637   0.215    0.830\nb           -0.01017    0.04606  -0.221    0.825\n\nResidual standard error: 1.037 on 498 degrees of freedom\nMultiple R-squared:  9.786e-05, Adjusted R-squared:  -0.00191 \nF-statistic: 0.04874 on 1 and 498 DF,  p-value: 0.8254\n\n\n\n\n\n\n\n\n\n\nRegression to mediocrity\n\n\n\nAn unusually large deviation is likely to be followed by one not so unusually large."
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#learning-checks",
    "href": "NTI/NTI-Lesson31.html#learning-checks",
    "title": "Math 300R NTI Lesson 31",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#section",
    "href": "NTI/NTI-Lesson31.html#section",
    "title": "Math 300R NTI Lesson 31",
    "section": "31.1",
    "text": "31.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson31.html#documenting-software",
    "href": "NTI/NTI-Lesson31.html#documenting-software",
    "title": "Math 300R NTI Lesson 31",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html",
    "href": "NTI/NTI-Lesson19.html",
    "title": "Math 300R NTI Lesson 19",
    "section": "",
    "text": "19.1 Distinguish between the two settings for decision-making:\n\nPrediction: predict an outcome for an individual\nRelationship: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\n19.2 Given a research question, identify whether it corresponds to a prediction setting or a relationship setting."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#reading",
    "href": "NTI/NTI-Lesson19.html#reading",
    "title": "Math 300R NTI Lesson 19",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#lesson",
    "href": "NTI/NTI-Lesson19.html#lesson",
    "title": "Math 300R NTI Lesson 19",
    "section": "Lesson",
    "text": "Lesson\nThis lesson marks the beginning of a new phase of the course. Thus far, we’ve worked with techniques* for data wrangling, graphics, and regression modeling. Now we address the question of what a regression model (and other information that we might have) can tell us about the real world.*\n\n\n\n\n\n\nSetup\n\n\n\nlibrary(mosaicData)\n\n\n\n\n\n\n\n\nGuided activity: House prices\n\n\n\nHave students do the calculations for the first model, answering the questions that follow. After this is complete, have students do the calculations for the second model and answer those questions.\nThe mosaicData::SaratogaHouses data frame contains information about the sales price and various attributes of about 1700 houses. (See ?SaratogaHouses for a detailed description.)\nDo a regression of price ~ bedrooms and explain what the regression coefficients mean.\n\nlm(price ~ bedrooms, data=SaratogaHouses) |> coefficients()\n\n(Intercept)    bedrooms \n   59862.96    48217.81 \n\n\n\nWhat are the units of the intercept and of the bedrooms coefficient? *Intercept in dollars, bedrooms in dollars per bedroom.\nInterpret what the coefficients indicate about the price of houses and bedrooms. Each additional bedroom adds about $50000 to the value of a house.\nAccording to the model, predict what would be the sales price (at the time the data was collected, 2006) of a house with two bedrooms? \\(59863 + 2\\times 48218\\)\n\nOf course, bedrooms are not the only important thing about a house. Let’s include livingArea along with bedrooms in the model.\n\nlm(price ~ livingArea + bedrooms, data=SaratogaHouses) |> coefficients()\n\n(Intercept)  livingArea    bedrooms \n  36667.895     125.405  -14196.769 \n\n\n\nWhat are the units of the livingArea coefficient? dollars per square foot\nWhat does this model say about the value of adding a bedroom? It seems to reduce the value of the house by about $15,000.\n\nBased on exactly the same data, the two models seem to give contradictory statements about the value of an additional bedroom.\n\nIs one model right and the other wrong? If so, which one is right? (Explain your reasoning.) Both models are mathematically correct. But they need to be interpreted in different ways. Each is telling us something different about the real world.\nCould both models be right? If so, explain why the bedroom coefficients have opposite signs. We will need to develop some additional tools and concepts before we can take on this question.\n\nLearning how to interpret models in terms of what they say about the world is a major theme of this second half of Math 300.\nAnother, more technical question that we will address has to do with the precision of coefficients like 125.40 dollars per square foot. Might it actually be $200/ft2? How about $500/ft2? And how seriously should we take the sales value that we calculate by setting numbers for bedrooms and livingArea into the model?"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#learning-checks",
    "href": "NTI/NTI-Lesson19.html#learning-checks",
    "title": "Math 300R NTI Lesson 19",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#setup-1",
    "href": "NTI/NTI-Lesson19.html#setup-1",
    "title": "Math 300R NTI Lesson 19",
    "section": "Setup",
    "text": "Setup\nThe math300 package will be needed for lessons 20 through 39.\n\nlibrary(math300)"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#objective-2",
    "href": "NTI/NTI-Lesson19.html#objective-2",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.1 (Objective 2)",
    "text": "19.1 (Objective 2)\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\nSolution\n\nPrediction and (2) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price."
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#objective-2-1",
    "href": "NTI/NTI-Lesson19.html#objective-2-1",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.2 (Objective 2)",
    "text": "19.2 (Objective 2)\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\nSolution\n\nPrediction\nRelationship\nRelationship"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section",
    "href": "NTI/NTI-Lesson19.html#section",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.3",
    "text": "19.3\nFit a model to some data. Write down the function implied by the coefficients.\nEvaluate the function for:\n\na=7, b=9\nand so on.\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#section-1",
    "href": "NTI/NTI-Lesson19.html#section-1",
    "title": "Math 300R NTI Lesson 19",
    "section": "19.4",
    "text": "19.4\nAbout the summarization of models. Pipe the model fit into any of four functions:\n\n%>% coefficients()\n%>% broom::tidy()\n%>% rsquared()\n%>% confint()\n\nREDO confint() so that the columns are named lower, middle, upper\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson19.html#documenting-software",
    "href": "NTI/NTI-Lesson19.html#documenting-software",
    "title": "Math 300R NTI Lesson 19",
    "section": "Documenting software",
    "text": "Documenting software\n\nFile creation date: 2022-10-14\nR version 4.2.1 (2022-06-23)\ntidyverse package version: 1.3.2"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html",
    "href": "NTI/NTI-Lesson28.html",
    "title": "Math 300R NTI Lesson 28",
    "section": "",
    "text": "28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#reading",
    "href": "NTI/NTI-Lesson28.html#reading",
    "title": "Math 300R NTI Lesson 28",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#lesson",
    "href": "NTI/NTI-Lesson28.html#lesson",
    "title": "Math 300R NTI Lesson 28",
    "section": "Lesson",
    "text": "Lesson\nWe’ve talked about explanatory variables and the response variable. Sometimes, we have one or a few explanatory variables that we care about, but recognize that others may be playing a role in the formation of the outcome. The explanatory variables that we don’t care about are called covariates. A covariate is nothing more than an explanatory variable in which we don’t have a direct interest.\nToday’s lesson is about whether using covariates can change the prediction error, either for better (a smaller prediction error) or for worse (a bigger prediction error).\nTo illustrate, consider dag04 in which multiple variables contribute to an outcome:\n\ndag_draw(dag04)\n\n\n\n\nIt might seem evident that, to predict d, using a, b, and c as explantory variables will produce narrower prediction intervals than using just one or two of the variables. We can confirm this intuition—we’ll do it with out-of-sample RMS error.\n\nTraining <- sample(dag04, size=500)\nmod1 <- lm(d ~ b, data = Training)\nmod2 <- lm(d ~ a + b + c, data = Training)\nTesting <- sample(dag04, size=1000)\nmod_eval(mod1, data = Testing) %>%\n  summarize(rms = sqrt(mean((d - model_output)^2)))\n\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.74\n\nmod_eval(mod2, data = Testing) %>%\n  summarize(rms = sqrt(mean((d - model_output)^2)))\n\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.01\n\n\nUsing the covariates reduces prediction error.\n\n\n\n\n\n\nAutomating model comparison\n\n\n\nRather than having to go through the same commands over and over again, let’s write a function that will compare the prediction error of different models. compare_rms_error() is currently defined in the _startup.R file.\nHow about in a situation like dag05:\n\ndag_draw(dag05)\n\n\n\n\n\ncompare_rms_error(dag05, n=500, d ~ c, d ~ b, d ~ a, d ~ a + b + c, in_sample=TRUE)\n\n[1] 0.9937855 1.4066611 1.7067076 0.9918937\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\ndag_draw(dag06)\n\n\n\n\n\nIn dag06, which are the best explanatory variables for predicting d?\nCan d and b help in predicting a?\n\n\n\nDo these principles hold for in-sample prediction error?\n\ncompare_rms_error(dag05, n=500, d ~ c, d ~ b, d ~ a, d ~ a + b + c, in_sample=FALSE)\n\n[1] 0.9932812 1.4725469 1.7431287 0.9963097\n\n\n\nLearning Checks\n\n\n28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#learning-checks",
    "href": "NTI/NTI-Lesson28.html#learning-checks",
    "title": "Math 300R NTI Lesson 28",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#section",
    "href": "NTI/NTI-Lesson28.html#section",
    "title": "Math 300R NTI Lesson 28",
    "section": "28.1",
    "text": "28.1\nConsider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution"
  },
  {
    "objectID": "NTI/NTI-Lesson28.html#documenting-software",
    "href": "NTI/NTI-Lesson28.html#documenting-software",
    "title": "Math 300R NTI Lesson 28",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html",
    "href": "NTI/NTI-Lesson29.html",
    "title": "Math 300R NTI Lesson 29",
    "section": "",
    "text": "29.1 Correctly define “covariate”.\n29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction."
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#reading",
    "href": "NTI/NTI-Lesson29.html#reading",
    "title": "Math 300R NTI Lesson 29",
    "section": "Reading",
    "text": "Reading\nTBD"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lesson",
    "href": "NTI/NTI-Lesson29.html#lesson",
    "title": "Math 300R NTI Lesson 29",
    "section": "Lesson",
    "text": "Lesson\n\n\n\n\n\n\nSummary\n\n\n\nIncluding covariates in a model can help, or can hurt. These are the conclusions we’re working toward:\n\nIn-sample, including covariates in a model always reduces the (in-sample) prediction error. The pattern is stronger the smaller the training data set.\nOut-of-sample, including covariates may or may not reduce prediction error. It depends on whether the covariates are genuinely connected to the response variable.\nIrrelevant covariates make (out-of-sample) prediction worse. This effect is strongest for small training data sets.\n\nRemember, since some of the conclusions depend on what variables are connected to what, we need to demonstrate the phenomena using a system where we know the structure.\nWe’ll come back to this topic, as a basis for ANOVA, when we do hypothesis testing. There we’ll look at the sum of squares, mean square, F and such.\n\n\nToday, we’ll work mostly with in-sample modeling. This reflects the case in the real world, where you have a data set but not usually an easy way to collect more data for testing.1\nLet’s work with dag04,dag05, and dag07 to illustrate some points about covariates.\n\ndag_draw(dag04)\n\n\n\ndag_draw(dag05)\n\n\n\ndag_draw(dag07)\n\n\n\n\nStart with dag04, where variables a, b, and c all contribute to the formation of d.\n\ncompare_rms_error(dag04, d ~ c, d~ b + c, d ~ a + b + c, n=50, in_sample = TRUE)\n\n[1] 1.6451623 1.3087042 0.9822509\n\n\nPrediction error gets smaller, the more covariates are included.\nThe situation can be different. In dag05, a, b, and c all contribute to d, but not separately. a and b communicate with d only via c. If c is in the model, a and b contribute nothing to reducing prediction error.\n\ncompare_rms_error(dag05, d ~ c, d~ b + c, d ~ a + b + c, n=50, in_sample = TRUE)\n\n[1] 1.105785 1.072284 1.071079\n\n\ndag07 is a case where we have covariates, but they aren’t actually connected to d. Will they reduce prediction error? We’ll use a very small sample size, \\(n=4\\), to make the situation obvious.\n\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = TRUE)\n\n[1] 3.735770e-01 3.647958e-01 1.571500e-01 3.295792e-16\n\n\n\n\n\n\n\n\nPattern shown by dag07 model\n\n\n\nConfirm these by running many simulations.\n\nThe prediction error gets smaller the more covariates are included in the model.\nThe last prediction error, with 4 terms in the model (don’t forget 1!) is zero. A perfect model?\n\n\n\n\nA change in accounting\nWe’ve been using RMS prediction error to quantify how well the response variable has een"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#learning-checks",
    "href": "NTI/NTI-Lesson29.html#learning-checks",
    "title": "Math 300R NTI Lesson 29",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.1",
    "href": "NTI/NTI-Lesson29.html#lc-29.1",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.1",
    "text": "LC 29.1\nIn dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_rms_error(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50, in_sample = TRUE)\n\n[1] 0.9247414 0.8672282 0.8056896 0.6919363\n\n\nd seems to block effect of a and b on c.\n\ncompare_rms_error(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5, in_sample = TRUE)\n\n[1] 1.1122452 1.0902534 0.8655307 0.2677090"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#lc-29.2",
    "href": "NTI/NTI-Lesson29.html#lc-29.2",
    "title": "Math 300R NTI Lesson 29",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = TRUE)\n\n[1] 4.689275e-01 4.188891e-01 3.603896e-01 1.416962e-16\n\nset.seed(101)\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = FALSE)\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "NTI/NTI-Lesson29.html#documenting-software",
    "href": "NTI/NTI-Lesson29.html#documenting-software",
    "title": "Math 300R NTI Lesson 29",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#section",
    "href": "NTI/NTI-Lesson38.html#section",
    "title": "Math 300R NTI Lesson 38",
    "section": "38.1",
    "text": "38.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#reading",
    "href": "NTI/NTI-Lesson38.html#reading",
    "title": "Math 300R NTI Lesson 38",
    "section": "Reading",
    "text": "Reading\nOne or more of these articles:\n\nA review of false discovery\nDiet and sex determination\nMost research findings false"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#lesson",
    "href": "NTI/NTI-Lesson38.html#lesson",
    "title": "Math 300R NTI Lesson 38",
    "section": "Lesson",
    "text": "Lesson\nDiscussion of article(s).\n\nWhat should the p-value become\nConsider dag07\n\n\n\n\n\nNode d is not connected to any of the other nodes. There should accordingly be a “null” relationship between d and the others. On the other hand, b and c are connected (although the connection is confounded with a).\nLet’s model d by b and look at the p-value:\n\nSample <- sample(dag07, size=50)\nlm(d ~ b, data=Sample) %>% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   0.0556    0.132      0.422   0.675\n2 b            -0.121     0.0985    -1.23    0.225\n\n\nThe p-value on the b coefficient is large, greater than the usual threshold of 0.05.\nOn the other hand, b and c are connected and the p-value (with this much data) is tiny.\n\nlm(c ~ b, data = Sample) %>% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -0.106     0.151    -0.705 4.84e- 1\n2 b             -1.55      0.113   -13.8   2.52e-18\n\n\nImagine a setting where a popular (but unproven!) hypothesis has emerged: that b and d are really related. 100 different research teams rush in to be the first to demonstrate, each generating their own experimental data. We’ll simulate this and collect the summary of the b coefficient w.r.t. d. [First show the statement without the do() to show what each row looks like. Then run the 100 trials and look for small p-values]\n\nAll_groups <- do(100) * {\n  lm(d ~ b, data=sample(dag07, size=50)) %>% \n  broom::tidy() %>%\n  filter(term == 'b')\n  }\n\nDid any of the groups get a “significant” result?\n\nAll_groups %>% \n  filter(p.value < 0.05)\n\n# A tibble: 4 × 7\n  term  estimate std.error statistic p.value  .row .index\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl> <int>  <dbl>\n1 b        0.298    0.105       2.84 0.00651     1     42\n2 b       -0.199    0.0889     -2.24 0.0297      1     76\n3 b        0.232    0.0934      2.48 0.0166      1     78\n4 b       -0.211    0.0989     -2.14 0.0375      1     99\n\n\nIn the context of 100 trials being done, it’s understandable that some of the groups happened to get a p-value < 0.05. But suppose that only the groups with small p-values publish their results? Then it looks as if they found a “significant” result.\nHow can we guard against this accidental generation of significant results? The standard answer in scientific work is to replicate the result: the labs should try again to confirm the result they got in the first study. (In practice, there are strong social/financial/career pressures against conducting such replications. These need to be overcome to guard against false discovery.)\nHere’s a simulation where each lab group runs the study twice. Do any get small p-values both times?\n\nReplicated_groups <- do(100) * {\n  do(2) * {\n    lm(d ~ b, data=sample(dag07, size=50)) %>% \n      broom::tidy() %>%\n      filter(term == 'b')\n    } %>% .$p.value\n} \nPairs <- Replicated_groups %>% \n  tidyr::pivot_wider(names_from = .row, values_from = result)\nPairs %>% filter(`1` < 0.05, `2` < 0.05)\n\n# A tibble: 0 × 3\n# … with 3 variables: .index <dbl>, 1 <dbl>, 2 <dbl>\n\n\nA better approach. As a rule of thumb, once you have a sample size \\(n\\) that gives a genuine p \\(\\approx 0.05\\), doubling \\(n\\) should reduce p by a factor of about 10. But if p is merely accidentally small, doubling the sample size won’t have any effect.\nA demonstration when there is a genuine relationship:\n\nlm(c ~ a, data = sample(dag07, size=5)) %>% broom::tidy() %>%\n  filter(term == 'a')\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 a         1.63     0.968      1.68   0.191\n\nlm(c ~ a, data = sample(dag07, size=10)) %>% broom::tidy() %>%\n  filter(term == 'a')\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 a        0.957     0.585      1.64   0.141\n\n\nLet’s re-run the simulation with \\(n\\) doubled, that is, size=100 compared to the previous size=50\n\nBigger_n <- do(100) * {\n  lm(d ~ b, data=sample(dag07, size=100)) %>% \n    broom::tidy() %>%\n    filter(term == 'b')\n  }\n\nAre these p-values smaller than in the trials with size=50?"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#learning-checks",
    "href": "NTI/NTI-Lesson38.html#learning-checks",
    "title": "Math 300R NTI Lesson 38",
    "section": "Learning Checks",
    "text": "Learning Checks"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#section-1",
    "href": "NTI/NTI-Lesson38.html#section-1",
    "title": "Math 300R NTI Lesson 38",
    "section": "38.1",
    "text": "38.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "NTI/NTI-Lesson38.html#documenting-software",
    "href": "NTI/NTI-Lesson38.html#documenting-software",
    "title": "Math 300R NTI Lesson 38",
    "section": "Documenting software",
    "text": "Documenting software"
  },
  {
    "objectID": "LC/LC-lesson37.html",
    "href": "LC/LC-lesson37.html",
    "title": "Learning Checks Lesson 37",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson23.html",
    "href": "LC/LC-lesson23.html",
    "title": "Learning Checks Lesson 23",
    "section": "",
    "text": "Vocabulary: Sampling distribution, standard error, sampling variability, sample size\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson22.html",
    "href": "LC/LC-lesson22.html",
    "title": "Learning Checks Lesson 22",
    "section": "",
    "text": "Consider these three data frames:\n\nOne <- sample(dag01, size=25)\nTwo <- do(10) * {\n  lm(y ~ x, data = sample(dag01, size=25)) %>%\n    coefficients()\n  }\nThree <- Two %>% \n  summarize(mx = mean(x), sx = sd(x))\n\n\nBoth One and Two have columns called x, but they stand for different things. Explain what the unit of observation is and what the values in x represent..\nThree does not have a column named x, but it is a summary of the x column from Two. What kind of summary.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nIn One, the x column contains the simulated of the x variable from dag01. The unit of observation is a single case, for instance a person for whom observations were made of x and y. The simulation involves generating 25 rows of data: one row for each of 25 people.\nIn Two, the x column is the regression coefficient on x from the simulation. Each row of Two corresponds to one trial in which regression is being performed on a sample of size 25 of simulated data from dag01.\nThree is a summary of the 10 trials in Two. The columns, named mx and sx, tell about the distribution of x across all the trials."
  },
  {
    "objectID": "LC/LC-lesson36.html",
    "href": "LC/LC-lesson36.html",
    "title": "Learning Checks Lesson 36",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson20.html",
    "href": "LC/LC-lesson20.html",
    "title": "Learning Checks Lesson 20",
    "section": "",
    "text": "?@sec-size-of-variable describes two very closely related summary quantities used to measures of the “size” of a variable: i. the variance and ii. the “standard deviation” (which is the square root of the variance.\n\nUsing software, what is the variance of the XXX variable in the YYY data frame? Make sure to include the units.\nWhat is the “standard deviation” of the XXX variable? Calculate this in two different ways: i. “by-hand” taking of the square root of the variance; ii. using the sd() software directly.\n\n[Repeat for a number of variables from different data frames.]"
  },
  {
    "objectID": "LC/LC-lesson20.html#section-1",
    "href": "LC/LC-lesson20.html#section-1",
    "title": "Learning Checks Lesson 20",
    "section": "20.2",
    "text": "20.2\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson34.html",
    "href": "LC/LC-lesson34.html",
    "title": "Learning Checks Lesson 34",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson35.html",
    "href": "LC/LC-lesson35.html",
    "title": "Learning Checks Lesson 35",
    "section": "",
    "text": "Given some classifier summaries, calculate the false-positive and false-negative rates as well as the sensitivity and specificity\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson21.html",
    "href": "LC/LC-lesson21.html",
    "title": "Learning Checks Lesson 21",
    "section": "",
    "text": "The following command will generate a data frame with 1000 rows from dag00 and calculate the variance of the x and y variables:\n\nsample(dag00, size=1000) %>%\n  summarize(vx = var(x), vy = var(y))\n\n# A tibble: 1 × 2\n     vx    vy\n  <dbl> <dbl>\n1  4.06  1.00\n\n\nCompare this result to the DAG tilde expressions\n\ndag00\n\n[[1]]\nx ~ eps(2) + 5\n\n[[2]]\ny ~ eps(1) - 7\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nIn the tilde expressions, eps(2) means to generate noise of magnitude 2.0.\n\nIs the argument to eps() specified in terms of the variance or the standard deviation?\nThe tilde expression for x specifies that the constant 5 is to be added to eps(2). Similarly, the constant -7 is added to y. How do these constants relate to the calculated magnitudes of x and y?\n\n\n\n\nThe standard deviation. For instance, x has noise of magnitude 2. The variance of x is 4, the square of 2.\nThe standard deviation (and therefore the variance) ignore such added constants."
  },
  {
    "objectID": "LC/LC-lesson21.html#section-1",
    "href": "LC/LC-lesson21.html#section-1",
    "title": "Learning Checks Lesson 21",
    "section": "21.2",
    "text": "21.2\n?@sec-signal-and-noise introduces the idea that variables consist of components. A simple breakdown is into two components: i. the part of the variable that is determined by other variables in the system (“signal”) and ii. the random part of the variable (“noise”). The section uses dag01 as an illustration of how a variable can be partly determined and partly random noise.\n\nWrite and execute a command that will generate 500 rows of simulated data from dag01 and will calculate the standard deviation of x and of y.\nWhat’s the magnitude of x in the simulated data? What’s the magnitude of y?\nDoes this change if you use data with 1000 or 20000 rows?\n\n\nSolution\n\nsample(dag01, size=500) %>% summarize(sx = sd(x), sy=sd(y))\nThe standard deviation of x is about 1, the standard deviation of y is about 1.8.\nNo, the values are roughly the same regardless of the size of the sample."
  },
  {
    "objectID": "LC/LC-lesson21.html#section-2",
    "href": "LC/LC-lesson21.html#section-2",
    "title": "Learning Checks Lesson 21",
    "section": "21.3",
    "text": "21.3\n[DRAW several DAG-like graphs, one of which should be undirected in all edges, one should be undirected on one or two edges (but not all), and one should be cyclic and another acyclic.]\nReferring to the graphs in the figure, say which ones are DAGs. If a graph is not a DAG, say whether that’s because it’s not directed or because it’s not cyclic.\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson21.html#section-3",
    "href": "LC/LC-lesson21.html#section-3",
    "title": "Learning Checks Lesson 21",
    "section": "21.4",
    "text": "21.4\nGenerate simulated data from dag01 with 1000 rows. Fit the regression model y ~ x to the data and examine the coefficients.\n\nHow do the coefficients relate to the tilde expressions that define dag01?\nInstead of using the regression model y ~ x, where y is the response variable, try the regression model x ~ y. Do the coefficients from x ~ y correspond in any simple way to the tilde expressions that define dag01?\n\n\nSolution\n\nsample(dag01, size=1000) %>%\n  lm(y ~ x, data = .)\n\n\nCall:\nlm(formula = y ~ x, data = .)\n\nCoefficients:\n(Intercept)            x  \n      4.009        1.503  \n\n\nThe intercept corresponds to the additive constant (4) in the y tilde expression. The x coefficient corresponds to the multiplier on x in the tilde expression.\nThe formula for x isn’t reflected by the coefficients.\nUsing x as the response variable:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .)\n\n\nCall:\nlm(formula = x ~ y, data = .)\n\nCoefficients:\n(Intercept)            y  \n    -1.8431       0.4615  \n\n\nThese coefficients do not appear in the dag01 tilde expressions."
  },
  {
    "objectID": "LC/LC-lesson25.html",
    "href": "LC/LC-lesson25.html",
    "title": "Learning Checks Lesson 25",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson31.html",
    "href": "LC/LC-lesson31.html",
    "title": "Learning Checks Lesson 31",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson19.html",
    "href": "LC/LC-lesson19.html",
    "title": "Learning Checks Lesson 19",
    "section": "",
    "text": "The math300 package will be needed for lessons 20 through 39.\n\nlibrary(math300)\n\nLoading required package: mosaic\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum"
  },
  {
    "objectID": "LC/LC-lesson19.html#objective-2",
    "href": "LC/LC-lesson19.html#objective-2",
    "title": "Learning Checks Lesson 19",
    "section": "19.1 (Objective 2)",
    "text": "19.1 (Objective 2)\nWhat are the two settings for decision making that we cover in this course?\nGive an example of each.\n\nSolution\n\nPrediction and (2) Relationship\n\n\nWhat will be the sales price of this house? “This house” is a shorthand way of saying “a house with these attributes.” The sales price will be the output of a prediction function that takes the various attributes as input and produces a sales price as output.\nIf I look for a house with an additional bathroom, how much will that change the sales price? This asks for the relationship between number of bathrooms and sales price."
  },
  {
    "objectID": "LC/LC-lesson19.html#objective-2-1",
    "href": "LC/LC-lesson19.html#objective-2-1",
    "title": "Learning Checks Lesson 19",
    "section": "19.2 (Objective 2)",
    "text": "19.2 (Objective 2)\nFor each of these research questions, say whether it is a prediction setting or a relationship setting.\n\nWhat’s the risk of falling ill?\nHow will the risk of falling ill change if we eat more broccholi?\nIs there any reason to believe, based on the evidence at hand, that we should look more deeply into the possible benefits of broccholi?\n\n\nSolution\n\nPrediction\nRelationship\nRelationship"
  },
  {
    "objectID": "LC/LC-lesson19.html#section",
    "href": "LC/LC-lesson19.html#section",
    "title": "Learning Checks Lesson 19",
    "section": "19.3",
    "text": "19.3\nFit a model to some data. Write down the function implied by the coefficients.\nEvaluate the function for:\n\na=7, b=9\nand so on.\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson19.html#section-1",
    "href": "LC/LC-lesson19.html#section-1",
    "title": "Learning Checks Lesson 19",
    "section": "19.4",
    "text": "19.4\nAbout the summarization of models. Pipe the model fit into any of four functions:\n\n%>% coefficients()\n%>% broom::tidy()\n%>% rsquared()\n%>% confint()\n\nREDO confint() so that the columns are named lower, middle, upper\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson30.html",
    "href": "LC/LC-lesson30.html",
    "title": "Learning Checks Lesson 30",
    "section": "",
    "text": "Dags with longer confounding pathways. Is there mixing when leaving out an element in the pathway. Mix up the directions of the arrows and show that the mixing occurs when the covariate is included in the model.\nRegression to the mean example.\nCollider?\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson24.html",
    "href": "LC/LC-lesson24.html",
    "title": "Learning Checks Lesson 24",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson32.html",
    "href": "LC/LC-lesson32.html",
    "title": "Learning Checks Lesson 32",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "LC/LC-lesson26.html",
    "href": "LC/LC-lesson26.html",
    "title": "Learning Checks Lesson 26",
    "section": "",
    "text": "Ideas"
  },
  {
    "objectID": "LC/LC-lesson26.html#section",
    "href": "LC/LC-lesson26.html#section",
    "title": "Learning Checks Lesson 26",
    "section": "26.1",
    "text": "26.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson27.html",
    "href": "LC/LC-lesson27.html",
    "title": "Learning Checks Lesson 27",
    "section": "",
    "text": "This is a QR day."
  },
  {
    "objectID": "LC/LC-lesson27.html#section",
    "href": "LC/LC-lesson27.html#section",
    "title": "Learning Checks Lesson 27",
    "section": "27.1",
    "text": "27.1\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/LC-lesson33.html",
    "href": "LC/LC-lesson33.html",
    "title": "Learning Checks Lesson 33",
    "section": "",
    "text": "Convert probability to odds and log odds, and vice versa.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "LC/Learning-checks.html",
    "href": "LC/Learning-checks.html",
    "title": "Learning Checks from Modern Dive",
    "section": "",
    "text": "LC 1.1 Block 1 Day 1\n\n\n\nRepeat the earlier installation steps, but for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package for data wrangling, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for generating easy-to-read tables in R. We’ll use these packages in the next section.\n\n\n\n\n\n\n\n\nLC 1.2 Block 1 Day 1\n\n\n\n“Load” the dplyr, nycflights13, and knitr packages as well by repeating the earlier steps.\n\n\nRun View(flights) in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note the uppercase V in View(). R is case-sensitive, so you’ll get an error message if you run view(flights) instead of View(flights)\n\n\n\n\n\n\nLC 1.3 Block 1 Day 1\n\n\n\nWhat does any ONE row in this flights dataset refer to?\n\nA. Data on an airline\nB. Data on a flight\nC. Data on an airport\nD. Data on multiple flights\n\n\n\n\n\n\n\n\n\nLC 1.4 Block 1 Day 1\n\n\n\nWhat are some other examples in this dataset (flights) of categorical variables? What makes them different than quantitative variables?\n\n\n\n\n\n\n\n\nLC 1.5 Block 1 Day 1\n\n\n\nWhat properties of each airport do the variables lat, lon, alt, tz, dst, and tzone describe in the airports data frame? Take your best guess.\n\n\n\n\n\n\n\n\nLC 1.6 Block 1 Day 1\n\n\n\nProvide the names of variables in a data frame with at least three variables where one of them is an identification variable and the other two are not. Further, create your own tidy data frame that matches these conditions.\n\n\n\n\n\n\n\n\nLC 1.7 Block 1 Day 1\n\n\n\nLook at the help file for the airports data frame. Revise your earlier guesses about what the variables lat, lon, alt, tz, dst, and tzone each describe."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-2-visualization",
    "href": "LC/Learning-checks.html#chapter-2-visualization",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 2: Visualization",
    "text": "Chapter 2: Visualization\n\n\n\n\n\n\nLC 2.1 Block 1 Day 2\n\n\n\nTake a look at both the flights and alaska_flights data frames by running View(flights) and View(alaska_flights). In what respect do these data frames differ? For example, think about the number of rows in each dataset.\n\n\n\n\n\n\n\n\nLC 2.2-2.6 Block 1 Day 2\n\n\n\nWhat are some practical reasons why dep_delay and arr_delay have a positive relationship?\nWhat variables in the weather data frame would you expect to have a negative correlation (i.e., a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here. Hint: Explore the weather dataset by using the View() function.\nWhy do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaska Air flights?\nWhat are some other features of the plot that stand out to you?\nCreate a new scatterplot using different variables in the alaska_flights data frame by modifying the example given.\n\n\n\n\n\n\n\n\nLC 2.7-2.8 Block 1 Day 2\n\n\n\nWhy is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot?\nAfter viewing Figure @ref(fig:alpha), give an approximate range of arrival delays and departure delays that occur most frequently. How has that region changed compared to when you observed the same plot without alpha = 0.2 set in Figure @ref(fig:noalpha)?\n\n\n\n\n\n\n\n\nLC 2.9-2.10 Block 1 Day 3\n\n\n\nLC 2.9 Take a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather). In what respect do these data frames differ?\nLC 2.10 View() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement, whereas the hour variable does not?\n\n\n\n\n\n\n\n\nLC 2.11-2.13 Block 1 Day 3\n\n\n\nLC 2.11 Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis?\nLC 2.12 Why are linegraphs frequently used when time is the explanatory variable on the x-axis?\nLC 2.12 Plot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013.\n\n\n\n\n\n\n\n\nLC 2.18-2.21 Block 1 Day 3\n\n\n\nWhat other things do you notice about this faceted plot? How does a faceted plot help us see relationships between two variables?\nWhat do the numbers 1-12 correspond to in the plot? What about 25, 50, 75, 100?\nFor which types of datasets would faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics.\nLC 2.21 Does the temp variable in the weather dataset have a lot of variability? Why do you say that?\n\n\n\n\n\n\n\n\nLC 2.22-2.25 Boxplots Block 1 Day 4\n\n\n\nLC 2.22 What does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point.\nLC 2.23 Which months have the highest variability in temperature? What reasons can you give for this?\nLC 2.24 We looked at the distribution of the numerical variable temp split by the numerical variable month that we converted using the factor() function in order to make a side-by-side boxplot. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative?\nLC 2.25 Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram?\n\n\n\n\n\n\n\n\nLC 2.26-2.29 Histograms Block 1 Day 4\n\n\n\nLC 2.26 Why are histograms inappropriate for categorical variables?\nLC 2.27 What is the difference between histograms and barplots?\nLC 2.28 How many Envoy Air flights departed NYC in 2013?\nLC 2.29 What was the 7th highest airline for departed flights from NYC in 2013? How could we better present the table to get this answer quickly?\n\n\n\n\n\n\n\n\nLC 2.30-2.31 Pie charts Block 1 Day 4\n\n\n\nLC 2.30 Why should pie charts be avoided and replaced by barplots?\nLC 2.31 Why do you think people continue to use pie charts?\n\n\n\n\n\n\n\n\nLC 2.32-2.37 Block 1 Day 4\n\n\n\nLC 2.32 What kinds of questions are not easily answered by looking at Figure @ref(fig:flights-stacked-bar) (2.23)?\nLC 2.33 What can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights?\nLC 2.34 Why might the side-by-side barplot be preferable to a stacked barplot in this case?\nLC 2.35 What are the disadvantages of using a dodged barplot, in general?\nLC 2.36 Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case?\nLC 2.37 What information about the different carriers at different airports is more easily seen in the faceted barplot?"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-3-wrangling",
    "href": "LC/Learning-checks.html#chapter-3-wrangling",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 3: Wrangling",
    "text": "Chapter 3: Wrangling\n\n\n\n\n\n\nLC 3.1 Block 1 Day 5\n\n\n\nWhat’s another way of using the “not” operator ! to filter only the rows that are not going to Burlington, VT nor Seattle, WA in the flights data frame? Test this out using the previous code.\n\n\n\n\n\n\n\n\nLC 3.2 Block 1 Day 5\n\n\n\nSay a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five-year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach?\n\n\n\n\n\n\n\n\nLC 3.3 Block 1 Day 5\n\n\n\nModify the earlier summarize() function code that creates the summary_temp data frame to also use the n() summary function: summarize(... , count = n()). What does the returned value correspond to?\n\n\n\n\n\n\n\n\nLC 3.4 Block 1 Day 5\n\n\n\nWhy doesn’t the following code work? Run the code line-by-line instead of all at once, and then look at the data. In other words, run summary_temp <- weather %>% summarize(mean = mean(temp, na.rm = TRUE)) first.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nsummary_temp <- weather %>%   \n  summarize(mean = mean(temp, na.rm = TRUE)) %>% \n  summarize(std_dev = sd(temp, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\nLC 3.5 Block 1 Day 6\n\n\n\nRecall from Chapter @ref(viz) when we looked at temperatures by months in NYC. What does the standard deviation column in the summary_monthly_temp data frame tell us about temperatures in NYC throughout the year?\n\n\n\n\n\n\n\n\nLC 3.6 Block 1 Day 6\n\n\n\nWhat code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC?\n\n\n\n\n\n\n\n\nLC 3.7 Block 1 Day 6\n\n\n\nRecreate by_monthly_origin, but instead of grouping via group_by(origin, month), group variables in a different order group_by(month, origin). What differs in the resulting dataset?\n\n\n\n\n\n\n\n\nLC 3.8 Block 1 Day 6\n\n\n\nHow could we identify how many flights left each of the three airports for each carrier?\n\n\n\n\n\n\n\n\nLC 3.9 Block 1 Day 6\n\n\n\nHow does the filter() operation differ from a group_by() followed by a summarize()?\n\n\n\n\n\n\n\n\nLC 3.10 Block 1 Day 6\n\n\n\nWhat do positive values of the gain variable in flights correspond to? What about negative values? And what about a zero value?\n\n\n\n\n\n\n\n\nLC 3.11 Block 1 Day 6\n\n\n\nCould we create the dep_delay and arr_delay columns by simply subtracting dep_time from sched_dep_time and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in flights.\n\n\n\n\n\n\n\n\nLC 3.12 Block 1 Day 76\n\n\n\nWhat can we say about the distribution of gain? Describe it in a few sentences using the plot and the gain_summary data frame values.\n\n\n\n\n\n\n\n\nLC 3.13 Block 1 Day 7\n\n\n\nLooking at Figure @ref(fig:reldiagram), when joining flights and weather (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of year, month, day, hour, and origin, and not just hour?\n\n\n\n\n\n\n\n\nLC 3.14 Block 1 Day 7\n\n\n\nWhat surprises you about the top 10 destinations from NYC in 2013?\n\n\n\n\n\n\n\n\nLC 3.15 Block 1 Day 7\n\n\n\nWhat are some advantages of data in normal forms? What are some disadvantages?\n\n\n\n\n\n\n\n\nLC 3.16 Block 1 Day 7\n\n\n\nWhat are some ways to select all three of the dest, air_time, and distance variables from flights? Give the code showing how to do this in at least three different ways.\n\n\n\n\n\n\n\n\nLC 3.17 Block 1 Day 7\n\n\n\nHow could one use starts_with(), ends_with(), and contains() to select columns from the flights data frame? Provide three different examples in total: one for starts_with(), one for ends_with(), and one for contains().\n\n\n\n\n\n\n\n\nLC 3.18 Block 1 Day 7\n\n\n\nWhy might we want to use the select function on a data frame?\n\n\n\n\n\n\n\n\nLC 3.19 Block 1 Day 7\n\n\n\nCreate a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013.\n\n\n::: {.callout-note icon=false} ## LC 3.20 Block 1 Day 7 Let’s now put your newly acquired data wrangling skills to the test!\nAn airline industry measure of a passenger airline’s capacity is the available seat miles, which is equal to the number of seats available multiplied by the number of miles or kilometers flown summed over all flights.\nFor example, let’s consider the scenario in Figure 1. Since the airplane has 4 seats and it travels 200 miles, the available seat miles are \\(4 \\times 200 = 800\\).\n\n\n\n\n\nFigure 1: Example of available seat miles for one flight.\n\n\n\n\nExtending this idea, let’s say an airline had 2 flights using a plane with 10 seats that flew 500 miles and 3 flights using a plane with 20 seats that flew 1000 miles, the available seat miles would be \\(2 \\times 10 \\times 500 + 3 \\times 20 \\times 1000 = 70,000\\) seat miles.\nUsing the datasets included in the nycflights13 package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints:\n\nCrucial: Unless you are very confident in what you are doing, it is worthwhile not starting to code right away. Rather, first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level pseudocode that is informal yet detailed enough to articulate what you are doing. This way you won’t confuse what you are trying to do (the algorithm) with how you are going to do it (writing dplyr code).\nTake a close look at all the datasets using the View() function: flights, weather, planes, airports, and airlines to identify which variables are necessary to compute available seat miles.\nFigure @ref(fig:reldiagram) showing how the various datasets can be joined will also be useful.\nConsider the data wrangling verbs in Table @ref(tab:wrangle-summary-table) as your toolbox! ::"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-4-tidy",
    "href": "LC/Learning-checks.html#chapter-4-tidy",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 4: Tidy",
    "text": "Chapter 4: Tidy\n::: {.callout-note icon=false} ## LC 4.1 Block 1 Day 8 What are common characteristics of “tidy” data frames? ::\n::: {.callout-note icon=false} ## LC 4.2 Block 1 Day 8 What makes “tidy” data frames useful for organizing data? ::\n::: {.callout-note icon=false} ## LC 4.3 Block 1 Day 8 Take a look at the airline_safety data frame included in the fivethirtyeight data package. Run the following:\n\nairline_safety\n\nAfter reading the help file by running ?airline_safety, we see that airline_safety is a data frame containing information on different airline companies’ safety records. This data was originally reported on the data journalism website, FiveThirtyEight.com, in Nate Silver’s article, “Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?”. Let’s only consider the variables airlines and those relating to fatalities for simplicity:\n\nairline_safety_smaller <- airline_safety %>% \n  select(airline, starts_with(\"fatalities\"))\nairline_safety_smaller\n\n# A tibble: 56 × 3\n   airline               fatalities_85_99 fatalities_00_14\n   <chr>                            <int>            <int>\n 1 Aer Lingus                           0                0\n 2 Aeroflot                           128               88\n 3 Aerolineas Argentinas                0                0\n 4 Aeromexico                          64                0\n 5 Air Canada                           0                0\n 6 Air France                          79              337\n 7 Air India                          329              158\n 8 Air New Zealand                      0                7\n 9 Alaska Airlines                      0               88\n10 Alitalia                            50                0\n# … with 46 more rows\n\n\nThis data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, in particular so that it has a variable fatalities_years indicating the incident year and a variable count of the fatality counts? ::\n::: {.callout-note icon=false} ## LC 4.4 Block 1 Day 9 Convert the dem_score data frame into a “tidy” data frame and assign the name of dem_score_tidy to the resulting long-formatted data frame. ::\n::: {.callout-note icon=false} ## LC 4.5 Block 1 Day 9 Read in the life expectancy data stored at https://moderndive.com/data/le_mess.csv and convert it to a “tidy” data frame. ::"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-5-regression",
    "href": "LC/Learning-checks.html#chapter-5-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 5: Regression",
    "text": "Chapter 5: Regression\n\n\n\n\n\n\nLC 5.1 Block 2 Day 1\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) being score but with age as the new explanatory variable \\(x\\). Remember, this involves three things:\n\nLooking at the raw data values.\nComputing summary statistics.\nCreating data visualizations.\n\nWhat can you say about the relationship between age and teaching scores based on this exploration?\n\n\n\n\n\n\n\n\nLC 5.2 Block 2 Day 1\n\n\n\nFit a new simple linear regression using lm(score ~ age, data = evals_ch5) where age is the new explanatory variable \\(x\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your earlier exploratory data analysis?\n\n\n\n\n\n\n\n\nLC 5.3 Block 2 Day 1\n\n\n\nGenerate a data frame of the residuals of the model where you used age as the explanatory \\(x\\) variable.\n\n\n\n\n\n\n\n\nLC 5.4 Block 2 Day 2\n\n\n\nConduct a new exploratory data analysis with the same explanatory variable \\(x\\) being continent but with gdpPercap as the new outcome variable \\(y\\). What can you say about the differences in GDP per capita between continents based on this exploration?\n\n\n\n\n\n\n\n\nLC 5.5 Block 2 Day 2\n\n\n\nFit a new linear regression using lm(gdpPercap ~ continent, data = gapminder2007) where gdpPercap is the new outcome variable \\(y\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis?\n\n\n\n\n\n\n\n\nLC 5.6 Block 2 Day 2\n\n\n\nUsing either the sorting functionality of RStudio’s spreadsheet viewer or using the data wrangling tools you learned in Chapter @ref(wrangling), identify the five countries with the five smallest (most negative) residuals? What do these negative residuals say about their life expectancy relative to their continents’ life expectancy?\n\n\n\n\n\n\n\n\nLC 5.7 Block 2 Day 2\n\n\n\nRepeat this process, but identify the five countries with the five largest (most positive) residuals. What do these positive residuals say about their life expectancy relative to their continents’ life expectancy?\n\n\n\n\n\n\n\n\nLC 5.8 Block 2 Day 3\n\n\n\nNote in Figure @fig:three-lines there are 3 points marked with dots and:\n\nThe “best” fitting solid regression line in blue\nAn arbitrarily chosen dotted red line\nAnother arbitrarily chosen dashed green line\n\n\n\n\n\n\nFigure 2: Regression line and two others.\n\n\n\n\nCompute the sum of squared residuals by hand for each line and show that of these three lines, the regression line in blue has the smallest value."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-6-multiple-regression",
    "href": "LC/Learning-checks.html#chapter-6-multiple-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 6: Multiple regression",
    "text": "Chapter 6: Multiple regression\n\n\n\n\n\n\nLC 6.1 Block 2 Day 4\n\n\n\nCompute the observed values, fitted values, and residuals not for the interaction model as we just did, but rather for the parallel slopes model we saved in score_model_parallel_slopes.\n\n\n\n\n\n\n\n\nLC 6.2\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age?\n\n\n\n\n\n\n\n\nLC 6.3\n\n\n\nConduct a new exploratory data analysis with the same outcome variable \\(y\\) debt but with credit_rating and age as the new explanatory variables \\(x_1\\) and \\(x_2\\). What can you say about the relationship between a credit card holder’s debt and their credit rating and age?\n\n\n\n\n\n\n\n\nLC 6.4\n\n\n\nFit a new simple linear regression using lm(debt ~ credit_rating + age, data = credit_ch6) where credit_rating and age are the new numerical explanatory variables \\(x_1\\) and \\(x_2\\). Get information about the “best-fitting” regression plane from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis?"
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-7-sampling",
    "href": "LC/Learning-checks.html#chapter-7-sampling",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 7: Sampling",
    "text": "Chapter 7: Sampling\n\n\n\n\n\n\nLC 7.1 Block 3 Day 1\n\n\n\nWhy was it important to mix the bowl before we sampled the balls?\n\n\n\n\n\n\n\n\nLC 7.2 Block 3 Day 1\n\n\n\nWhy is it that our 33 groups of friends did not all have the same numbers of balls that were red out of 50, and hence different proportions red?\n\n\n\n\n\n\n\n\nLC 7.3 Block 3 Day 1\n\n\n\nWhy couldn’t we study the effects of sampling variation when we used the virtual shovel only once? Why did we need to take more than one virtual sample (in our case 33 virtual samples)?\n\n\n\n\n\n\n\n\nLC 7.4 Block 3 Day 1\n\n\n\nWhy did we not take 1000 “tactile” samples of 50 balls by hand?\n\n\n\n\n\n\n\n\nLC 7.5 Block 3 Day 1\n\n\n\nLooking at Figure @ref(fig:samplingdistribution-virtual-1000), would you say that sampling 50 balls where 30% of them were red is likely or not? What about sampling 50 balls where 10% of them were red?\n\n\n\n\n\n\n\n\nLC 7.6 Block 3 Day 1\n\n\n\nIn Figure 7.9, we used shovels to take 1000 samples each, computed the resulting 1000 proportions of the shovel’s balls that were red, and then visualized the distribution of these 1000 proportions in a histogram. We did this for shovels with 25, 50, and 100 slots in them. As the size of the shovels increased, the histograms got narrower. In other words, as the size of the shovels increased from 25 to 50 to 100, did the 1000 proportions\n\nA. vary less,\nB. vary by the same amount, or\nC. vary more?\n\n\n\n\n\n\n\n\n\nLC 7.7 Block 3 Day 1\n\n\n\nWhat summary statistic did we use to quantify how much the 1000 proportions red varied?\n\nA. The interquartile range\nB. The standard deviation\nC. The range: the largest value minus the smallest.\n\n\n\n\n\n\n\n\n\nLC 7.8 Block 3 Day 2\n\n\n\nIn the case of our bowl activity, what is the population parameter? Do we know its value?\n\n\n\n\n\n\n\n\nLC 7.9 Block 3 Day 2\n\n\n\nWhat would performing a census in our bowl activity correspond to? Why did we not perform a census?\n\n\n\n\n\n\n\n\nLC 7.10 Block 3 Day 2\n\n\n\nWhat purpose do point estimates serve in general? What is the name of the point estimate specific to our bowl activity? What is its mathematical notation?\n\n\n\n\n\n\n\n\nLC 7.11 Block 3 Day 2\n\n\n\nHow did we ensure that our tactile samples using the shovel were random?\n\n\n\n\n\n\n\n\nLC 7.12 Block 3 Day 2\n\n\n\nWhy is it important that sampling be done at random?\n\n\n\n\n\n\n\n\nLC 7.13 Block 3 Day 2\n\n\n\nWhat are we inferring about the bowl based on the samples using the shovel?\n\n\n\n\n\n\n\n\nLC 7.14 Block 3 Day 2\n\n\n\nWhat purpose did the sampling distributions serve?\n\n\n\n\n\n\n\n\nLC 7.15 Block 3 Day 2\n\n\n\nWhat does the standard error of the sample proportion \\(\\widehat{p}\\) quantify?\n\n\n\n\n\n\n\n\nLC 7.16 Block 3 Day 2\n\n\n\nThe table that follows is a version of Table @ref(tab:comparing-n-2) matching sample sizes \\(n\\) to different standard errors of the sample proportion \\(\\widehat{p}\\), but with the rows randomly re-ordered and the sample sizes removed. Fill in the table by matching the correct sample sizes to the correct standard errors.\nStandard errors of \\(\\hat{p}\\) based on n = 25, 50, 100\n\n\n\nSample size\nStandard error of \\(\\hat{p}\\)\n\n\n\n\n\\(n=\\)\n0.94\n\n\n\\(n=\\)\n0.45\n\n\n\\(n=\\)\n0.69\n\n\n\nFor the following four Learning checks, let the estimate be the sample proportion \\(\\widehat{p}\\): the proportion of a shovel’s balls that were red. It estimates the population proportion \\(p\\): the proportion of the bowl’s balls that were red.\n\n\n\n\n\n\n\n\nLC 7.17 Block 3 Day 2\n\n\n\nWhat is the difference between an accurate and a precise estimate?\n\n\n\n\n\n\n\n\nLC 7.18 Block 3 Day 2\n\n\n\nHow do we ensure that an estimate is accurate? How do we ensure that an estimate is precise?\n\n\n\n\n\n\n\n\nLC 7.19 Block 3 Day 2\n\n\n\nIn a real-life situation, we would not take 1000 different samples to infer about a population, but rather only one. Then, what was the purpose of our exercises where we took 1000 different samples?\n\n\n\n\n\n\n\n\nLC 7.20 Block 3 Day 2\n\n\n\nFigure @ref(fig:accuracy-vs-precision) with the targets shows four combinations of “accurate versus precise” estimates. Draw four corresponding sampling distributions of the sample proportion \\(\\widehat{p}\\), like the one in the leftmost plot in Figure @ref(fig:comparing-sampling-distributions-3).\n\n\n\n\n\n\n\n\nLC 7.21 Block 3 Day 3\n\n\n\nThe Royal Air Force wants to study how resistant all their airplanes are to bullets. They study the bullet holes on all the airplanes on the tarmac after an air battle against the Luftwaffe (German Air Force).\n\n\n\n\n\n\n\n\nLC 7.22 Block 3 Day 3\n\n\n\nImagine it is 1993, a time when almost all households had landlines. You want to know the average number of people in each household in your city. You randomly pick out 500 phone numbers from the phone book and conduct a phone survey.\n\n\n\n\n\n\n\n\nLC 7.23 Block 3 Day 3\n\n\n\nYou want to know the prevalence of illegal downloading of TV shows among students at a local college. You get the emails of 100 randomly chosen students and ask them, “How many times did you download a pirated TV show last week?”.\n\n\n\n\n\n\n\n\nLC 7.24 Block 3 Day 3\n\n\n\nA local college administrator wants to know the average income of all graduates in the last 10 years. So they get the records of five randomly chosen graduates, contact them, and obtain their answers."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-8-confidence-intervals",
    "href": "LC/Learning-checks.html#chapter-8-confidence-intervals",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 8: Confidence intervals",
    "text": "Chapter 8: Confidence intervals\n\n\n\n\n\n\nLC 8.1 Block 3 Day 5\n\n\n\nWhat is the chief difference between a bootstrap distribution and a sampling distribution?\n\n\n\n\n\n\n\n\nLC 8.2 Block 3 Day 5\n\n\n\nLooking at the bootstrap distribution for the sample mean in Figure @ref(fig:one-thousand-sample-means), between what two values would you say most values lie?\n\n\n\n\n\n\n\n\nLC 8.3 Block 3 Day 6\n\n\n\nWhat condition about the bootstrap distribution must be met for us to be able to construct confidence intervals using the standard error method?\n\n\n\n\n\n\n\n\nLC 8.4 Block 3 Day 6\n\n\n\nSay we wanted to construct a 68% confidence interval instead of a 95% confidence interval for \\(\\mu\\). Describe what changes are needed to make this happen. Hint: we suggest you look at Appendix @ref(appendix-normal-curve) on the normal distribution.\n\n\n\n\n\n\n\n\nLC 8.5 Block 3 Day 8\n\n\n\nConstruct a 95% confidence interval for the median year of minting of all US pennies. Use the percentile method and, if appropriate, then use the standard-error method."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-9-hypothesis-testing",
    "href": "LC/Learning-checks.html#chapter-9-hypothesis-testing",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 9: Hypothesis testing",
    "text": "Chapter 9: Hypothesis testing\n\n\n\n\n\n\nLC 9.1 Block 4 Day 2\n\n\n\nWhy does the following code produce an error? In other words, what about the response and predictor variables make this not a possible computation with the infer package?\n\nlibrary(moderndive)\nlibrary(infer)\nnull_distribution_mean <- promotions %>%\n  specify(formula = decision ~ gender, success = \"promoted\") %>% \n  hypothesize(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\n\n\n\n\n\n\n\n\nLC 9.2 Block 4 Day 2\n\n\n\nWhy are we relatively confident that the distributions of the sample proportions will be good approximations of the population distributions of promotion proportions for the two genders?\n\n\n\n\n\n\n\n\nLC 9.3 Block 4 Day 2\n\n\n\nUsing the definition of p-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the promotion rates for males and females.\n\n\n\n\n\n\n\n\nLC 9.4 Block 4 Day 2\n\n\n\nDescribe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between the promotion rate of males and females using this study.\n\n\n\n\n\n\n\n\nLC 9.5 Block 4 Day 3\n\n\n\nWhat is wrong about saying, “The defendant is innocent.” based on the US system of criminal trials?\n\n\n\n\n\n\n\n\nLC 9.6 Block 4 Day 3\n\n\n\nWhat is the purpose of hypothesis testing?\n\n\n\n\n\n\n\n\nLC 9.7 Block 4 Day 3\n\n\n\nWhat are some flaws with hypothesis testing? How could we alleviate them?\n\n\n\n\n\n\n\n\nLC 9.8 Block 4 Day 3\n\n\n\nConsider two \\(\\alpha\\) significance levels of 0.1 and 0.01. Of the two, which would lead to a more liberal hypothesis testing procedure? In other words, one that will, all things being equal, lead to more rejections of the null hypothesis \\(H_0\\).\n\n\n\n\n\n\n\n\nLC 9.9\n\n\n\nConduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating. What was different and what was the same?\n\n\n\n\n\n\n\n\nLC 9.10\n\n\n\nWhat conclusions can you make from viewing the faceted histogram looking at rating versus genre that you couldn’t see when looking at the boxplot?\n\n\n\n\n\n\n\n\nLC 9.11\n\n\n\nDescribe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies.\n\n\n\n\n\n\n\n\nLC 9.12\n\n\n\nWhy are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres?\n\n\n\n\n\n\n\n\nLC 9.13\n\n\n\nUsing the definition of \\(p\\)-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the mean rating of romance to action movies.\n\n\n\n\n\n\n\n\nLC 9.14\n\n\n\nWhat is the value of the \\(p\\)-value for the hypothesis test comparing the mean rating of romance to action movies?\n\n\n\n\n\n\n\n\nLC 9.15\n\n\n\nTest your data wrangling knowledge and EDA skills:\n\nUse dplyr and tidyr to create the necessary data frame focused on only action and romance movies (but not both) from the movies data frame in the ggplot2movies package.\nMake a boxplot and a faceted histogram of this population data comparing ratings of action and romance movies from IMDb.\nDiscuss how these plots compare to the similar plots produced for the movies_sample data."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-10-inference-for-regression",
    "href": "LC/Learning-checks.html#chapter-10-inference-for-regression",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 10: Inference for regression",
    "text": "Chapter 10: Inference for regression\n\n\n\n\n\n\nLC 10.1 Block 4 Day 7\n\n\n\nContinuing with our regression using age as the explanatory variable and teaching score as the outcome variable.\n\nUse the get_regression_points() function to get the observed values, fitted values, and residuals for all 463 instructors.\nPerform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here.\n\n\n\n\n\n\n\n\n\nLC 10.2 Block 4 Day 8\n\n\n\nRepeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of stat = \"correlation\" in the calculate() function of the infer package."
  },
  {
    "objectID": "LC/Learning-checks.html#chapter-11-tell-your-story-with-data",
    "href": "LC/Learning-checks.html#chapter-11-tell-your-story-with-data",
    "title": "Learning Checks from Modern Dive",
    "section": "Chapter 11: Tell your story with data",
    "text": "Chapter 11: Tell your story with data\n\n\n\n\n\n\n\nLC 11.1 Block 4 Day 2\n\n\n\nRepeat the regression modeling in Subsection 11.2.3 and the prediction making you just did on the house of condition 5 and size 1900 square feet in Subsection 12.2.4, but using the parallel slopes model you visualized in Figure 11.6. Show that it’s $524,807!\n\n\n\n\n\n\n\n\nLC 11.2\n\n\n\nWhat date between 1994 and 2003 has the fewest number of births in the US? What story could you tell about why this is the case?"
  },
  {
    "objectID": "LC/LC-lesson29.html",
    "href": "LC/LC-lesson29.html",
    "title": "Learning Checks Lesson 29",
    "section": "",
    "text": "In dag04, build models to predict c from the other variables. Does one of those variables “block” the others?\n\nExplain how you know this from your models. Try to give an answer in everyday language as well.\nRepeat but use a very small sample size, say \\(n=5\\). Has your conclusion about blocking changed? Explain why.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ncompare_rms_error(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50, in_sample = TRUE)\n\n[1] 0.8580525 0.8220923 0.7443196 0.7114751\n\n\nd seems to block effect of a and b on c.\n\ncompare_rms_error(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5, in_sample = TRUE)\n\n[1] 0.508412550 0.253164615 0.130851042 0.007047309"
  },
  {
    "objectID": "LC/LC-lesson29.html#lc-29.2",
    "href": "LC/LC-lesson29.html#lc-29.2",
    "title": "Learning Checks Lesson 29",
    "section": "LC 29.2",
    "text": "LC 29.2\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-using stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\nFor out-of-sample, when working with new data, it’s not just a matter of being tricked into thinking covariates are useful when they’re not. Using irrelevant covariates can be genuinely harmful to the predictions.\nCompare these in-sample and out-of-sample results.\n\nset.seed(101)\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = TRUE)\n\n[1] 4.689275e-01 4.188891e-01 3.603896e-01 1.416962e-16\n\nset.seed(101)\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = FALSE)\n\n[1] 0.965495 1.434434 1.641881 1.591050\n\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?"
  },
  {
    "objectID": "LC/LC-lesson28.html",
    "href": "LC/LC-lesson28.html",
    "title": "Learning Checks Lesson 28",
    "section": "",
    "text": "Consider dag01, which shows a simple causal relationship between two variable.\n\ndag_draw(dag01)\n\n\n\n\nSo far as the size of prediction error is concerned, does it matter whether x is used to predict y or vice versa? Show the models and the results you use to come to your conclusion. ::: {.callout-note} ## Solution\n:::"
  },
  {
    "objectID": "LC/LC-lesson38.html",
    "href": "LC/LC-lesson38.html",
    "title": "Learning Checks Lesson 38",
    "section": "",
    "text": "Solution"
  },
  {
    "objectID": "Objectives/Obj-lesson-26.html",
    "href": "Objectives/Obj-lesson-26.html",
    "title": "Objectives (Day 26)",
    "section": "",
    "text": "26.2 Interpret prediction bands as a series of intervals, one for each value of the model input.\n26.3 Identify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t."
  },
  {
    "objectID": "Objectives/Obj-lesson-32.html",
    "href": "Objectives/Obj-lesson-32.html",
    "title": "Objectives (Day 32)",
    "section": "",
    "text": "32.2 Correctly re-draw DAG for an ideal experimental intervention.\n32.3 Use blocking to set assignment to treatment or control."
  },
  {
    "objectID": "Objectives/Obj-lesson-33.html",
    "href": "Objectives/Obj-lesson-33.html",
    "title": "Objectives (Day 33)",
    "section": "",
    "text": "33.2. Calculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\n33.3. Interpret effect size as stated in log odds."
  },
  {
    "objectID": "Objectives/Obj-lesson-27.html",
    "href": "Objectives/Obj-lesson-27.html",
    "title": "Objectives (Day 27)",
    "section": "",
    "text": "This is a QR day."
  },
  {
    "objectID": "Objectives/Obj-lesson-19.html",
    "href": "Objectives/Obj-lesson-19.html",
    "title": "Objectives (Day 19)",
    "section": "",
    "text": "Prediction: predict an outcome for an individual\nRelationship: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\n19.2 Given a research question, identify whether it corresponds to a prediction setting or a relationship setting."
  },
  {
    "objectID": "Objectives/Obj-lesson-31.html",
    "href": "Objectives/Obj-lesson-31.html",
    "title": "Objectives (Day 31)",
    "section": "",
    "text": "31.2 Construct appropriate DAG to match a narrative hypothesis."
  },
  {
    "objectID": "Objectives/Obj-lesson-25.html",
    "href": "Objectives/Obj-lesson-25.html",
    "title": "Objectives (Day 25)",
    "section": "",
    "text": "25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error."
  },
  {
    "objectID": "Objectives/Obj-lesson-24.html",
    "href": "Objectives/Obj-lesson-24.html",
    "title": "Objectives (Day 24)",
    "section": "",
    "text": "24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation."
  },
  {
    "objectID": "Objectives/Obj-lesson-30.html",
    "href": "Objectives/Obj-lesson-30.html",
    "title": "Objectives (Day 30)",
    "section": "",
    "text": "30.2 Choose whether to include covariate depending on form of DAG"
  },
  {
    "objectID": "Objectives/Obj-lesson-34.html",
    "href": "Objectives/Obj-lesson-34.html",
    "title": "Objectives (Day 34)",
    "section": "",
    "text": "34.2. Cross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\n34.3. Calculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome."
  },
  {
    "objectID": "Objectives/Obj-lesson-20.html",
    "href": "Objectives/Obj-lesson-20.html",
    "title": "Objectives (Day 20)",
    "section": "",
    "text": "20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples."
  },
  {
    "objectID": "Objectives/Obj-lesson-21.html",
    "href": "Objectives/Obj-lesson-21.html",
    "title": "Objectives (Day 21)",
    "section": "",
    "text": "21.2 Having selected a response and one or more explanatory variables, identify other DAG notes as covariates.\n21.3 Generate data from simulations and use the data to model the relationships."
  },
  {
    "objectID": "Objectives/Obj-lesson-35.html",
    "href": "Objectives/Obj-lesson-35.html",
    "title": "Objectives (Day 35)",
    "section": "",
    "text": "35.2 Understand sensitivity and specificity as conditional probabilities.\n35.3 Calculate false-positive and false-negative rates for a given prevalence."
  },
  {
    "objectID": "Objectives/Obj-lesson-23.html",
    "href": "Objectives/Obj-lesson-23.html",
    "title": "Objectives (Day 23)",
    "section": "",
    "text": "23.2 Infer sampling variation from a regression table: “standard error” of a model coefficient.\n23.3 Construct and interpret confidence intervals on a model coefficient.\n23.4. Understand and use scaling of confidence interval length as a function of \\(n\\)."
  },
  {
    "objectID": "Objectives/Obj-lesson-37.html",
    "href": "Objectives/Obj-lesson-37.html",
    "title": "Objectives (Day 37)",
    "section": "",
    "text": "37.2 Interpret correctly from regression/ANOVA reports\n37.3 Traditional names for hypothesis tests in different “textbook” settings.\n37.4. Distinguish between p-value and effect size, that is, “significance” and “substance.”"
  },
  {
    "objectID": "Objectives/Obj-lesson-36.html",
    "href": "Objectives/Obj-lesson-36.html",
    "title": "Objectives (Day 36)",
    "section": "",
    "text": "36.2 Contrast hypothesis testing versus Bayesian framework."
  },
  {
    "objectID": "Objectives/Obj-lesson-22.html",
    "href": "Objectives/Obj-lesson-22.html",
    "title": "Objectives (Day 22)",
    "section": "",
    "text": "22.2 Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\n22.3 Graphically display the distribution of summaries and generate a compact numerical description (“confidence interval”) of the sampling distribution."
  },
  {
    "objectID": "Objectives/Obj-lesson-38.html",
    "href": "Objectives/Obj-lesson-38.html",
    "title": "Objectives (Day 38)",
    "section": "",
    "text": "38.2 Estimate how overall p-value should change when study is replicated."
  },
  {
    "objectID": "Objectives/Obj-lesson-29.html",
    "href": "Objectives/Obj-lesson-29.html",
    "title": "Objectives (Day 29)",
    "section": "",
    "text": "29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction."
  },
  {
    "objectID": "Objectives/Obj-lesson-28.html",
    "href": "Objectives/Obj-lesson-28.html",
    "title": "Objectives (Day 28)",
    "section": "",
    "text": "28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "",
    "text": "This site holds the proposal for the Spring 2023 version of Math 300."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Background",
    "text": "Background\nUp through Spring 2022, Math 300 was organized around the Moore and Notz textbook: Statistics: Concepts and controversies 10/e. This book was designed for a non-technical audience of “consumers of statistics” but is dramatically outdated. For instance, it has no data science content and introduces only primitive statistical methods. A course with such shortcomings seems inappropriate for cadets going on to be officers who will inevitably have to work with modern data and methods.\nIn Fall 2022, Math 300 switched to a very different book, Ismay and Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. The ModernDive book introduces computing on data in an accessible but modern way. It is the only well-known statistics text based on a data-science perspective. Nonetheless, the statistical inference portions of the book regress to the same sort of primitive statistical methods from Concepts and Controversies.\nTo support the Fall 2022 course using ModernDive, a complete set of roughly 35 Notes to Instructors (NTI) was written by Prof. Bradley Warner along with problem sets and other needed materials and deployed for the course.\nThis proposal is for additional improvements to Math 300, building on the Fall 2022 course but replacing the statistical inference portions of the course with more contemporary and general-purpose inference techniques and support for concepts and methods relevant to decision-making.\nIn the following, I refer to three different versions of Math 300:\n\nThe Fall 2022 version of the course, using the ModernDive book, will be called Math 300.\nThe previous version of the course, as taught for several years before Fall 2022, will be called 300CC, which refers to the textbook then used, Concepts & Controversies.\nThe course proposed in this document, a revision of part of Math 300, will be called Math 300R. The R stands for “revised.”"
  },
  {
    "objectID": "index.html#overall-goals-of-math-300r",
    "href": "index.html#overall-goals-of-math-300r",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Overall goals of Math 300R",
    "text": "Overall goals of Math 300R\nThe design of a course revision needs to take into account several factors:\n\nThe target audience’s anticipated technical ability and motivation and, therefore, the appropriate pedagogy and the balance between theory and practice to use in the course.\nInstitutional goals that inform the prioritization of the topics included in the course.\nConstraints of class time and internal coherence of the course, that is, using later topics to reinforce student learning of the earlier topics.\n\nLater, in the rationale section section of this document, I describe how I came to the following conclusions, but for now, a simple statement will suffice.\n\nThe target audience is humanities and social science majors, many of whom will not be confident in the use of calculus but all of whom have had previous exposure to R in the core calculus course.\nInstitutional goals (as revealed by discussions with humanities and social science departments and the wording of the catalog description of Math 300) include a substantial emphasis on data science techniques (data wrangling and visualization) and the use of statistical concepts and methods to support decision-making."
  },
  {
    "objectID": "index.html#statistical-topics-and-framework",
    "href": "index.html#statistical-topics-and-framework",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Statistical topics and framework",
    "text": "Statistical topics and framework\nTransitioning from Math 300CC to Math 300 has already accomplished many data science goals. This proposal centers on the statistical topics/methods to be covered and the path through them.\nThe class-time demands of the new emphasis on data science techniques in Math 300 (and retained in Math 300R) dictate that the statistical concepts and methods be taught more compactly than in Math 300CC. Low-priority, legacy topics from Math 300CC should be dropped. (The GAISE report provides some guidance here.) We can use to advantage that students in Math 300 already see many modeling-related topics in Math 141Z/142Z. Since students already have a background in model-building and computing, we can choose statistical topics that relate well to decision-making.\nA traditional path for statistical methods starts with descriptive statistics (e.g., standard deviation) and then presents “1-sample” statistics (e.g., mean, proportion) and inferential techniques (confidence interval, hypothesis test) in that context. Next comes the inferential techniques for the analogous “2-sample” statistics (difference in mean, difference in proportion), followed by inference techniques for regression.\nThis path is unnecessarily long for our students since regression encompasses all the traditional methods.1 Framing statistical inference in the context of regression avoids the need to teach method-specific calculations or cover the variety of formats for non-regression test results. Regression is part of the data scientist’s standard toolbox and relates well to more advanced data techniques such as machine learning. The ModernDive textbook uses regression as the segue from the first block (about data wrangling and visualization) to the third block (about inference).\nAdditional streamlining comes from motivating statistical inference using a simulation approach. Simulation draws on two conceptually simple data operations: resampling and permutation (shuffling). This approach is well established in the statistics community and is considered by many (including the ModernDive authors) to be a better pedagogy than the traditional formula-and-distribution presentation of statistical methods. Since Math 300 (and 300R) students will already have worked with wrangling and visualization, they will be well prepared to work with the data generated by repeated simulation trials.\nThe focus on decision-making in 300R appears in the addition of new concepts and techniques treated minimally in traditional statistics courses. These include risk, prediction (and its close cousin classification), causality, and confounding. Introductory epidemiology courses provide a model for teaching about risk, causation, and confounding. The pedagogy for these topics in Math 300R comes from the epidemiology course I introduced at Macalester. In addition, Math 300R draws on my decade of experience teaching causality as part of an introductory statistics course. (See the causation chapter of my Statistical Modeling text.)\nThe Statistical Modeling pedagogy for causality uses directed acyclic graphs (DAGs) and causal simulations based on them. Unlike resampling and permutation, which re-arrange existing data, the DAG simulations generate synthetic data with specified properties (such as effect sizes). Simulations allow a concrete demonstration of the extent to which regression techniques can and cannot recover causal information from data.\nThe DAG-simulation approach lends itself naturally to the demonstration of statistical phenomena such as sampling variation and estimation of prediction error. As an example, consider the statistical fallacy of regression to the mean, as with Galton’s finding about comparing children’s heights to their parents’. The natural hypothesis that heights are determined by genetic and other factors is represented by this DAG:\n\\[\\epsilon \\rightarrow parent \\longleftarrow GENES \\longrightarrow child \\leftarrow \\nu\\]\nIn this DAG there is no causal mechanism included for “regression to the mean.” However, Galton’s empirical finding is replicated by data from the DAG-simulation."
  },
  {
    "objectID": "index.html#sec-broad-structure",
    "href": "index.html#sec-broad-structure",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Scope of the proposed changes",
    "text": "Scope of the proposed changes\nMath 300R will retain the first 17 lessons of Math 300. All teaching materials for this part of the course will be used unaltered. (Exception: revisions to Math 300 the Fall 2022 teaching team deems appropriate. Such revisions are not part of this proposal.)\nThe following 19 lessons are entirely refactored and based on new readings, NTIs, exams, and other materials. Objectives for each of these 19 lessons are itemized here.\n\nThe corresponding ModernDive chapters are not used in Math 300R.\nThe software used is the same as that used in the first half of the ModernDive book, specifically the ggplot2 graphics package and the tidyverse data wrangling packages. However, the infer package used in the second half of ModernDive is dropped.\n\nThe theme of the refactored 19 lessons is “informing decisions with data.” Statistical approaches that can inform decision-making include anticipating the impact of interventions, predicting individual outcomes, and the quantification of risk. These are all included in Math 300R.\nTopics to be de-emphasized are the algebra of computing confidence intervals and p-values and the (controversial) role of p-values as a guide to practical “significance.” About half of a traditional course is about the construction of confidence intervals in various settings and, more or less equivalently, the conversion of data into p-values. However, in the contemporary era, when “observational” data are collected en masse, p-values can become very small (“significant”) even when the relationship under study is slight and insubstantial.\nConfounding and methods for dealing with it (statistical adjustment, experiment) are treated substantially in Math 300R. Decision-making about interventions often relies on understanding causal effects. The possibility of confounding is a major source of skepticism about making causal judgments. In a world where much data is observational, the sweeping principles that “correlation is not causation” and “no causation without experimentation” do not support making responsible conclusions about causal connections and the need to make decisions even when data cannot provide a definitive answer. Decision-makers need this support."
  },
  {
    "objectID": "index.html#rationale",
    "href": "index.html#rationale",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Rationale for course revisions",
    "text": "Rationale for course revisions\n\nRelationship to Math 357 and Math 377\nDFMS offers three courses satisfying the statistics component of the Academy’s core requirements: Math 300, Math 357, and Math 377. In designing 300R, attention should be paid to the reasons for supporting three distinct courses. The catalog copy lays out the differences in terms of intended student major, software, mathematical background, and orientation to data science.\nIntended student major: The catalog says, “Math 300 is designed primarily for majors in the Social Sciences and Humanities.” while “Math 356 is primarily designed for cadets in engineering, science, or other technical disciplines. Math majors and Operations Research majors will take Math 377.” Math 377 is also the intended course for prospective Data Science majors, although this is not in the catalog.\nSoftware: The catalog does not describe any software component for either Math 300 or Math 357, but states that, in Math 377, “modern software appropriate for data analysis will be used.” In reality, as of Fall 2022, much the same software is used in all three courses: R with the dplyr package for data wrangling, ggplot2 for data visualization, and “R/Markdown” for creating computationally active documents.\nOne difference between Math 300 and 357/377 relates to computer programming. Both 357 and 377 include content about the underlying structure of the R language, object types, the construction of functions, and arrays and iteration. In contrast, Math 300 is based on a small set of command patterns using data frames. Students see R in Math 300 more or less as an extension of what they learned in 141Z/142Z; what’s added is a few statistical and data-wrangling functions and a handful of new graphics types.\nStudents’ mathematical background: Math 377 explicitly refers to “calculus-based probability.” Math 300 and 357 share identical catalog copy, though in reality Math 357 and Math 377 use the same textbook. Calculus is indeed necessary for the probability topics in Math 357 and 377. My interpretation is that Math 300 should serve as a safe haven for those who lack confidence in their calculus skills. Both the Fall 2022 edition of Math 300 and the proposed Math 300R serve this role as safe haven.\nOrientation to Data Science: Starting with the Fall 2022 edition, Math 300 develops and draws on data-science skills for wrangling and visualization. In this, the new Math 300 is in line with both Math 357 and 377.\nThe above analysis indicates that Math 300 and 300R should diverge from Math 357/377 in these ways:\n\nMath 300R should make little or no use of calculus operations.\nMath 300R should include little consideration of probability distributions or (non-automated) calculations with any but the simplest.\nMath 300R should be computational, but should not draw heavily on computer programming skills such as types of objects, arrays, indexing, and loop-style iteration. Use of R/Markdown documents should be considered as a pedagogical choice, and retained or discontinued based on how it contributes to student success in the other areas of the course.\n\nIn addition, I suggest that …\n\nMath 300R include some work with assembling/curating data using spreadsheets and basic data cleaning with spreadsheets. Awareness of the ubiquity of data errors and a basic understanding of how to deal with such errors is an important component of working with data. (This is not to suggest that data analysis, modeling, and graphical depiction be taught using spreadsheets, which are notoriously unreliable, difficult, and limiting for such purposes. Spreadsheets are, however, appropriate for the phase where non-tabular data is transcribed into a tabular arrangement.)\n\n\n\nInstitutional goals\nIt can be difficult to translate broadly stated institutional goals to apply them to a single course. However, catalog descriptions of programs and individual courses provide some assistance. Here is the catalog copy for Math 300 (which is identical to the catalog description of Math 357).\n\nMath 300. Introduction to Statistics. An introduction in probability and statistics for decision-makers. Topics include basic probability, statistical inference, prediction, data visualization, and data management. This course emphasizes critical thinking among decision-makers, preparing future officers to be critical consumers of data. (Emphasis added.)\n\nI interpret the final sentence as a description of the overall objective of the course:\n\nOverall objective: Prepare officers to use data to inform decisions.\n\nReturning to the idea that the topics listed in the catalog copy ought to be interpreted as serving the overall objective of the course, let’s consider those topics one at a time:\n\ndata management\ndata visualization\nprediction\nstatistical inference\nbasic probability\n\n\nStrictly speaking, as a term of art the phrase “data management” is business jargon describing enterprise-level activities that are unrelated to the other items on the list. It would be unheard of to include it, in this strict sense, in a statistics course. I believe the intent of the phrase to be better served by terms like “data wrangling,” “data cleaning,” “database querying,” and such which make up an important part of “data science.” Data wrangling is a major feature of Math 300 launched and is covered using professional level computing tools well suited to both small and large data. But whatever “data management” might reasonably be taken to mean, it was utterly ignored in Math 300CC.\n“Data visualization” is generally taken to be the process of using graphics to discover and highlight patterns shown in data. Math 300CC included only statistical graphics such as histograms, box-and-whisker plots, and basic “scatter plots.” Math 300 adds to this modern modes of graphics such as transparency, color, and faceting that make it possible to display relationships among multiple variables. The software used in Math 300 is the professional-level ggplot2 which provides the ability to increase the sophistication and generality of data display, using for example density graphics such as violin plots. As such, Math 300 is a big step on the road to rich data visualization. Some of these will be introduced in Math 300R in the second half of the course.\n“Prediction” is a central paradigm used in the important area of “machine learning.” It is also an often used method used to inform decision making and characterize risk, for instance, by indicating the distribution of plausible outcomes. Math 300CC emphasized paradigms such as hypothesis testing and confidence intervals that are not aligned with making and interpreting predictions. Math 300 focuses on these same paradigms. Math 300R will treat prediction as a central statistical path, as well as highlighting its proper use, interpretation, and evaluation.\n“Statistical inference” is traditionally taken to mean the calculation and interpretation of hypothesis tests and confidence intervals in various simple settings. Such settings include the “difference between two means,” the “correlation coefficient,” and the “slope of a regression line.” Math 300CC introduced a handful of such settings, providing distinct formulas for each of them. The “controversies” referred to in the title Concepts and controversies includes the problematic interpretation of “p-values” and the need to use random sampling and/or random assignment in data collection to get “correct” results. Math 300 retains the emphasis on confidence intervals and p-values in the simple settings, but emphasizes a more general and accessible methodology based on bootstrapping and permutation tests.\n\nUnfortunately, appealing to random sampling/assignment is often whistling past the graveyard, since these idealized data collection processes are rarely available. Instead, professionals include “covariates” in their data collection in order to “adjust” for the factors that would have been scrambled into insignificance by random sampling/assignment if it had been available. Math 300R incorporates covariate methods and highlights the importance of identifying appropriate covariates.\n\n“Basic probability” can mean different things to different people. In most introductory statistics courses it refers to the construction, calculation, and study of named distributions such as the binomial, normal, chi-squared, t, etc. Such distributions play an important role in the statistical theory of confidence intervals and hypothesis testing. That is, they are support for statistical inference. Math 300CC followed the traditional pattern of having students memorize which distribution is relevant to which setting and using printed tables for calculation. As described earlier, Math 300 provides a much more natural route to inference through bootstrapping and permutation tests.\n\nWhat’s left out in this conception of basic probability is the support for decision making. Essential to this is the proper use of “conditional probability.” Math 300R emphasizes appropriate use and interpretation of conditional probability, seen most clearly in the “classifiers” part of the course.\n\n\nFaculty opinions\nInsofar as faculty internalize the goals of the institution, their views can point to ways that existing courses do and do not reflect those goals.\nWithin DFMS and other departments, there is a general discontent that Math 300CC was not doing what it ought to. Reasons for this can be seen by examining the textbook used in Math 300CC. The book has clear deficiencies, among which are:\n\nthe material is out of date and does not reflect any of the consensus recommendations (such as GAISE) developed in the last 30 years.\nit does not use data at any level beyond hand calculation.\nit does not deal with decision making at any serious level. (The only decision formally supported is whether or not to reject the Null hypothesis.)\n\nThe opinions of faculty outside DFMS can also be an important guide to institutional priorities. In AY 2021-22 I contacted the departments in the social sciences and humanities. Three of these—history, political science, economics—responded with interest. Discussion with groups of faculty from these departments elucidated a number of points:\n\nThe faculty most highly valued the development of data-science skills such as computing for data wrangling and data visualization.\nThe then-current version of Math 300 did not contribute to the development of such skills.\nMath 357 is not seen as an appropriate alterative to Math 300, both because of perceived difficulty of 357 and because faculty do not value the emphasis on probability distributions seen in 357.\n\nFrom my experience at Macalester and in conducting reviews at many colleges, I am often wary of the motivation of faculty in other departments. These can represent a desire for service courses like Math 300 to cover discipline-specific techniques. However, the faculty I spoke to also had an eye on what their students will need for their post-graduation jobs. Particularly the USAF officers drew on their field experience in areas such as military intelligence.\nBased on these findings, the group of faculty planning for revisions to Math 300 made an easy decision: replace the textbook with one oriented to data science. We selected the ModernDive book, which is unique among introductory statistics textbooks in starting out with data wrangling and visualization. This change of textbook addresses the “use data” part of the course objective stipulated above.\nThe other part of the objective—inform decisions—remains problematic even with the switch in the Math 300 textbook. Discussions I had with the ModernDive authors made clear that their purpose in writing the book was to provide a way to introduce data science into introductory statistics, but that they stuck to the traditional hypothesis-testing/confidence-interval framework in order not to make the change too daunting for instructors thinking of adopting the text. In other words, they were not trying to turn the topic toward decision-making with data, the motivation of the ideas presented in this proposal for Math 300R."
  },
  {
    "objectID": "index.html#plan-of-work",
    "href": "index.html#plan-of-work",
    "title": "Math 300R: Additional improvement of Math 300",
    "section": "Plan of work",
    "text": "Plan of work\n\nEarly October 2022: Preliminary approval, with appropriate modifications, of the proposed objectives.\nOctober 2022: DTK will draft new day-by-day NTIs for the second half of the course in the same style as the existing NTIs for the first half of the course. In the process of drafting, there will likely be some re-arrangement and modification of the objectives in (1).\nNovember 2022: With the draft NTIs in hand, a faculty team will make a more detailed examination of the proposed objectives. I recommend that this examination be structured as a set of hour-long discussions, one for each of the five divisions described in ?@sec-topics.\nNovember/December 2022: DTK (and others, as interested) will assemble student readings to replace the second half of ModernDive. Much of the content already exists in the form of a draft textbook by DTK. These will be re-arranged to correspond to the day-to-day objectives as determined in (3).\nJanuary/February 2023: The first 18 lessons of 300R will be taught as a repeat of those lessons from Math 300 Fall 2022. DTK will participate mainly as an observer.\nJanuary/February 2023: Revision and refinement will be made of the readings and NTIs in (3) and (4) above.\nMarch/April 2023: Teaching the new lessons. DTK will participate as an instructor for these lessons."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html",
    "href": "Reading-notes/Reading-notes-lesson-38.html",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "",
    "text": "Ask, and it shall be given you; seek, and ye shall find; knock, and it shall be opened unto you: For every one that asketh receiveth; and he that seeketh findeth; and to him that knocketh it shall be opened. – Matthew 7:7-8\nThe modeling techniques we’ve covered are surprisingly powerful at identifying patterns in data. With power comes responsibility. This chapter is about how spurious patterns can arise in data and processes you can use to help ensure that the patterns your models identify are genuine.\nIt’s well known that people are particularly adept at finding patterns. To see this, spend a minute or two with Figure 1, which shows x-y pairs generated by a complex mathematical procedure called the Mersenne-Twister algorithm. How many of the structures created by Mersenne-Twister algorithm can you identify by eye? Take five of the stronger-looking patterns: clusters of points, large empty areas, strings of dots, etc. Write down a list of the patterns you spotted, including the coordinate location of each, a short description (e.g. “arc of dots”), and your subjective sense of how strong or convincing that pattern is.\nWith your list in hand, look at Figure 2 at the end of this section, which displays another n = 1000 x-y pairs generated by the same mathematical procedure. You’re going to check which of the patterns you found in the testing data are confirmed by the training data. Go through your list, looking at each location where you found a pattern in the training data and checking whether a similar pattern appears at that location in the testing data.\nWere any of the patterns you saw in the training data confirmed by the testing data?\nThere’s no denying that the patterns you saw were in the data. But the Mersenne-Twister algorithm is specifically designed not to produce regular patterns. Any that you saw were accidental alignments in the particular sample of data from the algorithm.\nThe “patterns” abstractly referred to in the previous paragraphs appear in data. In data used for modeling, a pattern might be a relationship or correlation between two or more variables, or a cluster of rows in a data frame that have similar values for a response variable and explanatory variables.\nTraining models on data can encode the underlying patterns. For instance, a pattern in the data might result in a model generating detailed predictions or demonstrating a strong effect size of one variable on another.\nA valid pattern is one that steadily appears from one sample of data to another (so long as the sample is big enough). Such consistency suggests that the pattern reflects some genuine aspect of the system generating the data. A false or accidental pattern is one that appears in a sample of data, but is unlikely to show up in another sample. This inconsistency indicates that conclusions based on this pattern are unlikely to be applicable in the future or in new situations.\nThe obvious, direct way to check the validity of a pattern encoded by a model is to see if the same pattern occurs in new data, data that was not used in building the initial model. Lesson 22 took this approach by constructing a sampling distribution of a statistic such as an effect size. To create a sampling distribution, we train many models on different subsets of a data set.\nWhen working with prediction models, the sign of a valid pattern is that the quality of the predictions – perhaps measured with a root-mean-square-error or a sensitivity/specificity – remains consistent when we calculate it on new data. A prediction that shows very small error on the data used to train the model but large error on new data is not a prediction that we can rely on in new settings.\nThe historical rapid growth in data analysis activity and the construction of data sets with large numbers of explanatory variables has made it easier to capture with models both valid patterns and false patterns. This makes it important to recognize that the false detection of patterns is possible whenever you train a model, to be aware of the characteristics of models and data that make false detection more likely, and to adopt procedures to mitigate the risk that the results of your work may not generalize beyond the particular sample of data you have in hand."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#example-falsely-discovering-purchasing-habits",
    "href": "Reading-notes/Reading-notes-lesson-38.html#example-falsely-discovering-purchasing-habits",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "Example: Falsely discovering purchasing habits",
    "text": "Example: Falsely discovering purchasing habits\nYou are a data scientist for an internet retailer, Potomac.com, which has just bought a national grocery chain, Austin Foods. You’re part of the team that is connecting the customer loyalty card data from Austin Foods with Potomac’s own large record of purchases. This is accomplished by offering a 10% Xdiscount for an item on Potomac to people who enter their Austin loyalty card number.\nPotomac’s management wants to create a cross-marketing program in which a customer shopping at Potomac will be offered coupons for Austin products. The hope is that the coupon discount will attract new customers to start shopping at Austin’s. In order for this to work, it’s best if the coupons are for products that the customer finds attractive.\nYour job is to build the coupon assignment system, that is, to figure out how to choose which products a customer is most likely to find attractive. To do this, you’ll create a set of classifiers that indicates the interest of a Potomac customer in an Austin product.\nYou’ve got data on 10,000 Potomac/Austin customers, that is, people whose records from Potomac and from Austin you can bring together. There are ten popular Austin products for which coupons can be offered. Among the 10,000 customers, about 16% have actually bought any given Austin product. You have built ten classifiers, one for each of the ten products. The input to the classifiers is 100 standard measures of a customer’s Potomac activity. The output of each classifier is the probability that the customer actually bought the corresponding Austin product.\nThe no-input classifier gives a probability of about 16% that the customer will buy the product. Management hopes that you will be able to segment the market to identify the products that a given person is much more likely to buy.\nIt’s a lot to ask of a person to sort through 100 potential explanatory variables to identify those that are predictive of buying a product. But it’s straightforward to use a model family that can learn on its own which variables are informative. You train the ten classifiers using a tree family of models.\n\nHeads up! The “data” has been created using random numbers, so that there are no actual relationships between the explanatory variables and the purchase outcomes. That is, no actual relationships aside from the accidental ones, such as the patterns encountered in Figure 1.\n\nTo illustrate how the coupon assignment system works, Table @ref(tab:some-results) shows an intermediate step in the calculation, where a probability for each of the ten products is calculated for each customer.\n\n\n\n\n\n\nTable @ref(tab:some-results) shows the output of the classifiers for just the first fifteen customers out of the 10,000 used to build the coupon selection system. For each person, all ten classifiers have been applied to estimate the probability that the person would buy each of the ten products. Highlighted in green are those products with a purchase probability greater than 40%.\nThe final output of the coupon assignment system is, for each customer, the identification of the specific products for which the probability is large. Reading Table @ref(tab:some-results), you’ll see that for person 1, product 9 merits a coupon. For person 2, products 2 and 10 merit a coupon. A winning product has not been identified for every customer, but you can’t please everyone.\n\n\n\nAttaching package: 'formattable'\n\n\nThe following objects are masked from 'package:scales':\n\n    comma, percent, scientific\n\n\n\n\n(ref:some-results-cap)\n \n\n\nCustomer ID\n\n  \n    product \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n  \n \n\n  \n    1 \n    9 \n    13 \n    10 \n    13 \n    5 \n    13 \n    8 \n    7 \n    13 \n    5 \n    44 \n    15 \n    8 \n    13 \n    13 \n  \n  \n    2 \n    16 \n    14 \n    7 \n    13 \n    11 \n    16 \n    13 \n    10 \n    11 \n    71 \n    8 \n    14 \n    73 \n    0 \n    16 \n  \n  \n    3 \n    89 \n    13 \n    10 \n    13 \n    11 \n    12 \n    100 \n    12 \n    10 \n    11 \n    10 \n    8 \n    14 \n    0 \n    9 \n  \n  \n    4 \n    9 \n    13 \n    22 \n    10 \n    14 \n    10 \n    3 \n    6 \n    12 \n    12 \n    12 \n    11 \n    14 \n    10 \n    8 \n  \n  \n    5 \n    14 \n    3 \n    10 \n    7 \n    11 \n    8 \n    11 \n    11 \n    15 \n    10 \n    11 \n    11 \n    14 \n    0 \n    8 \n  \n  \n    6 \n    16 \n    10 \n    7 \n    11 \n    64 \n    9 \n    25 \n    10 \n    8 \n    7 \n    5 \n    11 \n    11 \n    15 \n    12 \n  \n  \n    7 \n    10 \n    5 \n    9 \n    69 \n    13 \n    20 \n    13 \n    8 \n    8 \n    13 \n    73 \n    20 \n    6 \n    10 \n    13 \n  \n  \n    8 \n    14 \n    12 \n    15 \n    5 \n    4 \n    14 \n    13 \n    12 \n    4 \n    10 \n    14 \n    21 \n    14 \n    14 \n    13 \n  \n  \n    9 \n    12 \n    12 \n    14 \n    12 \n    12 \n    12 \n    14 \n    62 \n    12 \n    12 \n    80 \n    9 \n    0 \n    9 \n    12 \n  \n  \n    10 \n    9 \n    13 \n    6 \n    10 \n    12 \n    6 \n    17 \n    16 \n    6 \n    6 \n    25 \n    9 \n    5 \n    15 \n    7 \n  \n\n\n\n\n\n(ref:some-results-cap) The output of the ten classifiers for the first 15 customers. Green highlighting is used for those products which a given customer is likely to buy.\n\n\nWarning: `group_by_()` was deprecated in dplyr 0.7.0.\nℹ Please use `group_by()` instead.\nℹ See vignette('programming') for more help\nℹ The deprecated feature was likely used in the dplyr package.\n  Please report the issue at <\u001b]8;;https://github.com/tidyverse/dplyr/issues\u0007https://github.com/tidyverse/dplyr/issues\u001b]8;;\u0007>.\n\n\nTo test the performance of the system, we can look at the product/customer combinations for which a coupon was merited, and check how many of them actually corresponded to a purchase: it’s 74%. But for the combinations with no coupon, the purchase rate was only 11%.\nThe results are impressive. For about half of the customers, the coupon assignment system has identified customers/product combinations with a purchase probability of more than 40%. Often, the probability of purchase is considerably higher than 40%. Targeting each customer with a coupon for the right product is likely to generate a lot of new sales!\n\nSince data was generated using random numbers, we know that the “success” of the coupon assignment system is illusory. Later, we’ll see how the process was able to uncover so many accidental patterns from random data and list some things to look out for when modeling. But first, let’s provide a reliable method for you to identify when your results are based in accidental patterns: using testing data.\n\nA true measure of the performance of a model should be based not on the data on which the model was trained, but data which have been held back for use in testing and not used in training. For this example, we’ll use testing data consisting of 10,000 customers for whom we have the same 100 explanatory variables from the Potomac database and for whom we know if each customer purchased any of the ten products from Austin Foods. Only about 1 in 6 customers bought any single product from Austin. We want to see if the classifier assigns a high probability to those customers who did buy the product. If so, it means we can use just the 100 explanatory variables to find winning products for customers for whom we have no Austin purchasing data.\n\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\nℹ The deprecated feature was likely used in the tibble package.\n  Please report the issue at <\u001b]8;;https://github.com/tidyverse/tibble/issues\u0007https://github.com/tidyverse/tibble/issues\u001b]8;;\u0007>.\n\n\n\n\n(ref:purchase-test-cap)\n \n\n\nCustomer ID\n\n  \n    product \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n  \n \n\n  \n    1 \n    13 \n    7 \n    13 \n    17 \n    12 \n    8 \n    58 \n    13 \n    7 \n    7 \n    13 \n    9 \n    9 \n    13 \n    6 \n  \n  \n    2 \n    8 \n    19 \n    86 \n    13 \n    13 \n    9 \n    13 \n    18 \n    9 \n    75 \n    16 \n    13 \n    8 \n    13 \n    71 \n  \n  \n    3 \n    25 \n    12 \n    16 \n    0 \n    8 \n    10 \n    16 \n    10 \n    71 \n    8 \n    7 \n    80 \n    10 \n    10 \n    9 \n  \n  \n    4 \n    15 \n    13 \n    12 \n    11 \n    10 \n    9 \n    17 \n    14 \n    8 \n    3 \n    17 \n    14 \n    12 \n    11 \n    3 \n  \n  \n    5 \n    8 \n    10 \n    10 \n    38 \n    9 \n    13 \n    11 \n    10 \n    7 \n    11 \n    11 \n    9 \n    7 \n    14 \n    7 \n  \n  \n    6 \n    9 \n    10 \n    8 \n    60 \n    6 \n    4 \n    89 \n    9 \n    7 \n    11 \n    8 \n    10 \n    12 \n    8 \n    20 \n  \n  \n    7 \n    9 \n    15 \n    6 \n    20 \n    8 \n    100 \n    5 \n    10 \n    10 \n    9 \n    9 \n    13 \n    72 \n    8 \n    9 \n  \n  \n    8 \n    21 \n    12 \n    14 \n    14 \n    12 \n    12 \n    3 \n    11 \n    14 \n    13 \n    13 \n    14 \n    12 \n    12 \n    14 \n  \n  \n    9 \n    12 \n    9 \n    8 \n    9 \n    21 \n    6 \n    9 \n    71 \n    8 \n    8 \n    9 \n    8 \n    10 \n    9 \n    14 \n  \n  \n    10 \n    12 \n    16 \n    14 \n    6 \n    10 \n    12 \n    15 \n    10 \n    9 \n    17 \n    19 \n    7 \n    12 \n    9 \n    8 \n  \n\n\n\n\n\n(ref:purchase-test-cap) Similar to Table @ref(tab:some-results) but for the testing data.\n\n\n\nA valid evaluation of the performance of the system involves using the testing data rather than the training data. Figure @ref(fig:purchase-test) shows the assignment of coupons for the customers in the test data. Although coupons are assigned to these customers, the purchase rate for these items is only 16%, no different than the probability of purchase for no-coupon items. In other words, the coupon assignment system doesn’t work at all!"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#sources-of-false-discovery",
    "href": "Reading-notes/Reading-notes-lesson-38.html#sources-of-false-discovery",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "Sources of false discovery",
    "text": "Sources of false discovery\nHow did the coupon classifier system identify so many accidental patterns, patterns that existed in the training data but not in the testing data?\nOne source of false discovery stems from having multiple potential response variables. In the Potomac/Austin example, there were ten different classifiers at work, one for each of the ten Austin products. Even if the probability of finding an accidental pattern in one classifier is small, looking in ten different places dramatically increases the odds of finding something.\nSimilarly, having a large number of explanatory variables – we had 100 in the coupon classifier – provides many opportunities for false discovery. The probability of an accidental pattern between one outcome and one explanatory variable is small, but with many explanatory variables each being considered it’s much more likely to find something.\nA third source of false discovery at work in the coupon classifier relates to the family of models selected to implement the classifier. We used a tree model classifier capable of searching through the (many) explanatory variables to find ones that are associated with the response outcome. Unbridled, the tree model is capable of very fine stratification. Each coupon classifiers stratified the customers into about 200 levels. On average, then, there were about 50 customers in each strata. But there is variation, so many of the strata are much smaller, with ten or fewer customers. The small groups were constructed by the tree-building algorithm to have similar outcomes among the members, so it’s not surprising to see a very strong pattern in each group. For each classifier, about 15% of all customers fall into a strata with 20 or fewer customers.\nTo illustrate, Figure 3 shows the shape of the tree model for a typical coupon classifier. Each of the splits reflects an accidental alignment of the response variable with one of the explanatory variables. As more splits are made, the group of customers contained in the split becomes smaller. Many of the leaves on the tree contain just a handful of customers who accidentally had similar values for the several explanatory variables used in the splits.\n\n\n\n\n\nFigure 3: A sketch of one of the classifiers constructed for the coupon selection system. The tree-growing algorithm was allowed to keep going until the customer data was split up into very small strata.\n\n\n\n\nThe tree is too complex to be plausible as a real-world mechanism. None of the details in Figure 3 have any validity beyond the training data itself."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#identifying-false-discovery",
    "href": "Reading-notes/Reading-notes-lesson-38.html#identifying-false-discovery",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "Identifying false discovery",
    "text": "Identifying false discovery\nWe use data to build statistical models and systems such as the coupon-assignment machine. False discovery occurs when a pattern or model performance seen with one set of data does not generalize to other potential data sets.\nThe basic technique to avoid false discovery is called cross validation. One simple approach to cross validation splits the data frame into two randomly selected non-overlapping sets of rows: one for training and the other for testing. Use the training data to build the system. Use the testing data to evaluate the system’s performance.\nMost often, cross validation is used to test model prediction performance such as the root-mean-square error or the sensitivity and specificity of a classifier. This can be accomplished by taking the trained model and providing as input the explanatory variables from the testing data, then comparing the model output to the actual response variable values in the testing data. Note that using testing data in this way does not involve retraining the model on the testing data.\nHow big should the training set be compared to the testing set? For now, we’ll keep things simple and encourage use of a 50:50 split or something very close to that.\nThis is a simple and reliable approach that should always be used."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#false-discovery-and-multiple-testing",
    "href": "Reading-notes/Reading-notes-lesson-38.html#false-discovery-and-multiple-testing",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "False discovery and multiple testing",
    "text": "False discovery and multiple testing\nWhen the main interest is in an effect size, standard procedure calls for calculating a confidence interval on the effect. For example, a 2008 study examined the possible relationship between a woman’s diet before conception and the sex of the conceived child. The popular press was particularly taken by this result from the study:\n\nWomen producing male infants consumed more breakfast cereal than those with female infants. The odds ratio for a male infant was 1.87 (95% CI 1.31, 2.65) for women who consumed at least one bowl of breakfast cereal daily compared with those who ate less than or equal to one bowlful per week. [@fetal-sex-2008]\n\nThe model here is a classifier of the sex of the baby based on the amount of breakfast cereal eaten. The effect size tells the change in the odds of a male when the explanatory variable changes from one bowlful of cereal per week to one bowl per day (or more). This effect size is sensibly reported as a ratio of the two odds. A ratio bigger than one means that boys are more likely outcomes for the one-bowl-a-day potential mother than the one-bowl-a-week potential mother. The 95% confidence interval is given as 1.31 to 2.65. This confidence interval does not contain 1. In a conventional interpretation, this provides compelling evidence that the relationship between cereal consumption and sex is not a false pattern.\nBut the confidence interval is not the complete story. The authors are clear in stating their methodology: “Data of the 133 food items from our food frequency questionnaire were analysed, and we also performed additional analyses using broader food groups.” In other words, the authors had available more than 133 potential explanatory variables. For each of these explanatory variables, the study’s authors constructed a confidence interval on the odds ratio. Most of the confidence intervals included 1, providing no compelling evidence of a relationship between that food item and the sex of the conceived child. As it happens, breakfast cereal produced the confidence interval that was the most distant from an odds ratio of 1.\nLet’s look at the range of confidence intervals that can be found from studying 100 potential random variables that are each unrelated to the response variable. We’ll simulate a response randomly generated “sex” G and B where the odds of G is 1. Similarly, each explanatory variable will be a randomly generated “consumption” high or low where the odds of high is 1. A simple stratification of sex by consumption will generate the odds of G for those cases with consumption Y and also the odds of G for those cases with consumption N. Taking the ratio of these odds gives, naturally enough, the odds ratio. We can also calculate from the stratified data a 95% confidence interval on the odds ratio.\nSo that the results will be somewhat comparable to the results in @fetal-sex-2008, we’ll use a similar sample size, that is, n = 740. Table @ref(tab:sex-consumption-1) shows one trial of the simulation.\n\n\n\n\n\n\n(ref:sex-consumption-1-cap)\n\n\n\n\n\n\nhigh\n\n\nlow\n\n\n\n\n\n\nB\n\n\n165\n\n\n182\n\n\n\n\nG\n\n\n211\n\n\n182\n\n\n\n\n\n(ref:sex-consumption-1-cap) A stratification of sex outcome (B or G) on consumption (high or low) for one trial of the simulation described in the text.\nReferring to Table @ref(tab:sex-consumption-1), you can see that the odds of G when consumption is low is 182 / 182 = 1. The odds of G when consumption is high is 211/165 = 1.28. The 95% confidence interval on the odds ratio can be calculated. It is 0.95 to 1.73. Since that includes 1, the data underlying Table @ref(tab:sex-consumption-1) provide little or no evidence for a relationship between sex and consumption. This is exactly what we expect, since the simulation involves entirely random data.\nFigure 4 shows the 95% confidence interval on the odds ratio for 133 trials like that in Table @ref(tab:sex-consumption-1). The confidence interval from each trial is shown as a horizontal line. The large majority of them include 1. That’s to be expected because the data have been generated so that sex and consumption have no relationship except those arising by chance.\n\n\nWarning: geom_vline(): Ignoring `mapping` because `xintercept` was provided.\n\n\n\n\n\nFigure 4: Confidence intervals on the odds ratio comparing female and male birth rates for many trials of simulated data with no genuine relationship between the explanatory and response variables.\n\n\n\n\nNonetheless, out of 133 simulations there are six where the confidence interval does not include 1. These are shown in red. By necessity, one of the intervals will be the most extreme. If instead of numbering the simulations, we had labelled them with food items – e.g. grapefruit, breakfast cereal, toast – we would have a situation very similar to what seems to have happened in the sex-vs-food study. (For a more detailed analysis of the impact of multiple testing in @fetal-sex-2008, see @young-2009.)\nSuppose now that half of the data used in @fetal-sex-2008 had been held back as testing data. Using the training data, it would be an entirely legitimate practice to generate hypotheses about which specific food items might be related to the sex of the baby. The validity of any one selected hypothesis could then be established using the testing data without the ambiguity introduced by multiple testing. The testing data confidence interval can be taken at face value; the training data confidence interval cannot."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#example-organic-discovery",
    "href": "Reading-notes/Reading-notes-lesson-38.html#example-organic-discovery",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "Example: Organic discovery?",
    "text": "Example: Organic discovery?\nIt’s easy to find organic foods in many large grocery stores. Advocates of an organic diet are attracted by a view that it is sustainable, promotes small farms, and helps avoid contact with pesticides. There are also nay-sayers who make valid points, but that is not our purpose here. Informally, I find that many people and news reports point to the health benefits of an organic diet. Usually they believe that these benefits are an established fact.\nA 2018 New York Times article observed:\n\nPeople who buy organic food are usually convinced it’s better for their health, and they’re willing to pay dearly for it. But until now, evidence of the benefits of eating organic has been lacking. [@NYT-2018-10-23-Rabin]\n\nThe new evidence of health benefits is reported in an article in the Journal of the American Medical Association: Internal Medicine [@baudry-2018]\nDescribing the findings of the research, the Times article continued:\n\nEven after these adjustments [for covariates], the most frequent consumers of organic food had 76 percent fewer lymphomas, with 86 percent fewer non-Hodgkin’s lymphomas, and a 34 percent reduction in breast cancers that develop after menopause.\n\nThe study warrants being taken seriously: it involved about 70,000 French adults among whom 1340 cancers were noted. The summary of organic foot consumption was a scale from 0 to 32 and included 16 labeled products including dairy, meat and fish, eggs, coffee and tea, wine, vegetable oils, and sweets such as chocolate. Adjustment was made for a substantial number of covariates: age, sex, educational level, marital status, income, physical activity, smoking, alcohol intake, family history of cancer, body mass index, hormonal treatment for menopause, and others.\nYet … the reseach displays many of the features that can lead to false discovery. For instance, results were reported for four different types of cancer: breast, prostate, skin, lymphomas. The study reports p-values and hazard ratios1 comparing cancer rates among the four quartiles of the organic consumption index.\nComparing the most organic (average organic index 19.36/32) and the least organic (average index 0.72/32) groups the 95% confidence interval on the relative risk and p-values given in the study’s Table 4 are:\n\nBreast cancer: 0.66 - 1.16 (p = 0.38)\nProstate cancer: 0.61- 1.73 (p = 0.39)\nSkin cancer: 0.49 - 1.28 (p = 0.11)\nLymphomas: 0.07 - 0.69 (p = 0.05)\n\nYou might be surprised to see that the confidence interval on the relative risk for breast cancer includes 1.0, which suggests no evidence for an effect. As clearly stated in the report, the risk reduction for breast cancer is seen only in a subgroup of study participants: those who are postmenopausal. And even then, the confidence intervals continue to include 1.0:\n\nBreast cancer pre-menopausal: 0.67 - 1.52 (p = 0.85)\nBreast cancer post-menopausal: 0.53 - 1.18 (p = 0.18)\n\nSo where is the claimed 34% reduction in breast cancer cited in the New York Times article. It turns out the the study used two different indices of organic food consumption. The 0 to 32 scale which includes many items for which the amount consumed is very small (e.g., coffee, chocolate) and a “simplified, plant derived organic food score.” It’s only when you look at the full 0 to 32 scale that you see the reduction in post-menopausal breast cancer: the confidence interval is 0.45 to 0.96 (p = 0.03).\nWhat about cancer rates overall? For the 0 to 32 scale the risk ratio was 0.58 - 1.01 (p = 0.10). To see the claimed reduction clearly you need to look at the simplified food score which gives 0.63 - 0.89 (p < 0.005). And it’s only in comparing the highest-index quarter of participants with the lowerest quarter participants that any difference at all is seen in any type of cancer: the middle-half of participants show no difference in relative risk from the lowest-organic quarter of participants. (Because of this, had the study compared the highest quarter to the next highest quarter, they would have seen basically the same relative risks reported in the highest-to-lowest quarter comparison. Then the conclusion would have had a different flavor, perhaps to be reported as “Typical organic food consumption levels show no cancer benefits.”)\nA further source of potential false discovery stems from the study’s starting and stop times. It’s not clear that these were pre-defined; the reported results are intermediate to a longer follow up. The choice to report intermediate results is another way that the number of opportunities for false discovery is increased. And the choice is important: for the follow-up time used, about 2% of the participants developed cancer. In an earlier study of more than 600,000 middle-aged UK women (average age 59), the incidence of cancer was four times larger: 8.6%. [@bradbury-2014] That study did not find any relationship between organic food consumption and overall cancer rates, and found no relationship for 15 out of 16 different types of cancer. The exception is extremely interesting: non-Hodgkin lymphoma for which a similar result was found in the French study.\nSo is the study reported in the New York Times a matter of false discovery? It’s emotionally unsatisfying to discount a result about organic food and non-Hodgkin lymphoma simply because it’s part of a larger study that looked at many different combinations of cancer types and organic food indices. What if the researchers had only studied non-Hodgkin lymphoma – they would have gotten the same result and it wouldn’t have the deficiencies of being the strongest result of many possibilities. It would have stood on its own. But it doesn’t and we are left in a state of doubt."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#p_values",
    "href": "Reading-notes/Reading-notes-lesson-38.html#p_values",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "p-values and “significance”",
    "text": "p-values and “significance”\nFalse discovery is not a new problem. The traditional logic can be traced back to 1710, when John Arbuthnot was examining London birth records from 1629 to 1710. Arbuthnot was surprised to find that for each year males were more common than females. In interpreting this finding, Arbuthnot refered to the conventional wisdom that births of males and females are equally likely. If this were the case, in any one year there might, by chance, be more females than males or the other way around. While it’s theoretically possible that chance might produce the string of 82 years with more males, it’s very unlikely. “From whence it follows, that it is Art, not Chance, that governs,” Arburthnot wrote. In more modern language, Arburthnot concluded that the hypothesis of equal rates of male and female births was not consistent with the data. Arbuthnot’s “Chance” corresponds to false discovery, while “Art” is a valid discovery.\nArburthnot’s logic became a standard component of statistical method.\n\nSummarize the data into a single number called a “test statistic”. For Arburthnot the test statistic was the number of years where male births predominated, out of the 82 years being examined. The observed value of the test statistic was 82.\nState a “null hypothesis”, typically something that is the conventional wisdom. For Arburthnot, the null hypothesis was that male and female births are equally likely.\nCalculate a hypothetical quantity based on the null hypothesis: the probability that the test statistic produced in a world in which the null hypothesis holds true would be at least as large as the test statistic.\nIf the probability in (3) is small, one is entitled to “reject the null hypothesis.” Typically, “small” is defined as 0.05 or less.\n\nIn the 1890s, statistical pioneer Karl Pearson invented a test statistic he called \\(\\chi^2\\) (“chi”-squared, with “chi” pronounced “ki” as in “kite”) that can be applied in a variety of settings. In 1900, Pearson published a table [@pearson-1900] that makes it an easy matter to look up the probability in step (3) above. He called this theoretical probability “P”, a name that has stuck but is conventionally written as lower-case “p”.\nData scientists tend to work with “big data”, but for many applications of statistics, data is so scarce that use of separate training and testing data is impractical. For such small data, the calculation of a p-value can be a sensible guard against false discovery. Still, a p-value does not address any of the sources of false discovery outlined in the previous sections of this chapter. When used with small data and simple modeling methods, those sources of false discovery are not so much of a problem. In small data there won’t be multiple explanatory variables that can be searched and there won’t be a choice of response variables. This doesn’t eliminate all problems, since in small data results can depend critically on the inclusion or exclusion of a single row of data. The name “p hacking” has been given to the various ways that researchers can manipulate p-values to get them below 0.05.\nAnother problem with p-values stems from misinterpretation of the admittedly difficult logic that underlies them. The misinterpretations are encouraged by the use of the term “tests of significance” to the p-value method. Particularly galling is the use of the description “statistically significant” to describe a result where p < 0.05. The everyday meaning of “significant” as something of importance is in no way justified by p < 0.05. Instead, the practical importance or not is more clearly signaled by examining an effect size. (It’s extremely disappointing that journalists, who are writing for an audience that for the most part has no understanding of p-value methodology, use “significant” when reporting on the statistics of research findings. It would be more honest to use a neutral term such as “null-validated” or “p-validated” which does not confuse the statistical result with actual practical importance.)\nThe p-value methodology has little or nothing to contribute to data science practice. When data is big there is a much more straightforward method to guard against false discovery: cross validation. And when data is big there is another, more fundamental problem with p-values. They are calculated with reference to a specific null hypothesis of “no effect” or “no relationship.” More realistically, they should be calculated with respect to a hypothesis of “trivial (but potentially non-zero) effect”. There are all sorts of mechanisms in the world (such as common causes) that can create the appearance of some effect or relationship. No matter how trivial in size this is, with sufficient data the p-value will become small. To illustrate, Figure 5 shows the p-value as a function of the sample size n in a system with an R-squared of 0.001, which in most settings would be of no practical signficance.\n\n\nWarning: geom_hline(): Ignoring `mapping` because `yintercept` was provided.\n\n\n\n\n\nFigure 5: The p-value as a function of sample size n when the test statistic R-squared has the trivial value 0.001. The horizontal line shows the usual threshold for “significance” of p < 0.05."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-38.html#notes-in-draft",
    "href": "Reading-notes/Reading-notes-lesson-38.html#notes-in-draft",
    "title": "Math 300R Lesson 38 Reading Notes",
    "section": "NOTES IN DRAFT",
    "text": "NOTES IN DRAFT\n“Statistical crisis” in science\nhttps://www.americanscientist.org/article/the-statistical-crisis-in-science\nGarden of the Forking Paths\nIonedes"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-21.html",
    "href": "Reading-notes/Reading-notes-lesson-21.html",
    "title": "Math 300R Lesson 21 Reading Notes",
    "section": "",
    "text": "The theme of this second half of the course is one word: “inference.” To illustrate how inference is distinct from the data science methods—wrangling, visualization, modeling—you have been learning, consider Figure 1, copied from a well-regarded data-science text.\nYou’ve been to all the stepping stones on this path (with “transform” meaning “wrangling”), except the one on the far right: “Communicate.” I’d prefer if “communicate” were replaced with “inform.” One of the main uses of data science is to extract from data information that is useful for decision-making. The best information is that which reflects the true state of the world.\nStatistical inference is the body of concepts and techniques that help us be careful and responsible that our statistical results do actually reflect the true state of the world insofar as the data can tell us about it.\nExample: Imagine a salesperson bargaining with a customer. Successful salespeople want to have some idea about how much money the customer would be willing to spend. They try to predict this based on observations they can make about the customer. These might include the kind of car the customer drives, how the customer is dressed, age of the customer, etc. You might think that the prediction takes the form of a quantity: the amount of money. But it’s better if the prediction is in the form of a range. After all, the relationship between willingness to pay and the observed customer attributes is weak and uncertain. It’s not good if the prediction suggests more precision than is actually warranted.\nExample: Nutritionists are interested in helping people to make diet choices that will increase health. Often they do this by collecting data on what people eat and their health outcomes. A regression model might indicate, for instance, that “organic” food is associated with a reduction in risk of an illness by, say, 20%. Skeptics could point out that “correlation is not causation,” and that other factors, say family income, account for the association. To inform the consumers decision about spending on food, it’s helpful if the data modeling is structured in a way that can provide confidence that the link between food and health is causal.\nSince inference involves the relationship between the resulted gleaned from models and the state of the world, it’s helpful to have situations where the state of the world is well known. We are going to provide such situations in a simple, sure way: by simulation.\nTo emphasize that the simulation is only for developing an understanding of inference, and not for depicting the real world, we’ll use the word “gaming” to refer to using such simulations. The full process of gaming has four stages.\nWE’RE GOING TO USE SIMPLE NAMES like X, Y, A, B, C just to remind you that this is a simulation. Real-world data will usually have descriptive variable names, like height, age, score."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-21.html#sec-signal-and-noise",
    "href": "Reading-notes/Reading-notes-lesson-21.html#sec-signal-and-noise",
    "title": "Math 300R Lesson 21 Reading Notes",
    "section": "Signal and noise",
    "text": "Signal and noise\nA useful distinction is made in statistics—as it is in engineering—between signal and noise. For instance, it is usually assumed that response variables consist of a part that stems deterministically from the rest of the system (“signal”) and a random part (“noise”).\nAn important task in statistical modeling is to estimate the size of the noise. We’ll need to develop some statistical techniques to do this with real world data, but we can illustrate the ideas with DAG games.\nConsider the mechanism represented by dag01:\n\ndag01\n\n[[1]]\nx ~ eps()\n\n[[2]]\ny ~ 1.5 * x + 4 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\ndag_draw(dag01)\n\n\n\n\ndag01 has two variables, x and y. The tilde expression x ~ eps() means that x is generated entirely as random noise; that’s what the eps() function does. In contrast, the tilde expression y ~ 1.5 * x + 4 + eps() means that variable y will be composed of a part that depends on x and another part that is random noise.\n\n“depends on x”: 1.5*x + 4\n“random noise”: eps()\n\nIn contrast, the dag00 system involves two variables that are not connected in any way.\n\ndag00\n\n[[1]]\nx ~ eps(2) + 5\n\n[[2]]\ny ~ eps(1) - 7\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\ndag_draw(dag00)\n\n\n\n\n\n\n\n\n\n\neps() is a process\n\n\n\nThe idea of randomness is central to statistics. In simulating data from a DAG, we use a variety of random number generators. One of the most often used generators appears in the tilde expressions as eps().\nFrom the notation, you can see that eps() is an R function. By default, it is used without an input, just the bare () with nothing inside. So what could the output be?\nA random number generator, like eps(), produces its output as if from nothing. (There’s a lot of sophisticated mathematics behind this “nothing” but that doesn’t concern us here.) Each time eps() is used, it makes a new set of random numbers that are not connected to any previous output created by eps(). It’s best to think about random number generators like eps() as a process, for instance, rolls of dice or spins of a lottery wheel.\nBy default, the “size” of the random noise produced by eps() is 1. Some DAGs, such as dag01, explicitly specify the size of the noise. For instance, eps(2) means the noise will have size 2, eps(0.17) means the noise will have size 0.17."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-34.html",
    "href": "Reading-notes/Reading-notes-lesson-34.html",
    "title": "Math 300R Lesson 35 Reading Notes",
    "section": "",
    "text": "We all face many yes/no situations. A patient has a disease or does not. A credit card transaction is genuine or fraudulent. A classifier is a statistical model designed to predict the unknown outcome of a yes/no situation from information that is already available.\nConsider a credit-card company might building a classifier to predict at the time of the transaction whether a purchase of gasoline is fraudulent. The company knows how often and how much gasoline the individual cardholders buys, where the cardholder lives, whether the cardholder travels extensively, typical times of day for a purchase, and so on. Feature engineering is the process of using existing data—including, in our example, whether the purchase turned out to be fraudulent—to develop potential markers or signals of the outcome. For simplicity, imagine the features selected are the number of days since the last gasoline purchase and the distance from the last place of purchase.\nOnce potential features have been proposed, the engineers building the classifier assemble training and testing data sets. Suppose, for the purpose of illustration, that the training data has 2000 fraudulent transactions and 4000 non-fraudulent ones, and the testing set is about the same.\nThe word “assemble” was used intentionally to describe how the testing and training data were collected: a case-control study. Since the objective is to detect fraud, it is reasonable to have a lot of “yes” cases in the data. The “no” cases serve as a kind of control; they were included specifically to have balance in the data. If data had been collected as a simple random sample of credit card transactions, there would have been many, many more “no” cases than “yes.”\nWith such training data it is easy to build a statistical model with Fraud as the response variable. That model can then be evaluated on the testing data to produce a model output for each row:\nIt’s understandable that a classifier may not have perfect performance. After all, it iss trying to make a prediction based on limited data, and randomness may play a role.\nThere are different ways of making a mistake, and these different ways have very different consequences. One kind of mistake, called a “false positive”, involves a classifier output that’s positive (i.e. the classifier indicates fraud) but which is wrong. The consequence of this sort of mistake in the present example is a customer who has to find another way to pay for gasoline.\nThe other kind of mistake is called a “false negative”. Here, the classifer output is that the transaction is not fraudulent, but in actuality it was. The consequence of this kind of mistake is different: a successful theft.\nThe nomenclature signals that a mistake has been made with the word “false.” The kind of mistake is either “positive” or “negative”, corresponding to the output of the classifier.\nWhen the classifier gets things right, that is a “true” result. As with the false results, a true result is possible both for a “positive” and a “negative” classifier output. So the two ways of getting things right are called “true positive” and “true negative”.\nTabulating all 6000 rows of the testing data might produce something like this:"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-34.html#incidence",
    "href": "Reading-notes/Reading-notes-lesson-34.html#incidence",
    "title": "Math 300R Lesson 35 Reading Notes",
    "section": "Incidence",
    "text": "Incidence"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-34.html#sensitivity-and-specificity",
    "href": "Reading-notes/Reading-notes-lesson-34.html#sensitivity-and-specificity",
    "title": "Math 300R Lesson 35 Reading Notes",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\nExample: Accuracy of airport security screening\n\n\n\nAirplane passengers have, for decades, gone through a security screening process involving identity checks, “no fly” lists, metal detection, imaging of baggage, random pat-downs, and such. How accurate is such screening? Almost certainly, the accuracy is not as good as an extremely simple, no-input, alternative process: automatically identify every passenger as “not a security problem.” We can estimate the accuracy of the “not a security problem” classifier by guessing what fraction of airplane passengers are indeed a threat to aircraft. In the US alone, there are about 2.5 million airplane passengers each day and security problems of any sort rarely happen. So the accuracy of the no-input classifier is something like 99.999%.\nThe actual screening system, using metal detectors, baggage x-rays, etc. will have a lower accuracy. We know this since it regularly mis-identifies innocent people as security problems.\nThe problem here is not with airport security screening, but with the flawed use of accuracy as a measure of performance. Indeed, achieving super-high accuracy is not the objective of the security screening process. Instead, the objective is to deter security problems by convincing potential terrorists that they are likely to get caught before they can get on a plane. This has to do with the sensitivity of the system. The specificity of the system, although important to the everyday traveller, is not what deters the terrorist."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html",
    "href": "Reading-notes/Reading-notes-lesson-20.html",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "",
    "text": "A common task in statistical modeling is to break down a variable into components. For instance, a person’s height or intelligence or charm is presumably a combination of genetics and environment. In doing this breaking down, it’s convenient to be able to characterize the size of each component.\nThere are many possible ways to measure “size.” In this course, we will emphasize two, intimately related measures:\n\nvariance\nstandard deviation, which is simply the square root of variance.\n\nBoth variance and standard deviation are quantities, that is, a single number with associated units. The standard deviation of any variable has units that are exactly the same as the variable itself. For instance, the measured heights of a group of people is often measured in cm. So the units of the standard deviation of height will also be in cm.\nIn contrast, the variance, being the square of standard deviation, has units of the square of the units of the variable. The variance of height, for instance, will be measured in cm2. This will seem odd at first glance, but you have to get used to it: the variance of a variable has units that are the square of the units of the variable itself.\nKeep in mind also that variance and standard deviation are summaries of a variable. A variable in a data frame consists of multiple values, one for each row of the data frame. The variance or standard deviation of that variable will be just a single number, summarizing all of the values in the variable.\nAlmost always, people use software to do the calculations. The relevant R functions are sd() and var(). You can use these functions in a summarize() statement, for instance\n\nmtcars %>%\n  summarize(v = var(hp))\n\n         v\n1 4700.867\n\nmtcars %>% \n  summarize(s = sd(hp))\n\n         s\n1 68.56287\n\n\nRegrettably, the software does not indicate the units of the quantity. For that, you need to determine the units of the variable itself, typically by reading the documentation for the data frame.\n\n\n\n\n\n\nNote in draft\n\n\n\nVariance and standard deviation are about the average distance between values of the variable.\nAnother way to think about variance is as the average distance from the mean."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-20.html#using-do",
    "href": "Reading-notes/Reading-notes-lesson-20.html#using-do",
    "title": "Math 300R Lesson 20 Reading Notes",
    "section": "Using do( )",
    "text": "Using do( )\nTHERE’s a matter of the order of precedence of the operations in a pipe. Use the CURLY BRACES in the same way you would use parentheses in an arithmetic expression.\nWHY CURLY BRACES? Because parentheses mean something else in R: either application of a function to arguments or the usual arithmetic meaning."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html",
    "href": "Reading-notes/Reading-notes-lesson-23.html",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "",
    "text": "Case : a row in a data frame\nSample: a data frame.\nSummarized sample: lm(model, data=dataframe) %>% summary()\n\nThis is as far as we can go with real data. DAG simulation (gaming) let us go further:\n\nSample: a draw of \\(n\\) cases from the DAG: sample(DAG, size=50)\nTrial: a summarized sample: r trial <- function(n=50) {   lm(tilde, data=sample(DAG, size=n) %>% summary() }\nRepeated trials to see sampling:distribution: do(100) * trial()\nSummarize the repeated trials: e.g. standard deviation of trial-by-trial coefficients r     do(100) * trial() %>% summarize(sd = sd(coef_on_x)) We found that the standard deviation of trial-by-trial coefficient\n\n\nIs smaller if \\(n\\) is bigger.\nSpecifically, is proportional to \\(\\frac{1}{\\sqrt{n}}\\).\n\n\n\n\n\n\n\nFormulas for sampling distributions\n\n\n\nStatistics textbooks often give formulas for the standard deviation of the sampling distribution. The formulas have been constructed for many of the standard situations. To give you an idea of how this is done, let’s go over the very simplest situation.\nSystem: \\(\\epsilon \\longrightarrow y\\) with tilde xy~ 1.\nInterpretation: The coefficient on 1, that is, the “intercept” is an estimate of the mean of \\(y\\).\nTHIS IS JUST A SKETCH.\nWhen \\(n=1\\), that is, the mean of a sample of size \\(n=1\\), the standard deviation of the sampling distribution is just \\(sd(y)\\). Of course, we can’t estimate this from a single sample of size \\(n=1\\) because we need at least \\(n=2\\) to calculate a standard deviation. If we knew the DAG behind the data, we could read \\(sd(y)\\) from the DAG. Let’s imagine that we do and use the name \\(\\sigmal\\) for the standard deviation from many trials on the DAG. But if we have \\(n > 1\\), we could calculate the SD from the sample. Let’s call that \\(s\\), our estimate of sigma.\nWe also know that the SD of the sampling distribution scales as \\(\\frac{1}{n}\\).\n\n\n\nSample size\nSD of Intercept coefficient\n\n\n\n\n\\(n=1\\)\n\\(\\sigma\\) as stipulated\n\n\n\\(n=2\\)\n\\(\\sigma/\\sqrt{2}\\) estimated by \\(s/\\sqrt{2}\\)\n\n\n\\(n=3\\)\n\\(\\sigma/\\sqrt{3}\\) estimated by \\(s/\\sqrt{3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(\\sigma/\\sqrt{n}\\) estimated by \\(s/\\sqrt{n}\\)\n\n\n\nGIVE FORMULAS for y ~ 1, y ~ yesno, y ~ x\nThe challenge faced by the traditional statistics student is to look up the correct formula for the situation at hand. But the computer can figure out the right formula directly from the tilde model."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#bootstrapping",
    "href": "Reading-notes/Reading-notes-lesson-23.html#bootstrapping",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#regression-table",
    "href": "Reading-notes/Reading-notes-lesson-23.html#regression-table",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Regression table",
    "text": "Regression table"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-23.html#is-sampling-variation-the-issue",
    "href": "Reading-notes/Reading-notes-lesson-23.html#is-sampling-variation-the-issue",
    "title": "Math 300R Lesson 23 Reading Notes",
    "section": "Is sampling variation the issue?",
    "text": "Is sampling variation the issue?\n\nFrom the 2018 StatPREP newsletter\n\nIn 1996 my department chair handed me the first statistics textbook I had ever seen. That single gesture constituted my college’s faculty development program for teaching statistics. One of the earliest examples in the book was about the importance of random sampling. It included a picture of President Truman holding up the Chicago Tribune’s infamous “Dewey Defeats Truman” headline. It’s a good story, but hardly timely, having taken place 48 years earlier. Few of my students knew who Truman was and none of them knew anything about Dewey.\nOur students have grown up in an era of “scientific” polling. Being scientific, the results are reported with a margin of error, often ±3 percentage points, to help us know when conclusions are warranted and when not. Many of our statistics courses feature units on constructing a margin of error on a sample proportion, often with explicit reference to political polls. But, like Dewey defeating Truman, the story is no longer timely. The “error” in the “margin of error” is now only a small part of the unreliability of polls. Why?\nIn an unprecedented opening up of the process of polling, The New York Times is letting us observe, live, their polling for the 2018 mid-term elections. You’ll find a description of the project in a September 2018 column and the live action here. It’s worth watching.\nFor those of you reading this after the polling ends, I’ll describe the action. As I write this, 2,070,469 telephone calls have been made. In each Congressional district, the results from the past calls are laid out in a long line of circles, filled red or blue depending on the the recipient’s response. But only 1 or 2% of the dots are filled. The large majority are empty: no response. Each new call generates a wiggling box at the head of the line of dots. It wiggles until the end of the call. Almost always, the box turns into an unfilled circle.\nThe poll I’m watching now, New Jersey 3rd district, is in its early stage. 4250 calls producing 62 responses. The margin of error? There’s a simple but meaningful statement laid right on top of the grayed-out tally so far: “Don’t take this poll seriously until we reach at least 250 people. We’re at 62.”\nThe calls are made based on a random selection from the phone numbers known to be in the district. But the random selection hardly generates a random sample when the response rate is 2%. To get something that resembles the population, pollsters weight their results. The New York Times is weighting “by age, party registration, gender, likelihood of voting, race, education and region, mainly using data from voting records files compiled by L2, a nonpartisan voter file vendor.” And then there’s the “likely voter” model, an informed guess about what fraction of people in each weighting strata will actually vote. There’s a detailed explanation in this article on the site, where the faulty results from the 2016 presidential election are attributed to a failure to weight by education level.\nSeeing the polling process in such detail reveals our misconceptions about what’s important in statistics. The so-called “margin of error” is not an adequate indicator of the reliability of the poll. Instead, we need to be thinking about the factors used in weighting and the extent to which they capture the current configuration of political schisms. Polls are now about big, multivariable data (the “voting records compiled by L2”) and building models of turnout based on previous elections."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-33.html",
    "href": "Reading-notes/Reading-notes-lesson-33.html",
    "title": "Math 300R Lesson 33 Reading Notes",
    "section": "",
    "text": "[From The Model Thinker, p. 52]\nGain Framing: You have two options\nOption A) Win $400 for certain\nOption B) Win $1000 if a fair coin comes up heads and $0 if tails\nLoss Framing: You are given $1000 and have two options:\nOption a) Lose $600 for certain\nOption b) Lose $0 if a fair coin comes up heads and lose $1000 if tails.\nHyperbolic discounting: see pp 52-43\n“Prospect theory”, Kahneman and Tversky (1979) “Prospect theory: an analysis of decisions under risk,” Econometrica 47(2):263-291 link to paper"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-24.html",
    "href": "Reading-notes/Reading-notes-lesson-24.html",
    "title": "Math 300R Lesson 24 Reading Notes",
    "section": "",
    "text": "SHOW makeFun() applied to a model producing a model function.\nEffect size of an input is partial derivative of model function with respect to that input.\nEffect size is a rate: the change in output per unit change in input. It’s a measure of the size of a relationship."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-30.html",
    "href": "Reading-notes/Reading-notes-lesson-30.html",
    "title": "Math 300R Lesson 30 Reading Notes",
    "section": "",
    "text": "Causal Caution\n\n\n\nJust because you’ve calculated an effect size doesn’t mean that you have captured any sort of causal relationship between the variables. To illustrate, use dag01 and fit two different models: y ~ x and x ~ y.\n\nSample <- sample(dag01, size=500)\nlm(y ~ x, data = Sample)\n\n\nCall:\nlm(formula = y ~ x, data = Sample)\n\nCoefficients:\n(Intercept)            x  \n      4.071        1.515  \n\nlm(x ~ y, data = Sample)\n\n\nCall:\nlm(formula = x ~ y, data = Sample)\n\nCoefficients:\n(Intercept)            y  \n    -1.9022       0.4712  \n\n\nYou can’t tell from these coefficients whether x causes y or vice versa (or something entirely different). The words “correlation” or “association” are used when we don’t want to claim that there is a causal connection. Many statisticians will only use those words unless the data come from an experiment.\nWe’re going to use causal language (“relationship”, “effect,” etc.) because that is often the matter of concern to decision making. But using language doesn’t doesn’t make the connection causal."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html",
    "href": "Reading-notes/Reading-notes-lesson-25.html",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "",
    "text": "Use mod_eval()"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#a-model-as-function",
    "href": "Reading-notes/Reading-notes-lesson-25.html#a-model-as-function",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "A model as function",
    "text": "A model as function\nInconvenient because functions don’t know how to translate from the individual arguments to values taken from a data frame."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#feeding-inputs-into-a-model",
    "href": "Reading-notes/Reading-notes-lesson-25.html#feeding-inputs-into-a-model",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "Feeding inputs into a model",
    "text": "Feeding inputs into a model"
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-25.html#drawing-a-model-function",
    "href": "Reading-notes/Reading-notes-lesson-25.html#drawing-a-model-function",
    "title": "Math 300R Lesson 25 Reading Notes",
    "section": "Drawing a model function",
    "text": "Drawing a model function\nEvaluate the model at many inputs, then plot out model_output versus input.\nmod_plot() automates this."
  },
  {
    "objectID": "objective-list.html",
    "href": "objective-list.html",
    "title": "List of objectives",
    "section": "",
    "text": "19.1 Distinguish between the two settings for decision-making:\n\nPrediction: predict an outcome for an individual\nRelationship: characterize a relationship with an eye toward intervention or a better understanding of how a mechanism works.\n\n19.2 Given a research question, identify whether it corresponds to a prediction setting or a relationship setting.\n\n20.1. Understand that gaming is a way of improving our skills and identifying potential opportunities and problems.\n20.2 Characterize the “size” of a variable or of random noise using variance (or, equivalently, “standard deviation”).\n20.3 Distinguish between a sample, a summary of a sample, and a sample of summaries of samples.\n\n21.1 Determine whether a proposed graph is directed and acyclic.\n21.2 Having selected a response and one or more explanatory variables, identify other DAG notes as covariates.\n21.3 Generate data from simulations and use the data to model the relationships.\n\n22.1 Implement on the computer a procedure to generate a sample, calculate a regression model, and produce a summary.\n22.2 Iterate the procedure and collect the summaries across iterations. This collection is called the “sampling distribution.”\n22.3 Graphically display the distribution of summaries and generate a compact numerical description (“confidence interval”) of the sampling distribution.\n\n23.1 Use bootstrapping to estimate sampling variation.\n23.2 Infer sampling variation from a regression table: “standard error” of a model coefficient.\n23.3 Construct and interpret confidence intervals on a model coefficient.\n23.4. Understand and use scaling of confidence interval length as a function of \\(n\\).\n\n24.1 Estimate an effect size from a regression model of one and two variables.\n24.2 Construct a confidence interval on the effect size.\n24.3. Gaming: Evaluate whether confidence interval indicates that estimated effect size is consistent with simulation.\n\n25.1 Given a sample from a DAG simulation, construct a predictor function for a specified response variable.\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error.\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error.\n\n26.1 In evaluating a model function, generate a prediction interval.\n26.2 Interpret prediction bands as a series of intervals, one for each value of the model input.\n26.3 Identify the two components that make up a prediction error, one that scales with \\(n\\) and the other that doesn’t.\n\nThis is a QR day.\n\n28.1 Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n28.2 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)\n\n29.1 Correctly define “covariate”.\n29.2 Understand why including covariates—even spurious ones—always improves the appearance of model performance in in-sample testing.\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n\n30.1 Identify confounding in a DAG\n30.2 Choose whether to include covariate depending on form of DAG\n\n31.1 Distinguish “common cause” and “collider” forms of DAG.\n31.2 Construct appropriate DAG to match a narrative hypothesis.\n\n32.1 Properly use nomenclature of experiment.\n32.2 Correctly re-draw DAG for an ideal experimental intervention.\n32.3 Use blocking to set assignment to treatment or control.\n\n33.1. Distinguish between absolute and relative risk and identify when a change in risk is being presented as absolute or relative.\n33.2. Calculate and correctly interpret other presentations of differences in risk: population attributable fraction, NTT, odds ratio.\n33.3. Interpret effect size as stated in log odds.\n\n34.1. Build a classifier from case-control data.\n34.2. Cross-tabulate classifier results versus true state. Evaluate false-positive rate, false-negative rate, accuracy.\n34.3. Calculate different forms of conditional probability: p(A|B) versus p(B|A) and identify which form of conditional probability is useful for prediction of an individual’s outcome.\n\n35.1 Explain why case-control data may not give an proper measure of “prevalence.”\n35.2 Understand sensitivity and specificity as conditional probabilities.\n35.3 Calculate false-positive and false-negative rates for a given prevalence.\n\n36.1 Understand and use properly hypothesis testing nomenclature: test statistic, sampling distribution under the null, Type-1 and Type-2 error, rejection threshold, p-value\n36.2 Contrast hypothesis testing versus Bayesian framework.\n\n37.1 The permutation test\n37.2 Interpret correctly from regression/ANOVA reports\n37.3 Traditional names for hypothesis tests in different “textbook” settings.\n37.4. Distinguish between p-value and effect size, that is, “significance” and “substance.”\n\n38.1 Identify signs of false discovery in a research paper.\n38.2 Estimate how overall p-value should change when study is replicated."
  },
  {
    "objectID": "lessons.html",
    "href": "lessons.html",
    "title": "Math 300R day-by-day Lessons",
    "section": "",
    "text": "See Fall 2022 repository.\nData, graphics, wrangling\n\nData with R\nScatterplots\nLinegraphs, histograms, facets\nBoxplots and barcharts\nfilter and summarize\ngroup_by, mutate, arrange\njoin, select, rename, & top n\nImporting data\nCase study/review\nGR1 (chapters 1-4)\n\nRegression\n\nSLR: Continuous x\nSLR: Discrete x\nSLR: Related topics\nMultiple regression: Numerical & discrete\nMultiple regression: Two numerical\nMultiple regression: Related topics\nMultiple regression: Conclusion/review\nGR 2 (chapters 5-6)"
  },
  {
    "objectID": "lessons.html#new-lessons",
    "href": "lessons.html#new-lessons",
    "title": "Math 300R day-by-day Lessons",
    "section": "New lessons",
    "text": "New lessons\nVariation\n\nDecisions with data NTI : Objectives : LC : Reading\nReality versus gaming NTI : Objectives : LC : Reading\nDAGs, noise, and simulation NTI : Objectives : LC : Reading\nSampling variation NTI : Objectives : LC : Reading\nEstimate sampling variation from a single sample NTI : Objectives : LC : Reading\nEffect size NTI : Objectives : LC : Reading\nMechanics of prediction NTI : Objectives : LC : Reading\nConstructing a prediction interval NTI : Objectives : LC : Reading\nGR 3 (Lessons 19-26)\n\nInference\n\nCovariates NTI : Objectives : LC : Reading\nCovariates eat variance NTI : Objectives : LC : Reading\nConfounding NTI : Objectives : LC : Reading\nNon-causal correlation NTI : Objectives : LC : Reading\nExperiment and random assignment NTI : Objectives : LC : Reading\nMeasuring and accumulating risk NTI : Objectives : LC : Reading\nConstructing a classifier NTI : Objectives : LC : Reading\nAccounting for prevalence NTI : Objectives : LC : Reading\nHypothesis testing NTI : Objectives : LC : Reading\nCalculating a p-value NTI : Objectives : LC : Reading\nFalse discovery with hypothesis testing NTI : Objectives : LC : Reading\nGR 4 (lessons 28-38)\nReview"
  },
  {
    "objectID": "layout.html",
    "href": "layout.html",
    "title": "Layout of this site",
    "section": "",
    "text": "As of October 2022, the revisions are for lessons 19-37. Revisions, if any, to earlier lessons may be added. Eventually, the revised lessons and the unrevised lessons should be consolidated into a single site. This might occur in December 2022.\nThere are four main directories. In each directory, the contents are split up on a lesson-by-lesson basis.\n\nObjectives: Student-facing learning goals that make explicit the skills and understandings that students are expected to develop in the course of each lesson. These files are formatted in a specific manner that provides an ID to each objective. Software reads the files so that each objective can be referred to in any document without duplication. That is, the files in this Objectives directory are the source of truth for the objectives for each of the revised lessons. Any edits or additions to the objectives must be performed in the files in this directory.\nNTI: Lesson-by-lesson “Notes To Instructors.” These are intended to guide instructors through each lesson. They usually contain references to the objectives stored in the Objectives directory. In draft form, the NTIs typically contain notes that are to be moved eventually to the “Reading notes.”\nLC: Like Objectives, this directory contains lesson-by-lesson exercises, styled “learning checks” in the style of the Statistical Inference via Data Science textbook. Not all the learning checks need to be assigned. Which ones are assigned for each lesson will be noted in the NTIs.\nReading-notes: Textbook-like readings, also organized lesson-by-lesson (as opposed to the chapter organization typically found in textbooks.) Often, several lessons in sequence refer to the same statistical topic. Nonetheless, the reading notes are divided on a lesson basis. It is anticipated that these will be provided to students in an on-line format."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html",
    "href": "Reading-notes/Reading-notes-lesson-28.html",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "",
    "text": "Our method for modeling a real-world system is to identify a single response variable and treat that variable as a function of one, several, or many explanatory variables. Effect size (Lesson 24) lets you focus attention on an individual explanatory variable and how changes in that explanatory variable correspond to changes in the response variable.\nInsofar as the logic of effect size is to isolate a single explanatory variable of interest, you might wonder why not build a model that conditions the response variable just on that one explanatory variable? As you’ll see, this is generally the wrong way to go about things. The role of each explanatory variable takes place in a context set by other explanatory factors.\nThe context-setting factors are called “covariates”. In the sense of data, covariates are perfectly ordinary variables. They are called covariates only to highlight the role of these variables for putting in context some other explanatory variables of particular interest to the modeler."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#example-covariates-and-context-in-educational-outcomes",
    "href": "Reading-notes/Reading-notes-lesson-28.html#example-covariates-and-context-in-educational-outcomes",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Example: Covariates and context in educational outcomes",
    "text": "Example: Covariates and context in educational outcomes\nTo illustrate how covariates set context, consider an issue of interest to public policy-makers in many societies: How much money to spend on children’s education? In the United States, for instance, educational budget policy is set mainly on a state-by-state level. State lawmakers are understandably concerned with the quality of the public education provided, but they also have other concerns and constraints and constituencies who give budget priority to other matters.\nIn evaluating the various trade-offs they face, lawmakers would be helped by knowing how increased educational spending will shape educational outcomes. What can available data tell us? Unfortunately, there are various political constraints that work against states adopting and publishing data on a common measure of genuine educational outcome. Instead, we have high-school graduation rates, student grades, etc. These have some genuine meaning but also can reflect the way the system is gamed by administrators and teachers and which cannot be easily compared across states. At a national level, we have college admissions tests such as the ACT and SAT. Perhaps because these tests are administered by private organizations and not state governments, it’s possible to gather data on test-score outcomes on a state-by-state basis and collate these with public spending information.\nFigure 1 shows average SAT score in 2010 in each state versus expenditures per pupil in public elementary and secondary schools. Laid on top of the data is a flexible linear model (and its confidence band) of SAT score versus expenditure. The overall impression given by the model is that the relationship is negative, with lower expenditures corresponding to higher SAT scores. But the confidence bands are broad and it is possible to find a smooth path through the confidence band that has almost zero slope. Either way, the conventional wisdom that higher spending produces better school outcomes is not supported by this graph.\n\n\n\n\n\nFigure 1: State by state data (from 2010) on average score on the SAT college admissions test and expenditures for public education.\n\n\n\n\nThere are other factors that play a role in shaping education outcomes: poverty levels, parental education, how the educational money is spent (higher pay for teachers or smaller class sizes? administrative bloat?), and so on. Modeling educational outcomes solely by expenditures ignores these other factors.\nAt first glance, it’s tempting to ignore these additional factors. We may not have data on them. And insofar as our interest is in understanding the relationship between expenditures and education outcomes, we are not directly concerned with the additional factors. This lack of direct concern, however, doesn’t imply that we should totally ignore them but that we should do what we can to “hold them constant”.\nTo illustrate, let’s consider a factor on which we do have data: the fraction of eligible students (those in their last year of high school) who actually take the test. This varies widely from state to state. In a poor state where few students go to college the fraction can be very small (Alabama 8%, Arkansas 5%, Mississippi 4%, Louisiana 8%). In some states, the large majority of students take the SAT (Maine 93%, Massachusetts 89%, New York 89%). In states with low SAT participation rates, the students who do take the test are applying to schools with competitive admissions. Such strong students can be expected to be get high scores. In contrast, the scores in states with high participation rates reflect both strong and weak students; they will be lower on average than in the low-participation states.\nPutting the relationship between expenditure and SAT scores in the context of the fraction taking the SAT can be done by using fraction as a co-variate, that is, building the model SAT ~ expenditure + fraction rather than just SAT ~ expenditure. Figure 2) shows a model with fraction taken into account.\n\n\n\n\n\nFigure 2: The model of SAT score versus expenditures, including as a covariate the fraction of eligible students in the state who take the SAT.\n\n\n\n\nNote that the effect size of spending on SAT scores is positive when the expenditure level is less than $10,000 per pupil. And notice that when the fraction taking the SAT is near 0, the average scores don’t depend on expenditure. This suggests that among elite students, expenditure doesn’t make a discernable difference: it’s the students, not the schools that matter.\nThe relationship shown in Figure 1 is genuine. So is the very different relationship seen in Figure 2. How can the same data be consistent with two utterly different displays? The answer, perhaps unexpectedly, has to do with the connections among the explanatory variables. Whatever the relationship between each individual explanatory variable and the response variable, the appearance of that relationship will depend on how explanatory variables are connected to each other."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#connections-among-explanatory-variables",
    "href": "Reading-notes/Reading-notes-lesson-28.html#connections-among-explanatory-variables",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Connections among explanatory variables",
    "text": "Connections among explanatory variables\nTo demonstrate that the apparent relationship between an explanatory variable and a response variable – for instance, school expenditures and education outcomes – depends on the connections of the explanatory variable with other explanatory variables, let’s move away from the controversies of political issues and study some systems where everyone can agree exactly how the variables are connected. We’ll look at data produced by simulations where we specify exactly what the connections are.\nA simulation implements a hypothesis: a statement about that might or might not be true about the real world. As a starting point for our simulation, let’s imagine that education outcomes increase with school expenditures in a very simple way: each $1000 increase in school expenditures per pupil results in an average increase of 10 points in the SAT score: an effect size of 0.01 points per dollar. Thus, the imagined relationship is:\n\\[\\mbox{sat} = 1100 + 0.01 * \\mbox{dollar expenditure}\\]\nLet’s also imagine that the fraction of students taking the SAT test also influences the average test score with an effect size of -4 sat points per percentage point. Adding this effect into the simulation leads to an imagined relationship of\n\\[\\mbox{sat} = 1100 + 0.01 * \\mbox{dollar expenditure} - 4 * \\mbox{participation percentage} .\\]\nAnd, of course, there are other factors, but we’ll treat their effect as random with a typical size of ± 50 points.\nTo complete the simulation, we’ll need to set values for dollar expenditures and participation percentage. We’ll let the dollar expenditures vary randomly from $7000 to $18,000 from one state to another and the participation percentage vary randomly from 1 to 100 percentage points.\nNotice that in this simulation, both participation percentage and expenditures affect education outcomes, but there is no connection at all between the two explanatory variables. That is, the graphical causal network is that shown in Figure @ref(fig:school-sim-1).\n\ndag_school1\n\n[[1]]\nexpenditure ~ unif(7000, 18000)\n\n[[2]]\nparticipation ~ unif(1, 100)\n\n[[3]]\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + eps(50)\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\ndag_draw(dag_school1)\n\n\n\n\nFigure 3: A graphical causal network relating expenditures, participation percentage, and education outcome, where there is no connection between expenditures and participation.\n\n\n\n\nWe can generate simulated data and use the data to train models. ?@fig-school-data-1 shows the data and two different models.\n\nDat1 <- sample(dag_school1, size=500)\nmod1_1 <- lm(outcome ~ ns(expenditure,2), data = Dat1)\nmod1_2 <- lm(outcome ~ ns(expenditure,2) * participation, data = Dat1)\nmod_plot(mod1_1, interval=\"prediction\") %>%\n  gf_point(outcome ~ expenditure, data = Dat1)\nmod_plot(mod1_2, interval=\"prediction\") %>%\n  gf_point(outcome ~ expenditure, alpha=~participation, data = Dat1, inherit=FALSE)\n\n\n\n\nFigure 4: Data and models of the relationship between expenditures and education outcomes from a simulation in which expenditures and participation rate are unconnected as in Figure 3. - (a) The model outcome ~ expenditure - (b) The model with participation as a covariate: outcome ~ expenditure + participation Both models (a) and (b) show the same effect size for outcome with respect to expenditure.\n\n\n\n\n\n\n\nFigure 5: Data and models of the relationship between expenditures and education outcomes from a simulation in which expenditures and participation rate are unconnected as in Figure 3. - (a) The model outcome ~ expenditure - (b) The model with participation as a covariate: outcome ~ expenditure + participation Both models (a) and (b) show the same effect size for outcome with respect to expenditure.\n\n\n\n\nThe relationship between outcome and expenditure can be quantified by the effect size, which appears as the slope of the function. You can see that when the explanatory variables are unconnected, as in Figure 3, the functions have the same slope.\nNow consider a somewhat different simulation. Rather than expenditures and participation being unconnected (as in the causal diagram shown in Figure 3), in this new situation we will posit a connection between the two explanatory variables. We’ll image that there is some broad factor, labeled “culture” in ?@fig-school-sim-2, that influences both the amount of expenditure and the participation in the tests used to measure education outcome. For instance, “culture” might be the importance that the community places on education or the wealth of the community.\n\ndag_school2\n\n[[1]]\nculture ~ unif(-1, 1)\n\n[[2]]\nexpenditure ~ 12000 + 4000 * culture + eps(1000)\n\n[[3]]\nparticipation ~ (50 + 30 * culture + eps(15)) %>% pmax(0) %>% \n    pmin(100)\n\n[[4]]\noutcome ~ 1100 + 0.01 * expenditure - 4 * participation + eps(50)\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\ndag_draw(dag_school2)\n\n\n\n\nFigure 6: A DAG for school outcomes that links participation and expenditure as a function of culture.\n\n\n\n\nAgain, using data from this simulation, we can train models:\n\n\noutcome ~ expenditures, which has no covariates.\n\n\noutcome ~ expenditures + participation, which includes participation as a covariate.\n\n\n?@fig-school-data-2 shows the data from the new simulation (which is the same in both subplots) and the form of the function trained on the data. Now model (a) shows a very different relationship between expenditures and outcome than model (b).\n\n\n\n\n\nFigure 7: Similar to ?@fig-school-data-1 but using the simulation in which the explanatory variables – expenditure and participation – are connected by a common cause. The two models show very different relationships between outcomes and expenditures. Model (b) matches the mechanism used in the simulation, while that mechanism is obscured in model (a).\n\n\n\n\n\n\n\nFigure 8: Similar to ?@fig-school-data-1 but using the simulation in which the explanatory variables – expenditure and participation – are connected by a common cause. The two models show very different relationships between outcomes and expenditures. Model (b) matches the mechanism used in the simulation, while that mechanism is obscured in model (a).\n\n\n\n\nSince we know the exact mechanism in the simulation—outcome increases with expenditure—we know that model (b) matches the workings of the simulation while model (a) does not.\nFor the simulation where expenditure and participation share a common cause, failing to stratify on participation – that is, looking at the points in @fig:-school-data-2 (a) but ignoring color – gives an utterly different result than if the stratification includes participation."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#always-include-covariates",
    "href": "Reading-notes/Reading-notes-lesson-28.html#always-include-covariates",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Always include covariates?",
    "text": "Always include covariates?\nIt might be tempting at this point to conclude that your models should always include covariates. After all, for both simulations the model that included participation as a covariate gave the correct effect size of expenditures on outcomes. That turns out to be an over-simplification, as you’ll see in Learning Check XXX [about a collider]."
  },
  {
    "objectID": "Reading-notes/Reading-notes-lesson-28.html#causality-correlation",
    "href": "Reading-notes/Reading-notes-lesson-28.html#causality-correlation",
    "title": "Math 300R Lesson 28 Reading Notes",
    "section": "Causality & Correlation",
    "text": "Causality & Correlation\nCausality is about relationships among entities in the world, e.g. the immunological properties of the drug acetaminophen lead to a reduction in fever. Correlation is about relationships that are evident in data, which might or might not be due to direct causal connections. For example, people who take acetaminophen tend to have fever, but this is not because acetaminophen causes fever. Instead, people who are unwell, and perhaps have fever, are more likely to take acetaminophen than those who are asymptomatic.\nCorrelations are properly part of the evidence to support a claim or quantification of causation. Indeed, whenever there is a correlation between two variables, it’s likely that there is some chain of causal connections that links the two variables, even if that chain is not directly from one variable to the other. For instance, taking the flu vaccine is correlated with reduced mortality. Some of this correlation is due to the immunological properties of the vaccine itself. But some of the correlation results from healthy people being more likely to take the vaccine than sick people, and healthy people having a lower mortality than sick people.\nSeen as a pessimist, this chapter can help you understand some of the ways that correlations can be present without a direct causal pathway, and how you can be badly mislead if you rely purely on data without any causal theory of the way your system works in the real world.\nSeen as an optimist, this chapter is about ways of calculating effect sizes from data that allow you to incorporate knowledge of the causal connections amongst the variables in your data.\nThe field of statistics comprises both optimists and pessimists. Perhaps to oversimplify, the pessimists think the proper domain of statistics is data and stylized mathematical models, and ought not include speculative notions of causal connections in the real world. The only sort of causal connection that the pessimists will accept is that of the experimenter who sets the values of inputs, for example by giving one “treatment” group of patients a drug and another “control” group a “placebo”. This has been a highly productive attitude in statistics, resulting in the development of clever designs for experiments that give the most information with the least laboratory effort. Unfortunately, the no-causation-without-experimentation philosophy leaves us without recourse when working with a system where a controlled experiment is not feasible.\nPerhaps the outstanding historical example of the limits of the no-causation-without-experimentation philosophy relates to the health effects of smoking. Nowadays, the morbidity and mortality caused by tobacco smoking is mainstream knowledge. Among the other proofs of the causal relationship is the decline in mortality due to lung cancer amoung populations where smoking became much less popular. Until the mid 1960s, however, some statisticians were in the vanguard of challenging the idea of a causal connection between smoking and, e.g., lung cancer. Notably, Ronald Fisher, generally considered to be the leading statistical figure of the 20th century, vehemently and influentially criticized the evidence for the causal connection.\n\n\n\n\n\nFigure 9: Insisting that “correlation is not causation” can interfere with making useful judgements, as interpreted by Randall Munroe in his XKCD cartoon series.\n\n\n\n\nThe optimists, again to oversimplify, believe it is possible to make useful statements (e.g. “the class helped” in Figure 9) about the causal connections that underlie data. They emphasize that statistics can support decision making even when knowledge of causation is incomplete and uncertain.\nThe optimists and the pessimists use the same set of mathematical and statistical tools for data analysis, particularly the calculation of effect sizes. The difference between them is the range of legitimate conclusions that can be drawn. The pessimists place in the center the idea that “correlation is not causation” and that only controlled experiment can be a justification for making causal conclusions. (We’ll study experiment in Lesson 32.) The optimists also see the difference between correlation and causation: correlation is a mathematical property, causation is a physical one. And the optimists accept that controlled experiment is an excellent way to form strong conclusions. But they accept other sources of knowledge or theoretical speculations as potentially useful, and use effect-size calculations in a way that, contingent on that knowledge or speculation, creates through the process of data analysis situations analogous to those created in the laboratory by careful experimentation."
  }
]
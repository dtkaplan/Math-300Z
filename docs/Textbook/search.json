[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lessons in Statistical Thinking",
    "section": "",
    "text": "Preface\nLessons in Statistical Thinking is an update and reconsideration of the concepts and methods needed to extract information from data. Such an update is needed because the canon of traditional introductory statistics texts has long been obsolescent and fails to address the needs of the contemporary data scientist and decision-maker. That canon stems from an influential 1925 book, Ronald Fisher’s Statistical Methods for Research Workers. Research workers of that era typically ran small benchtop or field experiments with a dozen or fewer observations on each of two treatments. A first task with such small data is to rule out the possibility that calculated differences might reflect only the accidental arrangement of numbers into groups.\nPerhaps emblematic of the current dissatisfaction with small-data methods is the controversy over “statistical significance.” Although situated at the core of many statistics textbooks, significance testing has little to do with the meaning of “significant” as “important” or “relevant.” This article in the prestigious science journal Nature details the controversy. Figure 18.1 reproduces a cartoon from that article that puts the shortcomings of “statistical significance” in a historical context."
  },
  {
    "objectID": "index.html#statistical-thinking",
    "href": "index.html#statistical-thinking",
    "title": "Lessons in Statistical Thinking",
    "section": "Statistical thinking",
    "text": "Statistical thinking\nThe work of today’s data scientists is often to discover novel connections among multiple variables and to guide decision-making. It is common for data to be available in large masses from observations rather than experiments. One common purpose is “prediction,” which might be as simple as the uses of medical screening tests or as breathtaking as machine-learning techniques of “artificial intelligence.” Another pressing need from data analysis is to understand possible causal connections between variables.\nThe twenty lessons that follow describe a way of thinking that is historically novel, unfamiliar to most otherwise well-educated people, and incredibly useful for making sense of the world and what data can tell us about the world. Learning a new way of thinking is genuinely hard. To start, it will help to describe “statistical thinking” concisely. A definition I find useful is this:\n\nStatistic thinking is the explanation or description of variation in the context of what remains unexplained or undescribed.\n\nImplicit in this definition is a pathway for learning to think statistically:\n\nLearn how to use data to describe variation;\nSee how to measure “what remains undescribed” and use that measurement as a context for interpretation; and\nUnderstand how to link ideas of the process underlying data with appropriate description and modeling techniques."
  },
  {
    "objectID": "index.html#important-word-pairs",
    "href": "index.html#important-word-pairs",
    "title": "Lessons in Statistical Thinking",
    "section": "Important word pairs",
    "text": "Important word pairs\nMany of the vocabulary terms used in statistical thinking come in pairs. We list several such pairs below, in roughly the order they first appear in the Lessons. The pairs can be a reference while reading, but it is also helpful to return to this list to sharpen your understanding of the distinctions.\nExplanatory vs response variables. Models (in these Lessons) always involve a single response variable*. In contrast, models can have zero or more explanatory variables.\nVariable vs covariate. “Covariate” is another word for an explanatory variable. The word “covariate” signals that the variable is not itself of direct interest to the modeler but puts another explanatory variable in a correct context.\nCategorical vs quantitative variables. Always be aware of whether a model’s response variable is categorical or quantitative. When categorical, expect to use zero_one() to convert it to quantitative before modeling. In contrast, explanatory variables can be either categorical or quantitative.\nRegression model vs classifier. A regression model always has a quantitative response variable. A classifier has a categorical response variable. In these Lessons, as in much professional use of data, our categorical response variables will have two levels (e.g., healthy or sick, up or down, yes or no). In this situation, regression techniques suffice to build classifiers.\nModel vs model function. By “model,” we will almost always mean “regression model.” A regression model, typically constructed by the lm() function, contains various information useful to summarize the model. The “model function” provides the mechanism for one important task, calculating from values from the explanatory variables the corresponding model output.\nModel coefficient vs effect size. Model coefficients are numerical parameters. Training determines the appropriate values for the coefficients. In contrast, an effect size describes the relationship between the response variable and a selected explanatory variable.\nPoint estimate vs interval estimate. A point estimate is a single number. For instance, a model coefficient is a point estimate, as is the output from a model function. In contrast, interval estimates involve two numbers; one specifies the lower end of the interval and the other number specifies the upper end.\nPrediction interval vs confidence interval. A prediction interval describes the anticipated range of the actual result for which we have made a prediction, e.g., “tomorrow’s wind will be between 5 and 10 mph.” A confidence interval is often used to express the uncertainty in a coefficient or effect size."
  },
  {
    "objectID": "index.html#software-guide",
    "href": "index.html#software-guide",
    "title": "Lessons in Statistical Thinking",
    "section": "Software guide",
    "text": "Software guide\nThese Lessons use about a dozen new R functions. Some of these are used frequently in examples and exercises and are worth mastering. Others appear only in demonstrations.\n\n\n\n\n\n\nDemonstrations\n\n\n\nThese lessons contain demonstrations illustrating statistical concepts or data analysis strategies. We will place these in a distinctive box, of which this is an example.\nThe demonstrations will often contain new computer commands that perform tasks used in teaching statistics. However, readers are not expected to be able to construct such commands on their own.\n\n\n\nTraining models with data\n\nlm() arguments: i. tilde expression, ii. data= data frame.\nOccasionally, you will be directed to use glm() or model_train(), which work similarly to lm() but are specialized for models whose output is a probability.\nzero_one() converts a two-level categorical variable to a 0/1 encoding.\n\nSummarizing models. These invariably take as input a model produced by lm() (or glm()) and generate a summary report about that model.\n\ncoefficients(): displays model coefficients as a single number.\nconfint(): displays model coefficients as an interval with a lower and upper value.\nrsquared() reduces a model to a single number, called R2.\nregression_summary(), like confint(), but with more detail.\n\nEvaluating a model on inputs\n\nmodel_eval() takes a trained model (as produced by lm()) and calculates the model output in both a point form and an interval form. model_eval() can also display the residuals from training or evaluation data.\n\nGraphics\n\nmodel_plot() draws a graphic of a model’s function optionally with prediction or confidence intervals.\ngeom_violin() is a modern alternative to geom_boxplot().\n\nDAGs (directed, acyclic graphs)\n\nsample() collects simulated data from a DAG\ndag_draw() draws a picture of a DAG showing how the variables are connected.\n\nUsed within the summarize() data wrangling function:\n\nvar() computes the variance of a single variable.\n\n\n\n\n\n\n\n\nDemonstration\n\n\n\nHere are some of the command structures that appear in demonstrations. These explanations give a general idea of the tasks they perform.\n\ndo(10) * { command } causes the command to be executed repeatedly the indicated number of times. Such repetitions are useful when the command is a trial of a random process such as sampling, resampling, or shuffling.\nfunction(arguments) { set of commands } packages in a single unit a set of one or more commands. The packaging facilitates using them over and over again with specified arguments.\ngeom_errorbar() works much like geom_point() but draws vertical bars instead of dots. Bar-shaped glyphs depict intervals such as confidence or prediction intervals.\ngeom_ribbon() is like geom_line() but for intervals.\neffect_size() calculates the strength and direction of the input-output relationship between the response variable of a model and a selected one of the explanatory variables."
  },
  {
    "objectID": "bogus.html",
    "href": "bogus.html",
    "title": "1  Bogus",
    "section": "",
    "text": "Foobar"
  },
  {
    "objectID": "Reading-notes-lesson-19.html#regression-as-a-data-summary",
    "href": "Reading-notes-lesson-19.html#regression-as-a-data-summary",
    "title": "19  Preliminaries",
    "section": "Regression as a data summary",
    "text": "Regression as a data summary\nThe first half of this course emphasized data wrangling and visualization. The well-named summarize() function is the natural wrangling choice to compute summaries such as means, medians, or standard deviations. For instance, this command calculates four summary statistics on the net running time recorded in the TenMileRace data frame:\n\n\nTenMileRace %>%\n  summarize(ave = mean(net), middle = median(net), sd = sd(net), n = n())\n\n       ave middle       sd    n\n1 5599.065   5555 969.6564 8636\n\n\n\nsummarize() works hand-in-hand with group_by() to calculate groupwise summaries. The following wrangling statement, for instance, looks at the average net running time broken up according to the runner’s state of residence and presents the results from the fastest state downwards:\n\nTenMileRace %>%\n  group_by(state) %>%\n  summarize(ave = mean(net), middle = median(net), sd = sd(net), n = n()) %>%\n  arrange(ave) %>%\n  head(10)\n\n# A tibble: 10 × 5\n   state       ave middle    sd     n\n   <fct>     <dbl>  <dbl> <dbl> <int>\n 1 Australia 2872   2872    NA      1\n 2 Kenya     2934.  2874.  141.    14\n 3 Lithuania 2961   2961    NA      1\n 4 Japan     2992   2992   132.     2\n 5 Colombia  2998   2998    NA      1\n 6 Ethiopia  3185   3185   356.     2\n 7 EN        3251   3251    NA      1\n 8 Ukraine   3256   3256    NA      1\n 9 Russia    3267   3197   272.     3\n10 Romania   3287.  3297   162.     3\n\n\nWrangling is essential for many statistical purposes, not just summarizing but also setting up for making graphical displays, cleaning data, and assembling data from multiple sources.\nRegression modeling is used only for summarizing. The summary describes the relationship between the response and explanatory variables. Think of it as a kind of substitute for summarize() when you want to describe relationships.\nAs we saw above, summarize() and group_by() are two different stages of wrangling that, used together, produce a separate summary for each group. Regression modeling, however, offers a rich alternative to grouping. The spirit of what one accomplishes in wrangling with group_by() is achieved in regression by using additional explanatory variables.\n\n\n\n\n\n\nThe mean as summary\n\n\n\nIn its simplest mode, a regression can calculate means. The result will be identical to group_by()/summarize() but in a different format.\nHere is the mean net running time calculated using both approaches.\n\nTenMileRace %>% lm(net ~ 1, data=.) %>% coefficients()\n\n(Intercept) \n   5599.065 \n\nTenMileRace %>% summarize(mn = mean(net))\n\n        mn\n1 5599.065\n\n\n\nThe groupwise calculations also produce equivalent results, although the results are formatted in different ways.\n\nTenMileRace %>% lm(net ~ sex, data=.) %>% coefficients()\n\n(Intercept)        sexM \n  5916.3979   -635.6958 \n\nTenMileRace %>% group_by(sex) %>%summarize(mn = mean(net))\n\n# A tibble: 2 × 2\n  sex      mn\n  <fct> <dbl>\n1 F     5916.\n2 M     5281.\n\n\nRegression of this sort calculates the mean of a reference group and the difference in means between the two groups, whereas the wrangling command presents the mean of each group.\n\n\nNotice something new in the above command: the data=. argument inside lm(). The simple . is doing something important, carrying the output of the earlier stages of the pipeline into the data= argument of lm().In regression, “grouping” is extended to quantitative variables. For instance,\n\nTenMileRace %>% lm(net ~ age, data = .) %>% coefficients()\n\n(Intercept)         age \n5297.219248    8.189886 \n\n\nThis report indicates a trend of net running time increasing with age by about 8 seconds per year.\nThe group_by() function can use a quantitative variable, but the result is a different number for each group rather than a trend.\n\nTenMileRace %>% group_by(age) %>% summarize(mn = mean(net)) \n\n# A tibble: 68 × 2\n     age    mn\n   <int> <dbl>\n 1    10 5635 \n 2    12 5978 \n 3    13 5405.\n 4    14 5620.\n 5    15 5166.\n 6    16 5522.\n 7    17 5716.\n 8    18 5259.\n 9    19 4573 \n10    20 5218.\n# … with 58 more rows\n\n\nWith multiple grouping variables, say age and sex, the output of summarize() becomes increasingly complicated. For example:\n\nTenMileRace %>% group_by(sex, age) %>% summarize(mn = mean(net)) \n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 127 × 3\n# Groups:   sex [2]\n   sex     age    mn\n   <fct> <int> <dbl>\n 1 F        10 5635 \n 2 F        13 6831 \n 3 F        14 5941.\n 4 F        15 4854 \n 5 F        16 5678.\n 6 F        17 6011.\n 7 F        18 5720.\n 8 F        19 5285.\n 9 F        20 5970.\n10 F        21 6057.\n# … with 117 more rows\n\n\nRegression keeps things simpler, reporting on trends:\n\nTenMileRace %>% lm(net ~ sex + age, data = .) %>% coefficients() \n\n(Intercept)        sexM         age \n 5339.15545  -726.61948    16.89362 \n\n\nThe trend reported from this regression model is an increase in net of about 16 seconds per year of age. Regression can summarize relationships in more detailed ways as well. The following model looks at the trend with age separately for males and females:\n\nTenMileRace %>% lm(net ~ sex * age, data = .) %>% coefficients() \n\n(Intercept)        sexM         age    sexM:age \n5370.999953 -785.145163   15.961661    1.606559 \n\n\nHere, the age trend for women is an increase in net running time of 16 seconds per year of age, while for men, that increase is bigger, an extra 1.6 seconds per year of age.\nThere are good reasons why lm() organizes summaries the way it does. The lm() paradigm can make much more efficient use of data than group_by(). It also offers much more flexibility. lm() can handle multiple “grouping” variables together and even lets you “group” by quantitative variables."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#presentation-using-intervals",
    "href": "Reading-notes-lesson-19.html#presentation-using-intervals",
    "title": "19  Preliminaries",
    "section": "Presentation using intervals",
    "text": "Presentation using intervals\nStatistical thinking often involves quantifying uncertainty. Uncertainty appears where a newcomer to statistical thinking might not expect it. For example, consider “point” summaries such as the mean or median. So long as the arithmetic is correct, the result is inevitable; everyone doing the calculation will get the same result. The statistical thinker, however, includes the data collection process in the calculation. Each person carrying out his or her data collection process will get different results. The study-to-study variation calls for an interval display, where the interval covers the likely range of results.\nPrediction is another context benefiting from an interval display. Prediction is imperfect. The predicted result—for instance, the baby’s due date—is typically different from the actual outcome. The statistical thinker knows how to estimate the likely range of the difference between the predicted and actual outcomes.\n\n\n\n\n\n\nExample: 1.6 seconds per year?\n\n\n\nIt is appropriate to be skeptical of a claim that male runners slow down by 1.6 seconds per year compared to females. After all, people differ; some age more gently than others. As we will see in Lesson 20, the results presented from a regression model depend partly on the play of chance in determining the particular people represented in the data. It is helpful to know how much chance affects the results. A summary can indicate this by a range of plausible values, in other words, an “interval” summary. Here is an interval summary on the coefficients from the running time versus age model:\n\nTenMileRace %>% lm(net ~ sex * age, data = .) %>% confint() \n\n                  2.5 %      97.5 %\n(Intercept) 5269.766831 5472.233076\nsexM        -927.311125 -642.979202\nage           13.104803   18.818520\nsexM:age      -2.144359    5.357478\n\n\nNotice that the interval on sexM:age includes zero.\nConstruct interval summaries using the appropriate extractor on a regression model. For instance, confint() generates an interval summary suggesting there might be no difference in the age trend for males and females."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#point-plot-as-a-graphical-foundation",
    "href": "Reading-notes-lesson-19.html#point-plot-as-a-graphical-foundation",
    "title": "19  Preliminaries",
    "section": "Point plot as a graphical foundation",
    "text": "Point plot as a graphical foundation\nRegression models, which will be the primary means of summarizing data in these Lessons, always have a response variable and typically have one or more explanatory variables.1\nIn these Lessons, we will place graphical depictions of model summaries in the context of actual data. Consequently, the graphical frame will reflect the choice of response and explanatory variables. The vertical axis will always represent the response variable. The horizontal axis will represent one of the explanatory variables. A point plot will display the data, or a jitter plot when there are categorical variables to be shown.\nAnother aspect of our unified data graphic format is that it will always be a point plot or, closely related, a jitter plot.\n\n\n\n\n\n\nExample: When is the baby due?\n\n\n\nAs all expecting parents know, a baby’s “due date” is hardly exact. Pregnancies vary in length. What accounts for this variation?\nIn this example, we will entertain the hypothesis that experienced mothers have systematically different gestation periods than first-time mothers. An appropriate response variable is duration of gestation. The explanatory variable needs to measure “experience,” which is a vague idea. We will make it concrete by representing it by the number of the mother’s pregnancies before the one reported in the data.\nThe Gestation data frame records more than 1200 births. gestation records the length of the pregnancy and will be our response variable. parity gives the number of previous births to the mother, starting at zero for a first-time mother. Although parity is encoded as a number, it has only discrete values—0, 1, 2, … We will therefore graph it as a categorical variable, using jittering to avoid overplotting. There are not many rows with parity greater than five; we will focus on those.\n\nGestation %>% \n  filter(parity <= 5) %>%\n  #mutate(parity = as.character(parity)) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) \n\n\n\n\nFigure 19.1: Gestational period for pregnancies where the mother had five or fewer previous pregnancies. The width=0.2 controls the amount of horizontal jittering. We chose it to make the columns of data clear. There is no need to jitter in the vertical direction, so we set height=0\n\n\n\n\nThis graph shows some things at a glance. For example, a typical gestation period is about 275 days (about nine months), and it is much more common to have a low parity than a very high one."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#displaying-density",
    "href": "Reading-notes-lesson-19.html#displaying-density",
    "title": "19  Preliminaries",
    "section": "Displaying density",
    "text": "Displaying density\nIt is easy to see a pattern in Figure 19.1: It looks like mothers with high parity tend to have gestation periods more reliably close to 280 days than mothers with low parity. However, on the other hand, maybe this pattern is an illusion, an artifact of the small number of pregnancies with parity 3, 4, or 5 and, therefore, less opportunity to see extreme values for gestation.\nOne way to explore this idea is to plot the density of the dots as a function of gestation for each of the parity levels individually. A “violin” layer will make it easier to compare the distributions in the different columns, despite the unevenness in the case count. Figure 19.2 gives an example.\n\nGestation %>% \n  filter(parity <= 5) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) +\n  geom_violin(aes(group=parity), fill=\"blue\", alpha=0.2, color=NA)\n\n\n\n\nFigure 19.2: A violin plot. The long axis of the violin-like shape is oriented along the response-variable axis (that is, the vertical axis in our standard format). The width of the violin for each possible value of the response variable is proportional to the density of data near that value.\n\n\n\n\nThe violin plot is a more flexible display of the distribution of gestation period than a histogram. The histogram has all those bars that clutter up the display. Even worse, one of the axes in the frame of a histogram plot is “count” or maybe “density.” Such a frame is inconsistent with the response/explanatory axes used for the data. The violin is drawn in the no-mans-land between the different levels of parity, just as the jittering moves data away from a single vertical line into that same no-mans-land.\nThis idea of using the graphical no-mans-land between levels of a categorical explanatory variable is not new. You encountered it earlier when you drew box plots. ?fig-density-box adds a box-plot annotation layer on top of the violin-plot layer.\n\nGestation %>% \n  filter(parity <= 5) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) +\n  geom_violin(aes(group=parity), fill=\"blue\", alpha=0.2, color=NA) +\n  geom_boxplot(aes(group=parity), color=\"blue\", fill=NA, alpha=.5)\n\n\n\n\nFigure 19.3: A box and whisker plot uses the no-mans-land between levels of a categorical explanatory variable.\n\n\n\n\nIn practice, there is little reason to layer a box plot on top of a violin. The violin does the job nicely on its own."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#categorical-response-variables",
    "href": "Reading-notes-lesson-19.html#categorical-response-variables",
    "title": "19  Preliminaries",
    "section": "Categorical response variables",
    "text": "Categorical response variables\nRegression modeling will be a fundamental tool in these Lessons for summarizing data. Regression models always have a quantitative response variable, although explanatory variables can be either quantitative or categorical.\nOften, the modeling situation calls for a response variable that is categorical. Expert modelers can use specialized modeling methods to handle such situations. However, categorical response variables often have just two levels, e.g., Alive/Dead, Promoted/Not, or Win/Loss. We will name the general class of such variables as “yes/no” or, equivalently, “zero-one” variables.More formally, they are called “binomial” variables.\nYes/no response variables can be represented using 0 for one level and 1 for the other. This numerical “0/1 encoding” is directly suited for regression modeling and enables us to extend the scope of regression models. The output of the regression model is always numerical. Nothing in the regression technique restricts those outputs to exactly zero or one, even when the response variable is of the yes/no type. Usually, the modeler interprets such numerical output as probabilities or, more generally, as measures to be converted to probabilities.\n\n\n\n\n\n\nR technique: Using zero_one().\n\n\n\nThe zero_one() function converts a yes/no variable to the numerical zero-one format. zero_one() allows you to specify which of the two levels is represented by 1.\nTo illustrate, consider the mosaicData::Whickham data frame, which records a 1972-1974 survey, part of a study of the relationship between smoking and mortality. Twenty years after the initial survey, a follow-up established whether or not each person was still alive.\n\n\n?(caption)\n\n\n\n\n\n \n  \n    outcome \n    smoker \n    age \n  \n \n\n  \n    Alive \n    Yes \n    23 \n  \n  \n    Alive \n    Yes \n    18 \n  \n  \n    Dead \n    Yes \n    71 \n  \n  \n    Alive \n    No \n    67 \n  \n  \n    Alive \n    No \n    64 \n  \n  \n    Alive \n    Yes \n    38 \n  \n\n\n\n\n\n\nThe outcome variable in Whickham records the result of the follow-up survey. It is a categorical variable with levels “Alive” and “Dead.” To examine what the data have to say about the relationship between smoking and mortality, we construct a model with outcome as the response variable and smoking as an explanatory variable. Before doing so, we translate outcome into a zero-one format. Like this:\n\nWhickham %>% \n  mutate(alive = zero_one(outcome, one=\"Alive\"))\n\n\n\n\n\n \n  \n    outcome \n    smoker \n    age \n    alive \n  \n \n\n  \n    Alive \n    Yes \n    23 \n    1 \n  \n  \n    Alive \n    Yes \n    18 \n    1 \n  \n  \n    Dead \n    Yes \n    71 \n    0 \n  \n  \n    Alive \n    No \n    67 \n    1 \n  \n  \n    Alive \n    No \n    64 \n    1 \n  \n  \n    Alive \n    Yes \n    38 \n    1 \n  \n\n\n\n\n\nNote the correspondence between the outcome and the newly created alive variable.\n\n\n\n\n\n\n\n\n\nDemonstration: Predicting calorie content\n\n\n\nStarbucks is a famous coffee-shop franchise with more than 30,000 branches (as of 2021). People go to Starbucks for coffee, but they often buy something to eat as well. In this demonstration, we will look at the calorie content of Starbucks’ food offerings. As always, when conducting a statistical analysis, it is helpful to have in mind the motivation for the task. So we will imagine, tongue in cheek, that we want to make food recommendations for the calorie-conscious consumer.\nFirst, a point summary of the calories in the different types of food products available at Starbucks:\n\npoint_summary <- \n  df_stats(calories ~ type, \n           data = openintro::starbucks, mean)\npoint_summary\n\n  response          type     mean\n1 calories        bakery 368.7805\n2 calories    bistro box 377.5000\n3 calories hot breakfast 325.0000\n4 calories       parfait 300.0000\n5 calories        petite 177.7778\n6 calories         salad  80.0000\n7 calories      sandwich 395.7143\n\n\nThis summary supports the sensible advice to choose salads or smaller portions (type “petite”) to avoid calories. One might go further, for example, concluding that a sandwich is a poor choice (in terms of calorie content), so lean toward parfaits or hot breakfasts. We can even imagine someone concluding from this summary that a bistro box is a better calorie-conscious choice than a sandwich.\nFigure 19.4 shows the point summary, using the raw data to put things in context.\n\nopenintro::starbucks %>% \n  ggplot(aes(x=type, y=calories)) +\n  geom_jitter(width=0.2, alpha=0.5) +\n  geom_errorbar(data=point_summary, aes(ymin=mean, ymax=mean), \n                y=NA, color=\"blue\") +\n  geom_point(data=point_summary, aes(y=mean), color=\"red\")\n\n\n\n\nFigure 19.4: Calories of the various food items sold by Starbucks, annotated with point and interval summaries.\n\n\n\n\nPlotting the point summary in the context of the raw data shows at a glance that the point summary is not of any genuine use. For instance, using the point summary without the data, we might conclude that hot breakfasts are better than sandwiches. However, the data display suggests otherwise; there is just one low-calorie breakfast. The others are much like sandwiches.\nA point summary is compact but cannot represent the variation within each food type. An interval summary, as in Figure 19.5, does show this variation.\n\nopenintro::starbucks %>% \n  ggplot(aes(x=type, y=calories)) +\n  geom_jitter(width=0.2, alpha=0.5) +\n  geom_errorbar(data=point_summary, aes(ymin=mean, ymax=mean), \n                y=NA, color=\"blue\") \n\n\n\n\nFigure 19.5: Calories of the various food items sold by Starbucks, annotated with point and interval summaries.\n\n\n\n\nUnlike point summaries, interval summaries can overlap. Such overlap indicates that the groups are not all that different. Here, the interval summary indicates an appropriate conclusion; “Don’t make your diet choices based on food type. Look at the calorie content of individual items before choosing.”\nAdmittedly, in this simple setting the data themselves would lead to the conclusion. However, as we move into more complicated settings, it will become infeasible to see patterns quickly straight from the data."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#sec-size-of-variable",
    "href": "Reading-notes-lesson-20.html#sec-size-of-variable",
    "title": "20  Measuring and simulating variation",
    "section": "Measuring variation",
    "text": "Measuring variation\nA data frame records observations. In a properly constructed data frame, each row holds the observations from one “unit of observation.” Each column stores the values of one “variable,” one type of observation. The “unit of observation” might be a winner in a Scottish Hill race (Hill_racing), it might be a pregnancy and the resulting birth (Gestation), it might be a nurse participating in a survey (Whickham), or any of an infinite range of things. All the rows of a data frame record the same kind of unit of analysis.\nSimilarly, within a variable, all the recorded observations have the same type denominated in the same units: an age in years, a distance in kilometers, a price in dollars, or whatever.\nAlthough the unit of observation is the same type in every row, the units themselves can differ from one another. Consequently, the values recorded in a variable can differ; they are not all the same: they vary. The words “variable” and “variation” go hand in hand: a variable displays variation.1\nStatistical models decompose the response variable into components. Each component is associated with one or more explanatory variables, with one exception. The remaining component, called the “residual,” represents the part of the variation that is still unexplained. In order to make sure that we are not double-counting or omitting variation, it helps to measure the size of variation in each component.\nThere are many possible ways to measure the “size.” 2 In these Lessons, we will use a standard measure with an awkward name: the “standard deviation.” In practice, however, we will work mainly with the square of the standard deviation, called the “variance.”\nBoth variance and standard deviation are quantities, that is, a single number with associated units. The standard deviation of any variable has units that are the same as the variable itself. For instance, height is often denominated in cm. Therefore, the standard deviation of height, as it varies from person to person, will also be in cm.\nSince the variance is the square of the standard deviation, variance has units of the square of the units of the variable. So the variance of height, for instance, will be measured in cm2.\nRemember that variance and standard deviation are summaries of a variable. A variable in a data frame consists of multiple values, one for each row. The variance (or standard deviation) of that variable is a single number (with units), summarizing all of the values in the variable.\n\n\n\n\n\n\nWhy the variance?\n\n\n\nThe name “variance” is a good reminder of what it measures: the variation.\nBut why use a squared measure?\nConsider this familiar equation: \\(A^2 + B^2 = C^2\\). The Pythagorean Theorem states that the equation describes the lengths of the sides of a right triangle: \\(C\\) is the hypotenuse, while \\(A\\) and \\(B\\) are the other two sides of the triangle. Surprisingly, the Pythagorean Theorem is highly relevant to statistical models.\nOpenIntro chapters 5 & 6 point out that the linear modeling technique produces two columns of numbers: the fitted values and the residuals. These columns have the same number of rows as the data frame used for training. The residuals are the row-by-row numerical difference between the response variable and the fitted values.\nThese three columns of numbers—the response variable, the fitted model values, and the residuals—are exactly analogous to the three sides of a right triangle. (This is not an obvious fact, but it is important to remember.) In particular, the following numerical relationship is as true for linear models as it is for triangles: \\[\\text{sd(fitted)}^2 + \\text{sd(residuals)}^2 = \\text{sd(response)}^2\\] where sd() refers to the standard deviation. Consequently, sd()2 is the variance.\nThe particular mathematical definition of variance and the standard deviation makes the Pythagorean relationship always describe models constructed using the lm() technique. (One of the names used for this technique, least squares, provides a hint that the Pythagorean relationship applies.)\n\n\n\n\n\n\n\n\nCalculating variance\n\n\n\nAlmost always, people use software for calculations. The relevant R functions are sd() and var() and are used in a summarize() statement, for instance\n\nmtcars %>% summarize(v = var(hp))\n\n         v\n1 4700.867\n\nmtcars %>% summarize(s = sd(hp))\n\n         s\n1 68.56287\n\n\nRegrettably, the software does not indicate the units of the quantity. For that, read the documentation for the data frame.\n\n\nTo understand what is being calculated by var(), we will describe an algorithm. This algorithm is not numerically efficient, but it highlights the essential feature of variation.\nStarting material:\n\nA single column of numbers created by pulling out from the data frame the variable whose variance is to be calculated.\nA long roll of paper on which to write numbers, one after the other.\n\nRepeat a basic calculation for each and every row in the column of numbers. To illustrate, let us detail the basic calculation for the kth row.\n\nTake the data value from the kth row, and call it the “reference value.”\nSubtract the reference value from each and every other value in the column and square the results.\nWrite those numbers, all of them, on the roll.\n\nUsing the same roll of paper for all, carry out the basic calculation starting at each row in the data column. With this done, the paper roll contains many numbers, each of which is the square difference between the values from two rows in the data column. The mathematically inclined might like to know that there will be exactly \\(n(n-1)\\) numbers written on the roll. (The term \\(n-1\\) in that count might perk up the ears of statistics instructors.)\nThe final result—the variance—will be half the average of the numbers on the roll.\n\n\n\n\n\n\nDeconstructing “standard deviation”\n\n\n\n“Standard deviation” is an antique term and is misleading to people who think about “deviation” in the ordinary sense. Nonetheless, “standard deviation” is so widely used in statistics that we can hardly avoid it. To limit the confusion, we will deconstruct the term.\nStep 1 in the deconstruction makes clear what “standard” means:\n\nstandard deviation \\(=\\) accepted measure of deviation.\n\nStep 2 in the deconstruction replaces the archaic word “deviation” with something more descriptive:\n\nstandard deviation \\(=\\) accepted measure of variation in the variable."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#representing-causal-connections",
    "href": "Reading-notes-lesson-20.html#representing-causal-connections",
    "title": "20  Measuring and simulating variation",
    "section": "Representing causal connections",
    "text": "Representing causal connections\nOften, but not always, our interest in studying data is to reveal the causal connections between variables. Understanding causality is essential, for instance, if we are planning to intervene in the world and want to anticipate the consequences. Interventions are things like “increase the dose of medicine,” “stop smoking!”, “lower the budget,” “add more cargo to a plane (which will increase fuel consumption and reduce the range).”\nHistorically, mainstream statisticians were hostile to using data to explore causal relationships. (The one exception was experiment, which gathers data from an actual intervention in the world. See Lesson 32.) Statistics teachers encouraged students to use phrases like “associated with” or “correlated with” and reminded them that “correlation is not causation.”\nRegrettably, this attitude made statistics irrelevant to the many situations where intervention is the core concern and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists saw evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, “smoking is associated with lung cancer.” He speculated that smoking and lung cancer might have a common cause, perhaps genetic. Fisher argued that establishing causation requires running an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result.\nFortunately, around 1960, a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. Cornfield’s work prompted the development of a new area in statistics: “causal inference.”\nCausal inference is not about proving that one thing causes another but about formal ways to say something about how the world works that can be used, along with data, to make responsible conclusions about causal relationships.\nA core tool in thinking about causal connections is a mathematical structure called a “directed acyclic graph” (DAG, for short). DAGs are one of the most popular ways for statistical thinkers to express their ideas about what might be happening in the real world. Despite the long name, DAGs are very accessible to a broad audience.\nDAGs, despite the G for “graph,” are not about data graphics. The “graph” in DAG is a mathematical term of art; a suitable synonym is “network.” Mathematical graphs consist of a set of “nodes” and a set of “edges” connecting the nodes. For instance, Figure 20.1 shows three different graphs, each with five nodes labeled A, B, C, D, and E.\n\n\n\n\n\n\n\n(a) undirected graph\n\n\n\n\n\n\n\n(b) directed but cyclic\n\n\n\n\n\n\n\n(c) directed acyclic graph\n\n\n\n\nFigure 20.1: Graphs of various types\n\n\nThe nodes are the same in all three graphs of Figure 20.1, but each graph is different from the others. It is not just the nodes that define a graph; the edges (drawn as lines) are part of the definition as well.\nThe left-most graph in Figure 20.1 is an “undirected” graph; there is no suggestion that the edges run one way or another. In contrast, the middle graph has the same nodes and edges, but the edges are directed. An excellent way to think about a directed graph is that each node is a pool of water; each directed edge shows how the water flows between pools. This analogy is also helpful in thinking about causality: the causal influences flow like water.\nLook more carefully at the middle graph. There is a couple of loops; the graph is cyclic. In one loop, water flows from E to C to D and back again to E. The other loop runs B, C, D, E, and back to B. Such a flow pattern cannot exist without pumps pushing the water back uphill.\nThe rightmost graph reverses the direction of some of the edges. This graph has no cycles; it is acyclic. Using the flowing and pumped water analogy, an acyclic graph needs no pumps; the pools can be arranged at different heights to create a flow exclusively powered by gravity. The node-D pool will be the highest, E lower. C has to be lower than E for gravity to pull water along the edge from E to C. The node-B pool is the lowest, so water can flow in from E, C, and A.\nDirected acyclic graphs represent causal influences; think of “A causes B,” meaning that causal “water” flows naturally from A to B. In a DAG, a node can have multiple outputs, like D and E, and it might have multiple inputs, like B and C. In terms of causality, a node—like B—having multiple inputs means that more than one factor is responsible for the value of that node. A real-world example: the rising sun causes a rooster to crow, but so can another intruder to the coop.\nOften, nodes do not have any inputs. These are called “exogenous factors”at least by economists. The “genous” means “originates from.” “Exo” means “outside.” The value of an exogenous node is determined by something, just not something that we are interested in (or perhaps capable of) modeling. No edges are directed into an exogenous node since none of the other nodes influence its value.\nThe point of a DAG is to make a clear statement of a hypothesis about causation. Drawing a DAG does not mean that the hypothesis is correct, just that we believe the hypothesis is, in some sense, a possibility. Different people might have different beliefs about what causes what in real-world systems. Comparing their different DAGs can help, sometimes, to discuss and resolve the disagreement.\nWe are going to use DAGs for two distinct purposes. One purpose is to inform responsible conclusions from data about what causes what. The data on its own is insufficient to demonstrate the causal connections. However, data combined with a DAG can tell us something. Sometimes a DAG includes a causal connection that should create an association between variables. The DAG is incomplete if the association does not appear in the data.\nDAGs are also valuable aids for building models. For example, analysis of the paths in a DAG, as in Lesson 30, can tell us which explanatory variables to include and which to exclude from a model if our modeling goal is to represent the hypothetical causal connections.\nIn these Lessons, we have a second, entirely different, use for DAGs: learning modeling technique. Our approach will be to outfit DAGs with specific formulas representing the mechanism imbued in each node. DAGs equipped with formulas can be used to generate simulated data.3 Training a model on those data leads to a model function that we can compare to the DAG’s formulas. Then check whether the formulas and the model function match. This practice helps us learn what can go right or wrong in building a model, just as practice in an aircraft simulator trains pilots to handle real-world situations in real aircraft.\nWe start with a simple example. The DAG called dag09 has two exogenous nodes (a and b) and another node, c, that gets input from both a and b.\n\ndag_draw(dag09)\n\n\n\n\n\nprint(dag09)\n\na ~ exo()\nb ~ exo()\nc ~ binom(2 * a + 3 * b)\n\n\nThe dag_draw() command draws a picture of the graph. Printing the dag displays the formulas that set the values of the nodes.\nThe formulas for dag09 show that the nodes a and b are exogenous, their values set randomly and independently of one another by the exo() function. In contrast, the formula for node c says that the value of c will be a linear combination of the values of a and b, translated into a zero-one format.\nGenerate simulated data using the sample() function. For instance,\n\nsample(dag09, size=5)\n\n# A tibble: 5 × 3\n       a      b     c\n   <dbl>  <dbl> <dbl>\n1 -0.326  1.17      1\n2  0.552  0.619     0\n3 -0.675 -0.113     0\n4  0.214  0.917     1\n5  0.311 -0.223     0\n\n\nEach row in the sample is one trial; in each trial, the node’s formula sets the value for that node. For example, the formula might use the values of other nodes as input. Alternatively, the formula might specify that the node is exogenous, without input from any other nodes.\nModels can be trained on the simulated data using the same techniques as for any other data. To illustrate, here we generate a sample of size \\(n=10,000\\), then fit the model c ~ a + b and summarize by taking the coefficients.\n\nsample(dag09, size=10000) %>% \n  lm(c ~ a + b, data = .) %>%\n  confint()\n\n                2.5 %    97.5 %\n(Intercept) 0.4996543 0.5132192\na           0.1927018 0.2062122\nb           0.2945928 0.3081950\n\n\nThe coefficients on a and b are inconsistent with the dag09 formulas. This discrepancy suggests the existence of a flaw in the modeling technique. In the following box, we demonstrate another modeling technique that can do the job.\n\n\n\n\n\n\nDemonstration: Modeling binomial variables\n\n\n\nKeep in mind that this is just a demonstration. There is no need to master (or even understand) the calculations presented in this box.\nThe printed version of dag09 shows that the value of node c is a linear combination of a and b converted into a zero-one, binomial value. Unfortunately, the linear modeling trainer, lm(), is not well-tuned to work with binomial data. Another modeling technique, “logistic regression,” does a better job. The glm() function trains logistic regression models on data.\n\nsample(dag09, size=10000) %>% \n  glm(c ~ a + b, data = ., family=\"binomial\") %>%\n  confint()\n\nWaiting for profiling to be done...\n\n\n                  2.5 %     97.5 %\n(Intercept) -0.02973142 0.09550381\na            1.86979872 2.05982255\nb            2.97350879 3.23088485\n\n\nWhen we use the appropriate modeling technique, we can, in this case, recover the coefficients in the DAG formula: 2 for a and 3 for b.\n\n\n\n\n\n\n\n\nReality check: DAGs and data\n\n\n\nDAGs represent hypotheses about the connections between variables in the real world. They are a kind of scratchpad for constructing alternative scenarios and, as seen in Lesson 28, thinking about how models might go wrong in the face of a plausible alternative causal mechanism.\nIn this book, we extend the use of DAGs beyond their scope in professional statistics; we use them as simulations from which we can generate data. Such simulations provide one way to learn about statistical methodology.\nDAGs are aides to reasoning, scratchpads that help us play out the consequences of our hypotheses about possible real-world mechanisms. However, take caution to distinguish data from DAG simulations from data from reality.\nFinding out about the real world requires collecting data from the real world. The proper role of DAGs in real work is to guide model building from real data.\nIn this course, we sample from DAGs to learn statistical techniques. But never to make claims about real-world phenomena."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#samples-summaries-and-samples-of-summaries",
    "href": "Reading-notes-lesson-20.html#samples-summaries-and-samples-of-summaries",
    "title": "20  Measuring and simulating variation",
    "section": "Samples, summaries, and samples of summaries",
    "text": "Samples, summaries, and samples of summaries\nBeginners sometimes think that each row in a data frame is a sample. Better to say that each row is a “specimen.” A “sample” is a collection of specimens, the set of rows in a data frame.\nThe “sample size” is the number of rows. “Sampling” is the process of collecting the specimens to be put into the data frame.\nThe following command illustrates computing a summary of a sample from dag09.\n\nsample(dag09, size=10000) %>% \n  glm(c ~ a + b, data = ., family=\"binomial\") %>%\n  confint()\n\nWaiting for profiling to be done...\n\n\n                  2.5 %     97.5 %\n(Intercept) -0.07393373 0.04945966\na            1.90174806 2.09051562\nb            2.81158596 3.05507077\n\n\nAn essential question in statistics is how the summary depends on the incidental specifics of a particular sample. DAGs provide a convenient way to address this question since we can generate multiple samples from the same DAG, summarize each, and compare those summaries.\nTo generate a sample of summaries, re-run many trials of the summary. The do() function automates this process, accumulating the results from the trials in a single data frame: a “sample of summaries.” We will use do() mostly in demonstrations.\n\n\n\n\n\n\nDemonstration: Conducting many trials with do()\n\n\n\nIn this demonstration, we will revisit a model used earlier in this Lesson to see how much the coefficients vary from one sample to another. Each trial consists of drawing a sample from dag09, training a model, and summarizing with the model coefficients. Curly braces ({ and }) surround the commands needed for an individual trial.\nPreceding the curly braces, we have placed do(5) *. This instruction causes the trial to be repeated five times.\n\ndo(5) * {\n  sample(dag09, size=10000) %>% \n    glm(c ~ a + b, data = ., family=\"binomial\") %>%\n    coefficients()\n}\n\n    Intercept        a        b\n1 -0.01315189 2.041331 3.049575\n2 -0.01027258 1.987569 3.074509\n3  0.02787421 1.934953 3.044001\n4 -0.03509540 1.973983 2.969853\n5 -0.09887196 1.978861 2.971170\n\n\nThe five trials are collected together by do() into the five rows of a single data frame. Such a data frame can be considered a “sample of summaries.”\n\n\nOne of the things we will do with a “sample of summaries” is to … wait for it … summarize it. For instance, in the following code chunk, a sample of 40 summaries is stored under the name Trials. Then we will summarize Trials, in this case, to see how much the values of the a and b coefficients vary from trial to trial.\n\nTrials <- do(40) * {\n  sample(dag09, size=10000) %>% \n    glm(c ~ a + b, data = ., family=\"binomial\") %>%\n    coefficients()\n} \nTrials %>% \n  summarize(mean_a = mean(a), spread_a = sd(a), \n            mean_b = mean(b), spread_b = sd(b))\n\n    mean_a   spread_a   mean_b   spread_b\n1 2.004514 0.04364563 3.012044 0.06410626\n\n\nThe result of summarizing the trials is a “summary of a sample of summaries.” This phrase is admittedly awkward, but we will use this technique often: summarizing trials, where each trial is a “summary of a sample” Often, the clue will be the use of do(), which repeats trials as many times as you ask.\n\n::: {.callout-warning} ## For the statistically experienced reader\nWarning! This box contains mathematical formulas that are not needed for the course. The formulas might interest mathematically inclined statistics instructors, but others can skip this material.\nThe algorithm for the variance described previously is not used by any statistical software package; there are much faster ways to arrive at the result. One way to see this is to compare the traditional formula for the variance to the formula version of the above algorithm:\n\\[\\underbrace{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}_\\text{traditional} \\ \\ \\text{versus} \\ \\ \\underbrace{\\frac{1}{2n} \\sum_{i=1}^n \\left[\\frac{1}{n-1}\\sum_{j\\neq i} (x_i - x_j)^2\\right]}_\\text{our algorithm}\\] The inefficiency of the algorithm stems from the double sum. The advantage of the algorithm is conceptual and two-fold:\n\nThe \\(n-1\\) in the formula for the variance comes from the \\(j \\neq i\\) in the inner sum. Why not \\(\\sum_j=1^n\\)? Because that would put \\(n\\) zeros on the roll and bias the result downward. We want to average the square distance between each value and every other value.\nThere is no need to introduce the mean \\(\\bar{x}\\) of the values. Of course, \\(\\bar{x}\\) is easy and fast to calculate, so there is no numerical reason to avoid using it in the calculation. There is, however, a philosophical reason based on Stephen J. Gould’s observation, quoted at the start of this Lesson: “Variation is the hard reality. Means are the abstractions.”\n\nThe usual definition given for the variance uses the mean as the reference point for calculating deviations. This use imbues the mean with an unjustified veneer of reality. The pairwise square difference algorithm demonstrates that the mean is not needed to calculate the variance; it is just, as Gould says, a mathematical abstraction.\nThere is a factor \\(\\frac{1}{2}\\) in the formula for the variance based on the pairwise square-differences. The \\(\\frac{1}{2}\\) may seem inelegant to some readers. Stripping out the \\(\\frac{1}{2}\\), the quantity has a name dating from at least 1885: the modulus. Writing the modulus as \\(c = 2 s^2\\) makes the formula for the gaussian function cleaner than the formula usually given:\n\\[\\underbrace{\\frac{1}{\\sqrt{2 \\pi s^2}} \\exp\\left[-\\frac{(x-m)^2}{2s^2}\\right]}_\\text{using standard deviation}\\ \\ \\text{vs}\\ \\ \\ \\underbrace{\\frac{1}{\\sqrt{\\pi c^2}} \\exp\\left[-\\frac{(x-m)^2}{c^2}\\right]}_\\text{using modulus}\\] In mathematical and scientific nomenclature, “modulus” is roughly synonymous with “size.” For example, in algebra, \\(\\left|x\\right|\\) is the modulus of \\(x\\). “Modulus” is less off-putting than “standard deviation” and less redolent of the unfortunate early ties between statistics and eugenics. Looking back, perhaps “modulus” would have been the better choice for parameterizing the gaussian."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#sec-signal-and-noise",
    "href": "Reading-notes-lesson-21.html#sec-signal-and-noise",
    "title": "21  Signal and noise",
    "section": "Signal and noise",
    "text": "Signal and noise\nTo illustrate the statistical problem of signal and noise, let us turn to a DAG simulation: dag01. Here’s a sample from dag01:\n\nTiny <- sample(dag01, size=2)\n\n\n\n\n?(caption)\n\n\n\n# A tibble: 2 × 2\n       x     y\n   <dbl> <dbl>\n1 -0.326  2.84\n2  0.552  5.04\n\n\nThe DAG simulation implements a relationship between x and y. In statistics, this relationship is the signal.\n\nLook at the 2-row sample (?tbl-tiny-dag01) from the DAG and guess what the relationship might be.\n\nAny of an infinite number of possible relationships could account for the x and y data. The noise reduction problem of statistics is to make a guess that is as good as possible. Unfortunately, for a sample with \\(n=2\\), as “good as possible” is not very good!\nMore data—a bigger sample—gives us a better shot at revealing the relationship hidden by the noise. ?tbl-small-dag01 shows a sample of size \\(n=10\\):\n\nSmall <- sample(dag01, size=10)\n\n\n\n\n?(caption)\n\n\n\n# A tibble: 10 × 2\n         x     y\n     <dbl> <dbl>\n 1 -0.786  1.89 \n 2  0.0547 4.12 \n 3 -1.17   2.36 \n 4 -0.167  6.33 \n 5 -1.87   0.933\n 6 -0.120  2.93 \n 7  0.826  5.70 \n 8  1.19   5.90 \n 9 -1.09   2.13 \n10 -0.375  4.23 \n\n\nA careful perusal of the Small sample suggests some patterns. x is never larger than about 2 in magnitude and can be positive or negative. y is always positive. Furthermore, when x is negative, the corresponding y value is relatively small compared to the y values for positive x.\nA sample of size \\(n=10\\) provides more information than a sample of \\(n=2\\), so we can make a more informed guess about the relationship between variables x and y.\nHuman cognition is not well suited to looking at long columns of numbers. Often, we can make better use of our natural human talents by translating the sample into a graphic:\n\n\n\n\n\nCollecting more data can make the relationship clearer. Figure 21.1 displays an \\(n=10,000\\) sample.\n\nLarge <- sample(dag01, size=10000)\n\n\n\n\n\n\nFigure 21.1: With \\(n=10,000\\) rows, the relationship between x and y is evident graphically. (The original Small sample is shown in orange.)\n\n\n\n\nThere are many possible ways to describe the x-y relationship in Figure 21.1. For instance, we can see that when x is positive, y is almost always greater than 4, but for negative x, the value of y tends to be less than 4. Such a description might be apt for some purposes, but in these Lessons, we describe relationships by fitting models to data. For example, the following command uses the small sample (n=10) as the training data:\n\nlm(y ~ x, data = Small) %>% coefficients()  # n = 10 sample\n\n(Intercept)           x \n   4.262846    1.741758 \n\n\nThe coefficients provide the information needed to construct the formula for the model function: \\[y = 4.26 + 1.74 x\\ .\\] This formula is a guess of the signal—the relationship between the two variables in dag01. Unfortunately, the formula tells us nothing about the noise obscuring the signal nor how good the guess is.\nThe model coefficients produced by training the model on a much larger sample will presumably be a better guess:\n\nlm(y ~ x, data = Large) %>% coefficients() #  n = 10,000 sample\n\n(Intercept)           x \n   4.008928    1.495904 \n\n\nUnfortunately, we cannot tell from the coefficients how good the guess is.\nLuckily for us, since the data are a simulation from a DAG, we can see what the coefficients should be as well as the origin of the noise mixing in with the signal.\n\nprint(dag01)\n\nx ~ exo()\ny ~ 1.5 * x + 4 + exo()\n\n\nThe Large sample produced coefficients much closer than the Small sample to the mechanism in the DAG. The idea that larger samples lead to better accuracy has been appreciated since the 16th century and now has the prestige of being a “Law”: the Law of Large Numbers.\nHowever, “better accuracy” does not tell us whether the accuracy suffices for any given purpose. The model filters out some of the noise. However, the model coefficients still display a noisy legacy.\nThe challenge of real-world data is that we cannot open the black box that generated the data; all we have is the data! So how can we tell whether the data at hand are sufficient for giving a usefully accurate description of the actual relationships?\nThe key to the puzzle is the variation within the sample."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#measuring-variation",
    "href": "Reading-notes-lesson-21.html#measuring-variation",
    "title": "21  Signal and noise",
    "section": "Measuring variation",
    "text": "Measuring variation\nLesson 20 introduced the standard way to measure variation in a single variable: the variance or its square root, the standard deviation. For instance, we can measure the variation in the variables from the Large sample using sd() and var():\n\nLarge %>%\n  summarize(sx = sd(x), sy = sd(y), vx = var(x), vy = var(y))\n\n# A tibble: 1 × 4\n     sx    sy    vx    vy\n  <dbl> <dbl> <dbl> <dbl>\n1 0.983  1.78 0.966  3.16\n\n\nAccording to the standard deviation, the size of the x variation is about 1. The size of the y variation is about 1.7.\nLook again at the formulas that compose dag01:\n\nprint(dag01)\n\nx ~ exo()\ny ~ 1.5 * x + 4 + exo()\n\n\nThe formula for x shows that x is endogenous, its values coming from a random number generator, exo(), which, unless otherwise specified, generates noise of size 1.\nAs for y, the formula includes two sources of variation:\n\nThe part of y determined by x, that is \\(y = \\mathbf{1.5 x} + \\color{gray}{4 + \\text{exo()}}\\)\nThe noise added directly into y, that is \\(y = \\color{gray}{\\mathbf{1.5 x} + 4} + \\color{black}{\\mathbf{exo(\\,)}}\\)\n\nThe 4 in the formula does not add any variation to y; it is just a number.\nWe already know that exo() generates random noise of size 1. So the amount of variation contributed by the + exo() term in the DAG formula is 1. The remaining variation is contributed by 1.5 * x. The variation in x is 1 (coming from the exo() in the formula for x). A reasonable guess is that 1.5 * x will have 1.5 times the variation in x. So, the variation contributed by the 1.5 * x component is 1.5. The overall variation in y is the sum of the variations contributed by the individual components. This suggests that the variation in y should be \\[\\underbrace{1}_\\text{from exo()} + \\underbrace{1.5}_\\text{from 1.5 x} = \\underbrace{2.5}_\\text{overall variation in y}.\\] Simple addition! Unfortunately, the result is wrong. In the previous summary of the Large, we measured the overall variation in y as about 1.72.\nThe variance will give a better accounting than the standard deviation. Recall that exo() generates variation whose standard deviation is 1, so the variance from exo() is \\(1^2 = 1\\). Since x comes entirely from exo(), the variance of x is 1. So is the variance of the exo() component of y.\nTurn to the 1.5 * x component of y. Since variances involve squares, the variance of 1.5 * x works out to be \\(1.5^2\\, \\text{var(}\\mathit{x}\\text{)} = 2.25\\). Adding up the variances from the two components of y gives\n\\[\\text{var(}\\mathit{y}\\text{)} = \\underbrace{2.25}_\\text{from 1.5 exo()} + \\underbrace{1}_\\text{from exo()} = 3.25\\]\nThis result that the variance of y is 3.25 closely matches what we found in summarizing the y data generated by the DAG.\nThe lesson here: When adding two sources of variation, the variances of the individual sources add to form the overall variance of the sum. Just like \\(A^2 + B^2 = C^2\\)."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#dags-from-data",
    "href": "Reading-notes-lesson-21.html#dags-from-data",
    "title": "21  Signal and noise",
    "section": "DAGs from data",
    "text": "DAGs from data\nIn modeling data from dag01 we could recover a good approximation to the formula for y.\n\nLarge %>%\n  lm(y ~ x, data = .) %>%\n  coefficients()\n\n(Intercept)           x \n   4.008928    1.495904 \n\n\nA DAG describes the causal links between variables. Data modeling reveals the formula implementing the causal link in dag01. Nevertheless, it is wrong to think we can determine the DAG that generated the data from the data alone. Only if we already know the structure of the data-generation DAG can we recover the mechanism inside that DAG. For instance, another statistical thinker might believe that the causal mechanism behind the data is y causing x. Based on this assumption, she also can find the mechanism inside her hypothesized DAG:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .) %>%\n  coefficients()\n\n(Intercept)           y \n -1.8261448   0.4559782 \n\n\nA DAG is a hypothesis, a statement that might or might not be true. DAGs are part of the statistical apparatus for thinking responsibly about causality. Use a DAG—or, potentially, multiple DAGs—when the issue of what causes what is relevant to the purpose behind the work.\nWhen there are only two variables involved in the system under consideration—we will call them X and Y for simplicity—there are only two possible DAGs:\n\\[X \\rightarrow Y\\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\ X \\leftarrow Y\\] Our understanding of the world sometimes allows us to focus on one of these and not the other. Example: Does the rooster crowing cause the sun to rise, or does the rising sun cause the rooster to crow?\nBeyond the two DAGs \\(X \\rightarrow Y\\) and \\(X \\leftarrow Y\\), additional DAG possibilities can account for the relationship between X and Y. For instance, if we introduce another variable, C, located between X and Y, four other DAGs need to be considered:\n\\[X \\rightarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\leftarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\rightarrow C \\leftarrow Y\\]\nThere are many other DAG configurations involving three variables. To keep things simple, we will restrict things to DAGs where X might or might not cause Y, but Y never causes X.1 Figure 21.2 shows the ten configurations of 3-variable DAGs where Y does not cause X.\n\n\n\n\n\nFigure 21.2: Ten DAG configurations involving three variables X, Y, and C and where there is no causal path going from Y to X.\n\n\n\n\nWith the conceptual tool of DAGs, the statistical thinker can consider multiple possibilities for what might cause what. Sometimes she can discard some of the possibilities based on common sense. (Think: roosters and the sun.) However, in other settings, there may be possibilities that she does not favor but might be plausible to other people. In Lesson 28, we will explore how each configuration of DAG has implications for which model formulas can or cannot reveal the hypothesized causal mechanism."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#sampling-bias",
    "href": "Reading-notes-lesson-22.html#sampling-bias",
    "title": "22  Sampling and sampling variation",
    "section": "Sampling bias",
    "text": "Sampling bias\nCollecting a reliable sample is usually considerable work. An ideal is the “simple random sample” (SRS), where all of the items are available, but only some are selected—completely at random—for recording as data. Undertaking an SRS requires assembling a “sampling frame,” essentially a census. Then, with the sampling frame in hand, a computer or throws of the dice can accomplish the random selection for the sample.\nUnderstandably, if a census is unfeasible, constructing a perfect sampling frame is hardly less so. In practice, the sample is assembled by randomly dialing phone numbers or taking every 10th visitor to a clinic or similar means. Unlike genuinely random samples, the samples created by these practical methods are not necessarily representative of the larger group. For instance, many people will not answer a phone call from a stranger; such people are underrepresented in the sample. Similarly, the people who can get to the clinic may be healthier than those who cannot. Such unrepresentativeness is called “sampling bias.”\nProfessional work, such as collecting unemployment data, often requires government-level resources. Assembling representative samples uses specialized statistical techniques such as stratification and weighting of the results. We will not cover the specialized techniques in this introductory course, even though they are essential in creating representative samples. The table of contents of a classic text, William Cochran’s Sampling techniques shows what is involved.\nAll statistical thinkers, whether expert in sampling techniques or not, should be aware of factors that can bias a sample away from being representative. In political polls, many (most?) people will not respond to the questions. If this non-response stems from, for example, an expectation that the response will be unpopular, then the poll sample will not adequately reflect unpopular opinions. Such non-response bias can be significant, even overwhelming, in surveys.\nSurvival bias plays a role in many settings. The mosaicData::TenMileRace data frame provides an example, recording the running times of 8636 participants in a 10-mile road race and including information about each runner’s age. Can such data carry information about changes in running performance as people age? The data frame includes runners aged 10 to 87. Nevertheless, a model of running time as a function of age from this data frame is seriously biased. The reason? As people age, casual runners tend to drop out of such races. So the older runners are skewed toward higher performance. (We can see this by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.\n\n\n\n\n\n\nExamples: Returned to base\n\n\n\nAn inspiring story about dealing with survival bias comes from a World War II study of the damage sustained by bombers due to enemy guns. The sample, by necessity, included only those bombers that survived the mission and returned to base. The holes in those surviving bombers tell a story of survival bias. Shell holes on the surviving planes were clustered in certain areas, as depicted in Figure 22.1. The clustering stems from survivor bias. The unfortunate planes hit in the middle of the wings, cockpit, engines, and the back of the fuselage did not return to base. Shell hits in those areas never made it into the record.\n\n\n\n\n\n\n\n\n\nFigure 22.1: An illustration of shell-hole locations in planes that returned to base. Source: Wikipedia"
  },
  {
    "objectID": "Reading-notes-lesson-22.html#measuring-sampling-variation",
    "href": "Reading-notes-lesson-22.html#measuring-sampling-variation",
    "title": "22  Sampling and sampling variation",
    "section": "Measuring sampling variation",
    "text": "Measuring sampling variation\nSampling variation is a form of noise. Unlike some other forms of noise, modeling cannot filter out sampling variation or reduce its magnitude. Sampling variation is easiest to see by collecting multiple samples from the same source and summarizing each one. The summaries likely will vary from sample to sample: sampling variation.\nTypically, the data frame at hand is our only sample. With no other samples to compare it to, it may seem impossible to measure sampling variation. In this Lesson, we will use simulations from DAGs to study sampling variation. DAG simulations are suited to this because we can effortlessly collect as many samples as we wish from a DAG. In Lesson 23, we will use the knowledge gained from the simulations to see how to measure sampling variation even when there is only one sample.\nIn the spirit of starting simply, we return to dag01. This DAG is \\(\\mathtt{x}\\longrightarrow\\mathtt{y}\\). The causal formula setting the value of y is y ~ 4 + 1.5 * x + exo().\nIt is crucial to remember that sampling variation is not about the row-to-row variation in a single sample. Rather, it is about the variation in the summary from one sample to another. So our initial process for exploring sampling variation will be to carry out many trials, each trial being a summary of a sample.\n::: {.callout-warning} ## Demonstration: Samples and specimens\nTo illustrate, here is one trial using a sample of \\(n=25\\) and a simple model, y ~ 1.\n\nSample <- sample(dag01, size=25) \nSample %>% \n  lm(y ~ 1, data = .) %>%\n  coefficients()\n\n(Intercept) \n    4.68847 \n\n\nWe cannot see sampling variation directly in the above result because there is only one trial. The sampling variation becomes evident when we run many trials. In each trial, a new sample (of size \\(n=25\\) is taken and summarized.)\n\nTrials <- do(100) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coefficients()\n}\n\n\n\n\n\n\nTable 22.1:  Trials for seeing sampling variation. \n \n  \n    Intercept \n  \n \n\n  \n    3.750687 \n  \n  \n    4.552955 \n  \n  \n    3.947941 \n  \n  \n    4.008801 \n  \n  \n    3.578861 \n  \n  \n    4.006602 \n  \n  \n    4.259823 \n  \n  \n    3.869581 \n  \n  \n    3.917671 \n  \n  \n    3.932557 \n  \n  \n    3.727117 \n  \n  \n    4.094241 \n  \n  \n    3.649158 \n  \n  \n    4.017057 \n  \n  \n    3.511036 \n  \n  \n    4.278219 \n  \n  \n    3.862383 \n  \n  \n    3.835646 \n  \n  \n    4.343176 \n  \n  \n    4.094017 \n  \n  \n    4.664953 \n  \n  \n    3.586219 \n  \n  \n    3.730871 \n  \n  \n    3.876046 \n  \n  \n    4.279536 \n  \n\n\n\n\n\nTrials is a sample of summaries. (See Lesson 20). The row-to-row variation inTrials comes from sampling variation. We can summarize the variation in the sample of summaries. As always, our standard measure of variation is the standard deviation (or, equivalently, variance):\n\nTrials %>%\n  summarize(se = sd(Intercept))\n\n        se\n1 0.356543\n\n\nThis summary quantity, which we have named se, has a technical name in statistics: the standard error. The standard error is an ordinary standard deviation in a particular context: the standard deviation of a sample of summaries. The words standard error should be followed by a description of the summary and the size of the individual samples involved. Here it would be, “The standard error of the Intercept coefficient from a sample of size \\(n=25\\) is around 0.36.”\nIt is easy to confuse “standard error” with “standard deviation.” Adding to the potential confusion is another related term, the “margin of error.” To avoid this confusion, we will tend to use an interval description of the sampling variation called the “confidence interval.” However, for the present, we will continue with the standard error, sometimes written SE for short."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#se-depends-on-the-sample-size",
    "href": "Reading-notes-lesson-22.html#se-depends-on-the-sample-size",
    "title": "22  Sampling and sampling variation",
    "section": "SE depends on the sample size",
    "text": "SE depends on the sample size\nWe found an SE of 0.36 on the Intercept in a sample of size \\(n=25\\). We can see how the SE depends on sample size by repeating the trials for several different sizes, say, \\(n=25\\), 100, 400, 1600, 6400, 25,000, and 100,000.\nThe following command estimates the SE a sample of size 400:\n\nTrials <- do(1000) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coefficients()\n}\nTrials %>% summarize(se400 = sd(Intercept))\n\n      se400\n1 0.3538538\n\n\nWe repeated this process for each of the other sample sizes. Table 22.2 reports the results.\n\n\n\n\n\n\n\n\nTable 22.2:  Results of repeating the sampling variability trials for samples of varying sizes. \n \n  \n    n \n    se \n  \n \n\n  \n    25 \n    0.3600 \n  \n  \n    100 \n    0.1900 \n  \n  \n    400 \n    0.0910 \n  \n  \n    1600 \n    0.0430 \n  \n  \n    6400 \n    0.0230 \n  \n  \n    25000 \n    0.0110 \n  \n  \n    100000 \n    0.0056 \n  \n\n\n\n\n\nThere is a pattern in Table 22.2. Every time we double \\(n\\), the standard error goes down by a factor of 2, that is, \\(\\sqrt{4}\\). (The pattern is not exact because there is also sampling variation in the trials.)\nConclusion: The larger the sample size, the smaller the SE. For a sample size of \\(n\\), the SE will be proportional to \\(1/\\sqrt{\\strut n}\\)."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#the-confidence-interval",
    "href": "Reading-notes-lesson-22.html#the-confidence-interval",
    "title": "22  Sampling and sampling variation",
    "section": "The confidence interval",
    "text": "The confidence interval\nThe “confidence interval” is a more user-friendly format than SE for describing the amount of sampling variation. Being an interval, write it either as [lower, upper] or center\\(\\pm\\)half-width. These styles are equivalent; both styles are correct. (The preferred style can depend on the field or the journal publishing the report.)\nIn practice, confidence intervals are calculated using special-purpose software such as the confint() function, for instance:\n\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  confint()\n\n                  2.5 %      97.5 %\n(Intercept) -533.432471 -406.521402\ndistance     246.387096  261.229494\nclimb          2.493307    2.726209\n\n\nNotice that there is a separate confidence interval for each model coefficient. The sampling variation is essentially the same, but that variation appears different when translated to the various coefficients’ units.\n\n\n\n\n\n\nDemonstration: How many digits?\n\n\n\nThe confidence intervals on the model time ~ distance + climb, report the results to many digits. Such a report is appropriate for further calculations that might need doing, but it is usually not appropriate for a human reader.\nTo know how many digits are worth reporting to humans, look toward the standard error. The standard error is a part of a different kind of summary of a model: the “regression report.” We will only need to look at regression reports in the last few Lessons of the course. Here we want to point out how many digits are worth reporting to humans. That requires looking at the standard error itself.\nPreviously, we looked at the confidence intervals on coefficients from the Hill_racing model. Now we look at the regression summary, which contains the information on sampling variation in a different format.\n\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  regression_summary()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -470.     32.4        -14.5 9.92e- 46\n2 distance      254.      3.78        67.1 0        \n3 climb           2.61    0.0594      43.9 4.08e-304\n\n\nEach coefficient’s standard error appears in the std.error column of the regression summary.\nFor the human reader, only the first two significant digits of the standard error are worth reporting. (This is true regardless of the data and model design.) Here, the SE is 32 for the Intercept, 3.8 for the distance coefficient, and 0.059 for the climb coefficient. The confidence interval will be the coefficient (column labeled estimate) plus or minus “twice” the std.error. It is appropriate to round the confidence interval (for a human reader) to the first two significant digits of the standard error.\nFor example, the confidence interval on the distance coefficient will be \\(253.808295 \\pm 2 \\times 3.78433220\\). Keep only the digits before the first two significant digits of the SE, so the reported interval can be \\(253.8 \\pm 3.8\\)."
  },
  {
    "objectID": "Reading-notes-lesson-23.html#subsampling",
    "href": "Reading-notes-lesson-23.html#subsampling",
    "title": "23  Estimating sampling variation from a single sample",
    "section": "Subsampling",
    "text": "Subsampling\nTo “subsample” means to draw a smaller sample from a large one. “Small” and “large” are relative. For our example, we turn to the TenMileRace data frame containing the record of thousands of runners’ times in a race, along with basic information about each runner. There are many ways we could summarize TenMileRace. Any summary would do for the example. We will summarize the relationship between the runners’ ages and their start-to-finish times (variable net), that is, net ~ age. To avoid the complexity of a runner’s improvement with age followed by a decline, we will limit the study to people over 40.\n\nTenMileRace %>% filter(age > 40) %>%\n  lm(net ~ age, data = .) %>% coefficients()\n\n(Intercept)         age \n 4278.21279    28.13517 \n\n\nThe units of net are seconds, and the units of age are years. The model coefficient on age tells us how the net time changes for each additional year of age: seconds per year. Using the entire data frame, we see that the time to run the race gets longer by about 28 seconds per year. So a 45-year-old runner who completed this year’s 10-mile race in 3900 seconds (about 9.2 mph, a pretty good pace!) might expect that, in ten years, when she is 55 years old, her time will be longer by 280 seconds.\nIt would be asinine to report the ten-year change as 281.3517 seconds. The runner’s time ten years from now will be influenced by the weather, crowding, the course conditions, whether she finds a good pace runner, the training regime, improvements in shoe technology, injuries, and illnesses, among other factors. There is little or nothing we can say from the TenMileRace data about such factors.\nThere’s also sampling variation. There are 2898 people older than 40 in the TenMileRace data frame. The way the data was collected (radio-frequency interrogation of a dongle on the runner’s shoe) suggests that the data is a census of finishers. However, it is also fair to treat it as a sample of the kind of people who run such races. People might have been interested in running but had a schedule conflict, lived too far away, or missed their train to the start line in the city.\nWe see sampling variation by comparing multiple samples. To create those multiple samples from TenMileRace, we will draw, at random, subsamples of, say, one-tenth the size of the whole, that is, \\(n=290\\)\n\nOver40 <- TenMileRace %>% filter(age > 40)\nlm(time ~ age, data = Over40 %>% sample(size=290)) %>% coefficients()\n\n(Intercept)         age \n 4847.48393    18.30472 \n\nlm(time ~ age, data = Over40 %>% sample(size=290)) %>% coefficients()\n\n(Intercept)         age \n 5154.58190    15.63354 \n\n\nThe age coefficients from these two subsampling trials differ one from the other by about 0.5 seconds. To get a more systematic view, run more trials:\n\n# a sample of summaries\nTrials <- do(1000) * {\n  lm(time ~ age, data = sample(Over40, size=290)) %>% coefficients()\n}\n# a summary of the sample of summaries\nTrials %>%\n  summarize(se = sd(age))\n\n        se\n1 9.163039\n\n\nWe used the name se for the summary of samples of summaries because what we have calculated is the standard error of the age coefficient in a sample of size \\(n=290\\).\nIn Lesson 22 we saw that the standard error is proportional to \\(1/\\sqrt{\\strut n}\\), where \\(n\\) is the sample size. From the subsamples, know that the SE for \\(n=290\\) is about 9.0 seconds. This tells us that the SE for the full \\(n=2898\\) samples would be about \\(9.0 \\frac{\\sqrt{290}}{\\sqrt{2898}} = 2.85\\).\nSo the interval summary of the age coefficient—the confidence interval— is \\[\\underbrace{28.1}_\\text{age coef.} \\pm 2\\times\\!\\!\\!\\!\\!\\!\\! \\underbrace{2.85}_\\text{standard error} =\\ \\ \\ \\  28.1 \\pm\\!\\!\\!\\!\\!\\!\\!\\! \\underbrace{5.6}_\\text{margin of error}\\ \\  \\text{or, equivalently, 22.6 to 33.6}\\]"
  },
  {
    "objectID": "Reading-notes-lesson-23.html#bootstrapping",
    "href": "Reading-notes-lesson-23.html#bootstrapping",
    "title": "23  Estimating sampling variation from a single sample",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nThere is a trick, called “resampling,” to generate a random subsample of a data frame with the same \\(n\\) as the data frame: draw the new sample randomly from the original sample with replacement. An example will suffice to show what the “with replacement” does:\n\nexample <- c(1,2,3,4,5)\n# without replacement\nsample(example)\n\n[1] 1 4 3 5 2\n\n# now, with replacement\nsample(example, replace=TRUE)\n\n[1] 2 4 3 3 5\n\nsample(example, replace=TRUE)\n\n[1] 3 5 4 4 4\n\nsample(example, replace=TRUE)\n\n[1] 1 1 2 2 3\n\nsample(example, replace=TRUE)\n\n[1] 4 3 1 4 5\n\n\nThe “with replacement” leads to the possibility that some values will be repeated two or more times and other values will be left out entirely.\nThe calculation of the SE using resampling is called “bootstrapping.”\n\n\n\n\n\n\nDemonstration: Bootstrapping the standard error\n\n\n\nWe will apply bootstrapping to find the standard error of the age coefficient from the model time ~ age fit to the Over40 data frame.\nThere are two steps:\n\nRun many trials, each of which fits the model time ~ age using lm(). From trial to trial, the data used for fitting is a resampling of the Over40 data frame. The result of each trial is the coefficients from the model.\nSummarize the trials with the standard deviation of the age coefficients.\n\n\n# run many trials\nTrials <- do(1000) * {\n  lm(time ~ age, data = sample(Over40, replace=TRUE)) %>% \n       coefficients()\n}\n# summarize the trials to find the SE\nTrials %>% summarize(se = sd(age))\n\n        se\n1 2.859483"
  },
  {
    "objectID": "Reading-notes-lesson-24.html#categorical-outputs",
    "href": "Reading-notes-lesson-24.html#categorical-outputs",
    "title": "24  Effect size: Input to output",
    "section": "Categorical outputs",
    "text": "Categorical outputs\nSometimes the relevant effect size involves a categorical output variable. A case in point is the possible confounding of the Start/Stop feature with vehicle size. To investigate this, we should build a model with Start/Stop as the output and vehicle size as the input.\nIn this case, the issue of whether vehicle size causes Start/Stop is not essential. We are not concerned with the decisions made by automobile designers so much as with the possible confounding.\nWhen the output variable is categorical, it is not reasonable to calculate the change in output as the difference in categories. As before, “Yes” - “No” is not a number. Still, there is a meaningful and helpful way to quantify a change in a categorical output.\nThe essential insight is quantifying the change in output in terms of probabilities. For instance, a small effect size would reflect a slight chance of the output changing from one level to another.\nThe appropriate model type for a categorical output is to transform the output to a zero-one variable, as introduced in Lesson 19. We will present this in a demonstration here and return to the topic more fully in Lesson 34.\n\n\n\n\n\n\nDemonstration: Start/Stop and vehicle size\n\n\n\nAs described earlier, we are interested in the possibility that Start/Stop is available mainly on large, higher-fuel-consumption cars. If so, that would explain why the effect size we calculated of fuel cost with respect to Start/Stop was positive.\nThe model we build will have a zero-one encoding of Start/Stop as the response and the vehicle’s fuel cost as the explanatory variable.\n\nMPG <- MPG %>% \n    mutate(has_start_stop = zero_one(start_stop, one=\"Yes\"))\nMod4 <- lm(has_start_stop ~ EPA_fuel_cost, data = MPG)\nmodel_eval(Mod4, EPA_fuel_cost=c(1600, 2000))\n\n  EPA_fuel_cost   .output       .lwr     .upr\n1          1600 0.4901341 -0.4891981 1.469466\n2          2000 0.5207835 -0.4583924 1.499959\n\n\nThe .output here is interpreted as a probability of start_stop having the value “Yes.” (That is because we set one=\"Yes\" in the zero_one() conversion.) The model_eval() report indicates $400 per year increase in fuel cost is associated with a three percentage point increase in the probability of a vehicle having a Start/Stop feature. That is a small effect, so we see little support for our hypothesis that Start/Stop tends to be installed on larger, more fuel-efficient vehicles."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#multiple-explanatory-variables",
    "href": "Reading-notes-lesson-24.html#multiple-explanatory-variables",
    "title": "24  Effect size: Input to output",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\nWhen a model has more than one explanatory variable, each has a different effect size.\nAs an example, consider the price of books. We have some data that might be informative, moderndive::amazon_books. What is the effect size of page count on price. The appropriate model here is list_price ~ num_pages. The effect size is easy to compute:\n\nMod1 <- lm(list_price ~ num_pages, data = moderndive::amazon_books)\nmodel_eval(Mod1, num_pages = c(200, 400))\n\n  num_pages  .output       .lwr     .upr\n1       200 15.82014 -11.636987 43.27726\n2       400 19.79643  -7.637503 47.23037\n\n\nWe elected to compare 200-page books with 400-page books, simply because those seem like reasonable book lengths. However, the longer book costs about 4 dollars more. So the effect size, to judge from this model, is $4 divided by 200 more pages, which comes to 2 cents per page.\nAnother effect size is needed to address the question: Are hardcovers more expensive than paperbacks? The output is still price. But now, the input is categorical. In the moderndive::amazon_books data frame, the variable hard_paper has levels “P” and “H.” A possible model: list_price ~ hard_paper.\n\nMod2 <- lm(list_price ~ hard_paper, data = amazon_books)\nmodel_eval(Mod2, hard_paper = c(\"P\", \"H\"))\n\n  hard_paper  .output      .lwr     .upr\n1          P 17.13523 -10.62291 44.89338\n2          H 22.39393  -5.46052 50.24839\n\n\nA hardcover book costs about $5.25 more than a paperback book. Since the input is categorical, there is no change of input to divide by, so the effect size is $5.25 when going from a paperback to a hardcover.\nWe can look at the effects of page length and cover-type separately. Instead, we can include both as explanatory variables.\n\nMod3 <- lm(list_price ~ hard_paper + num_pages, data = amazon_books)\nmodel_eval(Mod3, hard_paper = c(\"P\", \"H\"),  num_pages=c(200, 400))\n\n  hard_paper num_pages  .output       .lwr     .upr\n1          P       200 14.52494 -12.641928 41.69182\n2          H       200 19.48253  -7.785720 46.75077\n3          P       400 18.43605  -8.709404 45.58151\n4          H       400 23.39363  -3.847698 50.63497\n\n\nThis output requires some interpretation. We have got short and long paperback books and short and long hardcover books. What should we compare to what?\nThe convention is to consider each of the two inputs separately and hold the other input constant when we compare.\nEffect size of num_pages on list_price. To hold hard_paper constant, we will compare the two rows of the model_eval() report that have a “P” value for hard_paper. The difference in output for these two rows is $3.90. The effect size divides by the change in input—200 pages—so the effect size is just under 2 cents per page. Effect size of hard_paper on list_price. This time we will hold num_pages constant, say at 200 pages. Comparing the corresponding rows in the model_eval() output shows a change in list price of $4.96 when going from paper back to hard cover. There is no special reason we decided to hold hard_paper constant at “P” rather than “H” or hold num_pages constant at 200 rather than 400. In general, the effect size will depend on the value being held constant. Choose a value that’s relevant to the purpose at hand.\nIn these Lessons we are building models with additive effects.That is what the + means in, say, list_price ~ hard_paper + num_pages. We do this to keep the effect-size story as simple as possible. (Occasionally, you will see examples with multiplicative effects, called “interactions.” The tilde expressions for such models involve * rather than +, as in list_price ~ hard_paper + num_pages."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#interval-estimates",
    "href": "Reading-notes-lesson-24.html#interval-estimates",
    "title": "24  Effect size: Input to output",
    "section": "Interval estimates",
    "text": "Interval estimates\nStatistical thinkers know that any estimate they make, including estimates of effect sizes, involves sampling variation. Consequently, give an interval estimate. The interval communicates to the decision-maker the uncertainty in the estimated quantity. Sophisticated decision-makers keep this uncertainty in mind, considering the range of outcomes likely from whatever use they make of effect size. On the other hand, statistically naive decision makers—even highly educated decision-makers can be statistically naive—look at the interval and sometimes ask the modeler, “Just give me a number. I don’t know what to do with two numbers.” Such a request might elicit a frank response: “If you don’t know what to do with two numbers, you also won’t know what to do with one number.” Unfortunately, that kind of frankness is not often well received; a reasonable alternative is: “The interval indicates the amount of uncertainty in the result. We’ll need to collect more data if you want to reduce the uncertainty.” (Lesson 29 introduces a not-always-available alternative to collecting more data: building a better model!)\nFor the additive models that we are mainly using in these Lessons, the effect size is identical to a model coefficient. For these models, the confidence interval on the coefficient is the confidence interval on the effect size. For instance,\n\nMod3 %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 7.03901789 14.1886534\nhard_paperH 1.55803436  8.3571295\nnum_pages   0.01023618  0.0288749"
  },
  {
    "objectID": "Reading-notes-lesson-25.html#the-prediction-machine",
    "href": "Reading-notes-lesson-25.html#the-prediction-machine",
    "title": "25  Mechanics of prediction",
    "section": "The prediction machine",
    "text": "The prediction machine\nA statistical prediction is the output of a kind of special-purpose machine. The inputs given to the machine are values for what we already know; the output is a value (or interval) for the as-yet-unknown aspects of the system.\nThere are always two phases involved in making a prediction. The first is building the prediction machine. The second phase is providing the machine with inputs for the individual case, turning the machine crank, and receiving the prediction as output.\nThese two phases require different sorts of data. Building the machine requires a “historical” data set that includes records from many instances where we already know two things: the values of the inputs and the observed output. The word “historical” emphasizes that the machine-building data must already have known values for each of the inputs and outputs of interest.\nThe evaluation phase—turning the crank of the machine—is simple. Take the input values for the individual to be predicted, put those inputs into the machine, and receive a predicted value as output. Those input values may come from pure speculation or the measured values from a specific case of interest."
  },
  {
    "objectID": "Reading-notes-lesson-25.html#building-and-using-the-machine",
    "href": "Reading-notes-lesson-25.html#building-and-using-the-machine",
    "title": "25  Mechanics of prediction",
    "section": "Building and using the machine",
    "text": "Building and using the machine\nTo illustrate building a prediction machine, we turn to a problem first considered quantitatively in the 1880s: the relationship between parents’ heights and their children’s heights at adulthood. The Galton data frame records the heights of about 900 children, along with their parents’ heights. Suppose we want to predict a child’s adult height (variable name: height) from his or her parents’ heights (mother and father). An appropriate model formula is height ~ mother + father. We use the model-training functionlm() to transform the model formula and the data into a model.\n\nMod1 <- lm(height ~ mother + father, data = Galton)\n\nAs the output of an R function, Mod1 is a computer object. It incorporates a variety of information organized in a somewhat complex way. There are several often-used ways to extract this information in ways that serve specific purposes.\nOne of the most common ways to see what is in a computer object like Mod1 is by printing:\n\nprint(Mod1)\n\n\nCall:\nlm(formula = height ~ mother + father, data = Galton)\n\nCoefficients:\n(Intercept)       mother       father  \n    22.3097       0.2832       0.3799  \n\n\nNewcomers to technical computing tend to confuse the printed form of an object with the object itself. For example, the Mod1 object contains many components, but the printed form displays only two: the model coefficients and the command used to construct the object.\nWe have already used some other functions to extract information from a model object. For instance,\n\nMod1 %>% coefficients()\n\n(Intercept)      mother      father \n 22.3097055   0.2832145   0.3798970 \n\nMod1 %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 13.8569119 30.7624990\nmother       0.1867750  0.3796540\nfather       0.2898301  0.4699639\n\nMod1 %>% regression_summary()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   22.3      4.31        5.18 2.74e- 7\n2 mother         0.283    0.0491      5.76 1.13e- 8\n3 father         0.380    0.0459      8.28 4.52e-16\n\n\nAnother extractor, model_eval(), is particularly convenient for prediction. Perhaps the most common use is to provide new input values to the model function, with model_eval() producing a data frame showing the output of the model function. To illustrate, here is how to calculate the predicted height of the child of a 63-inch-tall mother and a 68-inch father.\n\nMod1 %>% model_eval(mother = 63, father=68)\n\n  mother father  .output     .lwr     .upr\n1     63     68 65.98521 59.33448 72.63594\n\n\nThe data frame includes the input values along with a point value for the prediction (.output) and a prediction interval (.lwr to .upr).\nNaturally, the predictions depend on the explanatory variables used in the model. For example, here is a model that uses only sex to predict the child’s height:\n\nMod2 <- lm(height ~ sex, data = Galton)\nMod2 %>% model_eval(sex=c(\"F\", \"M\"))\n\n  sex  .output     .lwr     .upr\n1   F 64.11016 59.18024 69.04009\n2   M 69.22882 64.29928 74.15835\n\n\nThis model includes three explanatory variables:\n\nMod3 <- lm(height ~ mother + father + sex, data = Galton)\nMod3 %>% model_eval(mother=63, father=68, sex=c(\"F\", \"M\"))\n\n  mother father sex  .output     .lwr     .upr\n1     63     68   F 63.20546 58.97128 67.43964\n2     63     68   M 68.43141 64.19783 72.66499\n\n\nIn Lesson 26, we will look at the components that make up the prediction interval and some ways to use it."
  },
  {
    "objectID": "Reading-notes-lesson-26.html#where-does-the-prediction-interval-come-from",
    "href": "Reading-notes-lesson-26.html#where-does-the-prediction-interval-come-from",
    "title": "26  Constructing a prediction interval",
    "section": "Where does the prediction interval come from",
    "text": "Where does the prediction interval come from\nThe prediction interval has two distinct components:\n\nThe uncertainty in the model function and hence in the output of the model function.\nThe size of the residuals found when training the model.\n\nConsider first the model function. For the running-time model, we can construct the model function from the coefficients of the linear model. These are:\n\nTime_mod %>% coefficients()\n\n(Intercept)    distance       climb \n-469.976937  253.808295    2.609758 \n\n\nThe algebraic expression for the model function is straightforward: \\[t(d, c) \\equiv -470 + 254 d  + 2.61 c\\ .\\]\nThe statistical thinker knows that such coefficients have uncertainty due to sampling variation. That uncertainty is, of course, quantified by the confidence interval.\n\nTime_mod %>% confint()\n\n                  2.5 %      97.5 %\n(Intercept) -533.432471 -406.521402\ndistance     246.387096  261.229494\nclimb          2.493307    2.726209\n\n\nSince we cannot legitimately claim to know the values of the coefficients any better than indicated by these confidence intervals, we ought to temper our claims about the model function so that it reflects the uncertainty in the coefficients. For instance, we might provide an interval for the model output, using in an “upper” function the high ends of the confidence intervals on the coefficients and another “lower” function that uses the low ends of the confidence interval. Like this:\n\\[t_{upr}(d,c) \\equiv -407 + 261 d + 2.72 c\\\\\nt_{lwr}(d,c) \\equiv -533 + 246 d + 2.49 c\\]\nEvaluate both the lower and upper functions to get an interval on the model output. That would give us \\(t_{lwr}(10, 500) = 3172\\) and \\(t_{upr}(10, 500) = 3569\\).\nThis idea for generating the “lower” and “upper” functions has the right spirit but is not on target mathematically. The reason is that using the low end of the confidence interval for all coefficients is overly pessimistic; usually, the uncertainty in the different coefficients cancels out to some extent.\nThe mathematics for the correct “lower” and “upper” functions are well understood but too advanced for the general reader. For our purposes, it suffices to know that model_eval() knows how to do the calculations correctly.\nThe prediction interval produced by model_eval() includes both components (1) and (2) listed above. Insofar as we are interested in component (1) in isolation, the correct sort of interval—a confidence interval—can be requested from model_eval().\n\nTime_mod %>% \n    model_eval(distance=10, climb=500, interval=\"confidence\")\n\n  distance climb  .output     .lwr     .upr\n1       10   500 3372.985 3335.264 3410.706\n\n\nThis report shows that the confidence interval on the model output—that is, just component (1) of the prediction interval—is pretty narrow: 3335 seconds to 3411 seconds, or, in plus-or-minus format, \\(3373 \\pm 38\\) seconds.\nThe prediction interval—that is, the sum of components (1) and (2)—is comparatively huge: 1700 to 5100 seconds or, in plus-or-minus format, \\(3400 \\pm 1700\\) seconds. That is almost 50 times wider than the confidence interval.\nWhy is the prediction interval so much more comprehensive than the confidence interval? The confidence interval reports on the sampling variation of a model constructed as an average over all the data, the \\(n=2236\\) participants recorded in the Hill_racing data frame. However, each runner in Hill_racing has their own individual time: not an average but just for the individual. The individual value might be larger or smaller than the average. How much larger or smaller? The residuals for the model provide this information. As always, we can measure the individual-to-individual variation with the standard deviation.\n\nTime_mod %>% model_eval() %>% summarize(se_residuals = sd(.resid))\n\n  se_residuals\n1     870.6588\n\n\nKeeping in mind that the overall spread of the residuals is plus-or-minus “twice” the standard deviation of the residuals, we can say that the residuals indicate an additional uncertainty in the prediction for a runner of about \\(\\pm 1700\\) seconds. This \\(\\pm 1700\\) second is our estimate of the noise in the measurements. In contrast, the confidence interval is about the sampling variation in the signal.\nIn this case, the prediction interval is wholly dominated by noise; the sampling variability contributes only a tiny amount of additional uncertainty.\n\n\n\n\n\n\nExample: Graphics for the prediction interval\n\n\n\nWe shift the running scene from Scotland to Washington, DC. The race now is a single 10-miler with almost 9000 registered participants. We wish to predict the running time of an individual based on his or her age.\n\nAge_mod <- lm(net ~ age, data = TenMileRace)\n\nWe can see the prediction interval for an individual runner using mod_eval(). For example, here it is for a 23-year-old.\n\nAge_mod %>% model_eval(age=23)\n\n  age  .output     .lwr     .upr\n1  23 5485.587 3592.054 7379.119\n\n\nWe can also calculate the prediction interval for several different ages and graph out the results with the “errorbar” glyph:\n\nAge_mod %>% \n  model_eval(age=c(20,25,30,35,40,45,50,55,60,65,70,75)) %>%\n  ggplot(aes(x=age)) +\n  geom_errorbar(aes(ymin=.lwr, ymax=.upr))\n\n\n\n\nFor convenience, the model_plot() function will do this work for us, plotting the prediction interval along with the training data. We can also direct model_plot() to show the confidence interval.\n\nmodel_plot(Age_mod, x=age, interval=\"prediction\", data_alpha=0.05)\nmodel_plot(Age_mod, x=age, interval=\"confidence\", data_alpha=0.05)\n\n\n\n\n\n\n\n(a) prediction interval\n\n\n\n\n\n\n\n(b) confidence interval\n\n\n\n\nFigure 26.1: The prediction and model-function confidence intervals for the model net ~ age.\n\n\n\nSince we are looking at the intervals as a function of an input variable, what we formerly showed using the “errorbar” glyph is now shown using a ribbon or band.\n\n\nNotice that the prediction interval covers almost all of the data points. There are hundreds of data points outside the interval, but with almost 9000 rows in the TenMileRace data frame, an interval that covers 95% of the data will have about 450 rows outside the interval.\nSuch a prediction interval is of little use; it cannot give a precise prediction about the running time of an individual. The honest prediction of an individual’s outcome needs to reflect the spread of all the individuals with a similar age.\nIn contrast, the confidence band on the model function is pleasingly narrow and precise. It covers only a tiny fraction of the raw data. For this very reason, the confidence interval is inappropriate for presenting a prediction. As always, confidence intervals only show general trends in the data, not the range of results for an individual prediction. For instance, Figure 26.1 shows a clear upward trend in running time with age. There is no flat or negatively sloping line compatible with the confidence interval.\nTo summarize:\n\nWhen making a prediction, report a prediction interval.\nThe prediction interval is always larger than the confidence interval and is usually much larger.\n\nThe confidence interval is not for predictions. Use a confidence interval when looking at an effect size. Graphically, the confidence interval is to indicate whether there is an overall trend in the model."
  },
  {
    "objectID": "QR1.html",
    "href": "QR1.html",
    "title": "27  Review of Lessons 1-8",
    "section": "",
    "text": "Warning\n\n\n\nI’ll put learning challenges here. The class day will be given over to the QR."
  },
  {
    "objectID": "Reading-notes-lesson-28.html#all-other-things-being-equal",
    "href": "Reading-notes-lesson-28.html#all-other-things-being-equal",
    "title": "28  Covariates",
    "section": "All other things being equal",
    "text": "All other things being equal\nThe common phrase “all other things being equal” is a critical qualifier in describing relationships. To illustrate: A simple claim in economics is that a high price for a commodity reduces the demand. For example, increasing the price of heating fuel will reduce demand as people turn down thermostats to save money. Nevertheless, the claim can be considered obvious only with the qualifier all other things being equal. For instance, the fuel price might have increased because winter weather has increased the demand for heating compared to summer. Thus, higher prices may be associated with higher demand. Therefore, increased price may not be associated with lower demand unless holding other variables, such as weather conditions, constant.\nIn economics, the Latin equivalent of “all other things being equal” is sometimes used: “ceteris paribus”. So, the economics claim would be, “higher prices are associated with lower demand, ceteris paribus.”\nAlthough the phrase “all other things being equal” has a logical simplicity, it is impractical to implement “all.” So instead of the blanket “all other things,” it is helpful to consider just “some other things” to be held constant, being explicit about what those things are. Other phrases along the same lines are “taking into account …” and “controlling for ….” Those additional variables that are to be considered are called “covariates.\n\n\n\n\n\n\nExample: Covariates and Death\n\n\n\nThis news report appeared in 2007:\n\nHeart Surgery Drug Carries High Risk, Study Says. A drug widely used to prevent excessive bleeding during heart surgery appears to raise the risk of dying in the five years afterward by nearly 50 percent, an international study found. The researchers said replacing the drug—aprotinin, sold by Bayer under the brand name Trasylol—with other, cheaper drugs for a year would prevent 10,000 deaths worldwide over the next five years.\nBayer said in a statement that the findings are unreliable because Trasylol tends to be used in more complex operations, and the researchers’ statistical analysis did not fully account for the complexity of the surgery cases. The study followed 3,876 patients who had heart bypass surgery at 62 medical centers in 16 nations. Researchers compared patients who received aprotinin to patients who got other drugs or no antibleeding drugs. Over five years, 20.8 percent of the aprotinin patients died, versus 12.7 percent of the patients who received no antibleeding drug. [This is a 64% increase in the death rate.] When researchers adjusted for other factors, they found that patients who got Trasylol ran a 48 percent higher risk of dying in the five years afterward. The other drugs, both cheaper generics, did not raise the risk of death significantly.  The study was not a randomized trial, meaning that it did not randomly assign patients to get aprotinin or not. In their analysis, the researchers took into account how sick patients were before surgery, but they acknowledged that some factors they did not account for may have contributed to the extra deaths. - Carla K. Johnson, Associated Press, 7 Feb. 2007\n\nThe report involves several variables. Of primary interest is the relationship between (1) the risk of dying after surgery and (2) the drug used to prevent excessive bleeding during surgery. Also potentially important are (3) the complexity of the surgical operation and (4) how sick the patients were before surgery. Bayer disputes the published results of the relationship between (1) and (2) holding (4) constant, saying that it is also essential to hold variable (3) constant.\nThe total relationship involves a death rate of 20.8 percent of patients who got aprotinin versus 12.7 percent for the patients taking the generic drugs: an increase in the death rate by a factor of 1.64. However, when the researchers looked at a partial relationship (holding constant patient sickness before the operation), the effect size of aprotinin on mortality was less: a factor of 1.48. In other words, the model death ~ aprotinin shows a 64% increase in the death rate, but the model death ~ aprotinin + sickness shows a slightly smaller increase in death rate: 48%. The difference between the two estimates reflects doctors being more likely to give aprotinin to sicker patients.\nThe story’s last paragraph states that the choice of patients receiving aprotinin versus the generic drugs was not made at random. Some readers may find this reassuring. Why in the world would anyone prescribe a drug at random? The point, however, is to select randomly who gets which drug among the patients for whom the drugs would be appropriate. The phrase “randomized trial” used in the paragraph means specifically an experiment in which one treatment or the other—aprotinin versus the generic drugs—is assigned at random. The virtues of experiment and the vital role of random assignment are detailed in Lesson 32.\n\n\n“Significant” has a specialized meaning in statistical language. It is not a synonym for “important.” See Lessons 36 through 38"
  },
  {
    "objectID": "Reading-notes-lesson-28.html#letting-things-change-as-they-will",
    "href": "Reading-notes-lesson-28.html#letting-things-change-as-they-will",
    "title": "28  Covariates",
    "section": "Letting things change as they will",
    "text": "Letting things change as they will\nUsing covariates in models enables the relationship between a response and an explanatory variable to be described ceteris paribus, that is, “all other things being equal.” Another phrase used in news stories is “after adjusting for ….” This is appropriate since the all in “all other things” is, in reality, refers only to those particular factors used as the covariates in the model. So, Dr. Meyer’s foot width results might be stated in everyday language as, “After adjusting for foot width, she found no difference in the widths of girls’ and boys’ feet.”\nNot including covariates in a model amounts to “letting other things change as they will.” In Latin, this is “mutatis mutandis.” In the foot-width example, the model width ~ sex looks at the differences in foot width for the two sexes. However, sex is not the only thing associated with foot width. The model width ~ sex ignores all other factors than sex; it compares boys and girls mutatis mutandis, that is, letting other things change as they will. In this case, comparing boys and girls involves not just the possible differences in foot width but also the differences in other factors such as foot length and body weight.\n\n\n\n\n\n\nExample: One change can bring another\n\n\n\nI was once involved in a budget committee that recommended employee health benefits for the college where I worked. At the time, college employees who belonged to the college’s insurance plan received a generous subsidy for their health insurance costs. Employees who did not belong to the plan received no subsidy but were given a modest monthly cash payment. After the stock market crashed in 2000, the college needed to cut budgets. One proposal called for eliminating the cash payment to employees who did not belong to the insurance plan. Proponents of the plan claimed that this would save money without reducing health benefits. I argued that this claim was an “all other things being equal” analysis: how expenditures would change assuming the number of people belonging to the insurance plan remained constant. In reality, however, the policy change would play out mutatis matandis; the loss of the cash payment would cause some employees, who currently received health benefits through their spouse’s health plan, to switch to the college’s health plan. That is what happened, contributing to an overall increase in healthcare expenses.\n\n\n\n\n\n\n\n\nExample: Spending and student performance\n\n\n\nTo illustrate how covariates set context, consider an issue of interest to public policy-makers in many societies: How much money to spend on children’s education? State lawmakers in the US are understandably concerned with the quality of public education provided. However, they also have other concerns and constraints and constituencies who give budget priority to other matters.\nIn evaluating their various trade-offs, lawmakers could benefit by knowing how increased educational spending will shape educational outcomes. What can available data tell us? Unfortunately, there are various political constraints that work against states adopting and publishing data on a standard, genuine measure of educational outcome. Instead, we have high-school graduation rates, student grades, and other non-standardized data. These data might have some meaning but can also reflect system gaming by administrators and teachers, for which there is little systematic data.\nAlthough imperfect, college admissions tests such as the ACT and SAT provide consistent data between states. For example, Figure 28.1 shows the average SAT score in 2010 in each state versus expenditures per pupil in public elementary and secondary schools. Layered on top of the data is a flexible linear model (and its confidence band) of SAT score versus expenditure.\nThe overall impression given by the model is that the relationship is negative, with lower expenditures corresponding to higher SAT scores. However, the confidence band is broad; it is possible to find a smooth path with almost zero slope through the confidence band. Either way, this graph does not support the conventional wisdom that higher spending produces better school outcomes.\n\n\n\n\n\nFigure 28.1: State by state data (from 2010) on average SAT college admissions test scores and expenditures for public education.\n\n\n\n\nOf course, other factors play a role in shaping education outcomes: for instance, poverty levels, parental education, and how the educational money is spent (higher pay for teachers or smaller class sizes? administrative bloat?).\nAt first glance, it is tempting to ignore these additional factors. For instance, we may not have data on them. Moreover, as our interest is in understanding the relationship between expenditures and education outcomes, we are not directly concerned with the additional factors. However, the lack of direct concern does not imply that we should ignore the factors but that we should do what we can to “hold them constant”.\nTo illustrate, consider the fraction of eligible students (those in their last year of high school) who take the college admission test. This fraction varies widely from state to state. In a poor state where few students go to college, the fraction can be tiny (Alabama 8%, Arkansas 5%, Mississippi 4%, Louisiana 8%). In some other states, the large majority of students take the SAT (Maine 93%, Massachusetts 89%, New York 89%). In states with low SAT participation rates, the students who take the test tend to be those applying to schools with competitive admissions. Such strong students will get high scores. In contrast, the scores in states with high participation rates reflect both strong and weak students. Consequently, the scores will be lower on average than in the low-participation states.\nPutting the relationship between expenditure and SAT scores in the context of the fraction taking the SAT is accomplished with the model SAT ~ expenditure + fraction rather than just SAT ~ expenditure. Figure 28.2 shows a model with fraction as a covariate.\n\n\n\n\n\nFigure 28.2: The model of SAT score versus expenditures, including as a covariate the fraction of eligible students in the state who take the SAT.\n\n\n\n\nNote that the effect size of spending on SAT scores is positive when the expenditure level is less than $10,000 per pupil. Notice as well that when the fraction taking the SAT is tiny, the average scores do not depend on expenditure. This flat relationship suggests that, among elite students, state expenditure does not make a discernable difference. Perhaps the college-bound students in such states have other educational resources to draw on.\nThe relationship shown in Figure 28.1 is genuine. However, so is the very different relationship seen in Figure 28.2. How can the same data be consistent with two utterly different displays? The answer, perhaps unexpectedly, has to do with the connections among the explanatory variables. Whatever the relationship between an individual explanatory variable and the response variable, the appearance of that relationship will depend on which covariates the modeler chooses to include."
  },
  {
    "objectID": "Reading-notes-lesson-29.html#how-much-variation-is-explained",
    "href": "Reading-notes-lesson-29.html#how-much-variation-is-explained",
    "title": "29  Covariates eat variance",
    "section": "How much variation is explained",
    "text": "How much variation is explained\nWe start by returning to the definition of statistical thinking introduced at the start of these Lessons:\n\nStatistic thinking is the explanation or description of variation in the context of what remains unexplained or undescribed.\n\nIn this Lesson, we will work with a straightforward measure of “what remains unexplained or undescribed.” The fitted model values represent the explained part of the variation. The residuals are what is left over, the difference between the actual values of the response variable and the fitted model values.\nAs a reminder, we will construct a simple model of the list price of books as a function of the number of pages and whether the book is a paperback or hardcover.1\n\n\n\n\nPrice_model <- lm(list_price ~ num_pages + hard_paper, \n                  data = amazon_books)\n\nThe model_eval() function can extract the fitted model values and the residuals from the model. We show just a few rows here, but we will use the entire report from model_eval(). Remember that when model_eval() is not given input values, it uses the model training data as input.\n\nResults <- model_eval(Price_model)\n\n\n\n\n\n \n  \n    list_price \n    num_pages \n    hard_paper \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    12.95 \n    304 \n    P \n    16.60 \n    -3.65 \n    -10.73 \n    43.94 \n  \n  \n    15.00 \n    273 \n    P \n    15.98 \n    -0.98 \n    -11.36 \n    43.32 \n  \n  \n    1.50 \n    96 \n    P \n    12.45 \n    -10.95 \n    -14.97 \n    39.88 \n  \n  \n    15.99 \n    672 \n    P \n    23.95 \n    -7.96 \n    -3.57 \n    51.47 \n  \n  \n    30.50 \n    720 \n    P \n    24.91 \n    5.59 \n    -2.67 \n    52.49 \n  \n  \n    28.95 \n    460 \n    H \n    24.57 \n    4.38 \n    -2.89 \n    52.03 \n  \n\n\n\n\n\nThe first book in the training data is a 304-page paperback with a list price of $12.95. The fitted model value for that book is $16.60. (Ordinarily, we refer to the output of the model function simply as the “output” or the “model output.” However, the output of the model function, when applied to rows from the training data also called the fitted model value.)\nAt $16.60, the fitted model value is $3.65 higher than the list price. This difference is the residual for that book, the sign reflecting the definition \\[\\text{residual} \\equiv \\text{response value} - \\text{fitted model value}\\ .\\] When the residual is small in magnitude, the fitted model value is close to the response value. Conversely, a large residual means the model was way off target for that book.\nThe standard measure of the typical size of a residual is the standard deviation or, equivalently, the variance.\n\nResults %>% summarize(se_resids = sd(.resid), v_resids=var(.resid))\n\n  se_resids v_resids\n1  13.81885 190.9606\n\n\nAs always, the standard deviation is easier to read because it has sensible units, in this case, dollars. On the other hand, the variance has strange units (square dollars) because it is the square of the standard deviation. We will use the variance for measuring the typical size of a residual for the reasons described in Lesson 20; variances add nicely in a manner analogous to the Pythagorean Theorem.\nSimilarly, the total amount of variation in the list price is simply the variance of the list price:\n\nResults %>% summarize(v_response = var(list_price))\n\n  v_response\n1   206.5129\n\n\nA simple measure of how much of the variation in list price remains unexplained is the ratio of these variances \\(190.96/206.51 = 92.5\\%\\). More than 90% of the variation remains unexplained by the Price_model. This high fraction of unexplained variance suggests the model has little to tell us. In the spirit of putting a positive spin on things, statisticians typically work with the complement of the unexplained fraction. Since the unexplained fraction is 92.5%, the complement is 7.5%. This number is written R2 and pronounced “R-squared.” (It also has a formal name: the “coefficient of determination.” In Lesson 30, we will meet the inventor of the coefficient of determination, Sewall Wright, who is an early hero of causal reasoning.)\nR2 is such a widely used summary of how the explanatory variables account for the response variable that a software extractor calculates it and some related values.\n\nPrice_model %>% R2()\n\n    n k   Rsquared        F      adjR2\n1 317 2 0.07530934 12.78651 0.06941959\n\n\nMany modelers act as if their goal is to build a model that makes R2 as big as possible. Their thinking is that large R2 means that the explanatory variables account for much of the response variable’s variance. Unfortunately, it is a naive goal. Instead, always focus on the model’s suitability for the purpose at hand. Often, shooting for a large R2 imposes costs that can undermine the purpose for the model. Furthermore, even models with the largest possible R2 sometimes have nothing to say about the response variable."
  },
  {
    "objectID": "Reading-notes-lesson-29.html#getting-to-1",
    "href": "Reading-notes-lesson-29.html#getting-to-1",
    "title": "29  Covariates eat variance",
    "section": "Getting to 1",
    "text": "Getting to 1\nR2 can range from zero to one. Zero means that the model accounts for none of the variation in the response variable. We can construct such a model quickly enough: list_price ~ 1 has no explanatory variables and, therefore, no ability to distinguish one book from another.\n\nNull_model <- lm(list_price ~ 1, data = amazon_books)\nNull_model %>% R2()\n\n    n k Rsquared   F adjR2\n1 319 0        0 NaN     0\n\n\nWe are using the word “null” to name this model. “Null” is part of the statistics tradition. The dictionary definition of “null” is “having or associated with the value zero” or “lacking distinctive qualities; having no positive substance or content.”2\nIn the null model, the fitted model values are all the same; all the variation is in the residuals.\n\nNull_model %>% model_eval()\n\n\n\n\n\n \n  \n    list_price \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    12.95 \n    18.6 \n    -5.65 \n    -9.69 \n    46.89 \n  \n  \n    15.00 \n    18.6 \n    -3.60 \n    -9.69 \n    46.89 \n  \n  \n    1.50 \n    18.6 \n    -17.10 \n    -9.69 \n    46.89 \n  \n  \n    15.99 \n    18.6 \n    -2.61 \n    -9.69 \n    46.89 \n  \n  \n    30.50 \n    18.6 \n    11.90 \n    -9.69 \n    46.89 \n  \n  \n    28.95 \n    18.6 \n    10.35 \n    -9.69 \n    46.89 \n  \n\n\n\n\n\nAt the other extreme, where R2 = 1, the explanatory variables account for every bit of variation in the response variable. We can try various combinations of explanatory variables to see if we can accomplish this. For example, publisher explains 67% of the variation in list price.\n\nlm(list_price ~ publisher, data = amazon_books) %>% R2()\n\n    n   k  Rsquared        F     adjR2\n1 319 158 0.6749786 2.103008 0.3540199\n\n\nWe can also check whether author has anything to say about the list price.\n\nlm(list_price ~ author, data = amazon_books) %>% R2()\n\n    n   k  Rsquared        F     adjR2\n1 319 250 0.9434046 4.534044 0.7353333\n\n\nIncredible! How about if we use both publisher and author as explanatory variables? We get very close to R2 = 1.\n\nlm(list_price ~ publisher + author, data = amazon_books) %>%\n  R2()\n\n    n   k  Rsquared        F   adjR2\n1 319 281 0.9821609 7.249441 0.84668\n\n\nThe modeler discovering this tremendous explanatory power of publisher and author can be forgiven for thinking he or she has found a meaningful explanation. But, unfortunately, the high R2 is an illusion in this case.\nTo see why, consider another possible explanatory variable, the International Standard Book Number (ISBN). The ISBN is a ten- or thirteen-digit number that marks each book with a unique number.\n\nknitr::include_graphics(\"www/SM2-ISBN.png\")\n\n\n\n\n\nFigure 29.1: The ISBN number from one of the Project MOSAIC textbooks.\n\n\n\nThere is a system behind ISBNs, but despite the “N” standing for “number,” an ISBN is a character string or word (written using only digits). Consequently, the isbn_10 variable in amazon_booksis categorical.\n\nISBN_model <- lm(list_price ~ isbn_10, data = amazon_books)\nISBN_model %>% R2()\n\n    n   k Rsquared   F adjR2\n1 319 318        1 NaN   NaN\n\n\nThe isbn_10 explains all variation in the list price!\nGiven that the ISBN is, as we have said, an arbitrary sequence of characters, why does it do such a good job of accounting for the list price? The answer lies not in the content of the ISBN but in another fact: each book has a unique ISBN. As well, each book has a single price. So the ISBN identifies the price of each book. Cleverness is not involved; the list price could be anything, and the ISBN would still identify it precisely. The model coefficients store the whole set of ISBNs and the corresponding set of list prices.\nWe can substantiate the claim just made—that the list price could be anything at all—by synthesizing a data frame with random list prices:\n\namazon_books %>% \n  mutate(random_list_price = rnorm(nrow(.))) %>%\n  lm(random_list_price ~ isbn_10, data = .) %>%\n  R2()\n\n    n   k Rsquared   F adjR2\n1 319 318        1 NaN   NaN\n\n\nSimilar randomization can be accomplished by shuffling the isbn_10 column of the data frame so that each ISBN points to a random book. Of course, such shuffling destroys the link between the ISBN and the list price. Even so, the R2 remains high.\n\nlm(list_price ~ shuffle(isbn_10), data=amazon_books) %>% R2()\n\n    n   k Rsquared   F adjR2\n1 319 318        1 NaN   NaN\n\nlm(shuffle(list_price) ~ isbn_10, data=amazon_books) %>% R2()\n\n    n   k Rsquared   F adjR2\n1 319 318        1 NaN   NaN\n\n\nStatistical nomenclature is obscure here. So we will make up a name for such incidental alignment with no true explanatory power: the “ISBN-effect.”\nStatistical thinkers know to be aware of situations where categorical variables have many levels and check whether the ISBN effect is in play."
  },
  {
    "objectID": "Reading-notes-lesson-29.html#the-isbn-effect-as-a-benchmark",
    "href": "Reading-notes-lesson-29.html#the-isbn-effect-as-a-benchmark",
    "title": "29  Covariates eat variance",
    "section": "The ISBN effect as a benchmark",
    "text": "The ISBN effect as a benchmark\nShuffling an explanatory variable (while keeping the response variable in the original order) voids any possible explanatory connection between the two. An R2=0, as we get from any model of the form y ~ 1, signals that the 1 cannot account for any variation. However, this does not mean shuffling will lead to R2 = 0. Instead, there is a systematic relationship between the number of model coefficients associated with the shuffled variable, the sample size \\(n\\), and R2.\nWe can demonstrate this relationship by conducting many trials of modeling the list_price with a shuffled explanatory variable: either publisher, author, or isbn_10.\n\n\n\n\n\n\nDemonstration: Counting coefficients\n\n\n\nThe amazon_books data frame has \\(n=319\\) rows.3 In the next computing chunk, we fit the model list_price ~ publisher and collect the coefficients for counting:\n\nPublisher_model <- lm(list_price ~ shuffle(publisher),\n                      data=amazon_books)\nCoefficients <- Publisher_model %>% coefficients() %>% data.frame()\nnrow(Coefficients)\n\n[1] 159\n\n\nThere are 159 coefficients in the model, the first one being the “Intercept.”\n\nCoefficients %>% head()\n\n\n\n\nAltogether, there are \\(k=158\\) coefficients relating to shuffle(publisher).\n\n\n\n\n\nTable 29.1:  Confidence intervals on the first six of 161 coefficients in list_price ~ publisher \n \n  \n      \n    value \n  \n \n\n  \n    (Intercept) \n    14.95 \n  \n  \n    shuffle(publisher)Adams Media \n    0.05 \n  \n  \n    shuffle(publisher)Akashic Books \n    13.00 \n  \n  \n    shuffle(publisher)Aladdin \n    15.05 \n  \n  \n    shuffle(publisher)Albert Whitman & Company \n    -0.95 \n  \n  \n    shuffle(publisher)Alfred A. Knopf \n    0.05 \n  \n\n\n\n\nThe rule relating R2 to the number of coefficients associated is straightforward for shuffled explanatory variables: R2 will be random with mean value \\(\\frac{k}{n-1}\\). For the shuffle(publisher) model, the mean across many trials will be R2 = 158/324 = 0.49.\n\nPub_trials <- do(100) * {\n  lm(list_price ~ shuffle(publisher), data=amazon_books) %>%\n    R2()\n}\n\n\n\n\n\n\n\nFigure 29.2: 100 trials of R2 from list_price ~ shuffled(publisher). The theoretical value \\(k/n=160/324=0.49\\) is marked in red.\n\n\n\n:::\nWe can carry out similar trials for the models list_price ~ shuffle(author) and list_price ~ shuffle(isbn_10), which have \\(k=251\\) and \\(k=319\\) respectively.\n\n\n\n\n\nFigure 29.3: R2 from many trials of three models, list_price ~ shuffle(publisher) and ~ shuffle(author) and ~shuffle(isbn_10).\n\n\n\n\nThe blue diagonal line in Figure 29.3 shows the theoretical average R2 as a function of the number of model coefficients when the explanatory variable is randomized. R2 will always be 1.0 when \\(k=n\\), that is, when the number of coefficients is the same as the sample size.\nFigure 29.3 suggests a way to distinguish between R2 resulting from the ISBN-effect and R2 that shows some true explanatory power: Check if R2 is substantially above the blue diagonal line, that is, check if R2\\(\\gg \\frac{k}{n-1}\\) where \\(k\\) is the number of model coefficients."
  },
  {
    "objectID": "Reading-notes-lesson-29.html#the-f-statistic",
    "href": "Reading-notes-lesson-29.html#the-f-statistic",
    "title": "29  Covariates eat variance",
    "section": "The F statistic",
    "text": "The F statistic\n\\(k\\) and \\(n\\) provide the necessary context for proper interpretation of R2; all three numbers are needed to establish whether R2 \\(\\gg \\frac{k}{n-1}\\) to rule out the ISBN effect. The calculation is not difficult; the modeler always knows the size \\(n\\) of the training data and can find \\(k\\) as the number of coefficients in the model (not counting the Intercept term).\nPerhaps a little easier than interpreting R2 is the interpretation of another statistic, named F, which folds in the \\(k\\), \\(n\\), and R2 into a single number: \\[F \\equiv \\frac{n-k-1}{k} \\frac{\\text{R}^2}{1 - \\text{R}^2}\\] ?fig-amazon-book-shuffle-F is a remake of Figure 29.3 but using F instead of R2. The blue line, which had the formula R2\\(= k/(n-1)\\) in Figure 29.3, gets translated to the constant value 1.0 in ?fig-amazon-book-shuffle-F, regardless of \\(k\\). To decide when a model points to a connection stronger than the ISBN effect, the threshold F \\(> 3\\) is a good rule of thumb. (Lesson 37 introduces a more precise calculation for the F threshold, which is built into statistical software and presented as a “p-value.”)\n\nggplot(aes(x=k, y=F), data=All_trials) +\n  geom_jitter(width=15, alpha=.3) +\n  geom_abline(aes(intercept=1, slope=0), color=\"blue\") +\n  geom_point(aes(x=0, y=0), alpha=0) + \n  labs(subtitle=\"with shuffling\") +\n  ylab(expression(F)) + \n  scale_x_continuous(breaks=c(0,50, 100,158,200, 251,  318))\n\nWarning: Removed 100 rows containing missing values (geom_point).\n\n\n\n\n\n\n\n\n\n\n\nAdjusted R2\n\n\n\nSome fields, notably economics, prefer an alternative to F called “adjusted R2” (or \\(R^2_\\text{adj}\\)). The adjustment comes from moving the raw R2 downward and leftward, more-or-less in the direction of the blue line in Figure 29.3. This movement adjusts a raw \\(R^2\\) that lies on the blue line to \\(R^2_\\text{adj} = 0\\).\nWe leave the debate on the relative merits of using F or \\(R^2_\\text{adj}\\) their respective boosters. However, before getting wrapped up in such debates, it is worth pointing out that \\(R^2_\\text{adj}\\) is just a rescaling of F.\n\\[R^2_\\text{adj} = 1 - \\frac{n-1}{k} \\frac{R^2}{F}\\ .\\]"
  },
  {
    "objectID": "Reading-notes-lesson-29.html#comparing-models",
    "href": "Reading-notes-lesson-29.html#comparing-models",
    "title": "29  Covariates eat variance",
    "section": "Comparing models",
    "text": "Comparing models\nModelers are often in the position of having a model that they like but are contemplating adding one or more additional explanatory variables. To illustrate, consider the following models:\n\nModel 1: list_price ~ 1\nModel 2: list_price ~ 1 + hard_paper\nModel 3: list_price ~ 1 + hard_paper + num_pages\nModel 4: list_price ~ 1 + hard_paper + num_pages + weight_oz\n\n\n\n\n\n\n\nFigure 29.4: Nesting Russian dolls\n\n\n\nAll the explanatory variables in the smaller models also apply to the bigger models. Such sets are said to be “nested” in much the same way as for Russian dolls.\nFor a nested set of models, R2 can never decrease when moving from a smaller model to a larger one—almost always, there is an increase in R2. To demonstrate:\n\namazon_books <- amazon_books %>% \n  select(list_price, weight_oz, num_pages, hard_paper) %>%\n  filter(complete.cases(.))\nmodel1 <- lm(list_price ~ 1, data=amazon_books)\nmodel2 <- lm(list_price ~ 1 + weight_oz, data = amazon_books)\nmodel3 <- lm(list_price ~ 1 + weight_oz + num_pages, data=amazon_books)\nmodel4 <- lm(list_price ~ 1 + weight_oz + num_pages + hard_paper, data=amazon_books)\n\nR2(model1)\n\n    n k Rsquared   F adjR2\n1 309 0        0 NaN     0\n\nR2(model2)\n\n    n k  Rsquared        F     adjR2\n1 309 1 0.1558551 56.68166 0.1531055\n\nR2(model3)\n\n    n k  Rsquared        F     adjR2\n1 309 2 0.1662332 30.50456 0.1607838\n\nR2(model4)\n\n    n k  Rsquared        F     adjR2\n1 309 3 0.1697025 20.77941 0.1615357\n\n\nWhen adding explanatory variables to a model, a good question is whether the new variable(s) add to the ability to account for the variability in the response variable. R2 never goes down when moving from a smaller to a larger model, so we cannot rely on the increase in R2. A valuable technique called “Analysis of Variance” (ANOVA for short) looks at the incremental change in variance explained from a smaller model to a larger one. The increase can be presented as an F statistic. To illustrate:\n\nanova(model1, model2, model3, model4)\n\nAnalysis of Variance Table\n\nModel 1: list_price ~ 1\nModel 2: list_price ~ 1 + weight_oz\nModel 3: list_price ~ 1 + weight_oz + num_pages\nModel 4: list_price ~ 1 + weight_oz + num_pages + hard_paper\n  Res.Df   RSS Df Sum of Sq       F    Pr(>F)    \n1    308 54531                                   \n2    307 46032  1    8499.0 57.2516 4.565e-13 ***\n3    306 45466  1     565.9  3.8123   0.05179 .  \n4    305 45277  1     189.2  1.2744   0.25983    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFocus on the F column of the report. The move from Model 1 to Model 2 produces F=57, well above the threshold described above and clearly indicating that the weight_oz variable accounts for some of the list price. Moving from Model 2 to Model 3 creates a much less impressive F of 3.8. It is as if the added explanatory variable, num_pages, is just barely pulling its own “weight.” Finally, moving from Model 3 to Model 4 produces a below-threshold F of 1.3. In other words, in the context of weight_oz and num_pages, the hard_paper variable does not carry additional information about the list price.\nThe last column of the report, labeled Pr(>F), translates F into a universal 0 to 1 scale called a p-value. A large F produces a small p-value. The rule of thumb for reading p-values is that a value \\(p < 0.05\\) indicates that the added variable brings new information about the response variable. We will return to p-values and the controversy they have entailed in Lessons 36 through 38."
  },
  {
    "objectID": "Reading-notes-lesson-30.html#block-that-path",
    "href": "Reading-notes-lesson-30.html#block-that-path",
    "title": "30  Confounding",
    "section": "Block that path!",
    "text": "Block that path!\nLet us look more generally at the possible causal connections among three variables, which we will call X, Y, and C. We will stipulate that X points causally toward Y and that C is a possible covariate. Like all DAGs, there cannot be a cycle of causation. These conditions leave three distinct DAGs that do not have a cycle, shown in Figure 30.2.\n\n\n\n\n\n\n\n(a) C is a confounder.\n\n\n\n\n\n\n\n(b) C is a mechanism.\n\n\n\n\n\n\n\n(c) C is a consequence.\n\n\n\n\nFigure 30.2: Three different DAGs connecting X, Y, and C.\n\n\nC plays a different role in each of the three dags. In sub-figure (a), C causes both X and Y. In (b), part of the way that X influences Y is through C. We say, in this case, “C is a mechanism by which X causes Y. In sub-figure (c), C does not cause either X or Y. Instead, C is a consequence of both X and Y.1\nTo understand how a DAG informs whether or not to include a covariate, It will help to give general names to some of the sub-structures seen in the Figure 30.2 DAGs. ?fig-dag-paths shows some of these sub-structures, removing other links that are not part of the structure.\n\n\n\n\n\n\n\n(a) Direct causal link from X to Y\n\n\n\n\n\n\n\n(b) Causal path from X through C to Y\n\n\n\n\n\n\n\n(c) Correlating path connecting X and Y via C\n\n\n\n\n\n\n\n(d) C as a consequence of X and Y\n\n\n\n\nFigure 30.3: Sub-structures seen in Figure 30.2.\n\n\n\nA “direct causal link” between X and Y. There are no intermediate nodes.\nA “causal path” from C to X and on to Y. A causal path is one where, starting at the originating node, flow along the arrows can get to the terminal node, passing through all intermediate nodes.\nA “correlating path” from Y through X to C. Correlating paths are distinct from causal paths because, in a correlating path, there is no way to get from one end to the other by following the flows.\nA “collider” wealth. In other words, both X and Y are causes of C.\n\nLook back to Figure 30.2(a), where wealth is a confounder. A confounder is always an intermediate node in a correlating path.\n:::\nIncluding a covariate either blocks or opens the pathway on which that covariate lies. Which it will be depends on the kind of pathway. A causal path, as in Figure 30.3(b), is blocked by including the covariate. Otherwise, it is open. A correlating path (?fig-dags-path(c)) is similar: the path is open unless the covariate is included in the model. A colliding path, as in Figure 30.3(d), is blocked unless the covariate is included—the opposite of a causal path.\nOften, covariates are selected to block all paths except the direct link between the explanatory and response variable. This means do include the covariate if it is on a correlating path and do not include it if the covariate is at the collision point.\nAs for a causal path, the choice depends on what is to be studied. Consider the DAG drawn in Figure 30.2(b), reproduced here for convenience:\n\n\n\n\n\ngrass influences illness through two distinct paths:\n\nthe direct link from grass to illness.\nthe causal pathway from grass through wealth to illness.\n\nAdmittedly, it is far-fetched that choosing to green the grass makes a household wealthier, but focus on the topology of the DAG and not the unlikeliness of this specific causal scenario.\nThere is no way to block a direct link from an explanatory variable to a response. If there were a reason to do this, the modeler probably selected the wrong explanatory variable.\nBut there is a genuine choice to be made about whether to block pathway (ii). If the interest is the purely biochemical link between grass-greening chemicals and illness, then block pathway (ii). However, if the interest is in the total effect of grass and illness, including both biochemistry and the sociological reasons why wealth influences illness, then leave the pathway open.\n::: {.callout-warning} ## In draft: Some resources\nhttps://towardsdatascience.com/causal-effects-via-dags-801df31da794\nhttps://towardsdatascience.com/causal-effects-via-the-do-operator-5415aefc834a"
  },
  {
    "objectID": "Reading-notes-lesson-31.html#correlation",
    "href": "Reading-notes-lesson-31.html#correlation",
    "title": "31  Spurious correlation",
    "section": "Correlation",
    "text": "Correlation\nA dictionary is a starting point for understanding the use of a word. Here are four definitions of “correlation” from general-purpose dictionaries.\n\n“A relation existing between phenomena or things or between mathematical or statistical variables which tend to vary, be associated, or occur together in a way not expected on the basis of chance alone” Source: Merriam-Webster Dictionary\n\n\n“A connection between two things in which one thing changes as the other does” Source: Oxford Learner’s Dictionary\n\n\n“A connection or relationship between two or more things that is not caused by chance. A positive correlation means that two things are likely to exist together; a negative correlation means that they are not.” Source: Macmillan dictionary\n\n\n“A mutual relationship or connection between two or more things,” “interdependence of variable quantities.” Source: [Oxford Languages]\n\nAll four definitions use “connection” or “relation/relationship.” That is at the core of “correlation.” Indeed, “relation” is part of the word “correlation.” One of the definitions uses “causes” explicitly, and the everyday meaning of “connection” and “relation” tend to point in this direction. The phrase “one thing changes as the other does” is close to the idea of causality, as is “interdependence.:\nThree of the definitions use the words “vary,” “variable,” or “changes.” The emphasis on variation also appears directly in a close statistical synonym for correlation: “covariance.”\nTwo of the definitions refer to “chance,” that correlation “is not caused by chance,” or “not expected on the basis of chance alone.” These phrases suggest to a general reader that correlation, since not based on chance, must be a matter of fate: pre-determination and the action of causal mechanisms.\nWe can put the above definitions in the context of four major themes of these Lessons:\n\nQuantitative description of relationships\nVariation\nSampling variation\nCausality\n\nCorrelation is about relationships; the “correlation coefficient” is a way to describe a straight-line relationship quantitatively. The correlation coefficient addresses the tandem variation of quantities, or, more simply stated, how “one thing changes as the other does.”\nTo a statistical thinker, the concern about “chance” in the definitions is not about fate but reliability. Sampling variation can lead to the appearance of a pattern in some samples of a process that is not seen in other samples of that same process. Reliability means that the pattern will appear in a large majority of samples.\n\n\n\n\n\n\nNote\n\n\n\nOne of the better explanations of “correlation” appears in an 1890 article by Francis Galton, who invented the correlation coefficient. Since the explanation is more than a century old, some words will be unfamiliar to the modern reader. For example, a “clerk” is an office worker. An “omnibus” is merely a means of public transportation today.\n\nTwo clerks leave their office together and travel homewards in the same and somewhat unpunctual omnibus every day. They both get out of the omnibus at the same halting-place, and thence both walk by their several ways to their respective homes. … The upshot is that when either clerk arrives at his home later than his average time, there is some reason to expect that the other clerk will be late also, because the retardation of the first clerk may have been wholly or partly due to slowness of the omnibus on that day, which would equally have retarded the second clerk. Hence their unpunctualities are related. If the omnibus took them both very near to their homes, the relation would be very close. If they lodged in the same house and the omnibus dropped them at its door, the relation would become identity.\n\n\nThe problems of … correlation deal wholly with departures or variations ; they pay no direct regard to the central form from which the departures or variations are measured. If we were measuring statures, and had made a mark on our rule at a height equal to the average height of the race of persons whom we were considering, then it would be the distance of the top of each man’s head from that mark, upward or downward as the case might be, that is wanted for our use, and not its distance upward from the ground.1"
  },
  {
    "objectID": "Reading-notes-lesson-31.html#spurious-causation",
    "href": "Reading-notes-lesson-31.html#spurious-causation",
    "title": "31  Spurious correlation",
    "section": "Spurious causation",
    "text": "Spurious causation\n\n\n\n\n\n\nFigure 31.2: Two examples from the Spurious correlations website\n\n\n\nThe “Spurious correlations” website http://www.tylervigen.com/spurious-correlations provides entertaining examples of correlations gone wrong. The running gag is that the two correlated variables have no reasonable association, yet the correlation coefficient is very close to its theoretical maximum of 1.0. Typically, one of the variables is morbid, as in Figure 31.2.\n\n\n\n\n\n\nFigure 31.3: The telson and tergum are anatomical parts of the shrimp. Their locations are marked at the bottom. Source: Weldon 1888\n\n\n\nAccording to Aldrich (1995)^[John Aldrich (1994) “Correlations Genuine and Spurious in Pearson and Yule” Statistical Science 10(4) URL the idea of spurious correlations appears first in an 1897 paper by statistical pioneer and philosopher of science Karl Pearson. The correlation coefficient method was published only in 1888, and, understandably, early users encountered pitfalls. One very early user, W.F.R. Weldon, published a study in 1892 on the correlations between the sizes of organs, such as the tergum and telson in shrimp. (See Figure 31.3.)\n\n\n\n\n\n\n\n\n\nPearson noticed a distinctive feature of Weldon’s method. Weldon measured the tergum and telson as a fraction of the overall body length.\nFigure 31.4 shows one possible DAG interpretation where telson and tergum are not connected by any causal path. Similarly, length is exogenous with no causal path between it and either telson or tergum.\n\nshrimp_dag <- dag_make(\n  tergum ~ unif(min=2, max=3),\n  telson ~ unif(min=4, max=5),\n  length ~ unif(min=40, max=80), \n  x ~ tergum/length + exo(.01),\n  y ~ telson/length + exo(.01)\n)\n# dag_draw(shrimp_dag, seed=101, vertex.label.cex=1)\nknitr::include_graphics(\"www/telson-tergum.png\")\n\n\n\n\n\nFigure 31.4: DAG for the shrimp measurements.\n\n\n\nThe Figure 31.4 shows a hypothesis where there is no causal relationship between telson and tergum. Pearson wondered whether dividing those quantities by length to produce variables x and y, might induce a correlation. Weldon had found a correlation coefficient between x and y of about 0.6. Pearson estimated that dividing by length would induce a correlation between x and y of about 0.4-0.5, even if telson and tergum are not causally connected.\nWe can confirm Pearson’s estimate by sampling from the DAG and modeling y by x. The confidence interval on x shows a relationship between x and y. In 1892, before the invention of regression, the correlation coefficient would have been used. In retrospect, we know the correlation coefficient is a simple scaling of the x coefficient.\n\nSample <- sample(shrimp_dag, size=1000)\nlm(y ~ x, data=Sample) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 0.04576651 0.05227149\nx           0.61475492 0.75661135\n\ncor(y ~ x, data=Sample)\n\n[1] 0.514812\n\n\nPearson’s 1897 work precedes the earliest conception of DAGs by three decades. An entire century would pass before DAGs came into widespread use. However, from the DAG of Figure 31.4] in front of us, we can see that length is a common cause of x and y.\nWithin 20 years of Pearson’s publication, a mathematical technique called “partial correlation” was in use that could deal with this particular problem of spurious correlation. The key is that the model should include length as a covariate. The covariate correctly blocks the path from x to y via length.\n\nlm(y ~ x + length, data=Sample) %>% confint()\n\n                   2.5 %       97.5 %\n(Intercept)  0.150768730  0.163510774\nx           -0.036259801  0.083354349\nlength      -0.001397519 -0.001250762\n\n\nThe confidence interval on the x coefficient includes zero once length is included in the model. So the data, properly analyzed, show no correlation between telson and tergum.\nIn this case, “spurious correlation” stems from using an inappropriate method. This situation, identified 130 years ago and addressed a century ago, is still a problem for those who use the correlation coefficient. Although regression allows the incorporation of covariates, the correlation coefficient does not.\n\n\n\n\n\n\nTime series analysis\n\n\n\nSome spurious correlations, such as those presented on the eponymous website, can also be attributed to methodological error.\nOne source of error was identified in 1904 by F.E. Cave-Browne-Cave in her paper “On the influence of the time factor on the correlation between the barometric heights at stations more than 1000 miles apart,” published in the Proceedings of the Royal Society. “Miss Cave,” as she was referred to in 1917 and 1921, respectively by eminent statisticians William Sealy Gosset (who published under the name “Student”) and George Udny Yule, also offered a solution to the problem. Her solution is very much in the tradition of “time-series analysis,” a contemporary specialized area of statistics.\nThe unlikeliness of the correlations on the website is another clue to their origin as methodological. Nobody woke up one morning with the hypothesis that cheese consumption and bedsheet mortality are related. Instead, the correlation is the product of a search among many miscellaneous records. Imagine that data were available on 10,000 annually tabulated variables for the last decade. These 10,000 variables create the opportunity for 50 million pairs of variables. Even if none of these 50 million pairs have a genuine relationship, sampling variation will lead to some of them having a strong correlation coefficient.\nIn statistics, such a blind search is called the “multiple comparisons problem.” Ways to address the problem have been available since the 1950s. (We will return to this topic under the label “false discovery” in Lesson 38.) Multiple comparisons can be used as a trick, as with the website. However, multiple comparisons also arise naturally in some fields. For example, in molecular genetics, “micro-arrays” make a hundred thousand simultaneous measurements of gene expression. Correlations in the expression of two genes give a clue to cellular function and disease. With so many pairs available, multiple comparisons will be an issue."
  },
  {
    "objectID": "Reading-notes-lesson-31.html#correlation-implies-causation.",
    "href": "Reading-notes-lesson-31.html#correlation-implies-causation.",
    "title": "31  Spurious correlation",
    "section": "“Correlation implies causation.”",
    "text": "“Correlation implies causation.”\nFrancis Galton’s 1890 example of the clerks on the bus introduces “correlation” as a causality story. The bus trip causes variation in commute times. Two clerks riding the same bus will have correlated commute times. In the dictionary definitions of “correlation” at the start of the Lesson, the words “connection,” “relationship,” and “interdependence” suggests causal connections.\n\n\n\n\n\nInsofar as the dictionary definitions of correlation suggest a causal relationship, they are at odds with the statistical mainstream, which famously holds that “correlation does not imply causation.” This view is so entrenched that it appears on tee shirts, one style of which is available for sale by the American Statistical Association.\nThe statement “A is not B” can be valid only if we know what A and B are. We have a handle on the meaning of “correlation.” So what is the meaning of “causation?”\nDictionaries define “causation” using the word “cause.” So we look there for guidance.\n\nA person or thing that gives rise to an action, phenomenon, or condition. Source: Oxford Languages\n\n\nAn event, thing, or person that makes something happen. Source: Macmillan Dictionary\n\n\nA person or thing that acts, happens, or exists in such a way that some specific thing happens as a result; the producer of an effect. Source: Dictionary.com\n\nInterpreting these definitions requires making sense of “give rise to,” “makes happen,” or “happens as a result.” All of them are synonyms for “cause.”\nThis circularity produces a muddle. Centuries of philosophical debate have yet to clarify things much.\nStill, we can do something. The point of view of these Lessons is to support decision-making. Causation is a valuable concept for decision-making, particularly in cases where the decision-maker is considering an intervention. With this as an anchor, a pragmatic definition of “causation” is available:\n\nCausation describes a class of hypotheses that DAGs can represent. In that representation, a causal relationship between two nodes X and Y is marked by a causal path connecting X to Y. In Lesson 30, we defined “causal path” in terms of the directions of arrows in a DAG.2 A definitive demonstration of a causal relationship between X and Y is that intervening to change X results reliably in a change in Y, all other nodes not on the causal path being held constant. (Lesson 32 treats the methodology behind this definitive sign.)\n\nWhether or not a definitive demonstration is feasible is not directly relevant to the decision-maker. A decision-maker acts under the guidance of one or more hypotheses. A good rule of thumb for decision-makers is to be guided only by plausible hypotheses. Whether a hypothesis is plausible is a matter of informed belief. A definitive demonstration should sharpen that belief. If no such definitive demonstration is available, the decision-maker must rely on alternative sources for belief. Austin Bradford Hill (1898-1991), an epidemiologist and eminent statistician, famously published a list of nine criteria that support belief in a causal hypothesis.\nUsing my definition of causation, and in marked disagreement with many statisticians, I submit that\n\nCorrelation implies causation.\n\n“Correlation implies causation” is not the same as saying, “A correlation between A and B implies that A causes B.” That statement is false. For instance, it might be instead that B causes A. Alternatively, there might be a common cause C for both A and B. Or, C might be a collider between A and B.\nThere is no mechanism to produce correlation that I am aware of, other than the sources of spurious correlation described previously, that does not involve causation in some way.\n::: {.callout-note} ## So why do many statisticians say different?\nHistorically, the rise of the expression “correlation does not imply causation”—Figure 31.5 shows the ngram since the 1888 invention of the correlation coefficient—comes after the peak in the use of the word “correlation.”\n\n\n\n\n\nFigure 31.5: Google NGram showing the rise in the use of the phrases “correlation does not imply causation,” and “correlation is not causation” in recent decades.\n\n\n\n\nThe first documented use of the phrase is from 1900. It comes in a review of the second edition of a book, The Grammar of Science, by Karl Pearson (whom we have met before in this Lesson).\nThe Grammar of Science is a metaphysically oriented prescription for a new type of science. It posited that sciences such as physics or chemistry unnecessarily drew on metaphors for causation, such as “force.” Instead, the book advocated another framework as more appropriate, eschewing causation in favor of descriptions of “perceptions” with probability.\nPearson illustrates his antipathy toward causation with an example of an ash tree in his garden:\n\n[T]he causes of its growth might be widened out into a description of the various past stages of the universe. One of the causes of its growth is the existence of my garden, which is conditioned by the existence of the metropolis [London]; another cause is the nature of the soil, gravel approaching the edge of the clay, which again is conditioned by the geological structure and past history of the earth. The causes of any individual thing thus widen out into the unmanageable history of the universe. The Grammar of Science, 2/e, p. 131\n\nIt should not be surprising that the field of statistics, which uses probability very extensively as a description, and that developed correlation as a measure of probability, would advocate for more general use of its approach. In this spirit, I read “correlation does not imply causation” as “our new science framework of probability and correlation replaces the antiquated framework of causation.” Outside of statistics, however, probability is merely a tool; causation does indeed have practical use. All the more so for decision-makers."
  },
  {
    "objectID": "Reading-notes-lesson-32.html#from-sm2",
    "href": "Reading-notes-lesson-32.html#from-sm2",
    "title": "32  Experiment and random assignment",
    "section": "From SM2",
    "text": "From SM2\nOne of the most important ideas in science is “experiment”. In a simple, ideal form of an experiment, you cause one explanatory factor to vary, hold all the other conditions constant, and observe the response. A famous story of such an experiment involves Galileo Galilei (1564-1642) dropping balls of different masses but equal diameter from the Leaning Tower of Pisa.1 Would a heavy ball fall faster than a light ball, as theorized by Aristotle 2000 years previously? The quantity that Galileo varied was the weight of the ball, the quantity he observed was how fast the balls fell, the conditions he held constant were the height of the fall and the diameter of the balls. The experimental method of dropping balls side by side also holds constant the atmospheric conditions: temperature, humidity, wind, air density, etc.\nOf course, Galileo had no control over the atmospheric conditions. By carrying out the experiment in a short period, while atmospheric conditions were steady, he effectively held them constant.\nToday, Galileo’s experiment seems obvious. But not at the time. In the history of science, Galileo’s work was a landmark: he put observation at the fore, rather than the beliefs passed down from authority. Aristotle’s ancient theory, still considered authoritative in Galileo’s time, was that heavier objects fall faster.\nThe ideal of “holding all other conditions constant” is not always so simple as with dropping balls from a tower in steady weather. Consider an experiment to test the effect of a blood-pressure drug. Take two groups of people, give the people in one group the drug and give nothing to the other group. Observe how blood pressure changes in the two groups. The factor being caused to vary is whether or not a person gets the drug. But what is being held constant? Presumably the researcher took care to make the two groups as similar as possible: similar medical conditions and histories, similar weights, similar ages. But “similar” is not “constant.”"
  },
  {
    "objectID": "Reading-notes-lesson-32.html#dag-interpretation-of-experiment",
    "href": "Reading-notes-lesson-32.html#dag-interpretation-of-experiment",
    "title": "32  Experiment and random assignment",
    "section": "DAG interpretation of experiment",
    "text": "DAG interpretation of experiment\nAlbert Einstein is reputed to have said:\n\nA theory is something nobody believes, except the person who made it. An experiment is something everybody believes, except the person who made it.\n\nA graphical causal network is a kind of theory. As a theory, it’s natural for people to be skeptical about results stem from the theory. Experiments are more persuasive. Let’s consider what an experiment looks like when represented by a graphical causal networks.\nIn an experiment, you have some real-world system and a means to intervene physically on at least one of the variables in that system and to read out the response of the system to the intervention. You don’t necessarily know much about the actual structure of the real world system. In Figure 32.1 the real-world system is shown in the rounded box. The intervention is on X and the output is Y.\n\n\n\n\n\nFigure 32.1: An experiment is a system in which there is an intervention and an output.\n\n\n\n\nNote that in Figure 32.1 X is, potentially, affected by other variables in the system.\nIdeally, the experiment is set up to eliminate all other effects on X except the intervention as in Figure 32.2. And the intervention is done in a way that none of the variables in the system can have any effect on it, for instance by assigning the intervention using a computer random-number generator. The lovely thing about this configuration is that the correct model to capture the effect of X on Y is simply Y ~ X. Whatever different people might believe about the real-world mechanism doesn’t matter. The correct model is always Y ~ X. This is why Einstein’s statement, “An experiment is something everybody believes,” is justified.\n\n\n\n\n\nFigure 32.2: An ideal experiment is one where the only influence on X is the intervention. Any effect on X or the intervention of the other variables in the system has been eliminated. The input paths to X from C and D that appear in Figure 32.1 have been deleted by the experimenter. This is not always possible in practice.\n\n\n\n\nBut there is another part to Einstein’s statement: “… except the person who made it.” Why shouldn’t the experimenter believe her own experiment? The experimenter might know that she didn’t or couldn’t conduct an ideal experiment. She wasn’t actually able to eliminate the arrows D \\(\\rightarrow\\) X and C \\(\\rightarrow\\) X. The other variables in the system might also be influencing X as in Figure 32.1. In this situation, the right model may not be Y ~ X. In fact, for the particular system shown in Figure 32.1 the correct model would be Y ~ X + C + D. But how could the experimenter know this for sure if she didn’t know all about the real-world mechanism?\nIt turns out that for either of the causal systems in Figures 32.1 there is always a correct model to show the link between the intervention and output: Output ~ Intervention. Rather than modeling the output by the physical quantity X, model the output by the random numbers generated by the computer that were used to set the intervention. This modeling approach is called intent to treat.\nTypically, experiments are done using a specially constructed system that is thought to resemble the system on which the intervention will actually be done. Insofar as the experimental system does resemble the real-world system, the experimental results will anticipate the effect of the real-world intervention. But often it’s hard to establish that the experimental system is a match to the system on which the real-world intervention will be applied. As such, subjective belief is still a factor in accepting that the experiment will be informative about the real-world systems we work with.\n\nIt’s appropriate to show some humility about models and recognize that they can be no better than the assumptions that go into them. Useful object lessons are given by the episodes where conclusions from modeling (with careful adjustment for covariates) can be compared to experimental results. Some examples (from (freedman-editorial-2008?)):\n\nDoes it help to use telephone canvassing to get out the vote? Models suggest it does, but experiments indicate otherwise.\nIs a diet rich in vitamins, fruits, vegetables and low in fat protective against cancer, heart disease or cognitive decline? Models suggest yes, but experiments generally do not.\n\nThe divergence between models and experiment suggests that an important covariate has been left out of the models."
  },
  {
    "objectID": "Reading-notes-lesson-33.html#odds-and-log-odds",
    "href": "Reading-notes-lesson-33.html#odds-and-log-odds",
    "title": "33  Measuring and accumulating risk",
    "section": "Odds (and log odds)",
    "text": "Odds (and log odds)\nA probability—a number between 0 and 1—is the most used measure of the chances that something will happen, but it is not the only way nor the best for all purposes.\nAlso part of everyday language is the word “odds,” as in, “What are the odds?” to express surprise at an unexpected event.\nOdds are usually expressed in terms of two numbers, as in “3 to 2” or “100 to 1”, written more compactly as 3:2 and 100:1 respectively. The setting for odds is an even that might happen or not: the horse Fortune’s Chance might win the race, otherwise not; it might rain today, otherwise not; the Red Sox might win the World Series, otherwise not.\nThe format of a probability assigns a number between 0 and 1 to the chances that Fortune’s Chance will win, or that it will rain, or that the Red Sox will come out on top. If that number is called \\(p\\), then the chances of the “otherwise outcome” must be \\(1-p\\). The event with probability \\(p\\) would be reformatted into odds as \\(p:(1-p)\\). No information is lost if we treat the odds as a single number, the result of the division \\(p/(1-p)\\). Thus, when \\(p=0.25\\) the corresponding odds will be \\(0.25/0.75\\), in other words, 1/3.\nA big mathematical advantage to using odds is that the odds number can be anything from zero to infinity; it’s not bounded within 0 to 1. Even more advantageous for the purposes of accumulating risk is the logarithm of the odds, called “log odds.” We will come back to this later."
  },
  {
    "objectID": "Reading-notes-lesson-33.html#staying-in-bounds",
    "href": "Reading-notes-lesson-33.html#staying-in-bounds",
    "title": "33  Measuring and accumulating risk",
    "section": "Staying in bounds",
    "text": "Staying in bounds\n\n\n\n\n\n\nNote in draft\n\n\n\nMaybe move this to a earlier lesson. Not clear.\n\n\nThe linear models (lm()) we have mostly been using up until now accumulate the model output as a linear combination of model inputs. Consider, for instance, a simple model of fuel economy based on the horsepower and weight of a car:\n\nmpg_mod <- lm(mpg ~ hp + wt, data = mtcars) \nmpg_mod %>% coefficients()\n\n(Intercept)          hp          wt \n37.22727012 -0.03177295 -3.87783074 \n\n\nThese coefficients mean that the model output is a sum. For instance, a 100 horsepower car weighting 2500 pounds has a predicted fuel economy of 37.2 - 0.032*100 - 3.88*2.5=24.3 miles per gallon.1 If we’re interested in making a prediction, we often hide the arithmetic behind a computer function, but it is exactly this arithmetic:\n\nmod_eval(mpg_mod, hp = 100, wt = 2.5)\n\n   hp  wt model_output\n1 100 2.5      24.3554\n\n\nThe arithmetic, in principle, let’s us evaluate the model for any inputs, even ridiculous ones like a 10,000 hp car weighing 50,000 lbs. There is no such car, but there is a model output.2\n\nmod_eval(mpg_mod, hp=10000, wt = 50)\n\n     hp wt model_output\n1 10000 50    -474.3937\n\n\nThe prediction reported here means that such a car goes negative 474 miles on a gallon of gas. That’s silly. One way to deal with such silliness is to restrict the inputs to “reasonable” values.\nOften, a better way to avoid the silliness is to structure the model so that unreasonable outputs—such as negative miles per gallon—cannot happen. Figuring out how to do this draws on mathematical experience. In this case, modeling the logarithm of mpg means that a numerically negative output still corresponds to a positive mpg. If we want the model output denominated in miles-per-gallon rather than logarithmic units, we just need to exponentiate (exp()) the logarithmic output to return to the world of miles-per-gallon:\n\nmod_logmpg <- lm(log(mpg) ~ hp + wt, data = mtcars)\nmod_eval(mod_logmpg, hp=10000, wt=50) %>%\n  mutate(model_mpg = exp(model_output))\n\n     hp wt model_output   model_mpg\n1 10000 50     -21.6327 4.02753e-10\n\n\nThis “trick” of modeling the logarithm of output keeps the model output in bounds so far as mpg is concerned. There will never be a negative mpg output.3 ## Modeling log odds\nWhen a model output is intended to be interpreted as a probability, we have a similar problem. THATS what the LOG-ODDS transformation DOES. WITH LOG-ODDS we can model probability using linear combinations of inputs."
  },
  {
    "objectID": "Reading-notes-lesson-33.html#probability-as-prediction",
    "href": "Reading-notes-lesson-33.html#probability-as-prediction",
    "title": "33  Measuring and accumulating risk",
    "section": "Probability as prediction?",
    "text": "Probability as prediction?\nDOES IT MAKE SENSE TO FRAME a PREDICTION IN TERMS OF A PROBABILITY? So long as the probability is not exactly zero or one, observing either of the two kinds of events—e.g., yes/no, alive/dead, diseased/healthy—does not contradict the model output. So how can we judge if a model is on-target or not. Or, equivalently, how can we decide which of two models is better.\nIn Lesson 26 we introduced the idea of a prediction interval OUTPUT SHOULD ALMOST ALWAYS (95% of the time) be within the interval.\nThe problem for us now is to create something like the PREDICTION INTERVAL when the model output is a probability.\nINTRODUCE the variance as a measure of uncertainty. The variance of a probability is \\(p(1-p)\\).\n\n\n\n\n\n\nExample: A bookies’ calculations [NEEDS FIXING]\n\n\n\nThe most familiar use of “odds” is in gambling. For instance, a famous song lyric puts the odds of Valentine winning the horse rate “at five to nine.” Less musically, this odds is \\(5/9 = 0.5555\\), but the two-number format makes particular sense for keeping track of bets. Five-to-nine describes a bet of one unit. The second number, 9, specifies the amount the gambler is staking on the outcome. On a loss, the gambler loses that stake. On a win, the gambler gets back the stake and, in addition, gets the amount specified by the first number. So a winner at five to nine would leave the racetrack with an extra $5. But on a loss, the gambler leaves $9 behind.\nA “bookie” is someone who provides a service. You can go to a bookie to lay a bit. In drama, this might be done by telephone: “Lay $90 on Valentine” is all the gambler needs to communicate. No money has to change hands. On a win, the bookie will return $50 to the gambler. On a loss, the gambler has a debt of $90.\nA bookie is not a gambler; he’s an accountant who records numbers. The bookie arranges these numbers so that he makes money. To see this, imagine a horse race including Valentine, Paul Revere, and Epitaph. To start, the bookie specifies odds on each possible outcome, say 5:9 for Valentine, 1:3 for Paul Revere (a favorite!), and 1:2 on Epitaph.\nIf the bookie has a good nose, about a third of the stakes will be bet on each outcome. If not, as new bets come in the bookie raises or lowers the odds to encourage or discourage bets so that the roughly one-third of stakes are placed on each outcome. Suppose at the end of the day that $500 is staked on each of the three outcomes.\nWRONG WRONG WRONG. It needs to work that the winning returned for Valentine has to be less than the stakes on the other horses, and similarly for all horses. So if $100 is bet on Valentine we need $100 staked on the other horses.\nAdded up, these odds are \\(5+1+1=7\\) on the top and \\(9+2+1=12\\) on the bottom. It’s important—for the bookie—that the odds are arranged so that the bottom number is larger than the top number: 12 is larger than 11. Note that this method of adding is simpler than combining fractions. To add the fractions \\(1/2\\) and \\(1/3\\) gives \\(5/6\\). But to combine the odds \\(1:2\\) and \\(1:3\\) gives \\(2:5\\). One more detail is needed for a real-life bookies, taking into account the size of each bet. For instance, a $5 bet at 5:9 would be recorded as 25:45.\nNow the race is run. The winner is … well … from the bookie’s point of view it doesn’t matter who wins."
  },
  {
    "objectID": "Reading-notes-lesson-33.html#irrationality",
    "href": "Reading-notes-lesson-33.html#irrationality",
    "title": "33  Measuring and accumulating risk",
    "section": "“Irrationality”",
    "text": "“Irrationality”\n[From The Model Thinker, p. 52]\nGain Framing: You have two options\nOption A) Win $400 for certain\nOption B) Win $1000 if a fair coin comes up heads and $0 if tails\nLoss Framing: You are given $1000 and have two options:\nOption a) Lose $600 for certain\nOption b) Lose $0 if a fair coin comes up heads and lose $1000 if tails.\nHyperbolic discounting: see pp 52-43\n“Prospect theory”, Kahneman and Tversky (1979) “Prospect theory: an analysis of decisions under risk,” Econometrica 47(2):263-291 link to paper\n\n\n\n\n\n\nExample\n\n\n\nA subtle modification to the linear model architecture allows the modeller to guarantee that the output will be between zero and one. The modified architecture, called “logistic regression”, is therefore well suited to modeling categorical response variables, where the model output will be interpreted as a probability.\nFigure 33.1 shows a logistic model of survival as a function of age and smoking status. Notice that in the logistic model, the effect of smoking on survival is negative, particularly for people around age 50. The logistic architecture provides an intrinsic flexibility which avoids the undue influence of the very young and very old, for whom survival is close to 100% or 0 respectively regardless of smoking status.\n\n\n\nScale for 'y' is already present. Adding another scale for 'y', which will\nreplace the existing scale.\n\n\n\n\n\nFigure 33.1: A simple logistic model of survival versus age and smoking status.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Fraction attributable\n\n\n\nUS Federal law forbids employment discrimination based on age. (There are some exceptions, such as air-traffic controllers, whose mandatory retirement age is 56). In a discrimination lawsuit, data on who was and who was not laid-off was used to construct a model of the probability of layoff. The effect size is, as usual for a probability model, expressed in log odds.\n\nbaseline: risk of 20%, so log odds of of -1.4.\nage over 50, add log odds of 1 ± 0.3\nsoftware engineer, subtract log odds of 0.5 ± 0.25\npaid different from company average, subtract log odds of 0.2 ± 0.1 per $10,000 high than company average.\n\nThese estimates come from a logistic regression model laid_off ~ over50 +  software_engineer + pay_above_average.\n\nFor a laid-off employee over 50, what is the fraction attributable to age?\n\n\n\n\n\n\n\nSolution\n\n\n\nThe baseline risk of being laid off is 20%. For the employee aged over 50 years, the log odds of the risk is -1.4 + 1 ± 0.3, or -0.7 to -0.1. Translating these log odds into probabilities gives a risk of 33% to 47%, with the range reflecting the uncertainty in the effect size from the model. The estimated relative risk (risk ratio) for the employees over 50 ranges from 33/20 to 47/20, that is, from 1.65 to 2.35. The attributable fraction is \\((RR - 1) / RR\\) and therefore ranges from (1.65 - 1)/1.65 to (2.35 - 1) / 2.35 or 40% to 57%.\n\n\n\nWhat fraction of all layoffs can be attributed to age over 50? (Population attributable fraction.) Assume that one-third of the employees are over 50.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "Reading-notes-lesson-34.html#collecting-useful-data",
    "href": "Reading-notes-lesson-34.html#collecting-useful-data",
    "title": "34  Constructing a classifier",
    "section": "Collecting useful data",
    "text": "Collecting useful data\nConsider this news report and note the time lag between collection of the dietary explanatory variables and the response variable—whether the patient developed pancreatic cancer.\n\nHigher vitamin D intake has been associated with a significantly reduced risk of pancreatic cancer, according to a study released last week. Researchers combined data from two prospective studies that included 46,771 men ages 40 to 75 and 75,427 women ages 38 to 65. They identified 365 cases of pancreatic cancer over 16 years. Before their cancer was detected, subjects filled out dietary questionnaires, including information on vitamin supplements, and researchers calculated vitamin D intake. After statistically adjusting1 for age, smoking, level of physical activity, intake of calcium and retinol and other factors, the association between vitamin D intake and reduced risk of pancreatic cancer was still significant. Compared with people who consumed less than 150 units of vitamin D a day, those who consumed more than 600 units reduced their risk by 41 percent. - New York Times, 19 Sept. 2006, p. D6.\n\nThis was not an experiment; it was an observational study without any intervention to change anyone’s diet.\nIn building a classifier, we have a similar situation. Perhaps we can perform the blood test today, but that gives us only the test result, not the subject’s true condition. We might have to wait years for that condition to reveal itself. Only at that point can we measure the performance of the classifier.\nTo picture the situation, let’s imagine many people in the world, some of whom have the condition and some who don’t. On Day 1 of the study, we test everyone and get raw score on a scale from 0 to 40. The results are shown in Figure 34.1. Each glyph is a person. The varying locations are meant to help us later on; for now, just think of them as representing where each person lives in the world. The different shapes of glyph—circle, square, triangle—are meant to remind you that people are different from one another in age, gender, risk-factors, etc.\nEach person took a blood test. The raw result from that test is a score from 0 to 40. The distribution of scores is shown in the right panel of the figure. We also show the score in the world-plot; the higher the raw score, the more blue the glyph. On Day 1, it isn’t known who has the condition and who does not.\n\n\n\n\n\nFigure 34.1: Day 1: The people participating in the study to develop the classifier. Each has been given a blood test which gives a score from zero (gray) to one (blue).\n\n\n\n\nHaving recorded the raw test results for each person, we wait. In the pancreatic cancer study, they waited 16 years for the cancer to reveal itself. After the waiting period, we can add a new column to the original data; whether the person has the condition (C) or doesn’t (H). In Figure 34.2 people who developed the condition have been re-drawn with filled glyphs to make them easier to spot.\n\n\n\n\n\nFigure 34.2: The same people as in Figure 34.1, but now we know who turned out to have the condition and who didn’t. The people with the condition are re-drawn with filled glyphs.\n\n\n\n\nOnly now, when we have found out the C-or-H state of each person, can we compare the raw scores on the blood test for those were found to have the condition and those who were not."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#setting-the-threshold-for",
    "href": "Reading-notes-lesson-34.html#setting-the-threshold-for",
    "title": "34  Constructing a classifier",
    "section": "Setting the threshold for +",
    "text": "Setting the threshold for +\n\n\n\n\n\nFigure 34.3: The distribution of raw test scores. After we know the true condition, we can break down the test scores by condition.\n\n\n\n\nAt the end of year 16, we can now divide the study participants into two groups: those who developed the condition and those who didn’t.\nFigure 34.3 shows the distribution of raw test scores. The values are those recorded on Day 1, but after waiting to find out the patients’ conditions, we can subdivide them into those who have the condition (C) and those who don’t (H).\nThe classifier we aim to build is not yet complete, even though we have all the data we need. To finish the classifier, we need to identify a “threshold score.” Raw scores above this threshold will generate a + test; scores below the threshold generate a - test.\n\n\n\n\n\n\nFalse positives and false negatives\n\n\n\nSince each individual person is either C or H, and each has a test result that is + or -, we can divide the people into four distinct groups.\n\nC with a + test: This is called a “true positive”, since the test was + (that is, positive) and the test was right.\nH with a + test: This is a “false positive”; the test was + (positive) but wrong (false).\nH with a - test: A “true negative”\nC with a - test: A “false negative”\n\n\n\nIt is you, the modeler, who decides what the threshold should be. In setting it, you have two objectives that happen to conflict. You want to avoid both false negatives and false positives. You do this by setting the threshold low enough that a large fraction of the people in column C are above the threshold. Those below the threshold are the false negatives. But you also want to set the threshold low enough so that only a small fraction of the people in column H are above the threshold—such people are false positives.\n\n\n\n\n\n\nThe Loss Function\n\n\n\nIn order to set the threshold at an optimal level, it is important to measure the impact of the positive or negative test result. This impact of course will depend on whether the test is right or wrong about the person’s true condition. It is conventional to measure the impact as a “loss,” that is, the amount of harm that is done.\nIf the test result is right, there’s no loss. Of course, it’s not nice that a person is C, but a + test result will steer our actions to treat the condition appropriately: no loss in that.\nTypically, the loss of a false negative is reckoned as more than the loss of a false positive. A false negative will lead to failure to treat the person for a condition that he or she actually has.\nIn contrast, a false positive will lead to unnecessary treatment. This also is a loss that includes several components that would have been avoided if the test had be right. The cost of the treatment itself is one part of the loss. The harm that a treatment might do is another part of the loss. And the anxiety that the person and his or her family go through is still another part of the loss. These losses are not necessarily small. The woman who gets a false positive breast-cancer diagnosis will suffer from the effects of chemotherapy and the loss of breast tissue. The man who gets a false-positive prostate-cancer diagnosis may end up with urinary incontinence and impotence.\nThe aim in setting the threshold is to minimize the total loss. This will be the loss incurred due to false negative times the number of false negatives plus the loss incurred from a false positive times the number of false positives.\nThe term “loss function” is used to describe how the loss depends on whether the outcome is a false negative or a false positive.\nHowever, as you’ll see in Lesson 35, the number of false positives and false negatives depends on the “prevalence” of the condition. This can differ from group to group or location to location. Until we know the relevant prevalence and the loss function, we can’t sensibly pick a threshold."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#test-performance",
    "href": "Reading-notes-lesson-34.html#test-performance",
    "title": "34  Constructing a classifier",
    "section": "Test performance",
    "text": "Test performance\nAs pointed out above, to know the impact of the test you need to know both the loss function and the prevalence. Even so, there is a simple way to describe the performance of the test itself that will enable us to apply the test optimally for any prevalence or loss function.\nAssume for the moment that we have already selected a threshold that turns the raw score from the test into the + or - final result from the test.\nThe threshold lets us mark as + or - each of the people whose data we are using to develop the classifier. In terms of the graphics we have been drawing, the threshold turns what we were drawing as shades of blue to represent the raw score into a definite blue-or-not color code, as in Figure 34.4.\n\n\n\n\n\nFigure 34.4: Fixing the threshold turns the shades of blue into blue-or-not. Depending on the threshold, the picture will look somewhat different.\n\n\n\n\nThere’s not much more we can do at this point. We know who has a positive test and who a negative test, but we don’t know which people are C and which H. We therefore wait to get more information. This might involve waiting for the condition to reveal itself, 16 years in the pancreatic cancer study.\nThe day at last comes when we can identify the condition of each person in Figure 34.4. At this point, we can split the people in Figure 34.4 into a C group and an H group, as in Figure 34.5.\n\n\n\n\n\nFigure 34.5: The people in the classifier-building project split up according to their condition. Everyone in one group has condition C. (Note that all the glyphs are filled in since everyone has the condition.) Everyone in the other group has condition H. (The glyphs of the H people are hollow.)\n\n\n\n\nWith the people divided according to their actual condition, it is easy to measure how well the test performed. There are two, distinct measures: the “sensitivity” and the “specificity.” The word “distinct” is important. The sensitivity applies only to the people in the C group. We get absolutely no information about the specificity from the C group. It’s the H group that we will look at to find the specificity.\nBoth sensitivity and specificity describe the fraction of people who get the correct result from the test. But there are two distinct correct results, depending on which group, C or H, any person is in. Among the C people, a correct result is a positive test. Among the H people, a negative test is the correct result.\nFrom Figure 34.5 you can estimate by eye the sensitivity and specificity, keeping in mind that a blue symbol means a + test result. The sensitivity is the fraction of people in the C group who are marked by a blue symbol. The specificity is the fraction of people in the H group who are marked gray, the color for a - test. The vast majority of the C group are colored blue; counting marks puts the sensitivity at 90%. For the H group, most of the marks are gray. Count marks and you’ll find the specificity to be 85%."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#lc-34.a",
    "href": "Reading-notes-lesson-34.html#lc-34.a",
    "title": "34  Constructing a classifier",
    "section": "LC 34.A",
    "text": "LC 34.A\nThe following graph shows four hypothetical planets, each inhabited by a different population. Filled glyphs indicate people with condition C, hollow glyphs are for people with condition H. The blue glyphs have positive (+) test results, the gray glyphs have negative (-) test results.\n\n\n\n\n\n\nWhich planets is suited for measuring sensitivity and which for specificity?\nBy eye, estimate the numerical values for sensitivity and specificity."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#lc-34.b",
    "href": "Reading-notes-lesson-34.html#lc-34.b",
    "title": "34  Constructing a classifier",
    "section": "LC 34.B",
    "text": "LC 34.B\nAnother case of four hypothetical planets, but in a different solar system than LC 34.A.\n\n\n\n\n\n\nWhich planets is suited for measuring sensitivity and which for specificity?\nBy eye, estimate the numerical values for sensitivity and specificity."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#other-stuff",
    "href": "Reading-notes-lesson-34.html#other-stuff",
    "title": "34  Constructing a classifier",
    "section": "Other stuff",
    "text": "Other stuff\nConsider a credit-card company might building a classifier to predict at the time of the transaction whether a purchase of gasoline is fraudulent. The company knows how often and how much gasoline the individual cardholders buys, where the cardholder lives, whether the cardholder travels extensively, typical times of day for a purchase, and so on. Feature engineering is the process of using existing data—including, in our example, whether the purchase turned out to be fraudulent—to develop potential markers or signals of the outcome. For simplicity, imagine the features selected are the number of days since the last gasoline purchase and the distance from the last place of purchase.\nOnce potential features have been proposed, the engineers building the classifier assemble training and testing data sets. Suppose, for the purpose of illustration, that the training data has 2000 fraudulent transactions and 4000 non-fraudulent ones, and the testing set is about the same.\nThe word “assemble” was used intentionally to describe how the testing and training data were collected: a case-control study. Since the objective is to detect fraud, it is reasonable to have a lot of “yes” cases in the data. The “no” cases serve as a kind of control; they were included specifically to have balance in the data. If data had been collected as a simple random sample of credit card transactions, there would have been many, many more “no” cases than “yes.”\nWith such training data it is easy to build a statistical model with Fraud as the response variable. That model can then be evaluated on the testing data to produce a model output for each row:\n\n\n\nFraud\nFeature 1\nFeature 2\nModel output\n\n\n\n\nno\n6 days\n5 miles\n-\n\n\nno\n1 day\n250 miles\n+\n\n\nyes\n120 days\n75 miles\n-\n\n\nno\n5 days\n0 miles\n-\n\n\nyes\n0.2 days\n90 miles\n+\n\n\n\nIt’s understandable that a classifier may not have perfect performance. After all, it iss trying to make a prediction based on limited data, and randomness may play a role.\nThere are different ways of making a mistake, and these different ways have very different consequences. One kind of mistake, called a “false positive”, involves a classifier output that’s positive (i.e. the classifier indicates fraud) but which is wrong. The consequence of this sort of mistake in the present example is a customer who has to find another way to pay for gasoline.\nThe other kind of mistake is called a “false negative”. Here, the classifer output is that the transaction is not fraudulent, but in actuality it was. The consequence of this kind of mistake is different: a successful theft.\nThe nomenclature signals that a mistake has been made with the word “false.” The kind of mistake is either “positive” or “negative”, corresponding to the output of the classifier.\nWhen the classifier gets things right, that is a “true” result. As with the false results, a true result is possible both for a “positive” and a “negative” classifier output. So the two ways of getting things right are called “true positive” and “true negative”.\nTabulating all 6000 rows of the testing data might produce something like this:\n\n\n\nFraud\ntest +\ntest -\n\n\n\n\nyes\n1900\n100\n\n\nno\n50\n3950"
  },
  {
    "objectID": "Reading-notes-lesson-34.html#incidence",
    "href": "Reading-notes-lesson-34.html#incidence",
    "title": "34  Constructing a classifier",
    "section": "Incidence",
    "text": "Incidence"
  },
  {
    "objectID": "Reading-notes-lesson-34.html#sensitivity-and-specificity",
    "href": "Reading-notes-lesson-34.html#sensitivity-and-specificity",
    "title": "34  Constructing a classifier",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\nExample: Accuracy of airport security screening\n\n\n\nAirplane passengers have, for decades, gone through a security screening process involving identity checks, “no fly” lists, metal detection, imaging of baggage, random pat-downs, and such. How accurate is such screening? Almost certainly, the accuracy is not as good as an extremely simple, no-input, alternative process: automatically identify every passenger as “not a security problem.” We can estimate the accuracy of the “not a security problem” classifier by guessing what fraction of airplane passengers are indeed a threat to aircraft. In the US alone, there are about 2.5 million airplane passengers each day and security problems of any sort rarely happen. So the accuracy of the no-input classifier is something like 99.999%.\nThe actual screening system, using metal detectors, baggage x-rays, etc. will have a lower accuracy. We know this since it regularly mis-identifies innocent people as security problems.\nThe problem here is not with airport security screening, but with the flawed use of accuracy as a measure of performance. Indeed, achieving super-high accuracy is not the objective of the security screening process. Instead, the objective is to deter security problems by convincing potential terrorists that they are likely to get caught before they can get on a plane. This has to do with the sensitivity of the system. The specificity of the system, although important to the everyday traveller, is not what deters the terrorist."
  },
  {
    "objectID": "Reading-notes-lesson-35.html#prevalence-in-the-population",
    "href": "Reading-notes-lesson-35.html#prevalence-in-the-population",
    "title": "35  Accounting for prevalence",
    "section": "Prevalence in the population",
    "text": "Prevalence in the population\nIt often happens that the data used to train a classifier function is not representative of the population in which it will be used. For example, classifiers are often trained on “case-control” data. People with a particular condition, say, having strong symptoms of COVID-19 are easily recruited at health clinics. These are the “cases.”\nThe “controls” are people who don’t have any symptoms. These can be recruited by, say, door-to-door canvasing of a neighborhood. If we recruit 500 cases and 500 controls, then the prevalence of COVID-19 symptoms in our database will be 50%.\nAssume for the sake of an example that the sensitivity of an inexpensive, over-the-counter COVID-19 test is 90%, and the specificity is 95%. From these numbers, we can calculate the false-positive and false-negative rates.\nFalse negatives can only arise in the part of the population who have the condition. For the 500 cases in our database, the sensitivity of 90% will work out to 450 correct positive tests and 50 incorrect negative tests.\nFalse positives can only arise among the people who do not have the condition. For the 500 controls in our database, the specificity of 95% will work out to 475 correct negative tests and 25 incorrect positive tests.\nThere are 1000 people in the database, so the false-positive rate is 25/1000 = 2.5%. The false-negative rate is 50/1000 = 5%.\nHowever, these rates are not necessarily representative of the rates we will find when the test is used in the general population. Fortunately, we can calculate the false-positive and false-negative rates for a population with any given prevalence.\nImagine, say, that we think the prevalence of COVID-19 in the general population is 4%. This puts the false-negative rate—remember, false-negative only applies to people with the condition—at \\(4\\% \\times (1-\\text{sensitivity|) = 0.04 \\times 0.10 = 0.004$.\n\nIn that population, 96% do not have COVID-19. This, combined with the specificity, allows calculation of the false-positive rate: $96\\% \\times (1-\\text{specificity}) = 0.96 \\times 0.05 = 0.48.\n\n## Sensitivity and specificity as likelihoods\n\nIn Lesson 34, we used training data to estimate the sensitivity and specificity of the test in the C vs H classifier. In that example, the sensitivity was 90% and the specificity was 85%. We can use those facts to construct the test results on a hypothetical population with whatever *prevalence* we think is relevant to the actual population we will be testing. The arithmetic is straightforward, but we will use graphics to illustrate the calculations.\n\nFirst, we synthesis a data frame that has many rows, half corresponding to imaginary people with condition C and the other half with condition H. Next, we assign a *test outcome* for each of the people. We do this in a way that duplicates the sensitivity and specificity that we measured earlier. That is, for the C people, we randomly generate a + or - test result, setting the probability of a + result to be the measured *sensitivity*. We do similarly for the H people, the only difference being that the probability of a - result will be the *specificity*. Figure @fig-planets-of-C-and-H shows 2000 rows of simulated data, with 1000 C cases and 1000 H cases.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Knowing the specificity and sensitivity of the test, we can create hypothetical worlds to use in a simulation.)](Reading-notes-lesson-35_files/figure-html/fig-planets-of-C-and-H-1.png){#fig-planets-of-C-and-H width=672}\n:::\n:::\n\n\n@fig-planets-of-C-and-H shows the simulated data split up into two groups, one for the C cases and the other for the H cases.\n\nNow we are going to push hard on a fanciful metaphor in order to introduce some new terminology that will be important for Lessons 36 and 37. The metaphor is that C people originate in a *planet* populated exclusively by C people. The H people live on another planet, inhabited only by the H. On both planets, the classifier has been applied to everyone so that we know their test results: + is shown in blue, - in gray, as in Lesson 34.\n\nNeedless to say, these planets are imaginary. Let's call them \"**hypothetical**\" planets, planets where a particular hypothesis is known to apply. The hypothesis on the C planet is that all the inhabitants are C. Likewise for the H planet. The proportion of + test results on Planet C has been set to match the actual *sensitivity* of the classifier. Similarly, the - test results on Planet H match the actual *specificity* of the classifer.\n\nA new bit of statistical vocabulary that will be important in Lessons 36 and 37. A\n\"**likelihood**\" is a special kind of probability. Like all probabilities, it's a number between zero and one. Likelihoods are always conditional probabilities, that is a probability calculated on the assumption that a specific assumption holds. In terms of our metaphor, a likelihood is a probability calculated on a hypothetical planet.\n\nThe sensitivity is a likelihood. The hypothetical planet is Planet C, the planet where all inhabitants have condition C. On that planet, the sensitivity is the probability that a randomly selected inhabitant will get a test result +. But, since the probability is being calculated on the condition that all inhabitants have condition C, it is a conditional probability: a likelihood.\n\nThe specificity is similarly a likelihood. The hypothetical planet is H, where all inhabitants have condition H. The conditional probability is that of getting a - result on the hypothetical planet.\n\nWe will denote likelihoods using the letter ${\\cal L}$. The fanciful script reminds us that the likelihood is a fanciful probability, a probability on a hypothetical world.\n\nThe sensitivity is ${\\cal L}(+ \\text{ given } C)$. The specificity is ${\\cal L}(- \\text{ given } H)$. The \"given C\" means \"on hypothetical planet C. The \"given H\" means \"on hypothetical planet H.\" It would be just as correct to write $p(+\\text{ given }C)$ and $p(-\\text{ given }H)$, but I want to remind you that the the sensitivity and specificity apply only to their respective conditions, C and H. Also, phrases like \"assuming H, the probability of - is ...\" or \"under the assumption H, the probability of - is ....\"\n\n::: {.callout-note}\n## Constructing a world with a given prevalence\n\nNow to use the hypothetical planets C and H to construct a realistic world. By \"realistic\" we mean that the sensitivity and specificity are right *and* that the prevalence matches that of the target population in which the classifier is going to be used.\n\nAs notation, let's write the prevalence as $p(C)$. This is a probability, one that we will make up in order to define the target population. Notice that $p(C)$ does not have any condition. We knew it from previous studies of the target population. For this reason, it's appropriate to call $p(C)$ by a special name: the **prior** probability. The twin probability, $p(H)$ is also a prior probability. Since C and H are the only possibilities it must be that $p(H) = 1 - p(C)$.\n\nTo construct a realistic world, we're going to import people from hypothetical planets C and H. Remember, on C everyone has condition C; on H, everyone has condition H. How many people to import from each hypothetical planet? To put $N$ people on the realistic world, we will take $N p(C)$ from planet C, and $N (1-p(C))$ from planet H. The world thus populated, with $N=500$ and $p(C)$ set to 30% is shown in @fig-prevalence-world.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The world created to have prevalence $p(C) = 30\\%$ and $N=500$ inhabitants. This world has 150 immigrants from Planet C and 350 from Planet H.](Reading-notes-lesson-35_files/figure-html/fig-prevalence-world-1.png){#fig-prevalence-world width=672}\n:::\n:::\n\n\nWhile it's feasible to do the calculations as a simulation on a computer, the mathematics is not so tricky. For example, we can compute the false-positive and false-negative rates on the realistic world:\n\n- False positive rate:  $\\left(1 - \\underbrace{{\\cal L}(+ \\text{ given } C)}_\\text{sensitivity}\\right)\\cdot \\underbrace{p(C)}_\\text{prevalence}$\n\n- False negative rate: $\\left(1 - \\underbrace{{\\cal L}(- \\text{ given } H)}_\\text{specificity}\\right) \\cdot \\left(1-\\underbrace{p(C)}_\\text{prevalence}\\right)$\n\n:::\n\n## From the patient's point of view\n\nHaving created and populated the realistic world, we can compute quantities that are useful in communicating with patients and doctors. For example, suppose a patient has just gotten a positive test. How certain an indication is this of the patient's having the condition C?\n\nOn the realistic world, there are two groups who get a positive test. One group is the people who genuinely have C and who also have a positive test. These are the true positives. The relative size of this group is $P(+ \\text{ given } C) \\equiv \\text{proportion of true positives} = {\\cal L}(+ \\text{ given } C)\\cdot p(C)}\\).\nThe other group is the false positives, the people who have condition H but nonetheless received a + test. The relative size of this group is \\(P(+ \\text{ given } H) \\equiv \\text{proportion of false positives} = {\\cal L}(+ \\text{ given } H)\\cdot p(H)\\). We can calculate the size of this group since \\(p(H) = 1-p(C)\\) and \\({\\cal L}(+ \\text{ given } H) = 1 - {\\cal L}(+ \\text{ given } H)\\).\nConsequently, the probability that a + test result will be true is \\[\\frac{{\\cal L}(+ \\text{ given } C)\\cdot  p(C)}{{\\cal L}(+ \\text{ given } C)\\cdot p(C) + {\\cal L}(+ \\text{ given } H)\\cdot p(H)}\\]"
  },
  {
    "objectID": "Reading-notes-lesson-35.html#for-the-patient-with-a-test-result",
    "href": "Reading-notes-lesson-35.html#for-the-patient-with-a-test-result",
    "title": "35  Accounting for prevalence",
    "section": "For the patient with a + test result",
    "text": "For the patient with a + test result\nThe above formula looks daunting, but we know each of the quantities and can do the arithmetic.\n\n\n\n\n\n\nDemonstration\n\n\n\nFor instance, in the example in Lesson 34 we stipulated that the prevalence \\(p(C) = 4\\%\\). This, of course, implies that \\(p(H) = 96\\%\\). The sensitivity was stipulated at 90%, that is, \\({\\cal L}(+ \\text{ given } C) = 90\\%\\). The specificity was 85%, meaning that \\({\\cal L}(+ \\text{ given } H) = 15\\%\\). Putting these values together in the formula tells us that the probability that a person with a + test actually having the condition C is:\n\\[\\frac{0.90 \\cdot 0.04}{0.90 \\cdot 0.04 + 0.15\\cdot 0.96} = 20\\%\\] That’s good news to the Lesson 34 patient who gets a positive test result: He or she has only a 20% chance of having condition C.\nNow consider the situation of a patient in the world of ?fig-prevalence-world. There, the population has a prevalence of 30% rather than 4%. This changes the meaning of a + test. The probability of being C given a + test is:\n\\[\\frac{0.90 \\cdot 0.30}{0.90 \\cdot 0.30 + 0.15\\cdot 0.70} = 72\\%\\]\nIn a population where the prevalence is high, a + test is more likely to mean what it suggests.\n\n\nSince the reliability of a + result differs depending on the prevalence of C, it often happens that medical screening tests are recommended for one group of people but not for another.\nFor instance, the US Preventative Services Task Force (USPSTF) issues recommendations about a variety of medical screening tests. According to the Centers for Disease Control (CDC) summary:\n\nThe USPSTF recommends that women who are 50 to 74 years old and are at average risk for breast cancer get a mammogram every two years. Women who are 40 to 49 years old should talk to their doctor or other health care provider about when to start and how often to get a mammogram.\n\nRecommendations such as this can be baffling. Why recommend mammograms only for people 50 to 74? Why not for older women as well? And how come women 40-49 are only told to “talk to their doctor?”\nThe CDC summary needs decoding. For instance, the “talk to [your] doctor” recommendation really means, “We don’t think a mammogram is useful to you, but we’re not going to say that straight out because you’ll think we are denying you something. We’ll let your doctor take the heat, although typically if you ask for a mammogram, your doctor will order one for you. If you are a woman younger than 40, a mammogram is even less likely to give a useful result, so unlikely that we won’t even hint you should talk to a doctor.”\nThe reason mammograms are not recommended for women 40-49 is that the prevalence for breast cancer is much lower in that group of people than in the 50-74 group. The prevalence of breast cancer is even lower in women younger than 40.\nSo what about women 75+? The prevalence of breast cancer is high in this group, but at that age non-treatment is likely to be the most sensible option. Cancers can take a long while to develop from the stage identified on a mammogram, and at age 75+ it’s not likely to be the cause of eventual death.\nThe USPSTF web site goes into some detail about the reasoning for their recommendations. It’s worthwhile reading to see what considerations went into their decision-making process."
  },
  {
    "objectID": "Reading-notes-lesson-36.html",
    "href": "Reading-notes-lesson-36.html",
    "title": "36  Hypothesis testing",
    "section": "",
    "text": "We are nearing the end of our journey through the world of statistical thinking. But I think it’s time to get off the well-travelled road and take a detour of a few paragraphs. Detours are usually not the straightest path between two points, but they often have some advantage: safety, a scenic view or inspiring experience, and such.\nOur detour starts with a turn onto a lane marked with the word “orthodox.” There are, as you know, religions denominated by “orthodox.” But the word has a more general meaning that can be seen by translating the Greek origin words “ortho” and “doxa” into a familiar language: “ortho” means “straight” or “right”; “doxa” means “belief”. Synonyms for “belief” include creed, dogma, teaching, doctrine (which stems from the Latin for “to teach”), conviction, and article of faith. A line is straight and the phrase “the party line” indicates attitudes that fall into line with the party leadership. “Ortho” appears in “orthodonture” (straightening the teeth), “orthography” (correct spelling), and “orthogonal” (being at a right angle).1\nTo “true a wheel” means to set it straight, and the phrase “true believer” does not refer to someone whose beliefs are correct but rather to someone who stays in line with to a particular system of belief. There can be many different systems of belief, many different orthodoxies, and disagreeing parties can each have their distinct line.\nThere are two major orthodoxies of statistical thought, “frequentists” and “Bayesians,” a fact that’s important to keeping straight the variety of statistical nomenclature. The key distinction between the frequentist orthodoxy and that of the Bayesians, is their attitude toward the meaning of “probability.” One sect, the “frequentists,” bases their methodology in a supreme being they call the “population.” Coin flips are an example of a population; an abstractly infinite supply of events. The probability of “heads” is the frequency of a head turning up in \\(n\\) trials, where \\(n \\rightarrow\\infty\\). Flipping coins an infinite number of times is still a work in progress. Until it is complete, we need to work with the probability of heads as the proportion of a large but finite number of trials in which the outcome is “heads.”\nIn contrast, the “Bayesian” sect holds that a probability is a statement about belief. Different people, depending on their experiences, will rightly come to different conclusions about the probability of the outcome of an event. Bayesians will happily deal with a statement like, “the probability of rain tomorrow is 60%,” while frequentists might respond (while packing an umbrella in their knapsack for tomorrow’s weather) by pointing out that there is no “population” of “tomorrows” from which we can draw a sample to estimate the frequency of rain.\nFor a frequentist, the hypothesis “April showers bring May flowers” is an assumption. There is no actual population from which to draw many trials, so a probability cannot be assigned to the hypothesis. In the frequentist liturgy, to test the hypothesis means to make a simulation of the world. Typically the simulation implements a world in which it is hard coded that the connection described by the hypothesis does not exist. This no-connection hypothesis is generically called the “Null hypothesis.” In this case, the Null hypothesis states that there is no association between April showers and May flowers.\nConstructing a simulation that generates data from the Null hypothesis is easy. Take the real-world data recording the observations of April precipitation and May floration. Randomly shuffle the entries in the April column while holding the May column fixed. The shuffling destroys any systematic association between the April and May columns. Any measured association in the shuffled data is incidental and accidental.\nRun many trials of the shuffling simulation. In each trial, record the measured association. It’s reasonable to expect that the measured association across the trials will be close to zero. Indeed, you use the trial results to define what “close to zero” means. Then look back at the association found in the real-world, unshuffled data. If that observed association falls within the definition of “close to zero,” then it is not fair to insist that the real-data is inconsistent with the Null hypothesis. That is, you “fail to reject” the Null hypothesis. On the other hand, if the observed association falls outside the bounds of “close to zero,” then you are entitled to reject the Null hypothesis.\nNotice that the simulation mechanism provides a population— a “hypothetical planet” to use the language of Lesson 34—from which many samples could be drawn. Thus, it’s straight thinking for the frequentists to assign a probability to the event “a simulation trial will generate a result at least as extreme as what was observed in the real world.” This probability, in the lingo of hypothesis testing, is called a “p-value”.\nIn my opinion, it would have been better for everyone to rename the p-value as the \\({\\cal L}\\)-value. After all, it’s a likelihood, a proportion calculated on a hypothetical planet.\nFor many people, the orthodoxy that “hypothesis testing” can properly lead only to one of two conclusions—“reject” or “fail to reject” the Null hypothesis—seems stilted and overly rigid. Like others who are not aligned with orthodoxy, they will translate the orthodox result into language that they find reasonable and more comfortable. For instance, they will say that the p-value is the probability that the Null hypothesis is true. Or, they interpret a small probability for the Null hypothesis—what should properly lead to “rejecting” the Null hypothesis—as an indication that the original hypothesis is to be “accepted” as true. And “failing to reject” often gets (mis-)interpreted as meaning the original hypothesis is not true.\nThe ministers of orthodoxy—statistics professors, for example—will point out that such (mis-)statements are a sign of wrong thinking. The ministers exert such power as they have to set straight the strays among the flock, for instance by giving low scores on a course final examination. In practice, limited as it is to the examination room, this punishment rarely leads to a repentant and binding return to frequentist orthodoxy.\nThe Bayesian orthodoxy is more permissive. It accepts as legitimate statements involving probability, such as “Hypothesis A is less likely than Hypothesis B.”\nIn Lesson 35 we used a Bayesian approach to address two hypotheses that were relevant to a disease C. Once the data are in—say, a test result of + in the Lesson 35 example—we can use the likelihoods (sensitivity and specificity) and the disease prevalence to calculate the probability that, for a + test result, the patient actually has C.\nThe frequentists don’t disaggree with the legitimacy of a likelihood. That is just a probability under the assumption that a given hypothesis is true. Where they depart from the Bayesians is in accepting the “probability of a hypothesis.” For, in order to calculate the relative probability of two hypotheses, you need to combine the likelihoods with a prior idea of how likely the hypotheses were in the first place. Frequentists assert that any such prior ideas are subjective.\nFew people will disagree with the idea that good science ought to be objective. Yet allowing some scope for subjectivity can lead to better informed decision-making. To illustrate, let’s tell a story that illustrates the benefits of assigning a probability, subjective though it may be, to a hypothesis.\nImagine you are a doctor. A long-time patient comes to your clinic. His recent symptoms have him thinking that there is a strong chance he might have cancer. You, the doctor, ask questions about the patient’s symptoms and do a brief physical examination.\nA standard medical practice is to construct a “differential diagnosis”. In doing this, you construct a mental list of the medical disorders that might account for the symptoms and examination results. Since you have extensive training and experience, this list might be long and contain disorders that are relatively common and some that are rare.\nTo keep the story simple, let’s make the list of plausible disorders unrealistically short: the patient either has cancer or has the flu. Each of these is a hypothesis. Continuing the process of differential diagnosis, you, the doctor, consider what medical steps might point in favor of one hypothesis or the other. For instance, you could order a tissue biopsy. Less invasive and less costly, you could decide on ordering a magnetic resonance imaging (MRI) or, simpler, using a medical screening test for cancer. Or you might even direct the patient to take some over-the-counter flu treatment and report back in two weeks if the symptoms haven’t resolved.\nIn medical school, you learned a mantra: “When you hear hoofbeats, think horses, not zebras.” Zebras are very rare in most parts of the world, horses much less so. So horses are a much more likely source of hoofbeats than are zebras.\nTo make an appropriate decision, you consider what you know about the patient. Had he just finished a round of chemo-therapy six months ago? If so, an MRI might be a good choice. Is the patient elderly? The elderly as a group have a greater risk of developing cancer than the young. So for an elderly patient, you might decide on ordering a screening test. Does the patient have a history of good health and an active imagination? Then the flu medication might be the right road to go down.\nA Bayesian would interpret this decision-making process in terms of probabilities. You assign, based on your observations, a probability to each hypothesis. If the probability of the cancer hypothesis is much smaller than the probability of flu, and if the cost of making a mistake (as measured by a “loss function,” see Lesson 34) is small, the flu medication is a good next step. But if the probability of cancer is not so small, and the cost of a delay is high, then ordering the screening test seems appropriate.\n\n\n\n\n\nThe link between words for “straight” and thinking according to the rule book has ancient origins. The word “canon,” meaning the set of officially accepted writings, comes from the Sumerian word for a straight reed.↩︎"
  },
  {
    "objectID": "Reading-notes-lesson-37.html#incredibly-simple-interpretation",
    "href": "Reading-notes-lesson-37.html#incredibly-simple-interpretation",
    "title": "37  Calculating a p-value",
    "section": "“Incredibly simple” interpretation",
    "text": "“Incredibly simple” interpretation\nAs you will see, the p-value is always a number between zero and one. When the p-value is small, the conclusion is that the corresponding explanatory variable is contributing to explaining the variation in the response variable. That is, a p-value that’s small is justification for believing that there is a connection of some sort between the explanatory variable and the response variable.\n“Small” in the phrase “when the p-value is small” is most usually taken to mean \\(p < 0.05\\). But different fields have different standards for defining small. For instance, it’s common in psychology to consider \\(p < 0.10\\) as fairly small, while in physics, “small” means perhaps \\(p < 0.001\\) or even \\(p < 0.000001\\).\nIt may seem odd that there is no universal agreement about “small.” The reason is that p-values are part of a standard operating procedure for evaluating research results to know if they are worthy of publication.\nIn physics, laws and models are meant to be exact or close to exact. Lord Rutherford (1871-1935), an important physicist who won the Nobel prize in 1908, famously disparaged the use of statistics, reportedly saying, “If your experiment needs statistics, you should have done a better experiment.” This was in an era where the p-value standard operating procedure had not yet been invented. Today, when p-values are common in most fields, Rutherford’s distaste for statistical method is reflected in p-value thresholds like \\(p < 0.000001\\).\nIn other fields such as economics or psychology or clinical medicine, models are sought that are useful but without any expectation that they be exact. (In the 19th and early 20th century, psychologists and economists sometimes used the vocabulary of “law” to describe their findings, but “model” is more appropriate, because, unlike physics, the laws are not strictly enforced!) Often, in economics or psychology or medicine, the size of a sample used to train a model is less than, say, \\(n=100\\). And the units of observation—people or countries, for instance—are different one from the other, quite unlike, say, electrons, which are all the same. Consequently, sampling variation is often an important source of noise, obscuring relationships or even suggesting relationships that are not really there. (See Lesson 31.) This situation—small sample size, variation in observational units, and large sampling variation—would cause many useful findings to go unreported, as would happen if \\(p < 0.000001\\) were the standard. So a less stringent threshold for publication is used, most commonly 0.05."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#what-is-a-p-value",
    "href": "Reading-notes-lesson-37.html#what-is-a-p-value",
    "title": "37  Calculating a p-value",
    "section": "What is a p-value?",
    "text": "What is a p-value?\nA p-value is the result of a calculation based on data, but also involving a special hypothesis, called the “Null hypothesis.” The Null hypothesis is almost always a statement in line with the claim that “there is no relationship between these variables” or “nothing is going on.” For example, in a study about the effectiveness of a new drug, the Null hypothesis will be that the drug has no effect at all. Another example: In an economics study about the possible relationship between a country’s “corruption index” and interest rates, the Null hypothesis would be “corruption is unrelated to interest rates.”\nPerhaps it is helpful to envision the Null hypothesis as the belief of a skeptical devil, standing on the researcher’s shoulder and constantly whispering to the research that, “this study is useless, a waste of time, the result purely of sampling variation.” Note that in a world where researchers always took the devil’s advice, no study would be done. What motivates a researcher is a belief that the study will indeed produce results that are useful and represent something about the real world other than sampling variation, e.g. a relationship between two variables.\nThe calculation that results in a p-value is done under the assumption that the devil is right. The point is to see if the data are consistent with the devil’s skeptical position. If they are consistent, then the p-value will be large. If not, the p-value will be “small.”\nThe format of a p-value is that of a conditional probability. The condition is that the devil is right. The probability is that of seeing what the data analysis shows—typically summarized as a model coefficient—if the devil were right.\nActually, the probability reported in the p-value is not that of seeing the exact value of the model coefficient shown in the regression report. The probability also includes the events where the coefficient was larger in magnitude than the coefficient. Why? Because larger coefficients are stronger evidence that the devil is not right."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#the-world-of-the-null-hypothesis",
    "href": "Reading-notes-lesson-37.html#the-world-of-the-null-hypothesis",
    "title": "37  Calculating a p-value",
    "section": "The world of the Null hypothesis",
    "text": "The world of the Null hypothesis\nRecall that the Null hypothesis is the claim that “nothing is going on.” For a regression model, this amounts to saying that “there is no relationship between an explanatory variable and the response variable.” In order to help clarify the description in the previous section, let’s do an example calculation of a p-value. We will use for the example the possible relationship between a car’s fuel economy (mpg) and the maximum horsepower (hp) of the engine.\nA skeptic, such as the imaginary devil from the previous section, might argue this way: “The maximum horsepower is hardly ever used by a car. Instead, the driver throttles the engine so that it generates only that power needed to move the car along under the current conditions: acceleration, speed, wind, slope of the road. The maximum horsepower just affects the range of conditions under which the car can operate. But the fuel economy is based on a standard set of conditions which is the same for every car, regardless of the horsepower.”\nWe will pick up the action at the point where the study has been designed and the design implemented to produce data. For the example, we have the mtcars data frame in hand. As should be familiar at this point, the data are modeled and the model coefficient on the explanatory variable of interest is recorded. Looking at the regression report presented at the beginning of this Lesson, that coefficient is -0.0318 mpg/horsepower.\nThe data were collected in the real world, but that is not the world that’s relevant to the Null hypothesis. The world of the Null hypothesis is one where fuel economy is utterly unrelated to horsepower. To calculate the p-value, we construct a simulation of the Null-hypothesis world. But it is not sufficient for the simulation to generate Null-hypothesis data out of the blue. We want the simulation to be as much like the actual data as possible, except that there is no relationship between mpg and hp.\nPerhaps surprisingly, there is a very simple device for accomplishing this. It involves creating a new variable to use in place of hp in the model, but which is unrelated to mpg. Let’s call this new variable hp_null. We can generate hp_null by taking the values in hp and shuffling them. This randomized version of hp has no relationship to mpg because it is being dealt out to each row of the data frame at random.\nHere’s what the shuffling looks like, pretending for readability that there wee only ten rows in mtcars.\n\nSamp <- mtcars %>% \n  select(mpg, wt, hp) %>%\n  sample_n(size=10) %>%\n  mutate(hp_null = shuffle(hp))\nSamp\n\n                    mpg    wt  hp hp_null\nMerc 450SLC        15.2 3.780 180     230\nPorsche 914-2      26.0 2.140  91      52\nToyota Corolla     33.9 1.835  65     175\nHonda Civic        30.4 1.615  52     175\nCadillac Fleetwood 10.4 5.250 205      91\nPontiac Firebird   19.2 3.845 175     180\nHornet Sportabout  18.7 3.440 175      62\nDatsun 710         22.8 2.320  93      65\nMerc 240D          24.4 3.190  62      93\nChrysler Imperial  14.7 5.345 230     205\n\n\nWe’ll use such data, replacing the actual hp with the shuffled hp, to find the model coefficient on hp. This can be done concisely:\n\nset.seed(112)\nlm(mpg ~ wt + shuffle(hp), data = mtcars) %>%\n  regression_summary()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  34.2      2.93        11.7  1.65e-12\n2 wt           -4.95     0.626       -7.90 1.03e- 8\n3 shuffle(hp)   0.0120   0.00894      1.34 1.90e- 1\n\n\nIn this trial, the coefficient on the shuffled hp is 0.0120. Of course the coefficient might well be different if the trial were repeated. Let’s run 1000 trials, from each of which we’ll extract the coefficient on the shuffled hp.\n\nTrials <- do(1000) * { \n  lm(mpg ~ wt + shuffle(hp), data = mtcars) %>%\n  regression_summary() %>%\n  filter(term == \"shuffle(hp)\") %>%\n  select(estimate)\n}\n\nFigure 37.1 shows the distribution of the shuffled hp coefficient, compared to the coefficient we found from the original, unshuffled data.\n\ngf_jitter(estimate ~ 1, data = Trials, alpha=0.3, width=0.3) %>%\n  gf_violin(color=NA, fill=\"blue\", alpha=0.5, width=0.1) %>%\n  gf_lims(x=c(0,2)) %>%\n  gf_hline(yintercept = ~ -0.03177295, color=\"red\")\n\n\n\n\nFigure 37.1: The sampling distribution of the shuffled hp coefficient\n\n\n\n\nYou can see in Figure 37.1 that the coefficient on the shuffled hp is near zero, as would be expected since we enforced hp_null to be unrelated to mpg. Almost always, the coefficients on hp_null are in the interval \\(\\pm 0.02\\). That is to say, even though hp_null is unrelated to mpg, sampling variation will spread out the estimated coefficient away from the ideal of zero. The amount of spread due to sampling variation is \\(\\pm 0.02\\).\nThe estimated coefficient on hp in the original, unshuffled data is shown as a red horizontal line. You can see that this is farther from zero than any of the null-hypothesis trials. Since there were 1000 trials, the extreme nature of the coefficient from the original data let’s us eyeball the probability of that coefficient (or larger) coming out of the Null hypothesis simulation is something on the order of one-in-a-thousand. A detailed calculation—refer to the regression table at the start of this Lesson—puts that probability at \\(p = 0.0015\\)."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#what-to-conclude",
    "href": "Reading-notes-lesson-37.html#what-to-conclude",
    "title": "37  Calculating a p-value",
    "section": "What to conclude?",
    "text": "What to conclude?\nRemember always that the p-value is a probability calculated in a hypothetical world the world of the Null hypothesis. In the calculation, we are able to place the data in this hypothetical world by shuffling the explanatory variable.\nNo calculation done in the Null hypothesis world is going to tell us whether that hypothesis is correct or not. Nonetheless, that the Null hypothesis simulation did not generate a coefficient as large as that in the actual data suggests that the data are inconsistent with the Null hypothesis, that we can in the case of the hp coefficient regard the Null hypothesis as an implausible candidate to account for the data.\nAlmost always, newcomers to this p-value based scheme of hypothesis testing misinterpret the p-value to be the probability that the Null hypothesis is right. Small p-value would thus mean a small probability that the Null is right.\nBut suppose we want to do a calculation to produce something in the format “the probability that the Null is right?” The probability that decision-makers are usually interested in is the relative conditional probabilities for each of a set of hypotheses of interest. The “condition” under which these probabilities are calculated is, “given the data at hand.” Returning to the notation of Lesson 34, this is \\(p(H | \\text{data})\\), where \\(H\\) stands for each of the hypotheses of interest, say, that a drug has a large effect, a medium effect, no effect at all, or even a negative effect. The framework for calculation is called “Bayesian” statistics, the ideas of which date from the very beginnings of the emergence of statistical method.\nTo illustrate the Bayesian approach, return to Lesson 35 when we were evaluating the performance of classifiers. There, we had two hypotheses that were relevant. In the context of health, these might be \\(H_\\text{sick}\\) and \\(H_\\text{healthy}\\). The quantity of ultimate interest to the patient is p(sick given the test result). To calculate this probability we need to take a round-about route. We first find two completely different probabilities: p(test result given sick) and p(test result given healthy). In practice, these two probabilities are accessible: take a group of sick patients and see what fraction of them have positive tests, and take a different group of people who are healthy and see what fraction of that group have positive tests. With those two probabilities in hand, we take an estimate of the prevalence of sickness: p(sick). Then the probability of clinical interest, p(sick given test result) can be calculated using the Bayesian formula, just as we did in Lesson 35.\nIn contrast, the p-value is a probability in a different format: \\(p(\\text{summary(data)} | H_0)\\). Here, \\(H_0\\), the Null hypothesis, is indeed a specific hypothesis, but not any hypothesis that motivates the research. The quantity “summary(data)” is a particular summary computed from the data, say the sample mean or a regression coefficient.\nThe p-value probability is bound to be confusing on first sight (and, for most people, on second, third, and later sightings). After all, we know exactly what is the “summary(data)”; we just calculated it from the data! The probability of “summary(data)” is therefore 1, at least until you understand what is the event being summarized by the p-value probability.\nFor the p-value, the random event that lies behind the probability is a number generated by a process: Go to the world of the Null hypothesis, that boring world of “nothing happening” or “no relationship between variables.” Figure 37.2 lays out the different worlds involved in statistical inference using the metaphor of planets.\n\n\n\n\n\n\n\n(a) Planet Alt\n\n\n\n\n\n\n\n(b) The real world\n\n\n\n\n\n\n\n(c) Planet Sample\n\n\n\n\n\n\n\n(d) Planet Null\n\n\n\n\nFigure 37.2: The four planets of the statistical solar system.\n\n\nWhat motivates the work of collecting and modeling data is a hypothesis about the world. Typically, such hypotheses are simplistic, cartoon-like ideas about the shape of things.\nNaturally, our ultimate interest is in the real world. But we don’t have the whole Earth at hand; we have only a sample from it. The sample is something like the real world, but being a sample it is somewhat patchy, assembled from the \\(n\\) cases in our sample. Planet Sample lacks the detail of the real world, but each point on Planet Sample comes from a genuine place on Planet Earth.\nThe p-value is a probability computed on Planet Null, that boring world where nothing is going on and any perceived patterns are illusions, the appearance produced by random and shifting gusts of the winds of chance.\nAlmost all the work of calculating a p-value takes place on Planet Null. That work consists of simulation trials. Each trial involves taking a sample from Planet Null, summarizing it, and recording the result for later comparison the the summary calculated from Planet Sample.\nIt may seem perverse to base conclusions for real-world data on an imagined planet of no direct interest. And it is! At a minimum, we should put into competition at least two hypotheses: for instance Planet Alt and Planet Null. But in the world of the first half of the 20th century, when statistical analysis of data was just coming into the mainstream, it was impractical to compute the competing probabilities of the Bayesian style of reasoning. The reason: the computers and algorithms we use now had not been invented.\nIn addition, those early statisticians put a big premium on what they called “objectivity.” They did not think the subjective beliefs of researchers—the cartoon alternative hypothesis—should play any role in data analysis. The method they ended up inventing, p-values, was based only on a hypothesis that everyone could agree might be in play: the Null hypothesis. Unfortunately, the only valid conclusions that can be drawn from p-values are 1) “reject the Null hypothesis” and 2) “fail to reject the Null hypothesis.” These conclusions don’t guide us to favor any other particular hypothesis and so are inadequate to support decision-making in the real world. But the p-value conclusions can be the basis for a standard operating procedure: If the conclusion is “fail to reject the Null hypothesis,” don’t allow the work to be published.\nSo, standard operating procedures were based on the tools at hand. We will return to the mismatch between hypothesis testing and the contemporary world in Lesson 38."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#more-metaphors",
    "href": "Reading-notes-lesson-37.html#more-metaphors",
    "title": "37  Calculating a p-value",
    "section": "More metaphors?",
    "text": "More metaphors?\nUse this from Section 19.4 of Computational Probability and Statistics?\n\nA US court considers two possible claims about a defendant: she is either innocent or guilty. Imagine you are the prosecutor. If we set these claims up in a hypothesis framework, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. Your job as the prosecutor is to use evidence to demonstrate to the jury that the alternative hypothesis is the reasonable conclusion.\n\n\nThe jury considers whether the evidence under the null hypothesis, innocence, is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis).\n\n\nJurors examine the evidence under the assumption of innocence to see whether the evidence is so unlikely that it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person’s innocence. They are simply not convinced of the alternative that the person is guilty.\n\n\nThis is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.\n\n\nThere are two types of mistakes possible in this scenario, letting a guilty person go free and sending an innocent person to jail. The criteria for making the decision, reasonable doubt, establishes the likelihood of those errors.\n\n\nHypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.\n\n\nThere are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized below.\n\n\n\\[\n\\begin{array}{cc|cc} & & \\textbf{Test Conclusion} &\\\\\n& & \\text{do not reject } H_0 &  \\text{reject } H_0 \\text{ in favor of }H_A  \\\\\n\\textbf{Truth} & \\hline H_0 \\text{ true} & \\text{Correct Decision} &  \\text{Type 1 Error}  \\\\\n& H_A \\text{true} & \\text{Type 2 Error} & \\text{Correct Decision}  \\\\\n\\end{array}\n\\]\n\n\nA Type 1 error, also called a false positive, is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination (from the Case Study) and the commercial length studies, it is possible that we made a Type 1 error in one or both of those studies. A Type 2 error, also called a false negative, is failing to reject the null hypothesis when the alternative is actually true. A Type 2 error was not possible in the gender discrimination or commercial length studies because we rejected the null hypothesis.\n\n\nHypothesis tests are very easy to carry out. You don’t need to choose a particular hypothesis to test: it’s always the Null hypothesis. The test has only two possible conclusions: “rejecting the Null hypothesis” or “failing to reject the Null hypothesis.” The conclusion indicated by the test is signalled by a number called a “p-value.”\nIn this Lesson, we will demonstrate a general way to calculate p-values that has a simple logic. Even simpler, model-building software such as lm() will do the calculation for you. Often, there will be more than one p-value, each describing a different aspect of the model, so you have to identify which of the p-values is relevant for your purpose. This lesson will show you how.\nOnce you have the relevant p-value in hand, reaching the conclusion of the test is not hard, but it is confusing that there are different styles that are used in different journals or in different fields.\nWe’ll start by talking about the conclusion, then move on to how to select the relevant p-value, and finish by showing you the logic(s) behind the calculation of the p-value."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#the-conclusion",
    "href": "Reading-notes-lesson-37.html#the-conclusion",
    "title": "37  Calculating a p-value",
    "section": "The conclusion",
    "text": "The conclusion\nRemember that there are only two possible conclusions from a hypothesis test: reject or fail to reject the Null hypothesis. The translation between a p-value and a hypothesis-test conclusion can be stated very simply because the p-value is always a number between zero and one.\n\nA small p-value points toward rejecting the Null hypothesis. A large p-value points to “failing to reject” the Null.\n\nThe definition of “small” differs from one scientific field to another. Physics proudly declares “small” as something like 0.001. In most other fields, “small” is set at the threshold 0.05. Sometimes, as you’ll see in Lesson 38, the threshold for small should be adjusted downward, but for this Lesson we will do what is most common in many fields and use 0.05 as the dividing point between “small” and “not small.”\nCommon sense suggests that the result of a hypothesis test should be to show the p-value. After all, it’s easy to see if a number if greater or less than 0.05. But scientific convention says otherwise in part because “hypothesis testing” is a merger of two different traditions with different styles.\nIn principle, the result of a hypothesis test could be reported very simply either as “reject” or “fail.” Understandably, researchers prefer to use a notation that avoids the negative connotations of the everyday words “reject” and “fail.” Indeed, “reject” is often a very positive conclusion for a researcher. And “fail to reject” in no way means that the researcher did something wrong.\nPossibly the most common notation for the result of a hypothesis test is written \\(p < 0.05\\). Researchers like to show off a small p-value. In order to avoid violating the convention that the number itself is not reported directly, they will hint at the smallness by writing \\(p < 0.01\\) or \\(p < 0.001\\), whichever happens to be justified.\nIn some fields the result of the hypothesis test is reported using the * character. A single * is exactly the same as \\(p < 0.05\\). Vanity being an emotion that scientists share with the rest of us, sometimes ** is used to mean p < 0.01 and *** to mean p < 0.001.\nWhy not just report the p-value itself? One reason is that p-values can be minutely small (e.g. \\(< 10^{12}\\)) just because a lot of data is available.\n\n\n\n\n\n\nAvoiding “fail”\n\n\n\nNobody likes to summarize their work with the word “fail.” And so, when “fail to reject the Null hypothesis” is the correct conclusion, people express this in softer ways.\nIt’s very common for the conclusion “fail to reject the Null” simply not to be reported at all. Historically, and even today, some journals will not accept for publication a scientific article with the conclusion “fail to reject the Null.”\nConsider the situation of a researcher whose years-long project has led to a p-value of 0.07. To soften the blow of “fail to reject,” the researcher will report the p-value itself so that the reader can see how close it is to small. In some literatures, you will see language like “tending to significance” instead of “fail to reject.” In some fields, research publications will show the notation \\(p < 0.1\\). This also indicates failure to reject the null hypothesis.\nJournalists eager to publish reports about scientific work, but facing a p-value that is a little too large, will occasionally qualify their report with this phrase: “… although the work did not reach the rigorous scientific standard for statistical significance.”\nAll of these are dodges. There’s nothing “rigorous” about \\(p < 0.05\\) although seems unfair that a researcher who had a plausible idea and did the work to test it honestly does not get to publish that work and receive acknowledgement that they are a hard-working part of the overall scientific enterprise."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#which-p-value-is-relevant",
    "href": "Reading-notes-lesson-37.html#which-p-value-is-relevant",
    "title": "37  Calculating a p-value",
    "section": "Which p-value is relevant?",
    "text": "Which p-value is relevant?\nThe regression-table summary of a model conveniently presents a p-value for each and every coefficient in the model. For instance,\n\nlm(time ~ climb + distance + sex, data=Hill_racing) %>%\n  regression_summary()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -863.     33.1        -26.0 1.10e-130\n2 climb           2.61    0.0529      49.3 0        \n3 distance      254.      3.37        75.3 0        \n4 sexW          792.     32.9         24.1 8.76e-114\n\n\nEach term in the model has a coefficient, called the “estimate.” Like all estimates, that of the coefficient involves uncertainty due to sampling variation. This uncertainty is reported, as we’ve seen in Lessons 23 and 24, by the standard error. The column “statistic” in the regression table is an intermediate step in computing the final column: the p-value itself.\nThis example of a regression table shows that p-values can sometimes be very, very small. Such smallness is often mis-interpreted as indicating that a very powerful result has been found. This is simply nonsense, which is why the more dignified notation \\(p < 0.05\\) or * is to be preferred.\nWhich of these p-values is relevant depends on what you are interested in. If your focus is on the effect of the vertical climb on the winning time, then you should look at the p-value on the climb coefficient. If you’re interested in whether the race distance is a factor in determining the winning time, use the p-value on distance. The regression report gives p-values for every coefficient because it has no way of determining mathematically which variable is of interest and which variables are “covariates,” that is, not of direct interest.\nIt’s important to note that when looking at model coefficients, a simpler report of the confidence interval on the coefficient carries exactly the same information as the comparison \\(p < 0.05\\). Here’s that report on the hill racing model:\n\nlm(time ~ climb + distance + sex, data=Hill_racing) %>%\n  confint()\n\n                  2.5 %      97.5 %\n(Intercept) -927.815023 -797.864167\nclimb          2.505975    2.713461\ndistance     247.094192  260.316877\nsexW         726.981516  856.056286\n\n\nWhen a confidence interval does not include zero—that’s the case for all of the coefficients here—then the p-value is \\(p < 0.05\\). Most statisticians argue that the confidence interval is a better kind of report, since it also is informative about effect size. For instance, the p-value on climb, reported as 0, is only about the Null hypothesis. But the confidence interval, here [2.5 to 2.7] seconds per meter, gives insight into how much an extra meter of climb will prolong the race.\nReporting p-values rather than confidence intervals on coefficients has no scientific merit. But it can be hard to change traditions. And, as you’ll see in the next section, it’s not only model coefficients that have p-values."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#analysis-of-variance",
    "href": "Reading-notes-lesson-37.html#analysis-of-variance",
    "title": "37  Calculating a p-value",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nSometimes the interest is more general: Do any of these terms contribute to explaining variation in the response variable? In such situations, the appropriate p-value is one that compares one model to another. This style of p-value—not on the individual coefficients but on model terms—comes from a calculation called “analysis of variance.”\n\n\n\n\n\n\nWarning\n\n\n\nFILL THIS IN."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#a-simple-general-purpose-way-to-calculate-a-p-value",
    "href": "Reading-notes-lesson-37.html#a-simple-general-purpose-way-to-calculate-a-p-value",
    "title": "37  Calculating a p-value",
    "section": "A simple, general-purpose way to calculate a p-value",
    "text": "A simple, general-purpose way to calculate a p-value\nSince p-values are always given in the regression table and the ANOVA table, it’s rarely necessary (except in statistics classes!) to calculate one yourself.\nEven so, there is a simple method to calculate a p-value that provides good insight into what a p-value means. We will demonstrate this method so that you can gain that insight, but only in very specialized and esoteric circumstances would anyone use this method in place of reading the p-value from a regression table or ANOVA report.\n\n\n\n\n\n\nWarning\n\n\n\nPut shuffling here!"
  },
  {
    "objectID": "Reading-notes-lesson-37.html#the-hypothesis-testing-zoo",
    "href": "Reading-notes-lesson-37.html#the-hypothesis-testing-zoo",
    "title": "37  Calculating a p-value",
    "section": "The hypothesis testing zoo",
    "text": "The hypothesis testing zoo\nZoos are fun and interesting because of the great diversity of animals on display. The person who, looking in on a huge elephant or a long-legged crane or a chimpanze, drones, “Oh, another animal. That’s all.”\nAs statistics developed, early in the 20th century, distinct tests were developed for different kinds of situations. Each such test was given its own name, for example, a “t-test” or a “chi-squared test.” Honoring this history, statistics textbooks present hypothesis testing as if each test were a new and novel kind of animal.\nIn fact, almost all the different tests named in introductory statistics books are really just different manifestations of regression. Regression is to “animal” the way t-test is to “elephant.” An important theme in the history of statistics is that out of the diversity of statistical methods, almost all of them are encompassed by one method: regression modeling.\nIn these Lessons, we’ve focussed on that one method, rather than introducing all sorts of different formulas and calculations which, in the end, are just special cases of regression. Even so, most people who are taught statistics were never told that all the different methods fit into a single unified framework. Consequently, they use different names for the different methods. To communicate in a world where people learned the old-fashioned names, you have to be able to recognize those names know which regression model they refer to. In the table below, we will use different letters to refer to different kinds of explanatory and response variables.\nx and y - an ordinary quantitative variable\ngroup - a categorical variable with multiple (\\(\\geq 3\\)) levels.\nyesno - a categorical variable with exactly two levels (which can always be encoded as a zero-one quantitative variable)\n\n\n\n\n\n\n\nModel formula\ntraditional name\n\n\n\n\ny ~ 1\nt-test on a single mean\n\n\nyesno ~ 1\np-test on a single proportion.\n\n\ny ~ yesno\nt-test on the difference between two means\n\n\nyesno1 ~ yesno2\np-test on the difference between two proportions\n\n\ny ~ x\nt-test on a slope\n\n\ny ~ group\nANOVA test on the difference among the means of multiple groups\n\n\ny ~ group1 * group2\nTwo-way ANOVA\n\n\ny ~ x * yesno\nt-test on the difference between two slopes. (Note the *, indicating interaction)\n\n\n\nAnother named test, the z-test, is a special kind of t-test where you know the variance of a variable without having to calculate it from data. This situation hardly every arises in practice, and mostly it is used as a soft introduction to the t-test.\n\n\n\n\n\n\nThe chi-squared test\n\n\n\nMost statistics books include two versions of a test invented around 1900 that deals with counts at different levels of a categorical variable. This chi-squared test is genuinely different from regression. And, in theoretical statistics the chi-squared distribution has an important role to play.\nThe chi-squared test of independence could be written, in regression notation, as group1 ~ group2. But regression does not handle the case of a categorical variable with multiple levels.\nHowever, in practice the chi-squared test of independence is very hard to interpret except when one or both of the variables has two levels. This is because there is nothing analogous to model coefficients or effect size that comes from the chi-squared test.\nThe tendency in research, even when group1 has more than two levels, is to combine groups to produce a yesno variable. Chi-squared can be used with the response variable being yesno and almost all textbook examples are of this nature.\nBut for a yesno response variable, a superior, more flexible and more informative method is logistic regression.\n\n\nANOVA, which is always a comparison of two models, say y~1 versus y~group involves something called an F-test. For the simpler setting of the t-test, the model y~yesno, an F-test can also be done. Which to do, t or F? It turns out that t2 is exactly the same as F.\nStatistics textbooks usually include several different settings for “hypothesis tests.” I’ve just pulled a best-selling book off my shelf and find listed the following tests spread across eight chapters occupying about 250 pages.\n\nhypothesis test on a single proportion\nhypothesis test on the mean of a variable\nhypothesis test on the difference in mean between two groups (with 3 test varieties in this category)\nhypothesis test on the paired difference (meaning, for example, measurements made both before and after)\nhypothesis test on counts of a single categorical variable\nhypothesis test on independence between two categorical variables\nhypothesis test on the slope of a regression line\nhypothesis test on differences among several groups\nhypothesis test on R2"
  },
  {
    "objectID": "Reading-notes-lesson-38.html#interpreting-a-p-value",
    "href": "Reading-notes-lesson-38.html#interpreting-a-p-value",
    "title": "38  False discovery",
    "section": "Interpreting a p-value",
    "text": "Interpreting a p-value\nIf the simulation trials rarely or never produce an association as strong as that found in real-world observations, you can fairly conclude that the assumptions embedded in the simulation ought to be rejected as a proper description of the real world. On the other hand, if the simulation shows good accord with the real-world observations, you can … what? It’s not a good idea to claim the simulation is a correct description of the real world; it’s just a simulation. Instead, the proper statement is that you “fail to reject” the hypothesis hard-coded into the simulation."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#a-standard-operating-procedure",
    "href": "Reading-notes-lesson-38.html#a-standard-operating-procedure",
    "title": "38  False discovery",
    "section": "A standard operating procedure",
    "text": "A standard operating procedure"
  },
  {
    "objectID": "Reading-notes-lesson-38.html#what-people-want-to-know",
    "href": "Reading-notes-lesson-38.html#what-people-want-to-know",
    "title": "38  False discovery",
    "section": "What people want to know",
    "text": "What people want to know\nAnother major reason why statistical hypothesis testing can be difficult to get your head around is that many people have an intuitive idea about what they want to know when testing a hypothesis: whether the stated hypothesis is likely to be true. But statistical hypothesis is expressly designed to avoid making any statement about the probability that a hypothesis might be true or false. There is good reason for this since “truth” is a shakey concept philosophically.\nConsider this example of a hypothesis: “Drug X lowers blood pressure.” People being so different one from another, a hypothesis like this would not be refuted just because X raised the blood pressure of a person. A better statement might be, “Drug X typically lowers blood pressure.” Still better would be a more definite statement, “Drug X typically lowers blood pressure by around 10 mmHg.” Whether such a statement is true or not depends on the meaning of “typically” and “by around.”\nRather than looking for the truth or falsity of a hypotheses, statistical thinkers focus on a question in this form, “Is the statement that ‘Drug X typically lowers blood pressure by around 10 mmHg’ consistent with the observed facts?” Suppose, to illustrate, that the facts are the recorded change in blood pressure in 10 patients given drug X.\nConsider these measurements of change in blood pressure before and after administration of the drug: -5, -1, +7, -15, -3, -6, +1, -8, +2, 0\nWe’re seeing a reduction (a negative number) in most of the patients. The numbers are near -10, even if they are not exactly -10 all the time. When there’s an increase in blood pressure, it’s small.\nIn contrast, suppose the numbers were 5, 1, -7, 15, 3, 6, -1, 8, -2, 0. These number are inconsistent with the claim that the typical change is -10. Most of them are positive, sometimes by a lot. Of the negative numbers, none of them even reaches -10.\nIt would be better to have a quantitative way to measure “consistency with the observed facts.” Two changes to the way we frame hypotheses will help.\n\nBe more specific about “typical” and “by around.” For instance, here’s a very definite hypothesis: “In a group of patients with such-and-such condition, drug X lowers blood pressure by an average of -10 mmHg with a standard deviation of 7mmHg.”\n\n\nThere is, I think, a helpful analogy to be made between hypothesis testing and the familiar ways that we try to avoid information overload on the Internet.\n“Internet protocols” organize communication into standard format “packets” that are easily and rapidly transmitted, routed, and received. These packets make possible the vast web of connections that is the Internet. Anyone can put any digital content they like inside a packets; the protocols are neutral in this regard. The Internet protocols were not designed to determine what content is worth transmitting and what is worth receiving. We rely on other systems for that, mostly at the receiving end. There are spam filters to avoid email accounts being flooded with worthless or harmful messages. There are recommender systems that compare your history of music or movie streaming to that of others in order to identify what new content you might like. Search engines look inside web pages to identify connections and rank highly those pages that are linked to by other highly ranked pages. These systems leave creators free to follow their interests, ideas, and imaginations, while providing a little guidance to people who want to access some content but avoid being overcrowded by other content that is not worthwhile.\nHistorically, there were earlier waves of technology that increased the ability to communicate. Printing and postal systems emerged in the 13th and following systems. Before those innovations, communication was outrageously expensive, requiring hand-copying of manuscripts, couriers, and camel trains. Content was controlled to some extent by authorities: government censorship; church “indices” and spritual authorities; and often the authorities of those famous classical philosophers and poets whose work and thought was promulgated by early universities.\nAbout four centuries ago, such authorities were being challenged. It slowly became accepted to make judgements based on observations and to disregard antique authorities. Enlightened “scientists” communicated their discoveries in hand-written letters to one another.1 In the late 1600s, another, possibly more efficient means of communication was developed: scientific societies where members met and read aloud their work to an audience, and the journals of such societies which enabled mass communication to those scientists distant from the society’s meetings in time or space.\nEarly scientific journals are delightful collations on diverse and miscellaneous subjects. Everything seems to have been of interest to everyone. Publication was regulated by the recommendations of “members” of the society; new members were admitted by the consensus of earlier members.\nOver the centuries, the growth of scientific content and the specialization of methodology called for research findings to be sorted by area. But there was still need to regulate publication, to avoid distracting readers with worthless information.\nHand-in-hand with the scientific revolution’s reliance on observation and data rather than authority came the need to standardize methods for summarizing data. This might be called a “statistical protocol” by analogy to Internet protocols, but there is no wise governing body, only consensus and “accepted practice.”\nThe data from a bench-top experiment might consist of, say, six numbers: three from the treatment and three from controls. The arithmetic means of these two groups is practically certain to be non-zero, even if the the treatment had no effect. This meant that a means was needed to establish when the difference in means was large enough to suggest the two groups might be genuinely different and that the treatment did have an effect. The statistical protocol to decide such things needed to be simple: computers weren’t available and there were no courses to teach statistical method until the 1960s. In the 1930s, prominent statistical pioneer Ronald Fisher published a slim volume, Statistical Methods for Research Workers which laid out methods for managing and standardizing the calculations. Fisher’s authority was substantial but not absolute. Differing philosophical views also came to influence “accepted practice.”\nEarly statistics books and courses codified “accepted practice.” What emerged is the system of calculations that we call “hypothesis testing” and the ubiquitous p-value. Still, this was rooted in the need to avoid journals wasting library-shelf-space and reader time with experiments that produced arithmetic differences between groups that were accidental and not genuinely “significant.’\nThe now-codified accepted practice was in many ways similar to the protocols used by search engines and social media to direct our eyes and ears to content that might, possibly, be worthwhile. These systems are far from perfect, sometimes hiding good content or promoting worthless content. And, of course, the worth of content is a matter of personal interests and values, something that computer algorithms can mimic only imperfectly.\n“Hypothesis testing” is an ad hoc set of not always consistent concepts cobbled together by a unorganized community of independent researchers, steered perhaps by the perceived authority of one statistical celebrity or another. It is not a mathematically derived, highly optimized calculation of objective worth, just a simple means to deal with the fact that arithmetic differences are influenced by sampling variation and noise, and that a detected difference might not reliably point to a genuine difference between groups.\nIt is simply not possible to understand hypothesis testing in the same way you can understand differentiation or data wrangling.\nHypothesis testing emerged in an era of bench-top and agricultural experiments conducted by a small community of self-identified scientists working without central control. It might have been a practicable solution to the problem of information overload in that era of small data. But the protocol has been frozen in place by textbooks; each generation passing it along to the next as received wisdom, in much the same way as the views of classical philosophers and poets were passed down to later generations as authoritative and unchallengeable.\nSo lets step back from this frozen statistical protocol of hypothesis testing and point out inconsistencies and peculiarities that make it hard to make sense of and perhaps unsuited to the needs of handling information overload in todays world of big data and huge scientific enterprise.\nNULL versus Alternative hypothesis\nUse a threshold\nDon’t print the p-value\np-value is an inseparable tangle of the amount of data available and the effect size. With enough data, practically everything has a small p-value.\nHundreds of thousands (perhaps millions) of scientists churning out research results. A filter that eliminates 95% of the nonsense still lets through an unfathomable mass of content.\nSo many choices in research and analysis methods—which covariates to include, whether to exclude an inconvenient point as an outlier, multiple choices for the response variable, all combined with a professional priority to “publish or perish.”\n\n\n\n\n\n\nNote\n\n\n\nA short introduction to power and the alternative hypothesis.\nUse DAGs and ask for the p-value for different sample sizes.\n\n\n\n\nAsk, and it shall be given you; seek, and ye shall find; knock, and it shall be opened unto you: For every one that asketh receiveth; and he that seeketh findeth; and to him that knocketh it shall be opened. – Matthew 7:7-8\n\nThe modeling techniques we’ve covered are surprisingly powerful at identifying patterns in data. With power comes responsibility. This chapter is about how spurious patterns can arise in data and processes you can use to help ensure that the patterns your models identify are genuine.\nIt’s well known that people are particularly adept at finding patterns. To see this, spend a minute or two with Figure 38.1, which shows x-y pairs generated by a complex mathematical procedure called the Mersenne-Twister algorithm. How many of the structures created by Mersenne-Twister algorithm can you identify by eye? Take five of the stronger-looking patterns: clusters of points, large empty areas, strings of dots, etc. Write down a list of the patterns you spotted, including the coordinate location of each, a short description (e.g. “arc of dots”), and your subjective sense of how strong or convincing that pattern is.\n\n\n\n\n\nFigure 38.1: Training data (n = 1000)\n\n\n\n\nWith your list in hand, look at Figure 38.2 at the end of this section, which displays another n = 1000 x-y pairs generated by the same mathematical procedure. You’re going to check which of the patterns you found in the testing data are confirmed by the training data. Go through your list, looking at each location where you found a pattern in the training data and checking whether a similar pattern appears at that location in the testing data.\nWere any of the patterns you saw in the training data confirmed by the testing data?\nThere’s no denying that the patterns you saw were in the data. But the Mersenne-Twister algorithm is specifically designed not to produce regular patterns. Any that you saw were accidental alignments in the particular sample of data from the algorithm.\nThe “patterns” abstractly referred to in the previous paragraphs appear in data. In data used for modeling, a pattern might be a relationship or correlation between two or more variables, or a cluster of rows in a data frame that have similar values for a response variable and explanatory variables.\nTraining models on data can encode the underlying patterns. For instance, a pattern in the data might result in a model generating detailed predictions or demonstrating a strong effect size of one variable on another.\nA valid pattern is one that steadily appears from one sample of data to another (so long as the sample is big enough). Such consistency suggests that the pattern reflects some genuine aspect of the system generating the data. A false or accidental pattern is one that appears in a sample of data, but is unlikely to show up in another sample. This inconsistency indicates that conclusions based on this pattern are unlikely to be applicable in the future or in new situations.\nThe obvious, direct way to check the validity of a pattern encoded by a model is to see if the same pattern occurs in new data, data that was not used in building the initial model. Lesson 22 took this approach by constructing a sampling distribution of a statistic such as an effect size. To create a sampling distribution, we train many models on different subsets of a data set.\nWhen working with prediction models, the sign of a valid pattern is that the quality of the predictions – perhaps measured with a root-mean-square-error or a sensitivity/specificity – remains consistent when we calculate it on new data. A prediction that shows very small error on the data used to train the model but large error on new data is not a prediction that we can rely on in new settings.\nThe historical rapid growth in data analysis activity and the construction of data sets with large numbers of explanatory variables has made it easier to capture with models both valid patterns and false patterns. This makes it important to recognize that the false detection of patterns is possible whenever you train a model, to be aware of the characteristics of models and data that make false detection more likely, and to adopt procedures to mitigate the risk that the results of your work may not generalize beyond the particular sample of data you have in hand.\n\n\n\n\n\nFigure 38.2: Testing data (n = 1000)."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#example-falsely-discovering-purchasing-habits",
    "href": "Reading-notes-lesson-38.html#example-falsely-discovering-purchasing-habits",
    "title": "38  False discovery",
    "section": "Example: Falsely discovering purchasing habits",
    "text": "Example: Falsely discovering purchasing habits\nYou are a data scientist for an internet retailer, Potomac.com, which has just bought a national grocery chain, Austin Foods. You’re part of the team that is connecting the customer loyalty card data from Austin Foods with Potomac’s own large record of purchases. This is accomplished by offering a 10% Xdiscount for an item on Potomac to people who enter their Austin loyalty card number.\nPotomac’s management wants to create a cross-marketing program in which a customer shopping at Potomac will be offered coupons for Austin products. The hope is that the coupon discount will attract new customers to start shopping at Austin’s. In order for this to work, it’s best if the coupons are for products that the customer finds attractive.\nYour job is to build the coupon assignment system, that is, to figure out how to choose which products a customer is most likely to find attractive. To do this, you’ll create a set of classifiers that indicates the interest of a Potomac customer in an Austin product.\nYou’ve got data on 10,000 Potomac/Austin customers, that is, people whose records from Potomac and from Austin you can bring together. There are ten popular Austin products for which coupons can be offered. Among the 10,000 customers, about 16% have actually bought any given Austin product. You have built ten classifiers, one for each of the ten products. The input to the classifiers is 100 standard measures of a customer’s Potomac activity. The output of each classifier is the probability that the customer actually bought the corresponding Austin product.\nThe no-input classifier gives a probability of about 16% that the customer will buy the product. Management hopes that you will be able to segment the market to identify the products that a given person is much more likely to buy.\nIt’s a lot to ask of a person to sort through 100 potential explanatory variables to identify those that are predictive of buying a product. But it’s straightforward to use a model family that can learn on its own which variables are informative. You train the ten classifiers using a tree family of models.\n\nHeads up! The “data” has been created using random numbers, so that there are no actual relationships between the explanatory variables and the purchase outcomes. That is, no actual relationships aside from the accidental ones, such as the patterns encountered in Figure 38.1.\n\nTo illustrate how the coupon assignment system works, Table @ref(tab:some-results) shows an intermediate step in the calculation, where a probability for each of the ten products is calculated for each customer.\n\n\n\n\n\n\nTable @ref(tab:some-results) shows the output of the classifiers for just the first fifteen customers out of the 10,000 used to build the coupon selection system. For each person, all ten classifiers have been applied to estimate the probability that the person would buy each of the ten products. Highlighted in green are those products with a purchase probability greater than 40%.\nThe final output of the coupon assignment system is, for each customer, the identification of the specific products for which the probability is large. Reading Table @ref(tab:some-results), you’ll see that for person 1, product 9 merits a coupon. For person 2, products 2 and 10 merit a coupon. A winning product has not been identified for every customer, but you can’t please everyone.\n\n\n\nAttaching package: 'formattable'\n\n\nThe following objects are masked from 'package:scales':\n\n    comma, percent, scientific\n\n\n\n\n(ref:some-results-cap)\n \n\n\nCustomer ID\n\n  \n    product \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n  \n \n\n  \n    1 \n    11 \n    71 \n    7 \n    9 \n    63 \n    14 \n    9 \n    8 \n    0 \n    13 \n    7 \n    11 \n    9 \n    4 \n    11 \n  \n  \n    2 \n    6 \n    14 \n    20 \n    6 \n    14 \n    14 \n    14 \n    20 \n    14 \n    14 \n    14 \n    14 \n    14 \n    14 \n    14 \n  \n  \n    3 \n    15 \n    11 \n    11 \n    6 \n    15 \n    15 \n    12 \n    11 \n    11 \n    15 \n    10 \n    9 \n    8 \n    8 \n    5 \n  \n  \n    4 \n    6 \n    14 \n    11 \n    8 \n    9 \n    11 \n    6 \n    9 \n    13 \n    13 \n    13 \n    13 \n    6 \n    10 \n    11 \n  \n  \n    5 \n    11 \n    11 \n    11 \n    11 \n    11 \n    0 \n    13 \n    92 \n    7 \n    11 \n    13 \n    13 \n    43 \n    8 \n    9 \n  \n  \n    6 \n    13 \n    13 \n    13 \n    7 \n    9 \n    13 \n    10 \n    12 \n    10 \n    13 \n    13 \n    13 \n    10 \n    7 \n    13 \n  \n  \n    7 \n    15 \n    12 \n    6 \n    9 \n    9 \n    15 \n    11 \n    75 \n    9 \n    11 \n    9 \n    11 \n    4 \n    9 \n    16 \n  \n  \n    8 \n    30 \n    10 \n    10 \n    8 \n    6 \n    10 \n    12 \n    7 \n    6 \n    78 \n    86 \n    6 \n    6 \n    13 \n    8 \n  \n  \n    9 \n    67 \n    14 \n    8 \n    9 \n    10 \n    10 \n    10 \n    75 \n    11 \n    9 \n    0 \n    11 \n    15 \n    2 \n    9 \n  \n  \n    10 \n    19 \n    46 \n    6 \n    9 \n    10 \n    7 \n    42 \n    9 \n    10 \n    6 \n    16 \n    6 \n    9 \n    16 \n    14 \n  \n\n\n\n\n\n(ref:some-results-cap) The output of the ten classifiers for the first 15 customers. Green highlighting is used for those products which a given customer is likely to buy.\n\n\n\nTo test the performance of the system, we can look at the product/customer combinations for which a coupon was merited, and check how many of them actually corresponded to a purchase: it’s 74%. But for the combinations with no coupon, the purchase rate was only 11%.\nThe results are impressive. For about half of the customers, the coupon assignment system has identified customers/product combinations with a purchase probability of more than 40%. Often, the probability of purchase is considerably higher than 40%. Targeting each customer with a coupon for the right product is likely to generate a lot of new sales!\n\nSince data was generated using random numbers, we know that the “success” of the coupon assignment system is illusory. Later, we’ll see how the process was able to uncover so many accidental patterns from random data and list some things to look out for when modeling. But first, let’s provide a reliable method for you to identify when your results are based in accidental patterns: using testing data.\n\nA true measure of the performance of a model should be based not on the data on which the model was trained, but data which have been held back for use in testing and not used in training. For this example, we’ll use testing data consisting of 10,000 customers for whom we have the same 100 explanatory variables from the Potomac database and for whom we know if each customer purchased any of the ten products from Austin Foods. Only about 1 in 6 customers bought any single product from Austin. We want to see if the classifier assigns a high probability to those customers who did buy the product. If so, it means we can use just the 100 explanatory variables to find winning products for customers for whom we have no Austin purchasing data.\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\n\n\n(ref:purchase-test-cap)\n \n\n\nCustomer ID\n\n  \n    product \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n  \n \n\n  \n    1 \n    17 \n    8 \n    75 \n    16 \n    62 \n    15 \n    2 \n    11 \n    8 \n    24 \n    7 \n    15 \n    7 \n    13 \n    10 \n  \n  \n    2 \n    14 \n    20 \n    14 \n    14 \n    14 \n    14 \n    14 \n    6 \n    14 \n    13 \n    6 \n    14 \n    14 \n    13 \n    14 \n  \n  \n    3 \n    14 \n    22 \n    6 \n    8 \n    14 \n    10 \n    14 \n    13 \n    63 \n    11 \n    9 \n    11 \n    17 \n    13 \n    15 \n  \n  \n    4 \n    13 \n    13 \n    13 \n    13 \n    13 \n    13 \n    6 \n    9 \n    0 \n    13 \n    13 \n    13 \n    12 \n    0 \n    62 \n  \n  \n    5 \n    9 \n    13 \n    9 \n    85 \n    11 \n    11 \n    59 \n    9 \n    12 \n    7 \n    11 \n    11 \n    11 \n    13 \n    13 \n  \n  \n    6 \n    13 \n    13 \n    13 \n    14 \n    7 \n    7 \n    12 \n    9 \n    13 \n    11 \n    80 \n    13 \n    7 \n    7 \n    13 \n  \n  \n    7 \n    15 \n    11 \n    12 \n    6 \n    9 \n    0 \n    9 \n    15 \n    5 \n    7 \n    33 \n    12 \n    9 \n    12 \n    15 \n  \n  \n    8 \n    7 \n    13 \n    6 \n    7 \n    10 \n    6 \n    13 \n    14 \n    10 \n    100 \n    9 \n    8 \n    10 \n    13 \n    6 \n  \n  \n    9 \n    9 \n    12 \n    75 \n    58 \n    8 \n    11 \n    11 \n    9 \n    10 \n    8 \n    60 \n    11 \n    10 \n    10 \n    12 \n  \n  \n    10 \n    2 \n    10 \n    20 \n    2 \n    16 \n    12 \n    7 \n    75 \n    15 \n    16 \n    83 \n    15 \n    16 \n    56 \n    14 \n  \n\n\n\n\n\n(ref:purchase-test-cap) Similar to Table @ref(tab:some-results) but for the testing data.\n\n\n\nA valid evaluation of the performance of the system involves using the testing data rather than the training data. Figure @ref(fig:purchase-test) shows the assignment of coupons for the customers in the test data. Although coupons are assigned to these customers, the purchase rate for these items is only 16%, no different than the probability of purchase for no-coupon items. In other words, the coupon assignment system doesn’t work at all!"
  },
  {
    "objectID": "Reading-notes-lesson-38.html#sources-of-false-discovery",
    "href": "Reading-notes-lesson-38.html#sources-of-false-discovery",
    "title": "38  False discovery",
    "section": "Sources of false discovery",
    "text": "Sources of false discovery\nHow did the coupon classifier system identify so many accidental patterns, patterns that existed in the training data but not in the testing data?\nOne source of false discovery stems from having multiple potential response variables. In the Potomac/Austin example, there were ten different classifiers at work, one for each of the ten Austin products. Even if the probability of finding an accidental pattern in one classifier is small, looking in ten different places dramatically increases the odds of finding something.\nSimilarly, having a large number of explanatory variables – we had 100 in the coupon classifier – provides many opportunities for false discovery. The probability of an accidental pattern between one outcome and one explanatory variable is small, but with many explanatory variables each being considered it’s much more likely to find something.\nA third source of false discovery at work in the coupon classifier relates to the family of models selected to implement the classifier. We used a tree model classifier capable of searching through the (many) explanatory variables to find ones that are associated with the response outcome. Unbridled, the tree model is capable of very fine stratification. Each coupon classifiers stratified the customers into about 200 levels. On average, then, there were about 50 customers in each strata. But there is variation, so many of the strata are much smaller, with ten or fewer customers. The small groups were constructed by the tree-building algorithm to have similar outcomes among the members, so it’s not surprising to see a very strong pattern in each group. For each classifier, about 15% of all customers fall into a strata with 20 or fewer customers.\nTo illustrate, Figure 38.3 shows the shape of the tree model for a typical coupon classifier. Each of the splits reflects an accidental alignment of the response variable with one of the explanatory variables. As more splits are made, the group of customers contained in the split becomes smaller. Many of the leaves on the tree contain just a handful of customers who accidentally had similar values for the several explanatory variables used in the splits.\n\n\n\n\n\nFigure 38.3: A sketch of one of the classifiers constructed for the coupon selection system. The tree-growing algorithm was allowed to keep going until the customer data was split up into very small strata.\n\n\n\n\nThe tree is too complex to be plausible as a real-world mechanism. None of the details in Figure 38.3 have any validity beyond the training data itself."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#identifying-false-discovery",
    "href": "Reading-notes-lesson-38.html#identifying-false-discovery",
    "title": "38  False discovery",
    "section": "Identifying false discovery",
    "text": "Identifying false discovery\nWe use data to build statistical models and systems such as the coupon-assignment machine. False discovery occurs when a pattern or model performance seen with one set of data does not generalize to other potential data sets.\nThe basic technique to avoid false discovery is called cross validation. One simple approach to cross validation splits the data frame into two randomly selected non-overlapping sets of rows: one for training and the other for testing. Use the training data to build the system. Use the testing data to evaluate the system’s performance.\nMost often, cross validation is used to test model prediction performance such as the root-mean-square error or the sensitivity and specificity of a classifier. This can be accomplished by taking the trained model and providing as input the explanatory variables from the testing data, then comparing the model output to the actual response variable values in the testing data. Note that using testing data in this way does not involve retraining the model on the testing data.\nHow big should the training set be compared to the testing set? For now, we’ll keep things simple and encourage use of a 50:50 split or something very close to that.\nThis is a simple and reliable approach that should always be used."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#false-discovery-and-multiple-testing",
    "href": "Reading-notes-lesson-38.html#false-discovery-and-multiple-testing",
    "title": "38  False discovery",
    "section": "False discovery and multiple testing",
    "text": "False discovery and multiple testing\nWhen the main interest is in an effect size, standard procedure calls for calculating a confidence interval on the effect. For example, a 2008 study examined the possible relationship between a woman’s diet before conception and the sex of the conceived child. The popular press was particularly taken by this result from the study:\n\nWomen producing male infants consumed more breakfast cereal than those with female infants. The odds ratio for a male infant was 1.87 (95% CI 1.31, 2.65) for women who consumed at least one bowl of breakfast cereal daily compared with those who ate less than or equal to one bowlful per week. (fetal-sex-2008?)\n\nThe model here is a classifier of the sex of the baby based on the amount of breakfast cereal eaten. The effect size tells the change in the odds of a male when the explanatory variable changes from one bowlful of cereal per week to one bowl per day (or more). This effect size is sensibly reported as a ratio of the two odds. A ratio bigger than one means that boys are more likely outcomes for the one-bowl-a-day potential mother than the one-bowl-a-week potential mother. The 95% confidence interval is given as 1.31 to 2.65. This confidence interval does not contain 1. In a conventional interpretation, this provides compelling evidence that the relationship between cereal consumption and sex is not a false pattern.\nBut the confidence interval is not the complete story. The authors are clear in stating their methodology: “Data of the 133 food items from our food frequency questionnaire were analysed, and we also performed additional analyses using broader food groups.” In other words, the authors had available more than 133 potential explanatory variables. For each of these explanatory variables, the study’s authors constructed a confidence interval on the odds ratio. Most of the confidence intervals included 1, providing no compelling evidence of a relationship between that food item and the sex of the conceived child. As it happens, breakfast cereal produced the confidence interval that was the most distant from an odds ratio of 1.\nLet’s look at the range of confidence intervals that can be found from studying 100 potential random variables that are each unrelated to the response variable. We’ll simulate a response randomly generated “sex” G and B where the odds of G is 1. Similarly, each explanatory variable will be a randomly generated “consumption” high or low where the odds of high is 1. A simple stratification of sex by consumption will generate the odds of G for those cases with consumption Y and also the odds of G for those cases with consumption N. Taking the ratio of these odds gives, naturally enough, the odds ratio. We can also calculate from the stratified data a 95% confidence interval on the odds ratio.\nSo that the results will be somewhat comparable to the results in (fetal-sex-2008?), we’ll use a similar sample size, that is, n = 740. Table @ref(tab:sex-consumption-1) shows one trial of the simulation.\n\n\n\n\n\n\n(ref:sex-consumption-1-cap)\n\n\n\n\n\n\nhigh\n\n\nlow\n\n\n\n\n\n\nB\n\n\n165\n\n\n182\n\n\n\n\nG\n\n\n211\n\n\n182\n\n\n\n\n\n(ref:sex-consumption-1-cap) A stratification of sex outcome (B or G) on consumption (high or low) for one trial of the simulation described in the text.\nReferring to Table @ref(tab:sex-consumption-1), you can see that the odds of G when consumption is low is 182 / 182 = 1. The odds of G when consumption is high is 211/165 = 1.28. The 95% confidence interval on the odds ratio can be calculated. It is 0.95 to 1.73. Since that includes 1, the data underlying Table @ref(tab:sex-consumption-1) provide little or no evidence for a relationship between sex and consumption. This is exactly what we expect, since the simulation involves entirely random data.\nFigure 38.4 shows the 95% confidence interval on the odds ratio for 133 trials like that in Table @ref(tab:sex-consumption-1). The confidence interval from each trial is shown as a horizontal line. The large majority of them include 1. That’s to be expected because the data have been generated so that sex and consumption have no relationship except those arising by chance.\n\n\nWarning: geom_vline(): Ignoring `mapping` because `xintercept` was provided.\n\n\n\n\n\nFigure 38.4: Confidence intervals on the odds ratio comparing female and male birth rates for many trials of simulated data with no genuine relationship between the explanatory and response variables.\n\n\n\n\nNonetheless, out of 133 simulations there are six where the confidence interval does not include 1. These are shown in red. By necessity, one of the intervals will be the most extreme. If instead of numbering the simulations, we had labelled them with food items – e.g. grapefruit, breakfast cereal, toast – we would have a situation very similar to what seems to have happened in the sex-vs-food study. (For a more detailed analysis of the impact of multiple testing in (fetal-sex-2008?), see (young-2009?).)\nSuppose now that half of the data used in (fetal-sex-2008?) had been held back as testing data. Using the training data, it would be an entirely legitimate practice to generate hypotheses about which specific food items might be related to the sex of the baby. The validity of any one selected hypothesis could then be established using the testing data without the ambiguity introduced by multiple testing. The testing data confidence interval can be taken at face value; the training data confidence interval cannot."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#example-organic-discovery",
    "href": "Reading-notes-lesson-38.html#example-organic-discovery",
    "title": "38  False discovery",
    "section": "Example: Organic discovery?",
    "text": "Example: Organic discovery?\nIt’s easy to find organic foods in many large grocery stores. Advocates of an organic diet are attracted by a view that it is sustainable, promotes small farms, and helps avoid contact with pesticides. There are also nay-sayers who make valid points, but that is not our purpose here. Informally, I find that many people and news reports point to the health benefits of an organic diet. Usually they believe that these benefits are an established fact.\nA 2018 New York Times article observed:\n\nPeople who buy organic food are usually convinced it’s better for their health, and they’re willing to pay dearly for it. But until now, evidence of the benefits of eating organic has been lacking. (NYT-2018-10-23-Rabin?)\n\nThe new evidence of health benefits is reported in an article in the Journal of the American Medical Association: Internal Medicine (baudry-2018?)\nDescribing the findings of the research, the Times article continued:\n\nEven after these adjustments [for covariates], the most frequent consumers of organic food had 76 percent fewer lymphomas, with 86 percent fewer non-Hodgkin’s lymphomas, and a 34 percent reduction in breast cancers that develop after menopause.\n\nThe study warrants being taken seriously: it involved about 70,000 French adults among whom 1340 cancers were noted. The summary of organic foot consumption was a scale from 0 to 32 and included 16 labeled products including dairy, meat and fish, eggs, coffee and tea, wine, vegetable oils, and sweets such as chocolate. Adjustment was made for a substantial number of covariates: age, sex, educational level, marital status, income, physical activity, smoking, alcohol intake, family history of cancer, body mass index, hormonal treatment for menopause, and others.\nYet … the reseach displays many of the features that can lead to false discovery. For instance, results were reported for four different types of cancer: breast, prostate, skin, lymphomas. The study reports p-values and hazard ratios2 comparing cancer rates among the four quartiles of the organic consumption index.\nComparing the most organic (average organic index 19.36/32) and the least organic (average index 0.72/32) groups the 95% confidence interval on the relative risk and p-values given in the study’s Table 4 are:\n\nBreast cancer: 0.66 - 1.16 (p = 0.38)\nProstate cancer: 0.61- 1.73 (p = 0.39)\nSkin cancer: 0.49 - 1.28 (p = 0.11)\nLymphomas: 0.07 - 0.69 (p = 0.05)\n\nYou might be surprised to see that the confidence interval on the relative risk for breast cancer includes 1.0, which suggests no evidence for an effect. As clearly stated in the report, the risk reduction for breast cancer is seen only in a subgroup of study participants: those who are postmenopausal. And even then, the confidence intervals continue to include 1.0:\n\nBreast cancer pre-menopausal: 0.67 - 1.52 (p = 0.85)\nBreast cancer post-menopausal: 0.53 - 1.18 (p = 0.18)\n\nSo where is the claimed 34% reduction in breast cancer cited in the New York Times article. It turns out the the study used two different indices of organic food consumption. The 0 to 32 scale which includes many items for which the amount consumed is very small (e.g., coffee, chocolate) and a “simplified, plant derived organic food score.” It’s only when you look at the full 0 to 32 scale that you see the reduction in post-menopausal breast cancer: the confidence interval is 0.45 to 0.96 (p = 0.03).\nWhat about cancer rates overall? For the 0 to 32 scale the risk ratio was 0.58 - 1.01 (p = 0.10). To see the claimed reduction clearly you need to look at the simplified food score which gives 0.63 - 0.89 (p < 0.005). And it’s only in comparing the highest-index quarter of participants with the lowerest quarter participants that any difference at all is seen in any type of cancer: the middle-half of participants show no difference in relative risk from the lowest-organic quarter of participants. (Because of this, had the study compared the highest quarter to the next highest quarter, they would have seen basically the same relative risks reported in the highest-to-lowest quarter comparison. Then the conclusion would have had a different flavor, perhaps to be reported as “Typical organic food consumption levels show no cancer benefits.”)\nA further source of potential false discovery stems from the study’s starting and stop times. It’s not clear that these were pre-defined; the reported results are intermediate to a longer follow up. The choice to report intermediate results is another way that the number of opportunities for false discovery is increased. And the choice is important: for the follow-up time used, about 2% of the participants developed cancer. In an earlier study of more than 600,000 middle-aged UK women (average age 59), the incidence of cancer was four times larger: 8.6%. (bradbury-2014?) That study did not find any relationship between organic food consumption and overall cancer rates, and found no relationship for 15 out of 16 different types of cancer. The exception is extremely interesting: non-Hodgkin lymphoma for which a similar result was found in the French study.\nSo is the study reported in the New York Times a matter of false discovery? It’s emotionally unsatisfying to discount a result about organic food and non-Hodgkin lymphoma simply because it’s part of a larger study that looked at many different combinations of cancer types and organic food indices. What if the researchers had only studied non-Hodgkin lymphoma – they would have gotten the same result and it wouldn’t have the deficiencies of being the strongest result of many possibilities. It would have stood on its own. But it doesn’t and we are left in a state of doubt."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#p_values",
    "href": "Reading-notes-lesson-38.html#p_values",
    "title": "38  False discovery",
    "section": "p-values and “significance”",
    "text": "p-values and “significance”\nFalse discovery is not a new problem. The traditional logic can be traced back to 1710, when John Arbuthnot was examining London birth records from 1629 to 1710. Arbuthnot was surprised to find that for each year males were more common than females. In interpreting this finding, Arbuthnot refered to the conventional wisdom that births of males and females are equally likely. If this were the case, in any one year there might, by chance, be more females than males or the other way around. While it’s theoretically possible that chance might produce the string of 82 years with more males, it’s very unlikely. “From whence it follows, that it is Art, not Chance, that governs,” Arburthnot wrote. In more modern language, Arburthnot concluded that the hypothesis of equal rates of male and female births was not consistent with the data. Arbuthnot’s “Chance” corresponds to false discovery, while “Art” is a valid discovery.\nArburthnot’s logic became a standard component of statistical method.\n\nSummarize the data into a single number called a “test statistic”. For Arburthnot the test statistic was the number of years where male births predominated, out of the 82 years being examined. The observed value of the test statistic was 82.\nState a “null hypothesis”, typically something that is the conventional wisdom. For Arburthnot, the null hypothesis was that male and female births are equally likely.\nCalculate a hypothetical quantity based on the null hypothesis: the probability that the test statistic produced in a world in which the null hypothesis holds true would be at least as large as the test statistic.\nIf the probability in (3) is small, one is entitled to “reject the null hypothesis.” Typically, “small” is defined as 0.05 or less.\n\nIn the 1890s, statistical pioneer Karl Pearson invented a test statistic he called \\(\\chi^2\\) (“chi”-squared, with “chi” pronounced “ki” as in “kite”) that can be applied in a variety of settings. In 1900, Pearson published a table (pearson-1900?) that makes it an easy matter to look up the probability in step (3) above. He called this theoretical probability “P”, a name that has stuck but is conventionally written as lower-case “p”.\nData scientists tend to work with “big data”, but for many applications of statistics, data is so scarce that use of separate training and testing data is impractical. For such small data, the calculation of a p-value can be a sensible guard against false discovery. Still, a p-value does not address any of the sources of false discovery outlined in the previous sections of this chapter. When used with small data and simple modeling methods, those sources of false discovery are not so much of a problem. In small data there won’t be multiple explanatory variables that can be searched and there won’t be a choice of response variables. This doesn’t eliminate all problems, since in small data results can depend critically on the inclusion or exclusion of a single row of data. The name “p hacking” has been given to the various ways that researchers can manipulate p-values to get them below 0.05.\nAnother problem with p-values stems from misinterpretation of the admittedly difficult logic that underlies them. The misinterpretations are encouraged by the use of the term “tests of significance” to the p-value method. Particularly galling is the use of the description “statistically significant” to describe a result where p < 0.05. The everyday meaning of “significant” as something of importance is in no way justified by p < 0.05. Instead, the practical importance or not is more clearly signaled by examining an effect size. (It’s extremely disappointing that journalists, who are writing for an audience that for the most part has no understanding of p-value methodology, use “significant” when reporting on the statistics of research findings. It would be more honest to use a neutral term such as “null-validated” or “p-validated” which does not confuse the statistical result with actual practical importance.)\nThe p-value methodology has little or nothing to contribute to data science practice. When data is big there is a much more straightforward method to guard against false discovery: cross validation. And when data is big there is another, more fundamental problem with p-values. They are calculated with reference to a specific null hypothesis of “no effect” or “no relationship.” More realistically, they should be calculated with respect to a hypothesis of “trivial (but potentially non-zero) effect”. There are all sorts of mechanisms in the world (such as common causes) that can create the appearance of some effect or relationship. No matter how trivial in size this is, with sufficient data the p-value will become small. To illustrate, Figure 38.5 shows the p-value as a function of the sample size n in a system with an R-squared of 0.001, which in most settings would be of no practical signficance.\n\n\nWarning: geom_hline(): Ignoring `mapping` because `yintercept` was provided.\n\n\n\n\n\nFigure 38.5: The p-value as a function of sample size n when the test statistic R-squared has the trivial value 0.001. The horizontal line shows the usual threshold for “significance” of p < 0.05."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#notes-in-draft",
    "href": "Reading-notes-lesson-38.html#notes-in-draft",
    "title": "38  False discovery",
    "section": "NOTES IN DRAFT",
    "text": "NOTES IN DRAFT\n“Statistical crisis” in science\nhttps://www.americanscientist.org/article/the-statistical-crisis-in-science\nGarden of the Forking Paths\nIonedes"
  },
  {
    "objectID": "QR2.html",
    "href": "QR2.html",
    "title": "39  Review of Lessons 9-19",
    "section": "",
    "text": "Warning\n\n\n\nI’ll put learning challenges here. The class day will be given over to the QR."
  }
]
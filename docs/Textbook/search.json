[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lessons in Statistical Thinking",
    "section": "",
    "text": "Lessons in Statistical Thinking is an update and reconsideration of the concepts and methods needed to extract information from data. Such an update is needed because the canon of traditional introductory statistics texts has long been obsolescent and fails to address the needs of the contemporary data scientist and decision-maker. That canon stems from an influential 1925 book, Ronald Fisher’s Statistical Methods for Research Workers. Research workers of that era typically ran small benchtop or field experiments with a dozen or fewer observations on each of two treatments. A first task with such small data is to rule out the possibility that calculated differences might reflect only the accidental arrangement of numbers into groups.\nPerhaps emblematic of the current dissatisfaction with small-data methods is the controversy over “statistical significance.” Although situated at the core of many statistics textbooks, significance testing has little to do with the meaning of “significant” as “important” or “relevant.” This article in the prestigious science journal Nature details the controversy. Figure 18.1 reproduces a cartoon from that article that puts the shortcomings of “statistical significance” in a historical context."
  },
  {
    "objectID": "index.html#statistical-thinking",
    "href": "index.html#statistical-thinking",
    "title": "Lessons in Statistical Thinking",
    "section": "Statistical thinking",
    "text": "Statistical thinking\nThe work of today’s data scientists is often to discover novel connections among multiple variables and to guide decision-making. It is common for data to be available in large masses from observations rather than experiments. One common purpose is “prediction,” which might be as simple as the uses of medical screening tests or as breathtaking as machine-learning techniques of “artificial intelligence.” Another pressing need from data analysis is to understand possible causal connections between variables.\nThe twenty lessons that follow describe a way of thinking that is historically novel, unfamiliar to most otherwise well-educated people, and incredibly useful for making sense of the world and what data can tell us about the world."
  },
  {
    "objectID": "index.html#for-reference-important-word-pairs",
    "href": "index.html#for-reference-important-word-pairs",
    "title": "Lessons in Statistical Thinking",
    "section": "For reference: Important word pairs",
    "text": "For reference: Important word pairs\nMany of the vocabulary terms used in statistical thinking come in pairs. We list several such pairs below, in roughly the order they first appear in the Lessons. The pairs can be a reference while reading, but it is also helpful to return to this list to sharpen your understanding of the distinctions.\nExplanatory vs response variables. Models (in these Lessons) always involve a single response variable*. In contrast, models can have zero or more explanatory variables.\nVariable vs covariate. “Covariate” is another word for an explanatory variable. The word “covariate” signals that the variable is not itself of direct interest to the modeler but puts another explanatory variable in a correct context.\nCategorical vs quantitative variables. Always be aware of whether a model’s response variable is categorical or quantitative. When categorical, expect to use zero_one() to convert it to quantitative before modeling. In contrast, explanatory variables can be either categorical or quantitative.\nRegression model vs classifier. A regression model always has a quantitative response variable. A classifier has a categorical response variable. In these Lessons, as in much professional use of data, our categorical response variables will have two levels (e.g., healthy or sick, up or down, yes or no). In this situation, regression techniques suffice to build classifiers.\nModel vs model function. By “model,” we will almost always mean “regression model.” A regression model, typically constructed by the lm() function, contains various information useful to summarize the model. The “model function” provides the mechanism for one important task, calculating from values from the explanatory variables the corresponding model output.\nModel coefficient vs effect size. Model coefficients are numerical parameters. Training determines the appropriate values for the coefficients. In contrast, an effect size describes the relationship between the response variable and a selected explanatory variable.\nPoint estimate vs interval estimate. A point estimate is a single number. For instance, a model coefficient is a point estimate, as is the output from a model function. In contrast, interval estimates involve two numbers; one specifies the lower end of the interval and the other number specifies the upper end.\nPrediction interval vs confidence interval. A prediction interval describes the anticipated range of the actual result for which we have made a prediction, e.g., “tomorrow’s wind will be between 5 and 10 mph.” A confidence interval is often used to express the uncertainty in a coefficient or effect size."
  },
  {
    "objectID": "index.html#software-guide",
    "href": "index.html#software-guide",
    "title": "Lessons in Statistical Thinking",
    "section": "Software guide",
    "text": "Software guide\nThese Lessons use about a dozen new R functions. Some of these are used frequently in examples and exercises and are worth mastering. Others appear only in demonstrations.\n\n\n\n\n\n\nDemonstrations\n\n\n\nThese lessons contain demonstrations illustrating statistical concepts or data analysis strategies. We will place these in a distinctive box, of which this is an example.\nThe demonstrations will often contain new computer commands that perform tasks used in teaching statistics. However, readers are not expected to be able to construct such commands on their own.\n\n\n\nTraining models with data\n\nlm() arguments: i. tilde expression, ii. data= data frame.\nOccasionally, you will be directed to use glm() or model_train(), which work similarly to lm() but are specialized for models whose output is a probability.\nzero_one() converts a two-level categorical variable to a 0/1 encoding.\n\nSummarizing models. These invariably take as input a model produced by lm() (or glm()) and generate a summary report about that model.\n\ncoef(): displays model coefficients. Each coefficient is a single number.\nconf_interval(): displays model coefficients as an interval with a lower and upper value.\nrsquared() calculates the R2 of a model, and some related measures.\nregression_summary(), like conf_interval(), but with more detail.\n\nEvaluating a model on inputs\n\nmodel_eval() takes a trained model (as produced by lm()) and calculates the model output in both a point form and an interval form. model_eval() can also display the residuals from training or evaluation data.\n\nGraphics\n\nmodel_plot() draws a graphic of a model’s function optionally with prediction or confidence intervals.\ngeom_violin() is a modern alternative to geom_boxplot().\n\nDAGs (directed, acyclic graphs)\n\nsample() collects simulated data from a DAG\ndag_draw() draws a picture of a DAG showing how the variables are connected.\n\nUsed within the summarize() data wrangling function:\n\nvar() computes the variance of a single variable.\n\n\n\n\n\n\n\n\nDemonstration\n\n\n\nHere are some of the command structures that appear in demonstrations. These explanations give a general idea of the tasks they perform.\n\ndo(10) * { command } causes the command to be executed repeatedly the indicated number of times. Such repetitions are useful when the command is a trial of a random process such as sampling, resampling, or shuffling.\nfunction(arguments) { set of commands } packages in a single unit a set of one or more commands. The packaging facilitates using them over and over again with specified arguments.\ngeom_errorbar() works much like geom_point() but draws vertical bars instead of dots. Bar-shaped glyphs depict intervals such as confidence or prediction intervals.\ngeom_ribbon() is like geom_line() but for intervals.\neffect_size() calculates the strength and direction of the input-output relationship between the response variable of a model and a selected one of the explanatory variables."
  },
  {
    "objectID": "bogus.html",
    "href": "bogus.html",
    "title": "1  Bogus",
    "section": "",
    "text": "Foobar"
  },
  {
    "objectID": "Reading-notes-lesson-19.html#statistical-thinking",
    "href": "Reading-notes-lesson-19.html#statistical-thinking",
    "title": "19  Preliminaries",
    "section": "Statistical thinking",
    "text": "Statistical thinking\nThese lessons are about “statistical thinking,” a phrase which includes habits of mind, routine questions to ask, and understanding of which statistical measures are informative—and which not—in different contexts. The goal of statistical thinking is to understand “how and when we can draw valid inferences from data.” [Source] The word “valid” means several things at once: faithful to the data, consistent with the process used to assemble the data, and informative for the uses to which the inferences are to be directed.\nEvery person has a natural ability to think. We train our thinking skills by observing and emulating the logic and language of people and sources deemed authoritative. We have resources spanning several millennia to hone our ability to think. However, statistical thinking is a comparatively recent arrival on the intellectual scene, germinating and developing over only the last 150 years. As a result, hardly anything that we hear or read exemplifies statistical thinking.\nIn general, effective thinking requires us to grasp various intellectual tools, for example, logic. Our mode of logical thinking was promulgated by Aristotle (384–322 BC) and, to quote the Stanford Encyclopedia of Philosophy, “has had an unparalleled influence on the history of Western thought.” In the 2500 years since Aristotle’s time, the use of Aristotelian logic has been so pervasive that we expect any well-educated person to be able to identify logical thinking. For example, the statement “John’s car is red” has implications. Which of these two statements are among those implications? “That red car is necessarily John’s,” or “The blue car is not John’s car.” Not so hard!\nThe intellectual tools needed for statistical thinking are, by and large, unfamiliar and non-intuitive. These Lessons are intended to provide the tools you will need to engage in effective statistical thinking.\nTo get started, consider this headline from The Economist, a well-reputed international news magazine: “The pandemic’s indirect effects on small children could last a lifetime.” As support for this claim, the headlined article provides more detail. For instance:\n\n“Stress and distraction made some patients more distant. LENA, a charity in Colorado, has for years used wearable microphones to keep track of how much chatter babies and the care-givers exchange. During the pandemic the number of such \"conversations\" declined. ….”[g]etting lots of interaction in the early years of life is essential for healthy development, so these kinds of data \"are a red flag\".” The article goes on to talk of “children starved of stimulation at home …..”\n\nThis short excerpt might raise some questions. Think about it briefly and note what questions come to mind.\nFor those already along the road toward statistical thinking, the phrase, “the number of such conversations declined” might prompt this question: “By how much?” Similarly, reading the claim that “getting lots of interactions … is essential for healthy development,” your mind might insist on these questions: How much is “lots?” How does the decline in the number compare to “lots?”\nNot finding the answer to these questions in the article’s text, it would be sensible to look for the primary source of the information. In our Internet age, that’s comparatively easy to do. The LENA website includes an article, “COVID-era infants vocalize less and experience fewer conversational turns, says LENA research team.” The article contains two graphs. (?fig-lena-two-graphs)\n\nknitr::include_graphics(\"www/Lena-fig1.png\")\nknitr::include_graphics(\"www/Lena-fig2.png\")\n\n\n\n\n\n\nFigure 19.1: Graphics from the LENA website. The left is captioned, “Children from the COVID-era sample produced significantly fewer vocalizations than their pre-COVID peers.” The right, “The differences in vocalizations and turns were greatest among children from families in the lowest SES [socio-economic status] quartile.”\n\n\n\n\n\n\n\nFigure 19.2: Graphics from the LENA website. The left is captioned, “Children from the COVID-era sample produced significantly fewer vocalizations than their pre-COVID peers.” The right, “The differences in vocalizations and turns were greatest among children from families in the lowest SES [socio-economic status] quartile.”\n\n\n\n\n\n\nTo make any proper sense of the graphs in ?fig-lena-two-graphs, you need some basic technical knowledge. For example, what do the vertical bars in the graph mean? And the subcaptions, “t(628) = 3.03, p = 0.003” and “t(216)= 2.13, p=0.002”: What do they mean, if anything? Turning back to the text of The Economist, do these graphs justify raising a “red flag?” More basically, are these graphs the “data,” or is there more data behind the graphs? What would that data show?\nThe LENA article does not link to supporting data, that is, what lies behind the graphs in ?fig-lena-two-graphs. But the LENA article does point to other publications.\n\n“These findings from LENA support a growing body of evidence that babies born during the COVID pandemic are, on average, experiencing developmental delays. For example, researchers from the COMBO (COVID-19 Mother Baby Outcomes) consortium at Columbia University published findings in the January 2022 issue of JAMA Pediatrics showing that children born during the pandemic achieved significantly lower gross motor, fine motor, and personal-social scores at six months of age.”\n\nTo the statistical thinker, phrases like “red flag,” “growing body of evidence,” and “significantly lower” are weasel words, that is, terms “used in order to evade or retreat from a direct or forthright statement or position.” [Source] In ordinary thinking, such evasiveness or lack of forthrightness would naturally prompt concern about the reliability of the claim. It makes sense to look deeper, for instance, by checking out the JAMA article. Many people would be hesitant to do this, anticipating that the article would be incomprehensible and filled with jargon. An important reason to study statistical thinking is to tear down barriers to substantiating or debunking claims. In fact, the JAMA article contains very little that requires knowledge of pediatrics or the meaning of “gross motor, fine motor, and personal-social scores,” but a lot that depends on understanding statistical notation and convention and—more critical —the reasoning behind the conventions.\nThe tools of statistical thinking are the tools for making sense of data. Evaluating data is essential to determine whether to rely on claims supposedly based on those data. In the words of eminent engineer and statistician W. Edwards Demming: “In God we trust. All others must bring data.” And former President Ronald Reagan famously quoted a Russian proverb: “Trust, but verify.” Unfortunately, until you have the statistical thinking tools needed to interpret data reliably, all you can do is trust, not verify."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#defining-statistical-thinking",
    "href": "Reading-notes-lesson-19.html#defining-statistical-thinking",
    "title": "19  Preliminaries",
    "section": "Defining statistical thinking",
    "text": "Defining statistical thinking\nLearning a new way of thinking is genuinely hard. As you learn statistical thinking, it may help to have a concise definition. The following definition captures much of the essence of statistical thinking:\n\nStatistic thinking is the accounting for variation in the context of what remains unaccounted for.\n\nImplicit in this definition is a pathway for learning to think statistically:\n\nLearn how to measure variation;\nLearn how to account for variation;\nLearn how to measure what remains unaccounted for.\n\nThe next three sections briefly touch on each of these three topics."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#variation",
    "href": "Reading-notes-lesson-19.html#variation",
    "title": "19  Preliminaries",
    "section": "Variation",
    "text": "Variation\n\n\nVariation itself is nature’s only irreducible essence. Variation is the hard reality, not a set of imperfect measures for a central tendency. Means and medians are the abstractions. —– Stephen Jay Gould (1941- 2002), paleontologist and historian of science.\n\n\nTo illustrate variation, let’s consider a process fundamental to human life: gestation. We all know that human pregnancy “typically” lasts around nine-months, but that the duration isn’t known in advance.\nFigure 19.3 shows data from the Gestation data frame. In this data frame, each of the 1200 rows is one pregnancy and birth about which several measurements were made. The gestation variable records the length of the pregnancy (in days).\n\n\nCode\nGestation <- Gestation %>% \n  mutate(parity = ifelse(parity==0, \"first-time\", \"previous-preg\")) \nPlot1 <- Gestation %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) \nPlot1\n\n\n\n\n\nFigure 19.3: Gestational period for first-time mothers and mothers with a previous pregancy.\n\n\n\n\nFigure 19.3 divides the 1200 births in the Gestation data frame according to the variable parity, which describes whether or not the pregnancy is the mother’s first.\nThe variation in gestation is evident directly from the dots in the graph. One strategy for describing variation is to specify an interval: the span between a lower and an upper value. For instance,\n\nThe large majority of pregancies last between 250 and 300 days. Or,\nThe majority of pregnancies are between 275 and 290 days.\n\nA more subtle description avoids setting hard bounds in favor of saying which durations are common and which not. This common-or-not description is called a “distribution.” The “histogram” is a famous style of presentation of a distribution. Even elementary-school students are introduced to histograms; they are easy to draw. But we have more important concerns; we want to be able to show relationships between variables and we want, whenever possible, to put the graphical summaries of data as a layer on top of the data themselves. And we have the computer as a tool for making graphics. Consequently, our preferred format for displaying distributions is a smooth shape, oriented along the vertical axis. The width of the shape expresses how common is the corresponding region of the vertical axis. Figure 19.4 shows the density display layered on top of the pregnancy data. For reasons that may be evident, this sort of display is called a “violin plot.”\n\n\nCode\nPlot1 +\n  geom_violin(aes(group=parity), fill=\"blue\", alpha=0.65, color=NA)\n\n\n\n\n\nFigure 19.4: A violin plot. The long axis of the violin-like shape is oriented along the response-variable axis (that is, the vertical axis in our standard format). The width of the violin for each possible value of the response variable is proportional to the density of data near that value.\n\n\n\n\nThe shapes of the two violins in Figure 19.4 are similar, suggesting that the variation in the duration of pregnancy is about the same for first-time mothers as for mothers in a second or later pregnancy.\nThere is a strong link between the interval descriptions of variation and the density display. Suppose you specify the fraction of cases that you want to include in an interval description, say 50% or 80%. In terms of the violin, that fraction is a proportion of the overall area of the violin. For instance, the 50% interval would include the central 50% of the area of the violin, leaving 25% out at the bottom and another 25% out at the top. The 80% interval would leave out only 10% of the area at the top and bottom of the violin. This suggests that the interval style of describing variation really involves three numbers; the top and bottom of the interval as well as the selected percentage (say, 50% or 80%) used to find the location of the top and bottom.\nYet another style for describing variation—one that will take primary place in these Lessons—uses only a single-number. Perhaps the simplest way to imagine how a single number can capture variation is to think about the spread or distance between the top and bottom of an interval description. In taking such a distance as the measure of variation, we are throwing out some information. Taken together, the top and bottom of the interval describe two things: the location of the values and the spread among the values. These are both important, but it is the spread that gives a pure description of variation.\nEarly pioneers of statistics took some time to agree on a standard way of measuring the spread. For instance, should it be the spread between the top and bottom of a 50% interval or an 80% interval, or something else. In the end, the selected standard focussed on something more basic: the differences between pairs of individual values.\nIt works like this. For a data frame with \\(n=2\\) rows, the spread in a variable can be measured simply as the difference between the two values. For instance, suppose the gestation variable had only two entries, say, 267 and 293 days. The spread or distance between these is \\(293-267 = 26\\) days. Of course, we don’t intend to measure spread with a negative number. One solution is to use the absolute value of the difference. However, for subtle mathematical reasons relating to—of all things!—the Pythagorean theorem, we avoid the possibility of a negative spread by using the square of the difference, that is, \\((293 - 267)^2 = 676\\) days-squared.\nTo extend this very simple measure of variation to data with \\(n > 2\\) is simple: look at the square difference between every possible pair of values, then average. For instance, for \\(n=3\\) with values 267, 293, 284, look at the differences \\((267-293)^2, (267-284)^2\\) and \\((293-284)^2\\) and average them! This simple way of measuring variation is called the “modulus” and dates from 1885. Since then, statisticians have standardized on a closely related measure, the “variance,” which is the modulus divided by \\(\\sqrt{2}\\). Either one would work, but there are advantages to standardizing on one: the variance.\nCalculating the variance is straightforward, Here’s the variance of gestation:\n\nGestation %>%\n  summarize(variance = var(gestation))\n\n\n\n \n  \n    variance \n  \n \n\n  \n    256.887 \n  \n\n\n\n\nA consequence of the use of squaring in defining the variance is the units of the result. gestation is measured in days, so var(gestation) is measured in days2. The advantage to this will only become clear later in these Lessons. For now, you might prefer to think about the square-root of the variance, which has been given the name “standard deviation.”\n\nGestation %>%\n  summarize(standard_deviation = sd(gestation))\n\n\n\n \n  \n    standard_deviation \n  \n \n\n  \n    16.02769"
  },
  {
    "objectID": "Reading-notes-lesson-19.html#sec-accounting-for-variation",
    "href": "Reading-notes-lesson-19.html#sec-accounting-for-variation",
    "title": "19  Preliminaries",
    "section": "Accounting for variation",
    "text": "Accounting for variation\nThe word “account” has several related meanings.1\n\nTo “account for something” means “to be the explanation or cause of something.” [Oxford Languages]\nAn “account of something” is a story, a description, or an explanation, as in the Biblical account of the creation of the world.\nTo “take account of something” means “to consider particular facts, circumstances, etc. when making a decision about something.”\n\nSynonyms for “account” include “description,”report,” “version,” “story,” “statement,” “explanation,” “interpretation,” “sketch,” and “portrayal.” “Accountants” and their “account books” keep track of where money comes from and goes to.\nThese various nuances of meaning, from a simple arithmetical tallying up to an interpretation or version serve the purposes of statistical thinking well. When we “account for variation,” we are telling a story that tries to explain where the variation might have come from. An accounting of variation is not necessarily definitive, true, or helpful. Just as witnesses of an event can have different accounts, so there can be many accounts of the variation even of the same variable in the same data frame.\nThere are many formats for stories, many ways of organizing facts and data, and many ways of accounting for variance. In these Lessons, we will use regression modeling almost exclusively as our method of accounting. Here, for example, are two different accounts of gestation:\n\nlm(gestation ~ 1, data=Gestation) %>% coef()\n\n(Intercept) \n   279.3385 \n\nlm(gestation ~ parity, data = Gestation) %>% coef()\n\n        (Intercept) parityprevious-preg \n         281.261981           -2.585058 \n\n\nIn the R language, expressions like gestation ~ 1 and gestation ~ parity are called “tilde expressions.” They are the means by which the modeler specifies the structure of the model that is to be built. Training (or “fitting”) translates the model specification into an arithmetic formula that involves the explanatory variables and numerical coefficients.\nThe coefficients from a regression model are part of an accounting for variation. Learning how to read them is an important skill in statistical thinking. For instance, the coefficient from a model in the form y ~ 1 is always the average value of variable y. In contrast, in a model like y ~ x, the “intercept” is a baseline value and the x-coefficient describes what part of the variation in y can be credited to x.\n\n\n\n\n\n\nThe RESPEX graphics format\n\n\n\nFigure 19.3 is an example of what we call the RESPEX graphics style. Each RESPEX graphic is made to coordinate with aregression model of the data. Every regression model has a response variable. Likewise, every RESPEX graphic shows the response variable on the vertical axis. Similarly, RESPEX graphics place an explanatory variable on the horizontal axis. If there is more than one explanatory variable, they are encoded graphically using color then faceting.\nRESPEX stands for “RESPonse versus EXplanatory,” but you might like to think of it as data graphics drawn with “respect” to a model.\n\n\nRegression models always have a quantitative response variable, although explanatory variables can be either quantitative or categorical. But, often, the modeling situation calls for a response variable that is categorical. Expert modelers can use specialized modeling methods to handle such situations. However, some of the power of these specialized methods is available to the beginning modeler by a little trick. When categorical response variables have just two levels, e.g., Alive/Dead, Promoted/Not, or Win/Loss, they can be transformed to a numerical representation using 0 for one level and 1 for the other.\nWe will identify the such variables as being of type “yes/no” or, equivalently, “zero-one” variables. With the zero-one encoding\nThis numerical “0/1 encoding” is directly suited for regression modeling and enables us to extend the scope of regression models. The output of the regression model is always numerical. Nothing in the regression technique restricts those outputs to exactly zero or one, even when the response variable is of the yes/no type. Usually, the modeler interprets such numerical output as probabilities or, more generally, as measures to be converted to probabilities.\n\n\n\n\n\n\nR technique: zero_one().\n\n\n\nThe zero_one() function converts a yes/no variable to the numerical zero-one format. zero_one() allows you to specify which of the two levels is represented by 1.\nTo illustrate, consider the mosaicData::Whickham data frame, which records a 1972-1974 survey, part of a study of the relationship between smoking and mortality. Twenty years after the initial survey, a follow-up established whether or not each person was still alive. Here are a few rows from the data frame:\n\n\n\n\n \n  \n    outcome \n    smoker \n    age \n  \n \n\n  \n    Alive \n    Yes \n    23 \n  \n  \n    Alive \n    Yes \n    18 \n  \n  \n    Dead \n    Yes \n    71 \n  \n  \n    Alive \n    No \n    67 \n  \n  \n    Alive \n    No \n    64 \n  \n  \n    Alive \n    Yes \n    38 \n  \n\n\n\n\nThe outcome variable in Whickham records the result of the follow-up survey. It is a categorical variable with levels “Alive” and “Dead.” To examine what the data have to say about the relationship between smoking and mortality, we construct a model with outcome as the response variable and smoking as an explanatory variable. Before doing so, we translate outcome into a zero-one format. Like this:\n\nWhickham %>% \n  mutate(alive = zero_one(outcome, one=\"Alive\"))\n\n\n\n\n\n \n  \n    outcome \n    smoker \n    age \n    alive \n  \n \n\n  \n    Alive \n    Yes \n    23 \n    1 \n  \n  \n    Alive \n    Yes \n    18 \n    1 \n  \n  \n    Dead \n    Yes \n    71 \n    0 \n  \n  \n    Alive \n    No \n    67 \n    1 \n  \n  \n    Alive \n    No \n    64 \n    1 \n  \n  \n    Alive \n    Yes \n    38 \n    1 \n  \n\n\n\n\nNote the correspondence between the outcome and the newly created alive variable."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#variation-unaccounted-for",
    "href": "Reading-notes-lesson-19.html#variation-unaccounted-for",
    "title": "19  Preliminaries",
    "section": "Variation unaccounted for",
    "text": "Variation unaccounted for\nA model typically accounts for only some of the variation in a response variable. The remaining variation is called “residual variation.”\nConsider the model gestation ~ parity. In the next lines of code we build this model, training it with the Gestation data. Then we evaluate the model on the trained data. This amounts to using the model coefficients to generate a model output for each row in the training data, and can be accomplished with the model_eval() R function.\n\nModel <- lm(gestation ~ parity, data = Gestation)\nEvaluated <- model_eval(Model)\n\nUsing training data as input to model_eval().\n\n\n\n\nUsing training data as input to model_eval().\n\n\n\n\n \n  \n      \n    .response \n    parity \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    1218 \n    270 \n    previous-preg \n    278.6769 \n    -8.6769231 \n    247.2800 \n    310.0738 \n  \n  \n    1219 \n    275 \n    first-time \n    281.2620 \n    -6.2619808 \n    249.8322 \n    312.6917 \n  \n  \n    1220 \n    265 \n    previous-preg \n    278.6769 \n    -13.6769231 \n    247.2800 \n    310.0738 \n  \n  \n    1221 \n    291 \n    previous-preg \n    278.6769 \n    12.3230769 \n    247.2800 \n    310.0738 \n  \n  \n    1222 \n    281 \n    first-time \n    281.2620 \n    -0.2619808 \n    249.8322 \n    312.6917 \n  \n  \n    1223 \n    297 \n    previous-preg \n    278.6769 \n    18.3230769 \n    247.2800 \n    310.0738 \n  \n\n\n\n\n\n\n\n\n\n\nThe .response variable\n\n\n\nThe output from model_eval() repeats some columns from the data used for evaluation. For example, the explanatory variables are listed by name. (Here, the only explanatory variable is parity.) The response variable is also included, but given a generic name, .response to make it easy to distinguish it from the explanatory variables.\n\n\nTo see where the .output comes from, let’s look again at the model coefficients:\n\nModel %>% coef()\n\n        (Intercept) parityprevious-preg \n         281.261981           -2.585058 \n\n\nThe baseline value is 281.3 days. This applies to first-time mothers. For the other mothers, those with a previous pregnancy, the coefficient indicates that the model value is 2.6 days less than the baseline, or 279.7 days.\nThe output from model_eval() includes other columns of importance. For us, here, those are. the response variable itself (gestation, which has been given a generic name, .response) and the residuals from the model (.resid). There is a simple relationship between .response, .output and .resid:\n\\[\\mathtt{.response} = \\mathtt{.output} + \\mathtt{.resid}\\]\n\n\n\n\n\n\nDemonstration: Why the variance?\n\n\n\nThe subtle mathematical reasoning behind the choice of variance to measure variation is illuminated when we compute the variances of the three quantities in the previous equation.\n\nEvaluated %>%\n  summarize(var_response = var(.response),\n            var_output = var(.output),\n            var_resid  = var(.resid))\n\n\n\n \n  \n    var_response \n    var_output \n    var_resid \n  \n \n\n  \n    256.887 \n    1.273587 \n    255.6134 \n  \n\n\n\n\nThe variances of the output and residuals add up to equal, exactly, the variance of the response variable! This isn’t true for the standard deviations:\n\nEvaluated %>%\n  summarize(sd_response = sd(.response),\n            sd_output = sd(.output),\n            sd_resid  = sd(.resid))\n\n\n\n \n  \n    sd_response \n    sd_output \n    sd_resid \n  \n \n\n  \n    16.02769 \n    1.128533 \n    15.98791"
  },
  {
    "objectID": "Reading-notes-lesson-20.html#directed-acyclic-graphs",
    "href": "Reading-notes-lesson-20.html#directed-acyclic-graphs",
    "title": "20  Simulation and sampling variation",
    "section": "Directed Acyclic Graphs",
    "text": "Directed Acyclic Graphs\nA core tool in thinking about causal connections is a mathematical structure called a “directed acyclic graph” (DAG, for short). DAGs are one of the most popular ways for statistical thinkers to express their ideas about what might be happening in the real world. Despite the long name, DAGs are very accessible to a broad audience.\nDAGs, despite the G for “graph,” are not about data graphics. The “graph” in DAG is a mathematical term of art; a suitable synonym is “network.” Mathematical graphs consist of a set of “nodes” and a set of “edges” connecting the nodes. For instance, Figure 20.1 shows three different graphs, each with five nodes labeled A, B, C, D, and E.\n\n\n\n\n\n\n\n(a) undirected graph\n\n\n\n\n\n\n\n(b) directed but cyclic\n\n\n\n\n\n\n\n(c) directed acyclic graph\n\n\n\n\nFigure 20.1: Graphs of various types\n\n\nThe nodes are the same in all three graphs of Figure 20.1, but each graph is different from the others. It is not just the nodes that define a graph; the edges (drawn as lines) are part of the definition as well.\nThe left-most graph in Figure 20.1 is an “undirected” graph; there is no suggestion that the edges run one way or another. In contrast, the middle graph has the same nodes and edges, but the edges are directed. An excellent way to think about a directed graph is that each node is a pool of water; each directed edge shows how the water flows between pools. This analogy is also helpful in thinking about causality: the causal influences flow like water.\nLook more carefully at the middle graph. There is a couple of loops; the graph is cyclic. In one loop, water flows from E to C to D and back again to E. The other loop runs B, C, D, E, and back to B. Such a flow pattern cannot exist without pumps pushing the water back uphill.\nThe rightmost graph reverses the direction of some of the edges. This graph has no cycles; it is acyclic. Using the flowing and pumped water analogy, an acyclic graph needs no pumps; the pools can be arranged at different heights to create a flow exclusively powered by gravity. The node-D pool will be the highest, E lower. C has to be lower than E for gravity to pull water along the edge from E to C. The node-B pool is the lowest, so water can flow in from E, C, and A.\nDirected acyclic graphs represent causal influences; think of “A causes B,” meaning that causal “water” flows naturally from A to B. In a DAG, a node can have multiple outputs, like D and E, and it might have multiple inputs, like B and C. In terms of causality, a node—like B—having multiple inputs means that more than one factor is responsible for the value of that node. A real-world example: the rising sun causes a rooster to crow, but so can another intruder to the coop.\nOften, nodes do not have any inputs. These are called “exogenous factors”at least by economists. The “genous” means “originates from.” “Exo” means “outside.” The value of an exogenous node is determined by something, just not something that we are interested in (or perhaps capable of) modeling. No edges are directed into an exogenous node since none of the other nodes influence its value.\nFor simulating data, we go beyond drawing a graph of causal connections to outfit DAGs with specific formulas representing the mechanism imbued in each node. DAGs equipped with formulas can be used to generate simulated data.1 Training a model on those data leads to a model function that we can compare to the DAG’s formulas. Then check whether the formulas and the model function match. This practice helps us learn what can go right or wrong in building a model, just as practice in an aircraft simulator trains pilots to handle real-world situations in real aircraft.\nWe start with a simple example, dag08. The dag_draw() command draws a picture of the graph. Printing the dag displays the formulas that set the values of the nodes.\n\ndag_draw(dag08)\n\n\n\n\nThe graph shows that both c and x contribute to y.\n\nprint(dag08)\n\nc ~ exo()\nx ~ c + exo()\ny ~ x + c + 3 + exo()\n\n\nThe formulas show that x and c contribute equally to y, with coefficients of 1. To what extent can regression modeling recover this relationship from data?\nTo find out, we can generate simulated data using the sample() function. For instance,\n\nsample(dag08, size=5)\n\n\n\n \n  \n    c \n    x \n    y \n  \n \n\n  \n    -0.3260365 \n    0.8479298 \n    4.048341 \n  \n  \n    0.5524619 \n    1.1712517 \n    3.928869 \n  \n  \n    -0.6749438 \n    -0.7876782 \n    2.965133 \n  \n  \n    0.2143595 \n    1.1313877 \n    2.878928 \n  \n  \n    0.3107692 \n    0.0875099 \n    3.161596 \n  \n\n\n\n\nEach row in the sample is one trial; in each trial, the node’s formula sets the value for that node. For example, the formula might use the values of other nodes as input. Alternatively, the formula might specify that the node is exogenous, without input from any other nodes.\nModels can be trained on the simulated data using the same techniques as for any other data. To illustrate, here we generate a sample of size \\(n=50\\), then fit the model specification c ~ a + b and summarize by taking the coefficients.\n\nsample(dag08, size=50) %>% \n  lm(y ~ c + x, data = .) %>%\n  coef()\n\n(Intercept)           c           x \n  2.9451445   1.2606473   0.8235923 \n\n\nThe coefficients, including the intercept, are close, but not exactly right.\nIn Lessons -Chapter 21 and -Chapter 22 we will figure out how close we can expect the coefficients to be to the precise values implemented in the simulation."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#samples-summaries-of-samples-and-samples-of-summaries-of-samples",
    "href": "Reading-notes-lesson-20.html#samples-summaries-of-samples-and-samples-of-summaries-of-samples",
    "title": "20  Simulation and sampling variation",
    "section": "Samples, summaries of samples, and samples of summaries (of samples)",
    "text": "Samples, summaries of samples, and samples of summaries (of samples)\nBeginners sometimes think that each row in a data frame is a sample. Better to say that each row is a “specimen.” A “sample” is a collection of specimens, the set of rows in a data frame.\nThe “sample size” is the number of rows. “Sampling” is the process of collecting the specimens to be put into the data frame.\nThe following command illustrates computing a summary of a sample from dag08.\n\nsample(dag08, size=10000) %>% \n  lm(y ~ c + x, data = .) %>%\n  coef()\n\n(Intercept)           c           x \n  3.0070253   1.0100177   0.9934592 \n\n\nAn essential question in statistics is how the summary depends on the incidental specifics of a particular sample. DAGs provide a convenient way to address this question since we can generate multiple samples from the same DAG, summarize each, and compare those summaries.\nTo generate a sample of summaries, re-run many trials of the summary. The do() function automates this process, accumulating the results from the trials in a single data frame: a “sample of summaries.” We will use do() mostly in demonstrations.\n\n\n\n\n\n\nDemonstration: Conducting many trials with do()\n\n\n\nIn this demonstration, we will revisit a model used earlier in this Lesson to see how much the coefficients vary from one sample to another. Each trial consists of drawing a sample from dag08, training a model, and summarizing with the model coefficients. Curly braces ({ and }) surround the commands needed for an individual trial.\nPreceding the curly braces, we have placed do(5) *. This instruction causes the trial to be repeated five times.\n\ndo(5) * {\n  sample(dag08, size=50) %>% \n    lm(y ~ c + x, data = .) %>%\n    coef()\n}\n\n\n\n \n  \n    Intercept \n    c \n    x \n  \n \n\n  \n    3.019112 \n    0.6794641 \n    1.3353393 \n  \n  \n    3.006728 \n    0.9042066 \n    0.8406397 \n  \n  \n    2.966061 \n    1.1619847 \n    0.9307029 \n  \n  \n    2.866499 \n    1.0881640 \n    1.0769612 \n  \n  \n    3.080889 \n    1.1088753 \n    1.0009938 \n  \n\n\n\n\nThe five trials are collected together by do() into the five rows of a single data frame. Such a data frame can be considered a “sample of summaries.”\n\n\nOne of the things we will do with a “sample of summaries” is to … wait for it … summarize it. For instance, in the following code chunk, a sample of 40 summaries is stored under the name Trials. Then we will summarize Trials, in this case, to see how much the values of the a and b coefficients vary from trial to trial.\n\nTrials <- do(40) * {\n  sample(dag08, size=50) %>% \n    glm(y ~ c + x, data = .) %>%\n    coef()\n} \nTrials %>% \n  summarize(mean_c_coef = mean(c), spread_a = sd(c), \n            mean_x_coef = mean(x), spread_b = sd(c))\n\n\n\n \n  \n    mean_c_coef \n    spread_a \n    mean_x_coef \n    spread_b \n  \n \n\n  \n    0.9858736 \n    0.2215985 \n    1.022228 \n    0.2215985 \n  \n\n\n\n\nThe result of summarizing the trials is a “summary of a sample of summaries.” This phrase is admittedly awkward, but we will use this technique often: summarizing trials, where each trial is a “summary of a sample” Often, the clue will be the use of do(), which repeats trials as many times as you ask."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#causal-inference",
    "href": "Reading-notes-lesson-20.html#causal-inference",
    "title": "20  Simulation and sampling variation",
    "section": "Causal inference",
    "text": "Causal inference\nOften, but not always, our interest in studying data is to reveal or exploit the causal connections between variables. Understanding causality is essential, for instance, if we are planning to intervene in the world and want to anticipate the consequences. Interventions are things like “increase the dose of medicine,” “stop smoking!”, “lower the budget,” “add more cargo to a plane (which will increase fuel consumption and reduce the range).”\nHistorically, mainstream statisticians were hostile to using data to explore causal relationships. (The one exception was experiment, which gathers data from an actual intervention in the world. See Lesson 32.) Statistics teachers encouraged students to use phrases like “associated with” or “correlated with” and reminded them that “correlation is not causation.”\nRegrettably, this attitude made statistics irrelevant to the many situations where intervention is the core concern and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists saw evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, “smoking is associated with lung cancer.” He speculated that smoking and lung cancer might have a common cause, perhaps genetic. Fisher argued that establishing causation requires running an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result.\nFortunately, around 1960, a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. Cornfield’s work was an important step in the development of a new area in statistics: “causal inference.”\nCausal inference is not about proving that one thing causes another but about formal ways to say something about how the world works that can be used, along with data, to make responsible conclusions about causal relationships.\nAs you will see in Lesson -Chapter 30, DAGs are a major tools in causal inference, allowing you not only to represent a hypothesis about causal relationships, but to deduce what sorts of models will be able to reveal causal mechanisms.\nThe point of a DAG is to make a clear statement of a hypothesis about causation. Drawing a DAG does not mean that the hypothesis is correct, just that we believe the hypothesis is, in some sense, a possibility. Different people might have different beliefs about what causes what in real-world systems. Comparing their different DAGs can help, sometimes, to discuss and resolve the disagreement.\nWe are going to use DAGs for two distinct purposes. One purpose is to inform responsible conclusions from data about what causes what. The data on its own is insufficient to demonstrate the causal connections. However, data combined with a DAG can tell us something. Sometimes a DAG includes a causal connection that should create an association between variables. The DAG is incomplete if the association does not appear in the data.\nDAGs are also valuable aids for building models. For example, analysis of the paths in a DAG, as in Lesson 30, can tell us which explanatory variables to include and which to exclude from a model if our modeling goal is to represent the hypothetical causal connections.\nIn these Lessons, we have a second, entirely different, use for DAGs: learning modeling technique. Our approach will be to ::: {.callout-warning} ## Reality check: DAGs and data\nDAGs represent hypotheses about the connections between variables in the real world. They are a kind of scratchpad for constructing alternative scenarios and, as seen in Lesson 28, thinking about how models might go wrong in the face of a plausible alternative causal mechanism.\nIn this book, we extend the use of DAGs beyond their scope in professional statistics; we use them as simulations from which we can generate data. Such simulations provide one way to learn about statistical methodology.\nDAGs are aides to reasoning, scratchpads that help us play out the consequences of our hypotheses about possible real-world mechanisms. However, take caution to distinguish data from DAG simulations from data from reality.\nFinding out about the real world requires collecting data from the real world. The proper role of DAGs in real work is to guide model building from real data.\nIn this course, we sample from DAGs to learn statistical techniques. But never to make claims about real-world phenomena. :::"
  },
  {
    "objectID": "Reading-notes-lesson-21.html#sec-signal-and-noise",
    "href": "Reading-notes-lesson-21.html#sec-signal-and-noise",
    "title": "21  Signal and noise",
    "section": "Signal and noise",
    "text": "Signal and noise\nTo illustrate the statistical problem of signal and noise, let us turn to a DAG simulation: dag01. Here’s a sample from dag01:\n\nTiny <- sample(dag01, size=2)\n\n\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.3260365 \n    2.836001 \n  \n  \n    0.5524619 \n    5.043052 \n  \n\n\n\n\nThe DAG simulation implements a relationship between x and y. In statistics, this relationship is the signal.\n\nLook at the 2-row sample (?tbl-tiny-dag01) from the DAG and guess what the relationship might be.\n\nAny of an infinite number of possible relationships could account for the x and y data. The noise reduction problem of statistics is to make a guess that is as good as possible. Unfortunately, for a sample with \\(n=2\\), as “good as possible” is not very good!\nMore data—a bigger sample—gives us a better shot at revealing the relationship hidden by the noise. ?tbl-small-dag01 shows a sample of size \\(n=10\\):\n\nSmall <- sample(dag01, size=10)\n\n\n\n\n\n \n  \n    x \n    y \n  \n \n\n  \n    -0.7859732 \n    1.8888204 \n  \n  \n    0.0547389 \n    4.1153256 \n  \n  \n    -1.1725603 \n    2.3632792 \n  \n  \n    -0.1673128 \n    6.3287614 \n  \n  \n    -1.8650316 \n    0.9329524 \n  \n  \n    -0.1204402 \n    2.9310384 \n  \n  \n    0.8259787 \n    5.6981878 \n  \n  \n    1.1901595 \n    5.9006170 \n  \n  \n    -1.0914519 \n    2.1314570 \n  \n  \n    -0.3751124 \n    4.2296648 \n  \n\n\n\n\nA careful perusal of the Small sample suggests some patterns. x is never larger than about 2 in magnitude and can be positive or negative. y is always positive. Furthermore, when x is negative, the corresponding y value is relatively small compared to the y values for positive x.\nA sample of size \\(n=10\\) provides more information than a sample of \\(n=2\\), so we can make a more informed guess about the relationship between variables x and y.\nHuman cognition is not well suited to looking at long columns of numbers. Often, we can make better use of our natural human talents by translating the sample into a graphic:\n\n\n\n\n\nCollecting more data can make the relationship clearer. Figure 21.1 displays an \\(n=10,000\\) sample.\n\nLarge <- sample(dag01, size=10000)\n\n\n\n\n\n\nFigure 21.1: With \\(n=10,000\\) rows, the relationship between x and y is evident graphically. (The original Small sample is shown in orange.)\n\n\n\n\nThere are many possible ways to describe the x-y relationship in Figure 21.1. For instance, we can see that when x is positive, y is almost always greater than 4, but for negative x, the value of y tends to be less than 4. Such a description might be apt for some purposes, but in these Lessons, we describe relationships by fitting models to data.\nThe following command uses the small sample (n=10) as training data for a model y ~ x that accounts for y on the basis of x:\n\nlm(y ~ x, data = Small) %>% coef()  # n = 10 sample\n\n(Intercept)           x \n   4.262846    1.741758 \n\n\nThe coefficients provide the information needed to construct the model function: \\[y = 4.26 + 1.74 x\\ .\\] This mathematical formula is a guess of the signal—the relationship between the two variables in dag01. Unfortunately, the formula tells us nothing about the noise obscuring the signal nor how good the guess is.\nThe model coefficients produced by training the model on a much larger sample will presumably be a better guess:\n\nlm(y ~ x, data = Large) %>% coef() #  n = 10,000 sample\n\n(Intercept)           x \n   4.008928    1.495904 \n\n\nUnfortunately, we cannot tell from the coefficients how good the guess is.\nLuckily for us, since the data are a simulation from a DAG, we can see what the coefficients should be as well as the origin of the noise mixing in with the signal.\n\nprint(dag01)\n\nx ~ exo()\ny ~ 1.5 * x + 4 + exo()\n\n\nThe Large sample produced coefficients much closer than the Small sample to the mechanism in the DAG. The idea that larger samples lead to better accuracy has been appreciated since the 16th century and now has the prestige of being a “Law”: the Law of Large Numbers.\nHowever, “better accuracy” does not tell us whether the accuracy suffices for any given purpose. The model filters out some of the noise. However, the model coefficients still display a noisy legacy.\nThe challenge of real-world data is that we cannot open the black box that generated the data; all we have is the data! So how can we tell whether the data at hand are sufficient for giving a usefully accurate description of the actual relationships?\nThe key to the puzzle is the variation within the sample."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#measuring-variation",
    "href": "Reading-notes-lesson-21.html#measuring-variation",
    "title": "21  Signal and noise",
    "section": "Measuring variation",
    "text": "Measuring variation\nLesson 19 introduced the standard way to measure variation in a single variable: the variance or its square root, the standard deviation. For instance, we can measure the variation in the variables from the Large sample using sd() and var():\n\nLarge %>%\n  summarize(sx = sd(x), sy = sd(y), vx = var(x), vy = var(y))\n\n\n\n \n  \n    sx \n    sy \n    vx \n    vy \n  \n \n\n  \n    0.9830639 \n    1.779003 \n    0.9664146 \n    3.164851 \n  \n\n\n\n\nAccording to the standard deviation, the size of the x variation is about 1. The size of the y variation is about 1.7.\nLook again at the formulas that compose dag01:\n\nprint(dag01)\n\nx ~ exo()\ny ~ 1.5 * x + 4 + exo()\n\n\nThe formula for x shows that x is endogenous, its values coming from a random number generator, exo(), which, unless otherwise specified, generates noise of size 1.\nAs for y, the formula includes two sources of variation:\n\nThe part of y determined by x, that is \\(y = \\mathbf{1.5 x} + \\color{gray}{4 + \\text{exo()}}\\)\nThe noise added directly into y, that is \\(y = \\color{gray}{\\mathbf{1.5 x} + 4} + \\color{black}{\\mathbf{exo(\\,)}}\\)\n\nThe 4 in the formula does not add any variation to y; it is just a number.\nWe already know that exo() generates random noise of size 1. So the amount of variation contributed by the + exo() term in the DAG formula is 1. The remaining variation is contributed by 1.5 * x. The variation in x is 1 (coming from the exo() in the formula for x). A reasonable guess is that 1.5 * x will have 1.5 times the variation in x. So, the variation contributed by the 1.5 * x component is 1.5. The overall variation in y is the sum of the variations contributed by the individual components. This suggests that the variation in y should be \\[\\underbrace{1}_\\text{from exo()} + \\underbrace{1.5}_\\text{from 1.5 x} = \\underbrace{2.5}_\\text{overall variation in y}.\\] Simple addition! Unfortunately, the result is wrong. In the previous summary of the Large, we measured the overall variation in y as about 1.72.\nThe variance will give a better accounting than the standard deviation. Recall that exo() generates variation whose standard deviation is 1, so the variance from exo() is \\(1^2 = 1\\). Since x comes entirely from exo(), the variance of x is 1. So is the variance of the exo() component of y.\nTurn to the 1.5 * x component of y. Since variances involve squares, the variance of 1.5 * x works out to be \\(1.5^2\\, \\text{var(}\\mathit{x}\\text{)} = 2.25\\). Adding up the variances from the two components of y gives\n\\[\\text{var(}\\mathit{y}\\text{)} = \\underbrace{2.25}_\\text{from 1.5 exo()} + \\underbrace{1}_\\text{from exo()} = 3.25\\]\nThis result that the variance of y is 3.25 closely matches what we found in summarizing the y data generated by the DAG.\nThe lesson here: When adding two sources of variation, the variances of the individual sources add to form the overall variance of the sum. Just like \\(A^2 + B^2 = C^2\\) in the Pythagorean Theorem."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#dags-from-data",
    "href": "Reading-notes-lesson-21.html#dags-from-data",
    "title": "21  Signal and noise",
    "section": "DAGs from data",
    "text": "DAGs from data\nIn modeling data from dag01 we could recover a good approximation to the formula for y.\n\nLarge %>%\n  lm(y ~ x, data = .) %>%\n  coef()\n\n(Intercept)           x \n   4.008928    1.495904 \n\n\nA DAG describes the causal links between variables. Data modeling reveals the formula implementing the causal link in dag01. Nevertheless, it is wrong to think we can determine the DAG that generated the data from the data alone. Only if we already know the structure of the data-generation DAG can we recover the mechanism inside that DAG. For instance, another statistical thinker might believe that the causal mechanism behind the data is y causing x. Based on this assumption, she also can find the mechanism inside her hypothesized DAG:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .) %>%\n  coef()\n\n(Intercept)           y \n -1.8261448   0.4559782 \n\n\nA DAG is a hypothesis, a statement that might or might not be true. DAGs are part of the statistical apparatus for thinking responsibly about causality. Use a DAG—or, potentially, multiple DAGs—when the issue of what causes what is relevant to the purpose behind the work.\nWhen there are only two variables involved in the system under consideration—we will call them X and Y for simplicity—there are only two possible DAGs:\n\\[X \\rightarrow Y\\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\ X \\leftarrow Y\\] Our understanding of the world sometimes allows us to focus on one of these and not the other. Example: Does the rooster crowing cause the sun to rise, or does the rising sun cause the rooster to crow?\nBeyond the two DAGs \\(X \\rightarrow Y\\) and \\(X \\leftarrow Y\\), additional DAG possibilities can account for the relationship between X and Y. For instance, if we introduce another variable, C, located between X and Y, four other DAGs need to be considered:\n\\[X \\rightarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\leftarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\rightarrow C \\leftarrow Y\\]\nThere are many other DAG configurations involving three variables. To keep things simple, we will restrict things to DAGs where X might or might not cause Y, but Y never causes X.1 Figure 21.2 shows the ten configurations of 3-variable DAGs where Y does not cause X.\n\n\n\n\n\nFigure 21.2: Ten DAG configurations involving three variables X, Y, and C.\n\n\n\n\nWith the conceptual tool of DAGs, the statistical thinker can consider multiple possibilities for what might cause what. Sometimes she can discard some of the possibilities based on common sense. (Think: roosters and the sun.) However, in other settings, there may be possibilities that she does not favor but might be plausible to other people. In Lesson 28, we will explore how each configuration of DAG has implications for which model specifications can or cannot reveal the hypothesized causal mechanism."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#why-sample",
    "href": "Reading-notes-lesson-22.html#why-sample",
    "title": "22  Sampling and sampling variation",
    "section": "Why sample?",
    "text": "Why sample?\nSometimes a data frame is not a sample. This happens when the data frame contains a row for every member of an actual, finite “population.” Such a complete enumeration—the inventory records of a merchant, the records kept of student grades by the school registrar—has a technical name: a “census.” Famously, many countries conduct a census of the population in which they try to record every resident of the country. For example, the US, UK, and China carry out a census every ten years.\nIn a typical setting, it is unfeasible to record every possible unit of observation.1 Such incomplete records constitute a “sample.” One of the great successes of statistics is the means to draw useful information from a sample, at least when the sample is collected correctly.\nSampling is called for when we want to find out about a large group but lack time, energy, money, or the other resources needed to contact every group member. For instance, France collects samples at short intervals to collect up-to-date data while staying within a budget. The name used for the process—recensement en continu or “rolling census”—signals the intent. Over several years, the French rolling census contacts about 70% of the population.\nSometimes, as in quality control in manufacturing, the measurement process is destructive: the measurement process consumes the item. In a destructive measurement situation, it would be pointless to measure every single item. Instead, a sample will have to do."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#sampling-bias",
    "href": "Reading-notes-lesson-22.html#sampling-bias",
    "title": "22  Sampling and sampling variation",
    "section": "Sampling bias",
    "text": "Sampling bias\nCollecting a reliable sample is usually considerable work. An ideal is the “simple random sample” (SRS), where all of the items are available, but only some are selected—completely at random—for recording as data. Undertaking an SRS requires assembling a “sampling frame,” essentially a census. Then, with the sampling frame in hand, a computer or throws of the dice can accomplish the random selection for the sample.\nUnderstandably, if a census is unfeasible, constructing a perfect sampling frame is hardly less so. In practice, the sample is assembled by randomly dialing phone numbers or taking every 10th visitor to a clinic or similar means. Unlike genuinely random samples, the samples created by these practical methods are not necessarily representative of the larger group. For instance, many people will not answer a phone call from a stranger; such people are underrepresented in the sample. Similarly, the people who can get to the clinic may be healthier than those who cannot. Such unrepresentativeness is called “sampling bias.”\nProfessional work, such as collecting unemployment data, often requires government-level resources. Assembling representative samples uses specialized statistical techniques such as stratification and weighting of the results. We will not cover the specialized techniques in this introductory course, even though they are essential in creating representative samples. The table of contents of a classic text, William Cochran’s Sampling techniques shows what is involved.\nAll statistical thinkers, whether expert in sampling techniques or not, should be aware of factors that can bias a sample away from being representative. In political polls, many (most?) people will not respond to the questions. If this non-response stems from, for example, an expectation that the response will be unpopular, then the poll sample will not adequately reflect unpopular opinions. Such non-response bias can be significant, even overwhelming, in surveys.\nSurvival bias plays a role in many settings. The mosaicData::TenMileRace data frame provides an example, recording the running times of 8636 participants in a 10-mile road race and including information about each runner’s age. Can such data carry information about changes in running performance as people age? The data frame includes runners aged 10 to 87. Nevertheless, a model of running time as a function of age from this data frame is seriously biased. The reason? As people age, casual runners tend to drop out of such races. So the older runners are skewed toward higher performance. (We can see this by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.\n\n\n\n\n\n\nExamples: Returned to base\n\n\n\nAn inspiring story about dealing with survival bias comes from a World War II study of the damage sustained by bombers due to enemy guns. The sample, by necessity, included only those bombers that survived the mission and returned to base. The holes in those surviving bombers tell a story of survival bias. Shell holes on the surviving planes were clustered in certain areas, as depicted in Figure 22.1. The clustering stems from survivor bias. The unfortunate planes hit in the middle of the wings, cockpit, engines, and the back of the fuselage did not return to base. Shell hits in those areas never made it into the record.\n\n\n\n\n\nFigure 22.1: An illustration of shell-hole locations in planes that returned to base. Source: Wikipedia"
  },
  {
    "objectID": "Reading-notes-lesson-22.html#measuring-sampling-variation",
    "href": "Reading-notes-lesson-22.html#measuring-sampling-variation",
    "title": "22  Sampling and sampling variation",
    "section": "Measuring sampling variation",
    "text": "Measuring sampling variation\nSampling variation is a form of noise. Unlike some other forms of noise, modeling cannot filter out sampling variation or reduce its magnitude. Sampling variation is easiest to see by collecting multiple samples from the same source and summarizing each one. The summaries likely will vary from sample to sample: sampling variation.\nTypically, the data frame at hand is our only sample. With no other samples to compare it to, it may seem impossible to measure sampling variation. In this Lesson, we will use simulations from DAGs to study sampling variation. DAG simulations are suited to this because we can effortlessly collect as many samples as we wish from a DAG. In Lesson 23, we will use the knowledge gained from the simulations to see how to measure sampling variation even when there is only one sample.\nIn the spirit of starting simply, we return to dag01. This DAG is \\(\\mathtt{x}\\longrightarrow\\mathtt{y}\\). The causal formula setting the value of y is y ~ 4 + 1.5 * x + exo().\nIt is crucial to remember that sampling variation is not about the row-to-row variation in a single sample. Rather, it is about the variation in the summary from one sample to another. So our initial process for exploring sampling variation will be to carry out many trials, each trial being a summary of a sample."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#demonstration-sampling-trials",
    "href": "Reading-notes-lesson-22.html#demonstration-sampling-trials",
    "title": "22  Sampling and sampling variation",
    "section": "Demonstration: Sampling trials",
    "text": "Demonstration: Sampling trials\nA single sampling trial consists of taking a random sample and computing a sample statistic. To illustrate, here is one trial using a sample size \\(n=25\\) and a simple model modification, y ~ 1.\n\nSample <- sample(dag01, size=25) \nSample %>% \n  lm(y ~ 1, data = .) %>%\n  coef()\n\n(Intercept) \n   3.360868 \n\n\nWe cannot see sampling variation directly in the above result because there is only one trial. The sampling variation becomes evident when we run many trials. In each trial, a new sample (of size \\(n=25\\) is taken and summarized.)\n\nTrials <- do(500) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coef()\n}\n\nGraphics provide a nice way to visualize the sampling variation. Figure 22.2 shows the results from the set of trials.\n\n\n\n\n\nFigure 22.2: The sampling distribution as shown by 500 trials. Each dot is one trial where the model specification y~1 is fitted to a sample from dag01 of size \\(n=25\\).\n\n\n\n\nThe variance of the sampling distribution, that is, the sampling variance, is:\n\nTrials %>%\n  summarize(sampling_variance = var(Intercept))\n\n\n\n \n  \n    sampling_variance \n  \n \n\n  \n    0.122632 \n  \n\n\n\n\nOften, statisticians prefer to use the square root of the sampling variance, which has a technical name in statistics: the standard error. The standard error is an ordinary standard deviation in a particular context: the standard deviation of a sample of summaries. The words standard error should be followed by a description of the summary and the size of the individual samples involved. Here it would be, “The standard error of the Intercept coefficient from a sample of size \\(n=25\\) is around 0.36.”\nIt is easy to confuse “standard error” with “standard deviation.” Adding to the potential confusion is another related term, the “margin of error.” To avoid this confusion, we will tend to use an interval description of the sampling variation called the “confidence interval.” However, for the present, we will continue with the standard error, sometimes written SE for short."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#se-depends-on-the-sample-size",
    "href": "Reading-notes-lesson-22.html#se-depends-on-the-sample-size",
    "title": "22  Sampling and sampling variation",
    "section": "SE depends on the sample size",
    "text": "SE depends on the sample size\nWe found an SE of 0.36 on the Intercept in a sample of size \\(n=25\\). We can see how the SE depends on sample size by repeating the trials for several different sizes, say, \\(n=25\\), 100, 400, 1600, 6400, 25,000, and 100,000.\nThe following command estimates the SE a sample of size 400:\n\nTrials <- do(1000) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coef()\n}\nTrials %>% summarize(svar400 = var(Intercept),\n                     se400 = sd(Intercept))\n\n\n\n \n  \n    svar400 \n    se400 \n  \n \n\n  \n    0.12766 \n    0.3572954 \n  \n\n\n\n\nWe repeated this process for each of the other sample sizes. Table 22.1 reports the results.\n\n\n\n\n\n\n\nTable 22.1:  Results of repeating the sampling variability trials for samples of varying sizes. \n \n  \n    n \n    samping_variance \n    standard_error \n  \n \n\n  \n    25 \n    0.1296000 \n    0.3600 \n  \n  \n    100 \n    0.0361000 \n    0.1900 \n  \n  \n    400 \n    0.0082810 \n    0.0910 \n  \n  \n    1600 \n    0.0018490 \n    0.0430 \n  \n  \n    6400 \n    0.0005290 \n    0.0230 \n  \n  \n    25000 \n    0.0001210 \n    0.0110 \n  \n  \n    100000 \n    0.0000314 \n    0.0056 \n  \n\n\n\n\n\nThere is a pattern in Table 22.1. Every time we quadruple \\(n\\), the sampling variance goes down by a factor of four. Consequently, the standard error—which is just the square-root of the sampling variance—goes down by a factor of 2, that is, \\(\\sqrt{4}\\). (The pattern is not exact because there is also sampling variation in the trials, which are really just a sample of all possible trials.)\nConclusion: The larger the sample size, the smaller the sampling variance. For a sample of size \\(n\\), the sampling variance will be proportional to \\(1/n\\). Or, in terms of the standard error: For a sample size of \\(n\\), the SE will be proportional to \\(1/\\sqrt{\\strut n}\\)."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#the-confidence-interval",
    "href": "Reading-notes-lesson-22.html#the-confidence-interval",
    "title": "22  Sampling and sampling variation",
    "section": "The confidence interval",
    "text": "The confidence interval\nThe “confidence interval” is a more user-friendly format than SE for describing the amount of sampling variation. Being an interval, write it either as [lower, upper] or center\\(\\pm\\)half-width. These styles are equivalent; both styles are correct. (The preferred style can depend on the field or the journal publishing the report.)\nIn practice, confidence intervals are calculated using special-purpose software such as the conf_interval() function, for instance:\n\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    -533.432471 \n    -406.521402 \n  \n  \n    distance \n    246.387096 \n    261.229494 \n  \n  \n    climb \n    2.493307 \n    2.726209 \n  \n\n\n\n\nNotice that there is a separate confidence interval for each model coefficient. The sampling variation is essentially the same, but that variation appears different when translated to the various coefficients’ units.\n\n\n\n\n\n\nDemonstration: How many digits?\n\n\n\nThe confidence intervals on the model time ~ distance + climb, report the results to many digits. Such a report is appropriate for further calculations that might need doing, but it is usually not appropriate for a human reader.\nTo know how many digits are worth reporting to humans, look toward the standard error. The standard error is a part of a different kind of summary of a model: the “regression report.” We will only need to look at regression reports in the last few Lessons of the course. Here we want to point out how many digits are worth reporting to humans. That requires looking at the standard error itself.\nPreviously, we looked at the confidence intervals on coefficients from the Hill_racing model. Now we look at the regression summary, which contains the information on sampling variation in a different format.\n\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  regression_summary()\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    -469.976937 \n    32.3582241 \n    -14.52419 \n    0 \n  \n  \n    distance \n    253.808295 \n    3.7843322 \n    67.06819 \n    0 \n  \n  \n    climb \n    2.609758 \n    0.0593826 \n    43.94821 \n    0 \n  \n\n\n\n\nEach coefficient’s standard error appears in the std.error column of the regression summary.\nFor the human reader, only the first two significant digits of the standard error are worth reporting. (This is true regardless of the data and model design.) Here, the SE is 32 for the Intercept, 3.8 for the distance coefficient, and 0.059 for the climb coefficient. The confidence interval will be the coefficient (column labeled estimate) plus or minus “twice” the std.error. It is appropriate to round the confidence interval (for a human reader) to the first two significant digits of the standard error.\nFor example, the confidence interval on the distance coefficient will be \\(253.808295 \\pm 2 \\times 3.78433220\\). Keep only the digits before the first two significant digits of the SE, so the reported interval can be \\(253.8 \\pm 3.8\\)."
  },
  {
    "objectID": "Reading-notes-lesson-23.html#sec-subsampling",
    "href": "Reading-notes-lesson-23.html#sec-subsampling",
    "title": "23  Confidence intervals from a single sample",
    "section": "Subsampling",
    "text": "Subsampling\nTo “subsample” means to draw a smaller sample from a large one. “Small” and “large” are relative. For our example, we turn to the TenMileRace data frame containing the record of thousands of runners’ times in a race, along with basic information about each runner. There are many ways we could summarize TenMileRace. Any summary would do for the example. We will summarize the relationship between the runners’ ages and their start-to-finish times (variable net), that is, net ~ age. To avoid the complexity of a runner’s improvement with age followed by a decline, we will limit the study to people over 40.\n\nTenMileRace %>% filter(age > 40) %>%\n  lm(net ~ age, data = .) %>% coef()\n\n(Intercept)         age \n 4278.21279    28.13517 \n\n\nThe units of net are seconds, and the units of age are years. The model coefficient on age tells us how the net time changes for each additional year of age: seconds per year. Using the entire data frame, we see that the time to run the race gets longer by about 28 seconds per year. So a 45-year-old runner who completed this year’s 10-mile race in 3900 seconds (about 9.2 mph, a pretty good pace!) might expect that, in ten years, when she is 55 years old, her time will be longer by 280 seconds.\nIt would be asinine to report the ten-year change as 281.3517 seconds. The runner’s time ten years from now will be influenced by the weather, crowding, the course conditions, whether she finds a good pace runner, the training regime, improvements in shoe technology, injuries, and illnesses, among other factors. There is little or nothing we can say from the TenMileRace data about such factors.\nThere’s also sampling variation. There are 2898 people older than 40 in the TenMileRace data frame. The way the data was collected (radio-frequency interrogation of a dongle on the runner’s shoe) suggests that the data is a census of finishers. However, it is also fair to treat it as a sample of the kind of people who run such races. People might have been interested in running but had a schedule conflict, lived too far away, or missed their train to the start line in the city.\nWe see sampling variation by comparing multiple samples. To create those multiple samples from TenMileRace, we will draw, at random, subsamples of, say, one-tenth the size of the whole, that is, \\(n=290\\)\n\nOver40 <- TenMileRace %>% filter(age > 40)\nlm(time ~ age, data = Over40 %>% sample(size=290)) %>% coef()\n\n(Intercept)         age \n5838.669106    1.151848 \n\nlm(time ~ age, data = Over40 %>% sample(size=290)) %>% coef()\n\n(Intercept)         age \n  4287.9052     32.0135 \n\n\nThe age coefficients from these two subsampling trials differ one from the other by about 0.5 seconds. To get a more systematic view, run more trials:\n\n# a sample of summaries\nTrials <- do(1000) * {\n  lm(time ~ age, data = sample(Over40, size=290)) %>% coef()\n}\n# a summary of the sample of summaries\nTrials %>%\n  dplyr::summarize(se = sd(age))\n\n\n\n \n  \n    se \n  \n \n\n  \n    8.93232 \n  \n\n\n\n\nWe used the name se for the summary of samples of summaries because what we have calculated is the standard error of the age coefficient from samples of size \\(n=290\\).\nIn Lesson 22 we saw that the standard error is proportional to \\(1/\\sqrt{\\strut n}\\), where \\(n\\) is the sample size. From the subsamples, know that the SE for \\(n=290\\) is about 9.0 seconds. This tells us that the SE for the full \\(n=2898\\) samples would be about \\(9.0 \\frac{\\sqrt{290}}{\\sqrt{2898}} = 2.85\\).\nSo the interval summary of the age coefficient—the confidence interval— is \\[\\underbrace{28.1}_\\text{age coef.} \\pm 2\\times\\!\\!\\!\\!\\!\\!\\! \\underbrace{2.85}_\\text{standard error} =\\ \\ \\ \\  28.1 \\pm\\!\\!\\!\\!\\!\\!\\!\\! \\underbrace{5.6}_\\text{margin of error}\\ \\  \\text{or, equivalently, 22.6 to 33.6}\\]"
  },
  {
    "objectID": "Reading-notes-lesson-23.html#bootstrapping",
    "href": "Reading-notes-lesson-23.html#bootstrapping",
    "title": "23  Confidence intervals from a single sample",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nThere is a trick, called “resampling,” to generate a random subsample of a data frame with the same \\(n\\) as the data frame: draw the new sample randomly from the original sample with replacement. An example will suffice to show what the “with replacement” does:\n\nexample <- c(1,2,3,4,5)\n# without replacement\nsample(example)\n\n[1] 1 4 3 5 2\n\n# now, with replacement\nsample(example, replace=TRUE)\n\n[1] 2 4 3 3 5\n\nsample(example, replace=TRUE)\n\n[1] 3 5 4 4 4\n\nsample(example, replace=TRUE)\n\n[1] 1 1 2 2 3\n\nsample(example, replace=TRUE)\n\n[1] 4 3 1 4 5\n\n\nThe “with replacement” leads to the possibility that some values will be repeated two or more times and other values will be left out entirely.\nThe calculation of the SE using resampling is called “bootstrapping.”\n\n\n\n\n\n\nDemonstration: Bootstrapping the standard error\n\n\n\nWe will apply bootstrapping to find the standard error of the age coefficient from the model time ~ age fit to the Over40 data frame.\nThere are two steps:\n\nRun many trials, each of which fits the model time ~ age using lm(). From trial to trial, the data used for fitting is a resampling of the Over40 data frame. The result of each trial is the coefficients from the model.\nSummarize the trials with the standard deviation of the age coefficients.\n\n\n# run many trials\nTrials <- do(1000) * {\n  lm(time ~ age, data = sample(Over40, replace=TRUE)) %>% \n       coef()\n}\n# summarize the trials to find the SE\nTrials %>% summarize(se = sd(age))\n\n\n\n \n  \n    se \n  \n \n\n  \n    2.859483"
  },
  {
    "objectID": "Reading-notes-lesson-23.html#confidence-intervals-from-software",
    "href": "Reading-notes-lesson-23.html#confidence-intervals-from-software",
    "title": "23  Confidence intervals from a single sample",
    "section": "Confidence intervals from software",
    "text": "Confidence intervals from software\nThe same mathematical process that powers regression modeling software such as lm() can be used to compute standard errors for model coefficients as part of the fitting process. So, for regression models, finding a confidence interval is just a matter of asking for it.\n[Note: Experienced R users will know that the “standard” function for calculating confidence intervals is confint(), which is used in exactly the same manner as conf_interval(). Regrettably, confint() does not create a data frame. In keeping with these Lessons use of data wrangling, the conf_interval() from the {math300} package reformats the output of confint() into a data frame.]\nThere are several ways to do the asking. In R, the conf_interval() function makes it easy to extract the confidence intervals on each coefficient from a model. For example:\n\nlm(net ~ age + sex, data = TenMileRace) |> conf_interval() \n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    5270.45170 \n    5407.85920 \n  \n  \n    age \n    15.04242 \n    18.74483 \n  \n  \n    sexM \n    -765.85978 \n    -687.37917 \n  \n\n\n\n\nEach row of the result reports the confidence interval for one coefficient from the model."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#effect-size-input-to-output",
    "href": "Reading-notes-lesson-24.html#effect-size-input-to-output",
    "title": "24  Effect size",
    "section": "Effect size: Input to output",
    "text": "Effect size: Input to output\nAn intervention changes something in the world. Some examples are the budget for a program, the dose of a medicine, or the fuel flow into an engine. The thing being changed is the input. In response, something else in the world changes, for instance, the reading ability of students, the patient’s serotonin levels (a neurotransmitter), or the power output from the engine. The thing that changes in response to the change in input is called the “output.”\n“Effect size” describes the change in the output with respect to the change in the input. The simplest case is when the output is a quantitative variable. In this case, the change in the output is a difference between two numbers. The form of the effect size depends on the input type. For example, for a quantitative input, the effect size will be a ratio, that is, a rate. (For calculus students: the effect size is a derivative of the output with respect to the input.)\nTo measure an effect size from data, construct a model with the output as the response variable and the input as an explanatory variable.\n\n\n\n\n\n\nExample: Fuel economy\n\n\n\nA person buying a car typically has multiple objectives in mind. Perhaps the buyer is deciding whether to order a more powerful engine. This decision has consequences, including a reduction in fuel economy. The decision variable—the engine size—is the input; the fuel economy is the output.\nSince both input and output are quantitative, the effect size will be a rate: change in fuel economy per change in engine size. To inform a decision, use data such as the math300::MPG data frame, which compares various car models. MPG records the engine size in terms of displacement, in liters. Fuel economy is listed in miles per gallon, differently for city versus highway driving.\nThe buyer is debating between a 2-liter and a 3-liter engine. Most driving will be in the city. To calculate the effect size, first build a model with the output (mpg_city) as the response variable and the input (displacement) as an explanatory variable.\n\nMod <- lm(mpg_city ~ displacement, data=MPG)\n\nSecond, evaluate that model for the range of inputs under consideration.\n\nmodel_eval(Mod, displacement=c(2, 3))\n\n\n\n \n  \n    displacement \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    2 \n    24.01437 \n    15.91915 \n    32.10959 \n  \n  \n    3 \n    20.86976 \n    12.77698 \n    28.96254 \n  \n\n\n\n\nThe change in the input from 3 liters displacement to 2 liters leads to a change in fuel economy of \\(24.0 - 20.9 = -3.1\\) miles per gallon. The change in displacement is \\(3 - 2 = 1\\) liters. The effect size is the ratio between the output change and the input change. Here, that is -3.1 miles per gallon per liter.\nThe decision-maker may be more concerned about the cost of driving than with the miles per gallon. Then the appropriate response variable might be EPA_fuel_cost, denominated in dollars per year.\n\nMod2 <- lm(EPA_fuel_cost ~ displacement, data=MPG)\nmodel_eval(Mod2, displacement=c(2, 3))\n\n\n\n \n  \n    displacement \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    2 \n    1585.887 \n    1000.649 \n    2171.125 \n  \n  \n    3 \n    1882.534 \n    1297.473 \n    2467.596 \n  \n\n\n\n\nThe change in output is about $300 per year. However, the change in input is still 1 liter. The effect size is, therefore, $300 per year per liter.\n\n\nSome decision variables are categorical. For instance, the buyer might like the idea of an engine that automatically turns off when the car is stopped at a light or in traffic. The start_stop variable, which has categorical levels “Yes” and “No,” records whether the car has this feature. Effect size estimation is slightly different when the input is categorical rather than quantitative. Still, build a model and compare the change in output to the change in input:\n\nMod3 <- lm(EPA_fuel_cost ~ start_stop, data=MPG)\nmodel_eval(Mod3, start_stop=c(\"No\", \"Yes\"))\n\n\n\n \n  \n    start_stop \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    No \n    1872.193 \n    916.0164 \n    2828.369 \n  \n  \n    Yes \n    1945.194 \n    989.0637 \n    2901.324 \n  \n\n\n\n\nIn this case, the change in output is $73 per year; the change in input is “Yes” - “No.” But, of course, it is meaningless to subtract one categorical level from another. Consequently, the effect size of start_stop on fuel cost cannot be quantified as a ratio. So, instead, the effect size is simply the difference in the output: a $73 per year increase with the Start/Stop feature.\nThe statistical thinker knows to pay attention to whether a calculated result makes sense. It seems unlikely that the Start/Stop feature causes more fuel to be consumed. Was there an error? Perhaps we did the subtraction backward? Check the report from model_eval() to make sure.\nHere, the problem is not arithmetic. However, there is another possibility. It might be that manufacturers include the Start/Stop feature with big cars but not little ones. Then, even if Start/Stop might save gas when everything else is held constant, because the big cars use more fuel than little cars, it only appears that Start/Stop hurts fuel economy. This theory is, at this point, speculation: a hypothesis. Such a mixture of effects—big versus small car mixed with availability of Start/Stop—is called “confounding.” In Lessons 28 through 30, we discuss identifying and dealing with possible confounding.\n\n\n\n\n\n\nConfounding?\n\n\n\nThe surprising positive effect size of the Start/Stop feature caused a double take and led us to think of ways to make sense of the result. Right now, we simply have a hypothesis that Start/Stop is associated with bigger cars. (We will check that out in a little bit.)\nThe effect size of annual fuel cost with respect to engine displacement, $300 per year per liter, did not surprise us. Perhaps it should have. After all, larger vehicles tend to have larger engines. This relationship might lead to confounding between vehicle size and engine displacement. We think we are looking at engine displacement, but instead, the effect might be due to vehicle size. Again, just a hypothesis at this point. The statistical thinker knows to consider possible confounding from the start."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#categorical-outputs",
    "href": "Reading-notes-lesson-24.html#categorical-outputs",
    "title": "24  Effect size",
    "section": "Categorical outputs",
    "text": "Categorical outputs\nSometimes the relevant effect size involves a categorical output variable. A case in point is the possible confounding of the Start/Stop feature with vehicle size. To investigate this, we should build a model with Start/Stop as the output and vehicle size as the input.\nIn this case, the issue of whether vehicle size causes Start/Stop is not essential. We are not concerned with the decisions made by automobile designers so much as with the possible confounding.\nWhen the output variable is categorical, it is not reasonable to calculate the change in output as the difference in categories. As before, “Yes” - “No” is not a number. Still, there is a meaningful and helpful way to quantify a change in a categorical output.\nThe essential insight is quantifying the change in output in terms of probabilities. For instance, a small effect size would reflect a slight chance of the output changing from one level to another.\nThe appropriate model type for a categorical output is to transform the output to a zero-one variable, as introduced in Lesson 19. We will present this in a demonstration here and return to the topic more fully in Lesson 34.\n\n\n\n\n\n\nDemonstration: Start/Stop and vehicle size\n\n\n\nAs described earlier, we are interested in the possibility that Start/Stop is available mainly on large, higher-fuel-consumption cars. If so, that would explain why the effect size we calculated of fuel cost with respect to Start/Stop was positive.\nThe model we build will have a zero-one encoding of Start/Stop as the response and the vehicle’s fuel cost as the explanatory variable.\n\nMPG <- MPG %>% \n    mutate(has_start_stop = zero_one(start_stop, one=\"Yes\"))\nMod4 <- lm(has_start_stop ~ EPA_fuel_cost, data = MPG)\nmodel_eval(Mod4, EPA_fuel_cost=c(1600, 2000))\n\n\n\n \n  \n    EPA_fuel_cost \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    1600 \n    0.4901341 \n    -0.4891981 \n    1.469466 \n  \n  \n    2000 \n    0.5207835 \n    -0.4583924 \n    1.499959 \n  \n\n\n\n\nThe .output here is interpreted as a probability of start_stop having the value “Yes.” (That is because we set one=\"Yes\" in the zero_one() conversion.) The model_eval() report indicates $400 per year increase in fuel cost is associated with a three percentage point increase in the probability of a vehicle having a Start/Stop feature. That is a small effect, so we see little support for our hypothesis that Start/Stop tends to be installed on larger, more fuel-efficient vehicles."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#multiple-explanatory-variables",
    "href": "Reading-notes-lesson-24.html#multiple-explanatory-variables",
    "title": "24  Effect size",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\nWhen a model has more than one explanatory variable, each has a different effect size.\nAs an example, consider the price of books. We have some data that might be informative, moderndive::amazon_books. What is the effect size of page count on price. The appropriate model here is list_price ~ num_pages. The effect size is easy to compute:\n\nMod1 <- lm(list_price ~ num_pages, data = moderndive::amazon_books)\nmodel_eval(Mod1, num_pages = c(200, 400))\n\n\n\n \n  \n    num_pages \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    200 \n    15.82014 \n    -11.636987 \n    43.27726 \n  \n  \n    400 \n    19.79643 \n    -7.637503 \n    47.23037 \n  \n\n\n\n\nWe elected to compare 200-page books with 400-page books, simply because those seem like reasonable book lengths. However, the longer book costs about 4 dollars more. So the effect size, to judge from this model, is $4 divided by 200 more pages, which comes to 2 cents per page.\nAnother effect size is needed to address the question: Are hardcovers more expensive than paperbacks? The output is still price. But now, the input is categorical. In the moderndive::amazon_books data frame, the variable hard_paper has levels “P” and “H.” A possible model: list_price ~ hard_paper.\n\nMod2 <- lm(list_price ~ hard_paper, data = amazon_books)\nmodel_eval(Mod2, hard_paper = c(\"P\", \"H\"))\n\n\n\n \n  \n    hard_paper \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    P \n    17.13523 \n    -10.62291 \n    44.89338 \n  \n  \n    H \n    22.39393 \n    -5.46052 \n    50.24839 \n  \n\n\n\n\nA hardcover book costs about $5.25 more than a paperback book. Since the input is categorical, there is no change of input to divide by, so the effect size is $5.25 when going from a paperback to a hardcover.\nWe can look at the effects of page length and cover-type separately. Instead, we can include both as explanatory variables.\n\nMod3 <- lm(list_price ~ hard_paper + num_pages, data = amazon_books)\nmodel_eval(Mod3, hard_paper = c(\"P\", \"H\"),  num_pages=c(200, 400))\n\n\n\n \n  \n    hard_paper \n    num_pages \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    P \n    200 \n    14.52494 \n    -12.641928 \n    41.69182 \n  \n  \n    H \n    200 \n    19.48253 \n    -7.785720 \n    46.75077 \n  \n  \n    P \n    400 \n    18.43605 \n    -8.709404 \n    45.58151 \n  \n  \n    H \n    400 \n    23.39363 \n    -3.847698 \n    50.63497 \n  \n\n\n\n\nThis output requires some interpretation. We have got short and long paperback books and short and long hardcover books. What should we compare to what?\nThe convention is to consider each of the two inputs separately and hold the other input constant when we compare.\nEffect size of num_pages on list_price. To hold hard_paper constant, we will compare the two rows of the model_eval() report that have a “P” value for hard_paper. The difference in output for these two rows is $3.90. The effect size divides by the change in input—200 pages—so the effect size is just under 2 cents per page. Effect size of hard_paper on list_price. This time we will hold num_pages constant, say at 200 pages. Comparing the corresponding rows in the model_eval() output shows a change in list price of $4.96 when going from paper back to hard cover. There is no special reason we decided to hold hard_paper constant at “P” rather than “H” or hold num_pages constant at 200 rather than 400. In general, the effect size will depend on the value being held constant. Choose a value that’s relevant to the purpose at hand.\nIn these Lessons we are building models with additive effects.That is what the + means in, say, list_price ~ hard_paper + num_pages. We do this to keep the effect-size story as simple as possible. (Occasionally, you will see examples with multiplicative effects, called “interactions.” The tilde expressions for such models involve * rather than +, as in list_price ~ hard_paper + num_pages."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#interval-estimates",
    "href": "Reading-notes-lesson-24.html#interval-estimates",
    "title": "24  Effect size",
    "section": "Interval estimates",
    "text": "Interval estimates\nStatistical thinkers know that any estimate they make, including estimates of effect sizes, involves sampling variation. Consequently, give an interval estimate. The interval communicates to the decision-maker the uncertainty in the estimated quantity. Sophisticated decision-makers keep this uncertainty in mind, considering the range of outcomes likely from whatever use they make of effect size. On the other hand, statistically naive decision makers—even highly educated decision-makers can be statistically naive—look at the interval and sometimes ask the modeler, “Just give me a number. I don’t know what to do with two numbers.” Such a request might elicit a frank response: “If you don’t know what to do with two numbers, you also won’t know what to do with one number.” Unfortunately, that kind of frankness is not often well received; a reasonable alternative is: “The interval indicates the amount of uncertainty in the result. We’ll need to collect more data if you want to reduce the uncertainty.” (Lesson 29 introduces a not-always-available alternative to collecting more data: building a better model!)\nFor the additive models that we are mainly using in these Lessons, the effect size is identical to a model coefficient. For these models, the confidence interval on the coefficient is the confidence interval on the effect size. For instance,\n\nMod3 %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    7.0390179 \n    14.1886534 \n  \n  \n    hard_paperH \n    1.5580344 \n    8.3571295 \n  \n  \n    num_pages \n    0.0102362 \n    0.0288749"
  },
  {
    "objectID": "Reading-notes-lesson-25.html#the-prediction-machine",
    "href": "Reading-notes-lesson-25.html#the-prediction-machine",
    "title": "25  Mechanics of prediction",
    "section": "The prediction machine",
    "text": "The prediction machine\nA statistical prediction is the output of a kind of special-purpose machine. The inputs given to the machine are values for what we already know; the output is a value (or interval) for the as-yet-unknown aspects of the system.\nThere are always two phases involved in making a prediction. The first is building the prediction machine. The second phase is providing the machine with inputs for the individual case, turning the machine crank, and receiving the prediction as output.\nThese two phases require different sorts of data. Building the machine requires a “historical” data set that includes records from many instances where we already know two things: the values of the inputs and the observed output. The word “historical” emphasizes that the machine-building data must already have known values for each of the inputs and outputs of interest.\nThe evaluation phase—turning the crank of the machine—is simple. Take the input values for the individual to be predicted, put those inputs into the machine, and receive a predicted value as output. Those input values may come from pure speculation or the measured values from a specific case of interest."
  },
  {
    "objectID": "Reading-notes-lesson-25.html#building-and-using-the-machine",
    "href": "Reading-notes-lesson-25.html#building-and-using-the-machine",
    "title": "25  Mechanics of prediction",
    "section": "Building and using the machine",
    "text": "Building and using the machine\nTo illustrate building a prediction machine, we turn to a problem first considered quantitatively in the 1880s: the relationship between parents’ heights and their children’s heights at adulthood. The Galton data frame records the heights of about 900 children, along with their parents’ heights. Suppose we want to predict a child’s adult height (variable name: height) from his or her parents’ heights (mother and father). An appropriate model specification is height ~ mother + father. We use the model-training functionlm() to transform the model specification and the data into a model.\n\nMod1 <- lm(height ~ mother + father, data = Galton)\n\nAs the output of an R function, Mod1 is a computer object. It incorporates a variety of information organized in a somewhat complex way. There are several often-used ways to extract this information in ways that serve specific purposes.\nOne of the most common ways to see what is in a computer object like Mod1 is by printing:\n\nprint(Mod1)\n\n\nCall:\nlm(formula = height ~ mother + father, data = Galton)\n\nCoefficients:\n(Intercept)       mother       father  \n    22.3097       0.2832       0.3799  \n\n\nNewcomers to technical computing tend to confuse the printed form of an object with the object itself. For example, the Mod1 object contains many components, but the printed form displays only two: the model coefficients and the command used to construct the object.\nWe have already used some other functions to extract information from a model object. For instance,\n\nMod1 %>% coef()\n\n(Intercept)      mother      father \n 22.3097055   0.2832145   0.3798970 \n\nMod1 %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    13.8569119 \n    30.7624990 \n  \n  \n    mother \n    0.1867750 \n    0.3796540 \n  \n  \n    father \n    0.2898301 \n    0.4699639 \n  \n\n\n\nMod1 %>% regression_summary()\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    22.3097055 \n    4.3068968 \n    5.179995 \n    3e-07 \n  \n  \n    mother \n    0.2832145 \n    0.0491382 \n    5.763635 \n    0e+00 \n  \n  \n    father \n    0.3798970 \n    0.0458912 \n    8.278209 \n    0e+00 \n  \n\n\n\n\nAnother extractor, model_eval(), is particularly convenient for prediction. Perhaps the most common use is to provide new input values to the model function, with model_eval() producing a data frame showing the output of the model function. To illustrate, here is how to calculate the predicted height of the child of a 63-inch-tall mother and a 68-inch father.\n\nMod1 %>% model_eval(mother = 63, father=68)\n\n\n\n \n  \n    mother \n    father \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    63 \n    68 \n    65.98521 \n    59.33448 \n    72.63594 \n  \n\n\n\n\nThe data frame includes the input values along with a point value for the prediction (.output) and a prediction interval (.lwr to .upr).\nNaturally, the predictions depend on the explanatory variables used in the model. For example, here is a model that uses only sex to predict the child’s height:\n\nMod2 <- lm(height ~ sex, data = Galton)\nMod2 %>% model_eval(sex=c(\"F\", \"M\"))\n\n\n\n \n  \n    sex \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    F \n    64.11016 \n    59.18024 \n    69.04009 \n  \n  \n    M \n    69.22882 \n    64.29928 \n    74.15835 \n  \n\n\n\n\nThis model includes three explanatory variables:\n\nMod3 <- lm(height ~ mother + father + sex, data = Galton)\nMod3 %>% model_eval(mother=63, father=68, sex=c(\"F\", \"M\"))\n\n\n\n \n  \n    mother \n    father \n    sex \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    63 \n    68 \n    F \n    63.20546 \n    58.97128 \n    67.43964 \n  \n  \n    63 \n    68 \n    M \n    68.43141 \n    64.19783 \n    72.66499 \n  \n\n\n\n\nIn Lesson 26, we will look at the components that make up the prediction interval and some ways to use it."
  },
  {
    "objectID": "Reading-notes-lesson-26.html#where-does-the-prediction-interval-come-from",
    "href": "Reading-notes-lesson-26.html#where-does-the-prediction-interval-come-from",
    "title": "26  Constructing a prediction interval",
    "section": "Where does the prediction interval come from",
    "text": "Where does the prediction interval come from\nThe prediction interval has two distinct components:\n\nThe uncertainty in the model function and hence in the output of the model function.\nThe size of the residuals found when training the model.\n\nConsider first the model function. For the running-time model, we can construct the model function from the coefficients of the linear model. These are:\n\nTime_mod %>% coef()\n\n(Intercept)    distance       climb \n-469.976937  253.808295    2.609758 \n\n\nThe algebraic expression for the model function is straightforward: \\[t(d, c) \\equiv -470 + 254 d  + 2.61 c\\ .\\]\nThe statistical thinker knows that such coefficients have uncertainty due to sampling variation. That uncertainty is, of course, quantified by the confidence interval.\n\nTime_mod %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    -533.432471 \n    -406.521402 \n  \n  \n    distance \n    246.387096 \n    261.229494 \n  \n  \n    climb \n    2.493307 \n    2.726209 \n  \n\n\n\n\nSince we cannot legitimately claim to know the values of the coefficients any better than indicated by these confidence intervals, we ought to temper our claims about the model function so that it reflects the uncertainty in the coefficients. For instance, we might provide an interval for the model output, using in an “upper” function the high ends of the confidence intervals on the coefficients and another “lower” function that uses the low ends of the confidence interval. Like this:\n\\[t_{upr}(d,c) \\equiv -407 + 261 d + 2.72 c\\\\\nt_{lwr}(d,c) \\equiv -533 + 246 d + 2.49 c\\]\nEvaluate both the lower and upper functions to get an interval on the model output. That would give us \\(t_{lwr}(10, 500) = 3172\\) and \\(t_{upr}(10, 500) = 3569\\).\nThis idea for generating the “lower” and “upper” functions has the right spirit but is not on target mathematically. The reason is that using the low end of the confidence interval for all coefficients is overly pessimistic; usually, the uncertainty in the different coefficients cancels out to some extent.\nThe mathematics for the correct “lower” and “upper” functions are well understood but too advanced for the general reader. For our purposes, it suffices to know that model_eval() knows how to do the calculations correctly.\nThe prediction interval produced by model_eval() includes both components (1) and (2) listed above. Insofar as we are interested in component (1) in isolation, the correct sort of interval—a confidence interval—can be requested from model_eval().\n\nTime_mod %>% \n    model_eval(distance=10, climb=500, interval=\"confidence\")\n\n\n\n \n  \n    distance \n    climb \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    10 \n    500 \n    3372.985 \n    3335.264 \n    3410.706 \n  \n\n\n\n\nThis report shows that the confidence interval on the model output—that is, just component (1) of the prediction interval—is pretty narrow: 3335 seconds to 3411 seconds, or, in plus-or-minus format, \\(3373 \\pm 38\\) seconds.\nThe prediction interval—that is, the sum of components (1) and (2)—is comparatively huge: 1700 to 5100 seconds or, in plus-or-minus format, \\(3400 \\pm 1700\\) seconds. That is almost 50 times wider than the confidence interval.\nWhy is the prediction interval so much more comprehensive than the confidence interval? The confidence interval reports on the sampling variation of a model constructed as an average over all the data, the \\(n=2236\\) participants recorded in the Hill_racing data frame. However, each runner in Hill_racing has their own individual time: not an average but just for the individual. The individual value might be larger or smaller than the average. How much larger or smaller? The residuals for the model provide this information. As always, we can measure the individual-to-individual variation with the standard deviation.\n\nTime_mod %>% model_eval() %>% summarize(se_residuals = sd(.resid))\n\nUsing training data as input to model_eval().\n\n\n\n\n \n  \n    se_residuals \n  \n \n\n  \n    870.6588 \n  \n\n\n\n\nKeeping in mind that the overall spread of the residuals is plus-or-minus “twice” the standard deviation of the residuals, we can say that the residuals indicate an additional uncertainty in the prediction for a runner of about \\(\\pm 1700\\) seconds. This \\(\\pm 1700\\) second is our estimate of the noise in the measurements. In contrast, the confidence interval is about the sampling variation in the signal.\nIn this case, the prediction interval is wholly dominated by noise; the sampling variability contributes only a tiny amount of additional uncertainty.\n\n\n\n\n\n\nExample: Graphics for the prediction interval\n\n\n\nWe shift the running scene from Scotland to Washington, DC. The race now is a single 10-miler with almost 9000 registered participants. We wish to predict the running time of an individual based on his or her age.\n\nAge_mod <- lm(net ~ age, data = TenMileRace)\n\nWe can see the prediction interval for an individual runner using mod_eval(). For example, here it is for a 23-year-old.\n\nAge_mod %>% model_eval(age=23)\n\n\n\n \n  \n    age \n    .output \n    .lwr \n    .upr \n  \n \n\n  \n    23 \n    5485.587 \n    3592.054 \n    7379.119 \n  \n\n\n\n\nWe can also calculate the prediction interval for several different ages and graph out the results with the “errorbar” glyph:\n\nAge_mod %>% \n  model_eval(age=c(20,25,30,35,40,45,50,55,60,65,70,75)) %>%\n  ggplot(aes(x=age)) +\n  geom_errorbar(aes(ymin=.lwr, ymax=.upr))\n\n\n\n\nFor convenience, the model_plot() function will do this work for us, plotting the prediction interval along with the training data. We can also direct model_plot() to show the confidence interval.\n\n### #| column: page-right\nmodel_plot(Age_mod, x=age, interval=\"prediction\", data_alpha=0.05)\nmodel_plot(Age_mod, x=age, interval=\"confidence\", data_alpha=0.05)\n\n\n\n\n\n\n\n(a) prediction interval\n\n\n\n\n\n\n\n(b) confidence interval\n\n\n\n\nFigure 26.1: The prediction and model-function confidence intervals for the model net ~ age.\n\n\n\nSince we are looking at the intervals as a function of an input variable, what we formerly showed using the “errorbar” glyph is now shown using a ribbon or band.\n\n\nNotice that the prediction interval covers almost all of the data points. There are hundreds of data points outside the interval, but with almost 9000 rows in the TenMileRace data frame, an interval that covers 95% of the data will have about 450 rows outside the interval.\nSuch a prediction interval is of little use; it cannot give a precise prediction about the running time of an individual. The honest prediction of an individual’s outcome needs to reflect the spread of all the individuals with a similar age.\nIn contrast, the confidence band on the model function is pleasingly narrow and precise. It covers only a tiny fraction of the raw data. For this very reason, the confidence interval is inappropriate for presenting a prediction. As always, confidence intervals only show general trends in the data, not the range of results for an individual prediction. For instance, Figure 26.1 shows a clear upward trend in running time with age. There is no flat or negatively sloping line compatible with the confidence interval.\nTo summarize:\n\nWhen making a prediction, report a prediction interval.\nThe prediction interval is always larger than the confidence interval and is usually much larger.\n\nThe confidence interval is not for predictions. Use a confidence interval when looking at an effect size. Graphically, the confidence interval is to indicate whether there is an overall trend in the model."
  },
  {
    "objectID": "QR1.html",
    "href": "QR1.html",
    "title": "27  Review of Lessons 19-26",
    "section": "",
    "text": "Warning\n\n\n\nI’ll put learning challenges here. The class day will be given over to the QR."
  },
  {
    "objectID": "Reading-notes-lesson-28.html#all-other-things-being-equal",
    "href": "Reading-notes-lesson-28.html#all-other-things-being-equal",
    "title": "28  Covariates",
    "section": "All other things being equal",
    "text": "All other things being equal\nThe common phrase “all other things being equal” is a critical qualifier in describing relationships. To illustrate: A simple claim in economics is that a high price for a commodity reduces the demand. For example, increasing the price of heating fuel will reduce demand as people turn down thermostats to save money. Nevertheless, the claim can be considered obvious only with the qualifier all other things being equal. For instance, the fuel price might have increased because winter weather has increased the demand for heating compared to summer. Thus, higher prices may be associated with higher demand. Therefore, increased price may not be associated with lower demand unless holding other variables, such as weather conditions, constant.\nIn economics, the Latin equivalent of “all other things being equal” is sometimes used: “ceteris paribus”. So, the economics claim would be, “higher prices are associated with lower demand, ceteris paribus.”\nAlthough the phrase “all other things being equal” has a logical simplicity, it is impractical to implement “all.” So instead of the blanket “all other things,” it is helpful to consider just “some other things” to be held constant, being explicit about what those things are. Other phrases along the same lines are “taking into account …” and “controlling for ….” Those additional variables that are to be considered are called “covariates.\n\n\n\n\n\n\nExample: Covariates and Death\n\n\n\nThis news report appeared in 2007:\n\nHeart Surgery Drug Carries High Risk, Study Says. A drug widely used to prevent excessive bleeding during heart surgery appears to raise the risk of dying in the five years afterward by nearly 50 percent, an international study found. The researchers said replacing the drug—aprotinin, sold by Bayer under the brand name Trasylol—with other, cheaper drugs for a year would prevent 10,000 deaths worldwide over the next five years.\nBayer said in a statement that the findings are unreliable because Trasylol tends to be used in more complex operations, and the researchers’ statistical analysis did not fully account for the complexity of the surgery cases. The study followed 3,876 patients who had heart bypass surgery at 62 medical centers in 16 nations. Researchers compared patients who received aprotinin to patients who got other drugs or no antibleeding drugs. Over five years, 20.8 percent of the aprotinin patients died, versus 12.7 percent of the patients who received no antibleeding drug. [This is a 64% increase in the death rate.] When researchers adjusted for other factors, they found that patients who got Trasylol ran a 48 percent higher risk of dying in the five years afterward. The other drugs, both cheaper generics, did not raise the risk of death significantly.  The study was not a randomized trial, meaning that it did not randomly assign patients to get aprotinin or not. In their analysis, the researchers took into account how sick patients were before surgery, but they acknowledged that some factors they did not account for may have contributed to the extra deaths. - Carla K. Johnson, Associated Press, 7 Feb. 2007\n\nThe report involves several variables. Of primary interest is the relationship between (1) the risk of dying after surgery and (2) the drug used to prevent excessive bleeding during surgery. Also potentially important are (3) the complexity of the surgical operation and (4) how sick the patients were before surgery. Bayer disputes the published results of the relationship between (1) and (2) holding (4) constant, saying that it is also essential to hold variable (3) constant.\nThe total relationship involves a death rate of 20.8 percent of patients who got aprotinin versus 12.7 percent for the patients taking the generic drugs: an increase in the death rate by a factor of 1.64. However, when the researchers looked at a partial relationship (holding constant patient sickness before the operation), the effect size of aprotinin on mortality was less: a factor of 1.48. In other words, the model death ~ aprotinin shows a 64% increase in the death rate, but the model death ~ aprotinin + sickness shows a slightly smaller increase in death rate: 48%. The difference between the two estimates reflects doctors being more likely to give aprotinin to sicker patients.\nThe story’s last paragraph states that the choice of patients receiving aprotinin versus the generic drugs was not made at random. Some readers may find this reassuring. Why in the world would anyone prescribe a drug at random? The point, however, is to select randomly who gets which drug among the patients for whom the drugs would be appropriate. The phrase “randomized trial” used in the paragraph means specifically an experiment in which one treatment or the other—aprotinin versus the generic drugs—is assigned at random. The virtues of experiment and the vital role of random assignment are detailed in Lesson 32.\n\n\n“Significant” has a specialized meaning in statistical language. It is not a synonym for “important.” See Lessons 36 through 38"
  },
  {
    "objectID": "Reading-notes-lesson-28.html#letting-things-change-as-they-will",
    "href": "Reading-notes-lesson-28.html#letting-things-change-as-they-will",
    "title": "28  Covariates",
    "section": "Letting things change as they will",
    "text": "Letting things change as they will\nUsing covariates in models enables the relationship between a response and an explanatory variable to be described ceteris paribus, that is, “all other things being equal.” Another phrase used in news stories is “after adjusting for ….” This is appropriate since the all in “all other things” is, in reality, refers only to those particular factors used as the covariates in the model. So, Dr. Meyer’s foot width results might be stated in everyday language as, “After adjusting for foot width, she found no difference in the widths of girls’ and boys’ feet.”\nNot including covariates in a model amounts to “letting other things change as they will.” In Latin, this is “mutatis mutandis.” In the foot-width example, the model width ~ sex looks at the differences in foot width for the two sexes. However, sex is not the only thing associated with foot width. The model width ~ sex ignores all other factors than sex; it compares boys and girls mutatis mutandis, that is, letting other things change as they will. In this case, comparing boys and girls involves not just the possible differences in foot width but also the differences in other factors such as foot length and body weight.\n\n\n\n\n\n\nExample: One change can bring another\n\n\n\nI was once involved in a budget committee that recommended employee health benefits for the college where I worked. At the time, college employees who belonged to the college’s insurance plan received a generous subsidy for their health insurance costs. Employees who did not belong to the plan received no subsidy but were given a modest monthly cash payment. After the stock market crashed in 2000, the college needed to cut budgets. One proposal called for eliminating the cash payment to employees who did not belong to the insurance plan. Proponents of the plan claimed that this would save money without reducing health benefits. I argued that this claim was an “all other things being equal” analysis: how expenditures would change assuming the number of people belonging to the insurance plan remained constant. In reality, however, the policy change would play out mutatis matandis; the loss of the cash payment would cause some employees, who currently received health benefits through their spouse’s health plan, to switch to the college’s health plan. That is what happened, contributing to an overall increase in healthcare expenses.\n\n\n\n\n\n\n\n\nExample: Spending and student performance\n\n\n\nTo illustrate how covariates set context, consider an issue of interest to public policy-makers in many societies: How much money to spend on children’s education? State lawmakers in the US are understandably concerned with the quality of public education provided. However, they also have other concerns and constraints and constituencies who give budget priority to other matters.\nIn evaluating their various trade-offs, lawmakers could benefit by knowing how increased educational spending will shape educational outcomes. What can available data tell us? Unfortunately, there are various political constraints that work against states adopting and publishing data on a standard, genuine measure of educational outcome. Instead, we have high-school graduation rates, student grades, and other non-standardized data. These data might have some meaning but can also reflect system gaming by administrators and teachers, for which there is little systematic data.\nAlthough imperfect, college admissions tests such as the ACT and SAT provide consistent data between states. For example, Figure 28.1 shows the average SAT score in 2010 in each state versus expenditures per pupil in public elementary and secondary schools. Layered on top of the data is a flexible linear model (and its confidence band) of SAT score versus expenditure.\nThe overall impression given by the model is that the relationship is negative, with lower expenditures corresponding to higher SAT scores. However, the confidence band is broad; it is possible to find a smooth path with almost zero slope through the confidence band. Either way, this graph does not support the conventional wisdom that higher spending produces better school outcomes.\n\n\n\n\n\nFigure 28.1: State by state data (from 2010) on average SAT college admissions test scores and expenditures for public education.\n\n\n\n\nOf course, other factors play a role in shaping education outcomes: for instance, poverty levels, parental education, and how the educational money is spent (higher pay for teachers or smaller class sizes? administrative bloat?).\nAt first glance, it is tempting to ignore these additional factors. For instance, we may not have data on them. Moreover, as our interest is in understanding the relationship between expenditures and education outcomes, we are not directly concerned with the additional factors. However, the lack of direct concern does not imply that we should ignore the factors but that we should do what we can to “hold them constant”.\nTo illustrate, consider the fraction of eligible students (those in their last year of high school) who take the college admission test. This fraction varies widely from state to state. In a poor state where few students go to college, the fraction can be tiny (Alabama 8%, Arkansas 5%, Mississippi 4%, Louisiana 8%). In some other states, the large majority of students take the SAT (Maine 93%, Massachusetts 89%, New York 89%). In states with low SAT participation rates, the students who take the test tend to be those applying to schools with competitive admissions. Such strong students will get high scores. In contrast, the scores in states with high participation rates reflect both strong and weak students. Consequently, the scores will be lower on average than in the low-participation states.\nPutting the relationship between expenditure and SAT scores in the context of the fraction taking the SAT is accomplished with the model SAT ~ expenditure + fraction rather than just SAT ~ expenditure. Figure 28.2 shows a model with fraction as a covariate.\n\n\n\n\n\nFigure 28.2: The model of SAT score versus expenditures, including as a covariate the fraction of eligible students in the state who take the SAT.\n\n\n\n\nNote that the effect size of spending on SAT scores is positive when the expenditure level is less than $10,000 per pupil. Notice as well that when the fraction taking the SAT is tiny, the average scores do not depend on expenditure. This flat relationship suggests that, among elite students, state expenditure does not make a discernible difference. Perhaps the college-bound students in such states have other educational resources to draw on.\nThe relationship shown in Figure 28.1 is genuine. However, so is the very different relationship seen in Figure 28.2. How can the same data be consistent with two utterly different displays? The answer, perhaps unexpectedly, has to do with the connections among the explanatory variables. Whatever the relationship between an individual explanatory variable and the response variable, the appearance of that relationship will depend on which covariates the modeler chooses to include."
  },
  {
    "objectID": "Reading-notes-lesson-29.html#how-much-variation-is-explained",
    "href": "Reading-notes-lesson-29.html#how-much-variation-is-explained",
    "title": "29  Covariates eat variance",
    "section": "How much variation is explained",
    "text": "How much variation is explained\nWe start by returning to the definition of statistical thinking introduced at the start of these Lessons:\n\nStatistic thinking is the explanation or description of variation in the context of what remains unexplained or undescribed.\n\nIn this Lesson, we will work with a straightforward measure of “what remains unexplained or undescribed.” The fitted model values represent the explained part of the variation. The residuals are what is left over, the difference between the actual values of the response variable and the fitted model values.\nAs a reminder, we will construct a simple model of the list price of books as a function of the number of pages and whether the book is a paperback or hardcover.1\n\n\n\n\nPrice_model <- lm(list_price ~ num_pages + hard_paper, \n                  data = amazon_books)\n\nThe model_eval() function can extract the fitted model values and the residuals from the model. We show just a few rows here, but we will use the entire report from model_eval(). Remember that when model_eval() is not given input values, it uses the model training data as input.\n\nResults <- model_eval(Price_model)\n\nUsing training data as input to model_eval().\n\n\n\n\n\n\n \n  \n    .response \n    num_pages \n    hard_paper \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    12.95 \n    304 \n    P \n    16.60 \n    -3.65 \n    -10.73 \n    43.94 \n  \n  \n    15.00 \n    273 \n    P \n    15.98 \n    -0.98 \n    -11.36 \n    43.32 \n  \n  \n    1.50 \n    96 \n    P \n    12.45 \n    -10.95 \n    -14.97 \n    39.88 \n  \n  \n    15.99 \n    672 \n    P \n    23.95 \n    -7.96 \n    -3.57 \n    51.47 \n  \n  \n    30.50 \n    720 \n    P \n    24.91 \n    5.59 \n    -2.67 \n    52.49 \n  \n  \n    28.95 \n    460 \n    H \n    24.57 \n    4.38 \n    -2.89 \n    52.03 \n  \n\n\n\n\nThe first book in the training data is a 304-page paperback with a list price of $12.95. The fitted model value for that book is $16.60. (Ordinarily, we refer to the output of the model function simply as the “output” or the “model output.” However, the output of the model function, when applied to rows from the training data also called the fitted model value.)\nAt $16.60, the fitted model value is $3.65 higher than the list price. This difference is the residual for that book, the sign reflecting the definition \\[\\text{residual} \\equiv \\text{response value} - \\text{fitted model value}\\ .\\] When the residual is small in magnitude, the fitted model value is close to the response value. Conversely, a large residual means the model was way off target for that book.\nThe standard measure of the typical size of a residual is the standard deviation or, equivalently, the variance.\n\nResults %>% summarize(se_resids = sd(.resid), v_resids=var(.resid))\n\n\n\n \n  \n    se_resids \n    v_resids \n  \n \n\n  \n    13.81885 \n    190.9606 \n  \n\n\n\n\nAs always, the standard deviation is easier to read because it has sensible units, in this case, dollars. On the other hand, the variance has strange units (square dollars) because it is the square of the standard deviation. We will use the variance for measuring the typical size of a residual for the reasons described in Lesson 20; variances add nicely in a manner analogous to the Pythagorean Theorem.\nA simple measure of how much of the variation in the response variable remains unexplained is the ratio of the variance of the residuals and the variance of the response variable.\n\nResults %>% summarize(unexplained_fraction = var(.resid)/var(.response))\n\n\n\n \n  \n    unexplained_fraction \n  \n \n\n  \n    0.9246907 \n  \n\n\n\n\nMore than 90% of the variation remains unexplained by the Price_model! This high fraction of unexplained variance suggests the model has little to tell us. In the spirit of putting a positive spin on things, statisticians typically work with the complement of the unexplained fraction. Since the unexplained fraction is 92.5%, the complement is 7.5%. This number is written R2 and pronounced “R-squared.” (It also has a formal name: the “coefficient of determination.” In Lesson 30, we will meet the inventor of the coefficient of determination, Sewall Wright, who is an early hero of causal reasoning.)\nR2 is such a widely used summary of how the explanatory variables account for the response variable that a software extractor calculates it and some related values.\n\nPrice_model %>% R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    317 \n    2 \n    0.0753093 \n    12.78651 \n    0.0694196 \n  \n\n\n\n\nMany modelers act as if their goal is to build a model that makes R2 as big as possible. Their thinking is that large R2 means that the explanatory variables account for much of the response variable’s variance. Unfortunately, it is a naive goal. Instead, always focus on the model’s suitability for the purpose at hand. Often, shooting for a large R2 imposes costs that can undermine the purpose for the model. Furthermore, even models with the largest possible R2 sometimes have nothing to say about the response variable."
  },
  {
    "objectID": "Reading-notes-lesson-29.html#getting-to-1",
    "href": "Reading-notes-lesson-29.html#getting-to-1",
    "title": "29  Covariates eat variance",
    "section": "Getting to 1",
    "text": "Getting to 1\nR2 can range from zero to one. Zero means that the model accounts for none of the variation in the response variable. We can construct such a model quickly enough: list_price ~ 1 has no explanatory variables and, therefore, no ability to distinguish one book from another.\n\nNull_model <- lm(list_price ~ 1, data = amazon_books)\nNull_model %>% R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    319 \n    0 \n    0 \n    NaN \n    0 \n  \n\n\n\n\nWe are using the word “null” to name this model. “Null” is part of the statistics tradition. The dictionary definition of “null” is “having or associated with the value zero” or “lacking distinctive qualities; having no positive substance or content.”2\nIn the null model, the fitted model values are all the same; all the variation is in the residuals.\n\nNull_model %>% model_eval()\n\nUsing training data as input to model_eval().\n\n\n\n\nUsing training data as input to model_eval().\n\n\n\n\n \n  \n    .response \n    .output \n    .resid \n    .lwr \n    .upr \n  \n \n\n  \n    12.95 \n    18.6 \n    -5.65 \n    -9.69 \n    46.89 \n  \n  \n    15.00 \n    18.6 \n    -3.60 \n    -9.69 \n    46.89 \n  \n  \n    1.50 \n    18.6 \n    -17.10 \n    -9.69 \n    46.89 \n  \n  \n    15.99 \n    18.6 \n    -2.61 \n    -9.69 \n    46.89 \n  \n  \n    30.50 \n    18.6 \n    11.90 \n    -9.69 \n    46.89 \n  \n  \n    28.95 \n    18.6 \n    10.35 \n    -9.69 \n    46.89 \n  \n\n\n\n\n\nAt the other extreme, where R2 = 1, the explanatory variables account for every bit of variation in the response variable. We can try various combinations of explanatory variables to see if we can accomplish this. For example, publisher explains 67% of the variation in list price.\n\nlm(list_price ~ publisher, data = amazon_books) %>% R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    319 \n    158 \n    0.6749786 \n    2.103008 \n    0.3540199 \n  \n\n\n\n\nWe can also check whether author has anything to say about the list price.\n\nlm(list_price ~ author, data = amazon_books) %>% R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    319 \n    250 \n    0.9434046 \n    4.534044 \n    0.7353333 \n  \n\n\n\n\nIncredible! How about if we use both publisher and author as explanatory variables? We get very close to R2 = 1.\n\nlm(list_price ~ publisher + author, data = amazon_books) %>%\n  R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    319 \n    281 \n    0.9821609 \n    7.249441 \n    0.84668 \n  \n\n\n\n\nThe modeler discovering this tremendous explanatory power of publisher and author can be forgiven for thinking he or she has found a meaningful explanation. But, unfortunately, the high R2 is an illusion in this case.\nTo see why, consider another possible explanatory variable, the International Standard Book Number (ISBN). The ISBN is a ten- or thirteen-digit number that marks each book with a unique number.\n\n\n\n\n\n\nFigure 29.1: The ISBN number from one of the Project MOSAIC textbooks.\n\n\n\nThere is a system behind ISBNs, but despite the “N” standing for “number,” an ISBN is a character string or word (written using only digits). Consequently, the isbn_10 variable in amazon_booksis categorical.\n\nISBN_model <- lm(list_price ~ isbn_10, data = amazon_books)\nISBN_model %>% R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    319 \n    318 \n    1 \n    NaN \n    NaN \n  \n\n\n\n\nThe isbn_10 explains all variation in the list price!\nGiven that the ISBN is, as we have said, an arbitrary sequence of characters, why does it do such a good job of accounting for the list price? The answer lies not in the content of the ISBN but in another fact: each book has a unique ISBN. As well, each book has a single price. So the ISBN identifies the price of each book. Cleverness is not involved; the list price could be anything, and the ISBN would still identify it precisely. The model coefficients store the whole set of ISBNs and the corresponding set of list prices.\nWe can substantiate the claim just made—that the list price could be anything at all—by synthesizing a data frame with random list prices:\n\namazon_books %>% \n  mutate(random_list_price = rnorm(nrow(.))) %>%\n  lm(random_list_price ~ isbn_10, data = .) %>%\n  R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    319 \n    318 \n    1 \n    NaN \n    NaN \n  \n\n\n\n\nSimilar randomization can be accomplished by shuffling the isbn_10 column of the data frame so that each ISBN points to a random book. Of course, such shuffling destroys the link between the ISBN and the list price. Even so, the R2 remains high.\n\nlm(list_price ~ shuffle(isbn_10), data=amazon_books) %>% R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    319 \n    318 \n    1 \n    NaN \n    NaN \n  \n\n\n\nlm(shuffle(list_price) ~ isbn_10, data=amazon_books) %>% R2()\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    319 \n    318 \n    1 \n    NaN \n    NaN \n  \n\n\n\n\nStatistical nomenclature is obscure here. So we will make up a name for such incidental alignment with no true explanatory power: the “ISBN-effect.”\nStatistical thinkers know to be aware of situations where categorical variables have many levels and check whether the ISBN effect is in play."
  },
  {
    "objectID": "Reading-notes-lesson-29.html#the-isbn-effect-as-a-benchmark",
    "href": "Reading-notes-lesson-29.html#the-isbn-effect-as-a-benchmark",
    "title": "29  Covariates eat variance",
    "section": "The ISBN effect as a benchmark",
    "text": "The ISBN effect as a benchmark\nShuffling an explanatory variable (while keeping the response variable in the original order) voids any possible explanatory connection between the two. An R2=0, as we get from any model of the form y ~ 1, signals that the 1 cannot account for any variation. However, this does not mean shuffling will lead to R2 = 0. Instead, there is a systematic relationship between the number of model coefficients associated with the shuffled variable, the sample size \\(n\\), and R2.\nWe can demonstrate this relationship by conducting many trials of modeling the list_price with a shuffled explanatory variable: either publisher, author, or isbn_10.\n\n\n\n\n\n\nDemonstration: Counting coefficients\n\n\n\nThe amazon_books data frame has \\(n=319\\) rows.3 In the next computing chunk, we fit the model list_price ~ publisher and collect the coefficients for counting:\n\nPublisher_model <- lm(list_price ~ shuffle(publisher),\n                      data=amazon_books)\nCoefficients <- Publisher_model %>% coef() %>% data.frame()\nnrow(Coefficients)\n\n[1] 159\n\n\nThere are 161 coefficients in the model, the first one being the “Intercept.” We will show only the first few.\n\nCoefficients %>% head()\n\n\n\n\n\n \n  \n      \n    value \n  \n \n\n  \n    (Intercept) \n    14.95 \n  \n  \n    shuffle(publisher)Adams Media \n    0.05 \n  \n  \n    shuffle(publisher)Akashic Books \n    13.00 \n  \n  \n    shuffle(publisher)Aladdin \n    15.05 \n  \n  \n    shuffle(publisher)Albert Whitman & Company \n    -0.95 \n  \n  \n    shuffle(publisher)Alfred A. Knopf \n    0.05 \n  \n\n\n\n\nAltogether, there are \\(k=160\\) coefficients relating to shuffle(publisher).\n\n\nThe theory relating R2 to the number of coefficients associated is straightforward for shuffled explanatory variables: R2 will be random with mean value \\(\\frac{k}{n-1}\\).\n\n\n\n\n\n\nDemonstration: The mean R2 across many trials\n\n\n\nFor the shuffle(publisher) model, the theoretical mean across many trials will be R2 = 158/324 = 0.49. The demonstration below confirms this using 100 trials:\n\nPub_trials <- do(100) * {\n  lm(list_price ~ shuffle(publisher), data=amazon_books) %>%\n    R2()\n}\nPub_trials %>% summarize(meanR2 = mean(Rsquared))\n\n\n\n \n  \n    meanR2 \n  \n \n\n  \n    0.4955462 \n  \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 29.2: 100 trials of R2 from list_price ~ shuffled(publisher). The theoretical value \\(k/n=160/324=0.49\\) is marked in red.\n\n\n\nWe can carry out similar trials for the models list_price ~ shuffle(author) and list_price ~ shuffle(isbn_10), which have \\(k=251\\) and \\(k=319\\) respectively.\n\n\n\n\n\nFigure 29.3: R2 from many trials of three models, list_price ~ shuffle(publisher) and ~ shuffle(author) and ~shuffle(isbn_10).\n\n\n\n\nThe blue diagonal line in Figure 29.3 shows the theoretical average R2 as a function of the number of model coefficients when the explanatory variable is randomized. R2 will always be 1.0 when \\(k=n\\), that is, when the number of coefficients is the same as the sample size.\nFigure 29.3 suggests a way to distinguish between R2 resulting from the ISBN-effect and R2 that shows some true explanatory power: Check if R2 is substantially above the blue diagonal line, that is, check if R2\\(\\gg \\frac{k}{n-1}\\) where \\(k\\) is the number of model coefficients."
  },
  {
    "objectID": "Reading-notes-lesson-29.html#the-f-statistic",
    "href": "Reading-notes-lesson-29.html#the-f-statistic",
    "title": "29  Covariates eat variance",
    "section": "The F statistic",
    "text": "The F statistic\n\\(k\\) and \\(n\\) provide the necessary context for proper interpretation of R2; all three numbers are needed to establish whether R2 \\(\\gg \\frac{k}{n-1}\\) to rule out the ISBN effect. The calculation is not difficult; the modeler always knows the size \\(n\\) of the training data and can find \\(k\\) as the number of coefficients in the model (not counting the Intercept term).\nPerhaps a little easier than interpreting R2 is the interpretation of another statistic, named F, which folds in the \\(k\\), \\(n\\), and R2 into a single number: \\[F \\equiv \\frac{n-k-1}{k} \\frac{\\text{R}^2}{1 - \\text{R}^2}\\] Figure 29.4 is a remake of Figure 29.3 but using F instead of R2. The blue line, which had the formula R2\\(= k/(n-1)\\) in Figure 29.3, gets translated to the constant value 1.0 in Figure 29.4, regardless of \\(k\\). To decide when a model points to a connection stronger than the ISBN effect, the threshold F \\(> 3\\) is a good rule of thumb. (Lesson 37 introduces a more precise calculation for the F threshold, which is built into statistical software and presented as a “p-value.”)\n\n\n\n\n\nFigure 29.4: Like Figure 29.3, but using the F statistic to summarize each trial.\n\n\n\n\n\n\n\n\n\n\nAdjusted R2\n\n\n\nSome fields, notably economics, prefer an alternative to F called “adjusted R2” (or \\(R^2_\\text{adj}\\)). The adjustment comes from moving the raw R2 downward and leftward, more-or-less in the direction of the blue line in Figure 29.3. This movement adjusts a raw \\(R^2\\) that lies on the blue line to \\(R^2_\\text{adj} = 0\\).\nWe leave the debate on the relative merits of using F or \\(R^2_\\text{adj}\\) their respective boosters. However, before getting wrapped up in such debates, it is worth pointing out that \\(R^2_\\text{adj}\\) is just a rescaling of F.\n\\[R^2_\\text{adj} = 1 - \\frac{n-1}{k} \\frac{R^2}{F}\\ .\\]"
  },
  {
    "objectID": "Reading-notes-lesson-29.html#sec-partial-R2",
    "href": "Reading-notes-lesson-29.html#sec-partial-R2",
    "title": "29  Covariates eat variance",
    "section": "Comparing models",
    "text": "Comparing models\nModelers are often in the position of having a model that they like but are contemplating adding one or more additional explanatory variables. To illustrate, consider the following models:\n\n\nModel 1: list_price ~ 1\nModel 2: list_price ~ 1 + hard_paper\nModel 3: list_price ~ 1 + hard_paper + num_pages\nModel 4: list_price ~ 1 + hard_paper + num_pages + weight_oz\n\n\n\n\n\n\n\n\nFigure 29.5: Nesting Russian dolls\n\n\n\nAll the explanatory variables in the smaller models also apply to the bigger models. Such sets are said to be “nested” in much the same way as for Russian dolls.\nFor a nested set of models, R2 can never decrease when moving from a smaller model to a larger one—almost always, there is an increase in R2. To demonstrate:\n\namazon_books <- amazon_books %>% \n  select(list_price, weight_oz, num_pages, hard_paper) %>%\n  filter(complete.cases(.))\nmodel1 <- lm(list_price ~ 1, data=amazon_books)\nmodel2 <- lm(list_price ~ 1 + weight_oz, data = amazon_books)\nmodel3 <- lm(list_price ~ 1 + weight_oz + num_pages, data=amazon_books)\nmodel4 <- lm(list_price ~ 1 + weight_oz + num_pages + hard_paper, data=amazon_books)\n\n\nR2(model1)\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    309 \n    0 \n    0 \n    NaN \n    0 \n  \n\n\n\nR2(model2)\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    309 \n    1 \n    0.16 \n    57 \n    0.15 \n  \n\n\n\nR2(model3)\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    309 \n    2 \n    0.17 \n    30 \n    0.16 \n  \n\n\n\nR2(model4)\n\n\n\n \n  \n    n \n    k \n    Rsquared \n    F \n    adjR2 \n  \n \n\n  \n    309 \n    3 \n    0.17 \n    21 \n    0.16 \n  \n\n\n\n\nWhen adding explanatory variables to a model, a good question is whether the new variable(s) add to the ability to account for the variability in the response variable. R2 never goes down when moving from a smaller to a larger model, so we cannot rely on the increase in R2. A valuable technique called “Analysis of Variance” (ANOVA for short) looks at the incremental change in variance explained from a smaller model to a larger one. The increase can be presented as an F statistic. To illustrate:\n\nanova_summary(model1, model2, model3, model4)\n\n\n\n \n  \n    term \n    df.residual \n    rss \n    df \n    sumsq \n    statistic \n    p.value \n  \n \n\n  \n    list_price ~ 1 \n    308 \n    54531 \n    NA \n    NA \n    NA \n    NA \n  \n  \n    list_price ~ 1 + weight_oz \n    307 \n    46032 \n    1 \n    8499 \n    57.2 \n    0.00 \n  \n  \n    list_price ~ 1 + weight_oz + num_pages \n    306 \n    45466 \n    1 \n    566 \n    3.8 \n    0.05 \n  \n  \n    list_price ~ 1 + weight_oz + num_pages + hard_paper \n    305 \n    45277 \n    1 \n    189 \n    1.3 \n    0.26 \n  \n\n\n\n\nFocus on the column named statistic. This records the F statistic. The move from Model 1 to Model 2 produces F=57, well above the threshold described above and clearly indicating that the weight_oz variable accounts for some of the list price. Moving from Model 2 to Model 3 creates a much less impressive F of 3.8. It is as if the added explanatory variable, num_pages, is just barely pulling its own “weight.” Finally, moving from Model 3 to Model 4 produces a below-threshold F of 1.3. In other words, in the context of weight_oz and num_pages, the hard_paper variable does not carry additional information about the list price.\nThe last column of the report, labeled Pr(>F), translates F into a universal 0 to 1 scale called a p-value. A large F produces a small p-value. The rule of thumb for reading p-values is that a value \\(p < 0.05\\) indicates that the added variable brings new information about the response variable. We will return to p-values and the controversy they have entailed in Lessons 36 through 38."
  },
  {
    "objectID": "Reading-notes-lesson-30.html#block-that-path",
    "href": "Reading-notes-lesson-30.html#block-that-path",
    "title": "30  Confounding",
    "section": "Block that path!",
    "text": "Block that path!\nLet us look more generally at the possible causal connections among three variables, which we will call X, Y, and C. We will stipulate that X points causally toward Y and that C is a possible covariate. Like all DAGs, there cannot be a cycle of causation. These conditions leave three distinct DAGs that do not have a cycle, shown in Figure 30.2.\n\n\n\n\n\n\n\n(a) C is a confounder.\n\n\n\n\n\n\n\n(b) C is a mechanism.\n\n\n\n\n\n\n\n(c) C is a consequence.\n\n\n\n\nFigure 30.2: Three different DAGs connecting X, Y, and C.\n\n\nC plays a different role in each of the three dags. In sub-figure (a), C causes both X and Y. In (b), part of the way that X influences Y is through C. We say, in this case, “C is a mechanism by which X causes Y. In sub-figure (c), C does not cause either X or Y. Instead, C is a consequence of both X and Y.1\nTo understand how a DAG informs whether or not to include a covariate, It will help to give general names to some of the sub-structures seen in the Figure 30.2 DAGs. ?fig-dag-paths shows some of these sub-structures, removing other links that are not part of the structure.\n\n\n\n\n\n\n\n(a) Direct causal link from X to Y\n\n\n\n\n\n\n\n(b) Causal path from X through C to Y\n\n\n\n\n\n\n\n(c) Correlating path connecting X and Y via C\n\n\n\n\n\n\n\n(d) C as a consequence of X and Y\n\n\n\n\nFigure 30.3: Sub-structures seen in Figure 30.2.\n\n\n\nA “direct causal link” between X and Y. There are no intermediate nodes.\nA “causal path” from C to X and on to Y. A causal path is one where, starting at the originating node, flow along the arrows can get to the terminal node, passing through all intermediate nodes.\nA “correlating path” from Y through X to C. Correlating paths are distinct from causal paths because, in a correlating path, there is no way to get from one end to the other by following the flows.\nA “collider” wealth. In other words, both X and Y are causes of C.\n\nLook back to Figure 30.2(a), where wealth is a confounder. A confounder is always an intermediate node in a correlating path.\nIncluding a covariate either blocks or opens the pathway on which that covariate lies. Which it will be depends on the kind of pathway. A causal path, as in Figure 30.3(b), is blocked by including the covariate. Otherwise, it is open. A correlating path (?fig-dags-path(c)) is similar: the path is open unless the covariate is included in the model. A colliding path, as in Figure 30.3(d), is blocked unless the covariate is included—the opposite of a causal path.\nOften, covariates are selected to block all paths except the direct link between the explanatory and response variable. This means do include the covariate if it is on a correlating path and do not include it if the covariate is at the collision point.\nAs for a causal path, the choice depends on what is to be studied. Consider the DAG drawn in Figure 30.2(b), reproduced here for convenience:\n\n\n\n\n\ngrass influences illness through two distinct paths:\n\nthe direct link from grass to illness.\nthe causal pathway from grass through wealth to illness.\n\nAdmittedly, it is far-fetched that choosing to green the grass makes a household wealthier, but focus on the topology of the DAG and not the unlikeliness of this specific causal scenario.\nThere is no way to block a direct link from an explanatory variable to a response. If there were a reason to do this, the modeler probably selected the wrong explanatory variable.\nBut there is a genuine choice to be made about whether to block pathway (ii). If the interest is the purely biochemical link between grass-greening chemicals and illness, then block pathway (ii). However, if the interest is in the total effect of grass and illness, including both biochemistry and the sociological reasons why wealth influences illness, then leave the pathway open.\n\n\n\n\n\n\nIn draft: Some resources\n\n\n\nhttps://towardsdatascience.com/causal-effects-via-dags-801df31da794\nhttps://towardsdatascience.com/causal-effects-via-the-do-operator-5415aefc834a"
  },
  {
    "objectID": "Reading-notes-lesson-31.html#correlation",
    "href": "Reading-notes-lesson-31.html#correlation",
    "title": "31  Spurious correlation",
    "section": "Correlation",
    "text": "Correlation\nA dictionary is a starting point for understanding the use of a word. Here are four definitions of “correlation” from general-purpose dictionaries.\n\n“A relation existing between phenomena or things or between mathematical or statistical variables which tend to vary, be associated, or occur together in a way not expected on the basis of chance alone” Source: Merriam-Webster Dictionary\n\n\n“A connection between two things in which one thing changes as the other does” Source: Oxford Learner’s Dictionary\n\n\n“A connection or relationship between two or more things that is not caused by chance. A positive correlation means that two things are likely to exist together; a negative correlation means that they are not.” Source: Macmillan dictionary\n\n\n“A mutual relationship or connection between two or more things,” “interdependence of variable quantities.” Source: [Oxford Languages]\n\nAll four definitions use “connection” or “relation/relationship.” That is at the core of “correlation.” Indeed, “relation” is part of the word “correlation.” One of the definitions uses “causes” explicitly, and the everyday meaning of “connection” and “relation” tend to point in this direction. The phrase “one thing changes as the other does” is close to the idea of causality, as is “interdependence.:\nThree of the definitions use the words “vary,” “variable,” or “changes.” The emphasis on variation also appears directly in a close statistical synonym for correlation: “covariance.”\nTwo of the definitions refer to “chance,” that correlation “is not caused by chance,” or “not expected on the basis of chance alone.” These phrases suggest to a general reader that correlation, since not based on chance, must be a matter of fate: pre-determination and the action of causal mechanisms.\nWe can put the above definitions in the context of four major themes of these Lessons:\n\nQuantitative description of relationships\nVariation\nSampling variation\nCausality\n\nCorrelation is about relationships; the “correlation coefficient” is a way to describe a straight-line relationship quantitatively. The correlation coefficient addresses the tandem variation of quantities, or, more simply stated, how “one thing changes as the other does.”\nTo a statistical thinker, the concern about “chance” in the definitions is not about fate but reliability. Sampling variation can lead to the appearance of a pattern in some samples of a process that is not seen in other samples of that same process. Reliability means that the pattern will appear in a large majority of samples.\n\n\n\n\n\n\nNote\n\n\n\nOne of the better explanations of “correlation” appears in an 1890 article by Francis Galton, who invented the correlation coefficient. Since the explanation is more than a century old, some words will be unfamiliar to the modern reader. For example, a “clerk” is an office worker. An “omnibus” is merely a means of public transportation today.\n\nTwo clerks leave their office together and travel homewards in the same and somewhat unpunctual omnibus every day. They both get out of the omnibus at the same halting-place, and thence both walk by their several ways to their respective homes. … The upshot is that when either clerk arrives at his home later than his average time, there is some reason to expect that the other clerk will be late also, because the retardation of the first clerk may have been wholly or partly due to slowness of the omnibus on that day, which would equally have retarded the second clerk. Hence their unpunctualities are related. If the omnibus took them both very near to their homes, the relation would be very close. If they lodged in the same house and the omnibus dropped them at its door, the relation would become identity.\n\n\nThe problems of … correlation deal wholly with departures or variations ; they pay no direct regard to the central form from which the departures or variations are measured. If we were measuring statures, and had made a mark on our rule at a height equal to the average height of the race of persons whom we were considering, then it would be the distance of the top of each man’s head from that mark, upward or downward as the case might be, that is wanted for our use, and not its distance upward from the ground.1"
  },
  {
    "objectID": "Reading-notes-lesson-31.html#spurious-causation",
    "href": "Reading-notes-lesson-31.html#spurious-causation",
    "title": "31  Spurious correlation",
    "section": "Spurious causation",
    "text": "Spurious causation\n\n\n\n\n\n\nFigure 31.2: Two examples from the Spurious correlations website\n\n\n\nThe “Spurious correlations” website http://www.tylervigen.com/spurious-correlations provides entertaining examples of correlations gone wrong. The running gag is that the two correlated variables have no reasonable association, yet the correlation coefficient is very close to its theoretical maximum of 1.0. Typically, one of the variables is morbid, as in Figure 31.2.\n\n\n\n\n\n\nFigure 31.3: The telson and tergum are anatomical parts of the shrimp. Their locations are marked at the bottom. Source: Weldon 1888\n\n\n\nAccording to Aldrich (1995)^[John Aldrich (1994) “Correlations Genuine and Spurious in Pearson and Yule” Statistical Science 10(4) URL the idea of spurious correlations appears first in an 1897 paper by statistical pioneer and philosopher of science Karl Pearson. The correlation coefficient method was published only in 1888, and, understandably, early users encountered pitfalls. One very early user, W.F.R. Weldon, published a study in 1892 on the correlations between the sizes of organs, such as the tergum and telson in shrimp. (See Figure 31.3.)\n\n\n\n\n\n\n\n\n\nPearson noticed a distinctive feature of Weldon’s method. Weldon measured the tergum and telson as a fraction of the overall body length.\nFigure 31.4 shows one possible DAG interpretation where telson and tergum are not connected by any causal path. Similarly, length is exogenous with no causal path between it and either telson or tergum.\n\nshrimp_dag <- dag_make(\n  tergum ~ unif(min=2, max=3),\n  telson ~ unif(min=4, max=5),\n  length ~ unif(min=40, max=80), \n  x ~ tergum/length + exo(.01),\n  y ~ telson/length + exo(.01)\n)\n# dag_draw(shrimp_dag, seed=101, vertex.label.cex=1)\nknitr::include_graphics(\"www/telson-tergum.png\")\n\n\n\n\n\nFigure 31.4: DAG for the shrimp measurements.\n\n\n\nThe Figure 31.4 shows a hypothesis where there is no causal relationship between telson and tergum. Pearson wondered whether dividing those quantities by length to produce variables x and y, might induce a correlation. Weldon had found a correlation coefficient between x and y of about 0.6. Pearson estimated that dividing by length would induce a correlation between x and y of about 0.4-0.5, even if telson and tergum are not causally connected.\nWe can confirm Pearson’s estimate by sampling from the DAG and modeling y by x. The confidence interval on x shows a relationship between x and y. In 1892, before the invention of regression, the correlation coefficient would have been used. In retrospect, we know the correlation coefficient is a simple scaling of the x coefficient.\n\nSample <- sample(shrimp_dag, size=1000)\nlm(y ~ x, data=Sample) %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    0.0457665 \n    0.0522715 \n  \n  \n    x \n    0.6147549 \n    0.7566114 \n  \n\n\n\ncor(y ~ x, data=Sample)\n\n[1] 0.514812\n\n\nPearson’s 1897 work precedes the earliest conception of DAGs by three decades. An entire century would pass before DAGs came into widespread use. However, from the DAG of Figure 31.4] in front of us, we can see that length is a common cause of x and y.\nWithin 20 years of Pearson’s publication, a mathematical technique called “partial correlation” was in use that could deal with this particular problem of spurious correlation. The key is that the model should include length as a covariate. The covariate correctly blocks the path from x to y via length.\n\nlm(y ~ x + length, data=Sample) %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    0.1507687 \n    0.1635108 \n  \n  \n    x \n    -0.0362598 \n    0.0833543 \n  \n  \n    length \n    -0.0013975 \n    -0.0012508 \n  \n\n\n\n\nThe confidence interval on the x coefficient includes zero once length is included in the model. So the data, properly analyzed, show no correlation between telson and tergum.\nIn this case, “spurious correlation” stems from using an inappropriate method. This situation, identified 130 years ago and addressed a century ago, is still a problem for those who use the correlation coefficient. Although regression allows the incorporation of covariates, the correlation coefficient does not.\n\n\n\n\n\n\nTime series analysis\n\n\n\nSome spurious correlations, such as those presented on the eponymous website, can also be attributed to methodological error.\nOne source of error was identified in 1904 by F.E. Cave-Browne-Cave in her paper “On the influence of the time factor on the correlation between the barometric heights at stations more than 1000 miles apart,” published in the Proceedings of the Royal Society. “Miss Cave,” as she was referred to in 1917 and 1921, respectively by eminent statisticians William Sealy Gosset (who published under the name “Student”) and George Udny Yule, also offered a solution to the problem. Her solution is very much in the tradition of “time-series analysis,” a contemporary specialized area of statistics.\nThe unlikeliness of the correlations on the website is another clue to their origin as methodological. Nobody woke up one morning with the hypothesis that cheese consumption and bedsheet mortality are related. Instead, the correlation is the product of a search among many miscellaneous records. Imagine that data were available on 10,000 annually tabulated variables for the last decade. These 10,000 variables create the opportunity for 50 million pairs of variables. Even if none of these 50 million pairs have a genuine relationship, sampling variation will lead to some of them having a strong correlation coefficient.\nIn statistics, such a blind search is called the “multiple comparisons problem.” Ways to address the problem have been available since the 1950s. (We will return to this topic under the label “false discovery” in Lesson 38.) Multiple comparisons can be used as a trick, as with the website. However, multiple comparisons also arise naturally in some fields. For example, in molecular genetics, “micro-arrays” make a hundred thousand simultaneous measurements of gene expression. Correlations in the expression of two genes give a clue to cellular function and disease. With so many pairs available, multiple comparisons will be an issue."
  },
  {
    "objectID": "Reading-notes-lesson-31.html#correlation-implies-causation.",
    "href": "Reading-notes-lesson-31.html#correlation-implies-causation.",
    "title": "31  Spurious correlation",
    "section": "“Correlation implies causation.”",
    "text": "“Correlation implies causation.”\nFrancis Galton’s 1890 example of the clerks on the bus introduces “correlation” as a causality story. The bus trip causes variation in commute times. Two clerks riding the same bus will have correlated commute times. In the dictionary definitions of “correlation” at the start of the Lesson, the words “connection,” “relationship,” and “interdependence” suggests causal connections.\n\n\n\n\n\nInsofar as the dictionary definitions of correlation suggest a causal relationship, they are at odds with the statistical mainstream, which famously holds that “correlation does not imply causation.” This view is so entrenched that it appears on tee shirts, one style of which is available for sale by the American Statistical Association.\nThe statement “A is not B” can be valid only if we know what A and B are. We have a handle on the meaning of “correlation.” So what is the meaning of “causation?”\nDictionaries define “causation” using the word “cause.” So we look there for guidance.\n\nA person or thing that gives rise to an action, phenomenon, or condition. Source: Oxford Languages\n\n\nAn event, thing, or person that makes something happen. Source: Macmillan Dictionary\n\n\nA person or thing that acts, happens, or exists in such a way that some specific thing happens as a result; the producer of an effect. Source: Dictionary.com\n\nInterpreting these definitions requires making sense of “give rise to,” “makes happen,” or “happens as a result.” All of them are synonyms for “cause.”\nThis circularity produces a muddle. Centuries of philosophical debate have yet to clarify things much.\nStill, we can do something. The point of view of these Lessons is to support decision-making. Causation is a valuable concept for decision-making, particularly in cases where the decision-maker is considering an intervention. With this as an anchor, a pragmatic definition of “causation” is available:\n\nCausation describes a class of hypotheses that DAGs can represent. In that representation, a causal relationship between two nodes X and Y is marked by a causal path connecting X to Y. In Lesson 30, we defined “causal path” in terms of the directions of arrows in a DAG.2 A definitive demonstration of a causal relationship between X and Y is that intervening to change X results reliably in a change in Y, all other nodes not on the causal path being held constant. (Lesson 32 treats the methodology behind this definitive sign.)\n\nWhether or not a definitive demonstration is feasible is not directly relevant to the decision-maker. A decision-maker acts under the guidance of one or more hypotheses. A good rule of thumb for decision-makers is to be guided only by plausible hypotheses. Whether a hypothesis is plausible is a matter of informed belief. A definitive demonstration should sharpen that belief. If no such definitive demonstration is available, the decision-maker must rely on alternative sources for belief. Austin Bradford Hill (1898-1991), an epidemiologist and eminent statistician, famously published a list of nine criteria that support belief in a causal hypothesis.\nUsing my definition of causation, and in marked disagreement with many statisticians, I submit that\n\nCorrelation implies causation.\n\n“Correlation implies causation” is not the same as saying, “A correlation between A and B implies that A causes B.” That statement is false. For instance, it might be instead that B causes A. Alternatively, there might be a common cause C for both A and B. Or, C might be a collider between A and B.\nThere is no mechanism to produce correlation that I am aware of, other than the sources of spurious correlation described previously, that does not involve causation in some way.\n::: {.callout-note} ## So why do many statisticians say different?\nHistorically, the rise of the expression “correlation does not imply causation”—Figure 31.5 shows the ngram since the 1888 invention of the correlation coefficient—comes after the peak in the use of the word “correlation.”\n\n\n\n\n\nFigure 31.5: Google NGram showing the rise in the use of the phrases “correlation does not imply causation,” and “correlation is not causation” in recent decades.\n\n\n\n\nThe first documented use of the phrase is from 1900. It comes in a review of the second edition of a book, The Grammar of Science, by Karl Pearson (whom we have met before in this Lesson).\nThe Grammar of Science is a metaphysically oriented prescription for a new type of science. It posited that sciences such as physics or chemistry unnecessarily drew on metaphors for causation, such as “force.” Instead, the book advocated another framework as more appropriate, eschewing causation in favor of descriptions of “perceptions” with probability.\nPearson illustrates his antipathy toward causation with an example of an ash tree in his garden:\n\n[T]he causes of its growth might be widened out into a description of the various past stages of the universe. One of the causes of its growth is the existence of my garden, which is conditioned by the existence of the metropolis [London]; another cause is the nature of the soil, gravel approaching the edge of the clay, which again is conditioned by the geological structure and past history of the earth. The causes of any individual thing thus widen out into the unmanageable history of the universe. The Grammar of Science, 2/e, p. 131\n\nIt should not be surprising that the field of statistics, which uses probability very extensively as a description, and that developed correlation as a measure of probability, would advocate for more general use of its approach. In this spirit, I read “correlation does not imply causation” as “our new science framework of probability and correlation replaces the antiquated framework of causation.” Outside of statistics, however, probability is merely a tool; causation does indeed have practical use. All the more so for decision-makers."
  },
  {
    "objectID": "Reading-notes-lesson-32.html#replication",
    "href": "Reading-notes-lesson-32.html#replication",
    "title": "32  Experiment and random assignment",
    "section": "Replication",
    "text": "Replication\nTo understand some of the contribution that statistical thinking can make to experiment, recall our earlier definition:\n\nStatistic thinking is the explanation/description of variation in the context of what remains unexplained/undescribed.\n\nA key concept that statistical thinking brings to experiment is the idea of variation. Simply put, a good experiment should involve some variation. The simplest way to create variation is to repeat each experimental trial multiple times. This is called “replication.”"
  },
  {
    "objectID": "Reading-notes-lesson-32.html#example-replicated-bed-net-trials",
    "href": "Reading-notes-lesson-32.html#example-replicated-bed-net-trials",
    "title": "32  Experiment and random assignment",
    "section": "Example: Replicated bed net trials",
    "text": "Example: Replicated bed net trials\nOne way to improve the simple experiment bed net described above is to carry out many trials. One reason is that the results from any single trial might be shaped by accidental or particular circumstances: the weather in the trial area was less favorable to mosquito reproduction; another government agency decided to help out by spraying pesticides broadly, and so on. Setting up trials in different areas can help to balance out these influences.\nReplicated trials also allow us to estimate the size of the variability caused by the accidental or particular factors. To illustrate, suppose a single trial is done and the rate of malarial illness goes down by 5 percentage points. What can we conclude? The result is promising but we can’t rule out that it is due to accidental factors other than bed nets. Why not? Because we have no idea how much unexplained variation is in play.\n\n\n\n:::: {.column-margin}\n\n\n\n?tbl-bed-net shows data from four imagined trials of the effect of bed nets. (Reduction by a negative number, like -1, is an increase.) The mean reduction is 3 percentage points, but this number is not much use unless we can put it in the context of sampling variation. Conducting multiple trials gives us a handle on the amount of sampling variation. By We can easilyNow we know something about the amount of variation due to site-to-site factors. The replication introduces observed variation in results, the observed variation can be quantified and used to place the overall trend in context.\nUsing the regression framework makes it easy to estimate the amount of sampling variation. The mean reduction corresponds to the coefficient from the model reduction ~ 1.\n\nlm(reduction ~ 1, \n     data=Bed_net_data) %>% \n  coef()\n\n(Intercept) \n          3 \n\n\n\nlm(reduction ~ 1, \n     data=Bed_net_data) %>% \n  conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    1 \n    5 \n  \n\n\n\n\nThe observed 3 percentage point mean reduction in the incidence of malaria does stand out from the noise: the confidence interval does not include zero. In these (imagined) data, we have confidence that we have seen a signal."
  },
  {
    "objectID": "Reading-notes-lesson-32.html#control",
    "href": "Reading-notes-lesson-32.html#control",
    "title": "32  Experiment and random assignment",
    "section": "Control",
    "text": "Control\nHowever, there is still a problem with the design of the imagined bed-net experiment. What if the year the experiment was done was unusually dry, reducing the mosquito population and, with it, the rate of malaria infection? Then we don’t know whether the observed 3 point reduction is due to the weather or the bed nets, or even something else, e.g. better nutrition due to a drop in international prices for rice.\nWe need to measure what the change in malarial infection would have been without the bed-net intervention. Care needs to be taken here. If the trial sites were rural, it would not be appropriate to look at malarial rates in urban areas where there was no bed-net program. We want to compare the trial sites with non-trial sites where the intervention was not carried out, so-called “control” sites. The With_controls data frame imagines what data might look like if in half the sites no bed-net program was involved.\n\n\n\n\n\n\n\nTable 32.1:  With_controls \n \n  \n    site \n    reduction \n    nets \n  \n \n\n  \n    A \n    2 \n    control \n  \n  \n    B \n    8 \n    treatment \n  \n  \n    C \n    4 \n    treatment \n  \n  \n    D \n    1 \n    treatment \n  \n  \n    E \n    -1 \n    control \n  \n  \n    F \n    -2 \n    control \n  \n  \n    G \n    0 \n    control \n  \n  \n    H \n    2 \n    treatment \n  \n  \n    I \n    3 \n    treatment \n  \n  \n    J \n    2 \n    control \n  \n\n\n\n\n\nThe proper regression model for the With_controls data is reduction ~ treatment:\n\n\nlm(reduction ~ nets, \n       data=With_controls) %>% \n  coef() \n\n  (Intercept) netstreatment \n          0.2           3.4 \n\nlm(reduction ~ nets, \n       data=With_controls) %>% \n  conf_interval() \n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    -2.200 \n    2.6 \n  \n  \n    netstreatment \n    0.058 \n    6.7 \n  \n\n\n\n\nThe effect of the bed nets is summarized by the netstreatment coefficient, which compares the reduction between the treatment and control groups. In this new (imagined) data frame, the confidence interval on netstreatment touches very close to zero; the signal is just barely discernible from the noise.\nThe reader might wonder why, in moving to the controlled design, the ten sites were not all treated with nets and another ten or so sites found to use as the control. Perhaps, even, the control sites could be selected as villages nearby to the bed net villages.\nOne reason is pragmatic: the larger study would require more effort and money. The larger study might be worthwhile; larger \\(n\\) would presumably narrow the confidence interval. Another reason, to be expanded on in the next section, is that the treatment and control sites should be as similar as possible. This can be surprising hard to achieve. Other factors such as the enthusiasm or skepticism of the town leaders toward public-health interventions might be behind the choice of the original sites for the bed-net program. The control sites might be towns that turned down the original offer of the bed-net program and, accordingly, have different attitudes toward public health."
  },
  {
    "objectID": "Reading-notes-lesson-32.html#example-testing-the-salk-polio-vaccine",
    "href": "Reading-notes-lesson-32.html#example-testing-the-salk-polio-vaccine",
    "title": "32  Experiment and random assignment",
    "section": "Example: Testing the Salk polio vaccine",
    "text": "Example: Testing the Salk polio vaccine\nToday, most children are vaccinated against polio, though a smaller fraction than in previous years. This might be because symptomatic polio is very rare, lessening the perceived urgency of protecting against it. Partly, the reduction reflects the growth in the “anti-vax” movement, which became especially notable with the advent of COVID-19.\nThe first US polio epidemic occurred in 1916, just two years before the COVID-like “Spanish flu” pandemic.1 Up through the early 1950s, polio injured or killed hundreds of thousands of people, particularly children. Anxiety about the disease was similar to that seen in the first year of the COVID-19 pandemic.\nThere were many attempts to develop a vaccine against polio. Jonas Salk created the first really promising vaccine, the promise being based on laboratory tests. To establish the safety and effectiveness of the Salk vaccine, it needed to be tried in the field, with people. Two organizations, the US Public Health Service and the National Foundation for Infantile Paralysis got together to organize a clinical field trial which, all told, involved two-million students in grades 1 through 3.\nThe two studies involved both a treatment and a control group. In some school districts, students in grades 1 and 3 were held as controls. The treatment group was students in grade 2 whose parents gave consent. We will call this “Study 1.” In other school districts, the study design was different: the parents of all students in all three grades were asked for consent. The students with parental consent were then randomly split into two groups: a treatment and a control. Call this “Study 2.”\nThe Study 2 design might seem inefficient; it reduced the number of children receiving the vaccine because half of the children with parental consent were left unvaccinated. On the other hand, it might be that children from families who consent to be given a vaccine are different in a systematic way from children whose families refuse, just as today’s anti-vax families might be different from “pro-vax” families.\nAs reported in Freedman (1998)2, the different risk of symptomatic polio between children from consent versus refuse families became evident in the study. Table 32.2 shows the study results from the school districts which used half the consent group as controls.\nThe difference between treatment and control groups is very evident: a reduction from 71 cases per 100,000 children to 28 cases per 100,000. The no-consent children had a rate between the two, 46 per 100,000. Since both the “control” and “no consent” groups did not get the vaccine, one might expect those rates to be similar. That they are not shows that the “no-consent” children are systematically different from those children whose parents gave consent.\n\nIn the other branch of the study, Study 1, where no-consent 2nd-graders were used as control and vaccine was given to all whose parents did consent, the results (Table 32.3) were different because of confounding between treatment and consent.\n\n\n\n\nTable 32.2:  Results from Study 2. \n \n  \n    vaccine \n    size \n    rate \n  \n \n\n  \n    Treatment \n    200000 \n    28 \n  \n  \n    Control \n    200000 \n    71 \n  \n  \n    No consent \n    350000 \n    46 \n  \n\n\n\n\n\n\n\n\n\nTable 32.3:  Results from Study 1 \n \n  \n    vaccine \n    size \n    rate \n  \n \n\n  \n    Treatment \n    225000 \n    25 \n  \n  \n    No consent \n    125000 \n    44 \n  \n\n\n\n\n\nThe effect of the vaccine from Study 1 under-estimated the biological link between vaccination and reduction of polio risk."
  },
  {
    "objectID": "Reading-notes-lesson-32.html#random-assignment",
    "href": "Reading-notes-lesson-32.html#random-assignment",
    "title": "32  Experiment and random assignment",
    "section": "Random assignment",
    "text": "Random assignment\nThe example of the Salk vaccine trial is a chastening reminder that care must be taken when assigning treatment or control to the units in an experiment. Without such care, confounding enters into the picture. Merely the possibility of confounding is damaging to the experiment’s result; it invites skepticism and doubt.\n\n\n\n\n\n\nFigure 32.1: A DAG for the polio vaccine experiment.\n\n\n\nIt is illuminating to look at the vaccine trial as a DAG. The essential situation is diagrammed in Figure 32.1. The socio_economic node represents the idea that socio-economic status has an influence on susceptibility to symptomatic polio3 and also is a factor in shaping a family’s decision about giving consent.\nThe DAG in Figure 32.1 has two pathways between treatment and polio that can produce confounding:\n\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\rightarrow \\mathtt{polio}\\)\n\\(\\mathtt{treatment} \\leftarrow \\mathtt{consent} \\leftarrow \\mathtt{socio.economic} \\rightarrow \\mathtt{polio}\\)\n\n\n\n\n\n\n\nFigure 32.2: The DAG when consent \\(\\equiv\\) vaccine.\n\n\n\nThe approach emphasized in Lesson 30 to avoid such confounding is to block the relevant pathways. Both can be blocked by including consent as a covariate. However, in Study 1, assignment to vaccine was purely a matter of consent; consent and treatment are essentially the same variable. Figure 32.2 shows the corresponding DAG, where consent and treatment are merged into a single variable. Holding consent constant deprives the system of the explanatory variable and still introduces confounding through socio_economic.\nIn Study 2, all the children participating had parents give consent. This means that consent is not actually a variable; it doesn’t vary! The corresponding DAG, without consent as a factor, is drawn in Figure 32.3. This Study 2 DAG is unfolded; there are no confounding pathways! Thus the model polio ~ treatment is appropriate.\n\n\n\n\n\n\nFigure 32.3: The Study 2 DAG.\n\n\n\nThe assignment to treatment or control in Figure 32.3 is made by the people running the study. Although the DAG doesn’t show any inputs to assignment, the involvement of people in making the assignment opens up a possibility that their assignment of treatment or control might have been influenced by other factors, such as socio-economic status. To guard against this, or even skepticism raised by the possibility, experimentalists have developed a simple safeguard: “random assignment.” In random assignment, assignment is made by a computer generating random numbers. Nobody believes that the computer algorithm is influenced by socio-economic status or any other factor that might be connected to polio in any way.\n\n\n\n\n\n\nUnder construction"
  },
  {
    "objectID": "Reading-notes-lesson-32.html#blocking",
    "href": "Reading-notes-lesson-32.html#blocking",
    "title": "32  Experiment and random assignment",
    "section": "Blocking",
    "text": "Blocking"
  },
  {
    "objectID": "Reading-notes-lesson-33.html#risk",
    "href": "Reading-notes-lesson-33.html#risk",
    "title": "33  Measuring and accumulating risk",
    "section": "Risk",
    "text": "Risk\nIn everyday language, “risk” refers to a dangerous or unwelcome outcome. We talk about the “risk of heart disease” or the “risk of bankruptcy” or other financial loss. To apply risk to a positive outcome is non-idiomatic. For instance, for a person wanting to have a baby, we don’t talk about the “risk of pregnancy,” but about the “chances of becoming pregnant.”\nIn statistics, the word “risk” is similarly used for an unwelcome outcome. However, an additional component of meaning is added. “Risk” refers to the probability of the unwelcome outcome. In principle, though, it would be entirely equivalent to speak of the “probability of heart disease,” leaving the phrase “heart disease” to signal that the outcome is unwanted. We talk about the “risk of death” but never the “risk of survival.” Instead, we would say something like the “chances of survival.”\nThe outcomes described by “risk” are categorical. Generically, the levels of the categorical variable might be “unwelcome” and “not unwelcome,” but they might be more specifically named, say, “death” and “survival,” or “lung disease” and “not.”\nWe have been building models of such categorical output variables from the start of these Lessons. For the zero-one categorical variables we have emphasized, the model output is in the form of a probability: the probability of the outcome of the event being “one” (or whatever actual level “one” corresponds to.) If we assign one for “death” and zero for “survival,” the probability which is the output of a model is a risk, but other than the choice of zero-one assignment, there is no mathematical difference (in statistics) between a risk and a probability.\nIt often happens that risk depends on other factors, often called “risk factors.” In our modeling framework, such risk factors are merely explanatory variables. For instance, a study of the impact of smoking on health might use outcome represented by a categorical response variable with levels “death” or “survival.”\nTo summarize, for statistical thinkers a model of risk takes the usual form that we have used for models of zero-one categorical models. All the same issues apply: covariates, DAGs, confidence intervals, and so on. There is, however, a slightly different style for presenting effect sizes.\nUp until now, we have presented effect in terms of an arithmetic difference. As an example, we turn to the fuel-economy model introduced at the beginning of this lesson. Effect sizes are about changes. To look at the effect size of, say, weight (wt), we would calculate the model output for two cars that differ in weight (but are the same for the other explanatory variables). For instance, to know the change in fuel economy due to a 1000 pound change in weight, we can do this calculation:\n\nmod_eval(logmpg_mod, hp = 100, wt = c(2.5, 3.5)) %>%\n  mutate(mpg = exp(model_output))\n\n\n\n \n  \n    hp \n    wt \n    model_output \n    mpg \n  \n \n\n  \n    100 \n    2.5 \n    3.173411 \n    23.88884 \n  \n  \n    100 \n    3.5 \n    2.972875 \n    19.54803 \n  \n\n\n\n\nThe lighter car is predicted to get 24 mpg, the heavier car to get 19.5 mpg. The arithmetic difference in output \\(19.5 - 24 = -4.5\\) mpg is the effect of the 1000 pound increase in weight.\nThere is another way to present the effect, as a ratio or proportion. In this style, the effect of an addition 1000 pounds is \\(19.5 / 24 = 81\\%\\), that is, the heavier car can go only 81% of the distance that the lighter car will travel on the same amount of gasoline. (Stating an effect as a ratio is common in some fields. For example, economists use ratios when describing prices or investment returns.)\nIn presenting a change in risk—that is, the change in probability resulting from a change in some explanatory variable—both the arithmetic difference and arithmetic ratio forms are used. But there is a special terminology that is used to identify the two forms. A change in the form of an arithmetic difference is called an “absolute change in risk.” A change in the ratio form is called a “relative risk.”\nThe different forms—absolute change in risk versus relative risk—both describe the same change in risk. For most decision-makers, the absolute form is most useful. To illustate, suppose exposure to a toxin increases the risk of a disease by 50%. This would be a risk ratio of 1.5. But that risk ratio might be based on an absolute change in risk from 0.00004 to 0.00006, or it might be based on an absolute change in risk from 40% to 60%. The latter is a much more substantial change in risk and ought to warrant more attention from decision makers interested.\n\n\n\n\n\n\nOther ways to measure change in risk\n\n\n\nIt is important for measures of change in risk to be mathematically valid. But from among the mathematically valid measures, one wants to choose a form that will be the best for communicating with decision-makers. Those decision-makers might be the people in charge of establishing screening for diseases like breast cancer, or a judge and jury deciding the extent to which blame for an illness ought to be assigned to second-hand smoke.\nTwo useful ways to present a change in risk are the “number needed to treat” (NNT) and the “attributable fraction.” The NNT is useful for presenting the possible benefits of a treatment or screening test. Consider these data from the US Preventive Services Task Force which take the form of the number of breast-cancer deaths in a 10-year period avoided by mammography. The confidence interval on the estimated number is also given.\n\n\n\nAge\nDeaths avoided\nConf. interval\n\n\n\n\n40-49\n3\n0-9\n\n\n50-59\n8\n2-17\n\n\n60-69\n21\n11-32\n\n\n70-74\n13\n0-32\n\n\n\nThe table does not give the risk of death, but rather the absolute risk reduction. For the 70-74 age group this risk reduction is 13/100000 with a confidence interval of 0 to 32/100000.\nThe NNT is well named. It gives the number of people who must receive the treatment in order to avoid one death. Arithmetically, the NNT is simply the reciprocal of the absolute risk reduction. So, for the 70-74 age group the NNT is 100000/13 or 7700 or, stated as a confidence interval, [3125 to \\(\\infty\\)].\nFor a decision-maker, NNT presents the effect size in a readily understood way. For example, the 40-49 year-old group has an NTT of 33,000. The cost of the treatment could be presented in terms of anxiety prevented (mammography produces a lot of false positives) or monetary cost. The US Affordable Care Act requires health plans to fully cover the cost of a screening mammogram every one or two years for women over 40. Those mammograms each cost about $100-200. Consequently, the cost of mammography over the ten-year period (during which 5 mammograms might be performed) is roughly \\(5\\times \\$100 \\times 33000\\) or about $16 million per life saved.\nThe attributable fraction is a way of presenting a risk ratio—in other words, a relative risk—in a way that is more concrete than the ratio itself. Consider the effect of smoking on the risk of getting lung cancer. According to the US Centers for Disease Control, “People who smoke cigarettes are 15 to 30 times more likely to get lung cancer.” This statement directly gives the confidence interval on the relative risk: [15 to 30].\nThe attributable fraction refers to the proportion of disease in the exposed group—that is, smokers—to be attributed to expose. The general formula for attributable fraction is simple. If the risk ratio is denoted \\(RR\\), the attributable fraction is \\[\\text{attributable fraction} \\equiv \\frac{RR-1}{RR}\\] For a smoker who gets lung cancer, the confidence interval on the attributable fraction is [93% to 97%].\nFor second-hand smoke, the CDC estimates the risk ratio for cancer at [1.2 to 1.3]. For a person exposed to second-hand smoke who gets cancer, the attributable fraction is [17% to 23%]. Such attributions are useful for those, such as judges and juries, who need to assign a level of blame for a bad outcome."
  },
  {
    "objectID": "Reading-notes-lesson-33.html#accumulating-risk",
    "href": "Reading-notes-lesson-33.html#accumulating-risk",
    "title": "33  Measuring and accumulating risk",
    "section": "Accumulating risk",
    "text": "Accumulating risk\nThere can be multiple factors that contribute to risk. As a simple example of how different factors can contribute simultaneously, consider the following DAG:\n\ndag_draw(dag09)\n\n\n\n\n\nprint(dag09)\n\na ~ exo()\nb ~ exo()\nc ~ binom(2 * a + 3 * b)\n\n\nThe formulas for dag09 show that the nodes a and b are exogenous, their values set randomly and independently of one another by the exo() function. Both a and b contribute to the value of c. Ordinary stuff. But there is something new in this simulation, the binom() function. The output of binom() will always be either 0 or 1. If the input to binom() is zero, it’s completely random whether a 0 or 1 will result. The more positive the input to binom(), the larger the chance of a 1. Similarly, the more negative the input, the larger the chance of a 0. Like this:\n\n\n\nAttaching package: 'mosaicCalc'\n\n\nThe following object is masked from 'package:mosaicModel':\n\n    Runners\n\n\nThe following object is masked from 'package:stats':\n\n    D\n\n\n\n\n\nFigure 33.2: The binom() function looks to its input to determine the probability that the output will be 1 or 0.\n\n\n\n\nIn dag09, the input to binom() is 2*a + 3*b; both a and b are risk factors for c being 1. The larger that a is, the larger the risk. Similarly, the larger b, the larger the risk. The formula 2*a + 3*b accumulates the risk steming from a and b.\nLet’s generate a big sample—\\(n=10,000\\)—from dag09 and see if a simple model is able to capture the way a and b contribute to c:\n\nsample(dag09, size=10000) %>% \n  lm(c ~ a + b, data = .) %>%\n  conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    0.4882392 \n    0.5019308 \n  \n  \n    a \n    0.1858879 \n    0.1994226 \n  \n  \n    b \n    0.2910692 \n    0.3046348 \n  \n\n\n\n\nThe coefficients on a and b are inconsistent with the dag09 formulas. What’s wrong? The problem can be seen in Figure 33.2. The output of binom() is a probability. Probabilities have to stay within the bounds 0 to 1. But there is nothing in the formula c ~ a + b that enforces such bounds. If we are to model zero-one response variables like c, we will need a means to enforce the bounds."
  },
  {
    "objectID": "Reading-notes-lesson-33.html#probability-odds-and-log-odds",
    "href": "Reading-notes-lesson-33.html#probability-odds-and-log-odds",
    "title": "33  Measuring and accumulating risk",
    "section": "Probability, odds, and log odds",
    "text": "Probability, odds, and log odds\n\n\n\n\n\n\nUnder construction\n\n\n\n\n\n\nA probability—a number between 0 and 1—is the most used measure of the chances that something will happen, but it is not the only way nor the best for all purposes.\nAlso part of everyday language is the word “odds,” as in, “What are the odds?” to express surprise at an unexpected event.\nOdds are usually expressed in terms of two numbers, as in “3 to 2” or “100 to 1”, written more compactly as 3:2 and 100:1 or even 1.5 and 100, respectively. The setting for odds is an even that might happen or not: the horse Fortune’s Chance might win the race, otherwise not; it might rain today, otherwise not; the Red Sox might win the World Series, otherwise not.\nThe format of a probability assigns a number between 0 and 1 to the chances that Fortune’s Chance will win, or that it will rain, or that the Red Sox will come out on top. If that number is called \\(p\\), then the chances of the “otherwise outcome” must be \\(1-p\\). The event with probability \\(p\\) would be reformatted into odds as \\(p:(1-p)\\). No information is lost if we treat the odds as a single number, the result of the division \\(p/(1-p)\\). Thus, when \\(p=0.25\\) the corresponding odds will be \\(0.25/0.75\\), in other words, 1/3.\nA big mathematical advantage to using odds is that the odds number can be anything from zero to infinity; it’s not bounded within 0 to 1. Even more advantageous for the purposes of accumulating risk is the logarithm of the odds, called “log odds.” We will come back to this later.\nThe printed version of dag09 shows that the value of node c is a linear combination of a and b converted into a zero-one, binomial value. Unfortunately, the linear modeling trainer, lm(), is not well-tuned to work with binomial data. Another modeling technique, “logistic regression,” does a better job. The glm() function trains logistic regression models on data.\n\nsample(dag09, size=10000) %>% \n  glm(c ~ a + b, data = ., family=\"binomial\") %>%\n  conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    -0.1039978 \n    0.023108 \n  \n  \n    a \n    2.0353911 \n    2.237961 \n  \n  \n    b \n    3.0352614 \n    3.300165 \n  \n\n\n\n\nWhen we use the appropriate modeling technique, we can, in this case, recover the coefficients in the DAG formula: 2 for a and 3 for b. :::"
  },
  {
    "objectID": "Reading-notes-lesson-34.html#identifying-cases",
    "href": "Reading-notes-lesson-34.html#identifying-cases",
    "title": "34  Constructing a classifier",
    "section": "Identifying cases",
    "text": "Identifying cases\nConsider this news report and note the time lag between collection of the dietary explanatory variables and the response variable—whether the patient developed pancreatic cancer.\n\nHigher vitamin D intake has been associated with a significantly reduced risk of pancreatic cancer, according to a study released last week. Researchers combined data from two prospective studies that included 46,771 men ages 40 to 75 and 75,427 women ages 38 to 65. They identified 365 cases of pancreatic cancer over 16 years. Before their cancer was detected, subjects filled out dietary questionnaires, including information on vitamin supplements, and researchers calculated vitamin D intake. After statistically adjusting1 for age, smoking, level of physical activity, intake of calcium and retinol and other factors, the association between vitamin D intake and reduced risk of pancreatic cancer was still significant. Compared with people who consumed less than 150 units of vitamin D a day, those who consumed more than 600 units reduced their risk by 41 percent. - New York Times, 19 Sept. 2006, p. D6.\n\nThis was not an experiment; it was an observational study without any intervention to change anyone’s diet."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#the-training-sample",
    "href": "Reading-notes-lesson-34.html#the-training-sample",
    "title": "34  Constructing a classifier",
    "section": "The training sample",
    "text": "The training sample\nIn building a classifier, we have a similar situation. Perhaps we can perform the blood test today, but that gives us only the test result, not the subject’s true condition. We might have to wait years for that condition to reveal itself. Only at that point can we measure the performance of the classifier.\nTo picture the situation, let’s imagine many people enrolled in the study, some of whom have the condition and some who don’t. On Day 1 of the study, we test everyone and get raw score on a scale from 0 to 40. The results are shown in Figure 34.1. Each glyph is a person. The varying locations are meant to help us later on; for now, just think of them as representing where each person lives in the world. The different shapes of glyph—circle, square, triangle—are meant to remind you that people are different from one another in age, gender, risk-factors, etc.\nEach person took a blood test. The raw result from that test is a score from 0 to 40. The distribution of scores is shown in the right panel of the figure. We also show the score in the world-plot; the higher the raw score, the more blue the glyph. On Day 1, it isn’t known who has the condition and who does not.\n\n\n\n\n\nFigure 34.1: Day 1: The people participating in the study to develop the classifier. Each has been given a blood test which gives a score from zero (gray) to forty (blue).\n\n\n\n\nHaving recorded the raw test results for each person, we wait. In the pancreatic cancer study, they waited 16 years for the cancer to reveal itself.\n… waiting …\nAfter the waiting period, we can add a new column to the original data; whether the person has the condition (C) or doesn’t (H).\nFigure 34.2 shows the distribution of raw test scores for the C group and the H group. The scores are those recorded on Day 1, but after waiting to find out the patients’ conditions, we can subdivide them into those who have the condition (C) and those who don’t (H).\n\n\n\n\n\nFigure 34.2: The distribution of raw test scores. After we know the true condition, we can break down the test scores by condition."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#applying-a-threshold",
    "href": "Reading-notes-lesson-34.html#applying-a-threshold",
    "title": "34  Constructing a classifier",
    "section": "Applying a threshold",
    "text": "Applying a threshold\nTo finish the classifier, we need to identify a “threshold score.” Raw scores above this threshold will generate a \\({\\mathbb{P}}\\) test; scores below the threshold generate a \\({\\mathbb{N}}\\) test.\nWe can make a good guess at an appropriate threshold score from the presentation in the right panel of Figure 34.2. The objective in setting the threshold is to distinguish the C group from the H group. Setting the threshold at a score around 3 does a pretty good job.\nIt helps to give names to the two test results: \\({\\mathbb{P}}\\) and \\({\\mathbb{N}}\\). Anyone with a score above 3 has result \\({\\mathbb{P}}\\), anyone with a score below 3 has an \\({\\mathbb{N}}\\) result.\n\n\n\n\n\nFigure 34.3: Blue is a \\(\\mathbb{P}\\) result, gray a \\(\\mathbb{N}\\) result."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#false-positives-and-false-negatives",
    "href": "Reading-notes-lesson-34.html#false-positives-and-false-negatives",
    "title": "34  Constructing a classifier",
    "section": "False positives and false negatives",
    "text": "False positives and false negatives\nNARRATE Figure 34.3 to point out the gray dots in the C group and the blue dots in the H group. These are errors. But there are two kinds of errors.\n\nFalse-positive: blue dots in the H group. The “positive” refers to the \\({\\mathbb{P}}\\) test result, the “false” simply means the test result was wrong.\nFalse-negative: gray dots in the C group. The “negative” refers to the \\({\\mathbb{N}}\\) result. Again, the “false” means simply that the test result is out of line with the actual condition of the person.\n\nIn the training sample shown in Figure 34.3, there are 300 people altogether and 17 false-negatives. This gives a false-negative rate of about 6%. Similarly there are 30 false-negatives, a false-positive rate of 10%.\n\n\n\n\n\n\nFeature engineering: selling dog food\n\n\n\nNaturally, the objective when building a classifier is to avoid errors. One way to avoid errors is by careful “feature engineering.” Here, “features” refers to the inputs to the classifier model. Often, the designer of the classifier has multiple variables (“features”) to work with. (See example.) Choosing a good set of features can be the difference between a successful classifier and one that makes so many mistakes as to be useless.\nWe will use the name “Bullseye” to refer to a major, national, big-box retailing chain which sells, among many other products, dog food. Sales are largely determined by customer habits; people tend to buy where and what they have previously bought. There are many places to buy dog food, for instance pet supermarkets and grocery stores.\nOne strategy for increasing sales involves discount coupons. A steep discount provides a consumer incentive to try something new and, maybe, leads to consumers forming new habits. But, from a sales perspective, there is little point in providing discounts to people who already have the habit of buying dog food from the retailer. Instead, it is most efficient to provide the discount only to people who don’t yet have that habit\nThe Bullseye marketing staff decided to build a classifier to identify pet owners who already shop at Bullseye but do not purchase dog food there. The data available, from Bullseye’s “loyalty” program, consisted of individual customers’ past purchases of the tens of thousands of products sold at Bullseye.\nWhich of these many products to use as indicators of a customer’s potential to switch to Bullseye’s dog food? This is where feature engineering comes in. Searching through Bullseye’s huge database, the feature engineers identified that customers who buy dog food also buy carpet cleaner. But many people buy carpet cleaner who don’t buy dog food. The engineers searched for purchases might distinguish dog owners from other users of carpet cleaner.\nThe feature engineers’ conclusion: Send dog-food coupons to people who buy carpet cleaner but do not buy diapers. Admittedly, this will leave out the people who have both dogs and babies: these are false negatives. It will also lead to coupons being sent to petless, spill-prone people whose children, if any, have moved beyond diapers: false-positives."
  },
  {
    "objectID": "Reading-notes-lesson-34.html#threshold-sensitivity-and-specificity",
    "href": "Reading-notes-lesson-34.html#threshold-sensitivity-and-specificity",
    "title": "34  Constructing a classifier",
    "section": "Threshold, sensitivity and specificity",
    "text": "Threshold, sensitivity and specificity\nIn Figure 34.3 the threshold between \\({\\mathbb{P}}\\) and \\({\\mathbb{N}}\\) is set at a score of 3. That might have been a good choice, but it pays to take a more careful look.\nThat graph is hard to read because the scores have a very long-tailed distribution; the large majority of scores are below 2 but the scores go up to 40. To make it easier to compare scores between the C and H groups, Figure 34.4 shows the scores on a nonlinear axis. Each score is marked as a letter: “P” means \\({\\mathbb{P}}\\), “N” means \\({\\mathbb{N}}\\). False results are colored red.\n\n\n\n\n\n\n\n## PNplot(threshold=3)\nknitr::include_graphics(\"www/PN-threshold1.png\")\n\n\n\n\nFigure 34.4: Redrawing the participants’ scores from Figure 34.2 on a nonlinear axis. Color marks whether the classifier gave a correct output.\n\n\n\n\nMoving the threshold up would reduce the number of false-positives. At the same time, the larger threshold would increase the number of false-negatives. ?fig-two-thresholds shows what the situation would be if the threshold had been set at, say, 10 or 0.5.\n\n\n\n\n\n\nFigure 34.5: A higher threshold increases the number of false-negatives, but decreases false-positives.\n\n\n\n\n\n\n\n\n\nFigure 34.6: A lower threshold increases the number of false-positives, but decreases false-negatives.\n\n\n\nBy setting the threshold larger, the number of false-negatives (red Ns in ?fig-two-thresholds) increases, but the number of false-positives (red Ps) goes down. Setting the threshold lower reduces the number of false-negatives but increases the number of false-positives.\nThis trade-off between the number of false-positives and the number of false-negatives is characteristic of classifiers.\nFigure 34.7 shows the overall pattern for false results versus threshold. At a threshold of 0, all test results are \\({\\mathbb{P}}\\). Hence, none of the C group results are false; if there are no \\({\\mathbb{N}}\\) results, there cannot be any false-negatives. On the other hand, all of the H group are false-positives.\nIncreasing the threshold changes the results. At a threshold of 1, many of the H group—about 50%—are being correctly classified as \\({\\mathbb{N}}\\). Unfortunately, the higher threshold introduces some negative results for the C group. So the fraction of correct results in the C group goes down to about 90%. This pattern continues: raising the threshold improves the fraction correct in the H group and lowers the fraction correct in the C group.\nThere are two names given to the fraction of correct classifications, depending on whether one is looking at the C group or the H group. The fraction correct in the C group is called the “sensitivity” of the test. The fraction correct in the H group is the “specificity” of the test.\nThe sensitivity and the specificity, taken together, summarize the error rates of the classifier. Note that there are two error rates: one for the C group and another for the H group. Figure 34.7 shows that, depending on the threshold used, the sensitivity and specificity can be very different from one another.\n\n\n\n\n\nFigure 34.7: The choice of threshold determines the number of correct results.\n\n\n\n\nIdeally, both the sensitivity and specificity would be 100%. In practice, high sensitivity means lower specificity and vice versa.\nSensitivity and specificity will be particularly important in Lesson 35 when we take into consideration the prevalence, that is, the fraction of the population with condition C"
  },
  {
    "objectID": "Reading-notes-lesson-35.html#prevalence",
    "href": "Reading-notes-lesson-35.html#prevalence",
    "title": "35  Accounting for prevalence",
    "section": "Prevalence",
    "text": "Prevalence\nThe “prevalence” of C is the fraction of the population who have condition C. Prevalence is an important factor in the performance of a classifier.\nThe training sample used to construct the sensitivity and specificity curves in Figure 34.7.\n\n\n\n\n\n\n\n\n\n\nLesson 34 used a training sample, first shown in Figure 34.3 and duplicated here in the margin. The training sample allowed us look at the consequences of the choice of threshold used in the test. That training sample had roughly equal numbers of people from the C and H groups. It’s sensible to use such a training sample in order to make sure both the C and H groups are well represented.\nThe prevalence among the actual population is usually very different than in the training sample. Figure 35.1 illustrates the typical situation: many people in the H group and few people in the C group.\n\n\n\n\n\nFigure 35.1: The population on which the classifier will be used.\n\n\n\n\nThe prevalence can be seen by how densely the H group is populated compared to the C group. The prevalence depicted in Figure 35.1 is about 10%, that is, one in ten people has condition C. In real-world conditions, prevalence is often much lower, perhaps 0.1%. Indeed, epidemiologists often move alway from a percentage scale when quantifying prevalences, often using “cases per 100,000.”\nEven though the prevalence is different in Figure 34.3 than in Figure 34.3, the sensitivity is exactly the same. Likewise for the specificity.\nWe don’t usually have comprehensive testing of a population, so drawing a picture like Figure 35.1 has to be done theoretically based on the limited information available: prevalence (from surveys of the population) as well as sensitivity and specificity (from the training sample). This is easy to do.\nThe first step is to determine the number in the C group and in the H group using the population size. If the population size is \\(N\\), then the number in the C group will be \\(p(C) N\\). We are writing the prevalence here as a probability, the probability \\(p(C)\\) that a randomly selected person from the population has condition C. Similarly, the size of the H group is \\((1-p(C)) N\\).\n\n\n\n\n\nFigure 35.2: Knowing the prevalence allows us to determine the number of people in the C group and the number in the H group.\n\n\n\n\nConsider now the sensitivity. Sensitivity is relevant only to the C group; it tells the fraction in the C group who will be correctly classified. That’s enough information to know how many people in C to color blue (for \\(\\mathbb{P}\\)) or gray (for \\(\\mathbb{H}\\)).\nSimilarly, the specificity tells us what fraction among the H group to color blue and gray.\nThis is how Figure 35.1 was generated: specifying population size \\(N\\), prevalence \\(p(C)\\), and sensitivity and specificity. The false-positives are the blue dots in the H group, the false-negatives are the gray dots in the C group."
  },
  {
    "objectID": "Reading-notes-lesson-35.html#from-the-patients-point-of-view",
    "href": "Reading-notes-lesson-35.html#from-the-patients-point-of-view",
    "title": "35  Accounting for prevalence",
    "section": "From the patient’s point of view",
    "text": "From the patient’s point of view\nFigure 35.1 is drawn from the perspective of the epidemiologist or test developer. But it doesn’t directly provide information of use to the patient, simply because the patient has only a test result (\\(\\mathbb{P}\\) or \\(\\mathbb{H}\\)) but no definitive knowledge of the actual condition (C or H).\nRe-organizing the epidemiologist’s graph can put it in a form relevant to the patient. Instead of plotting people by C or H, we can plot them by \\(\\mathbb{P}\\) or \\(\\mathbb{H}\\). This perspective is shown in Figure 35.3, which is exactly the same people as in ?fig-divided-by-condition2 but arranged differently.\n\n\n\n\n\nFigure 35.3: The population on which the classifier will be used.\n\n\n\n\nFor the patient who has gotten a \\(\\mathbb{P}\\) result, the left panel of Figure 35.3 is highly informative. The patient can see that only a small fraction of the people testing \\(\\mathbb{P}\\) actually have condition C. (The people with C are shown as filled symbols.)\nThe test result \\(\\mathbb{P}\\) is not definitive, it is merely a clue."
  },
  {
    "objectID": "Reading-notes-lesson-35.html#likelihood",
    "href": "Reading-notes-lesson-35.html#likelihood",
    "title": "35  Accounting for prevalence",
    "section": "Likelihood",
    "text": "Likelihood\nA “clue” is a piece of information or an observation that tells something about a mystery, but not usally everything. As an example, consider a patient who has just woken up from a coma and doesn’t know what month it is. It is a mystery. With no information at all, it is almost equally likely to be any month. So the hypotheses in contention might be labeled Jan, Feb, March, and so on.\nThe person looks out the window and observes snow falling. The observation of snow is a clue. It tells something about what month it might be, but not everything. For instance, the possibility that it is July becomes much less likely if snow has been observed; the possibility that it is February (or January or March) becomes more likely.\nStatistical thinkers often have to make use of clues. Suppose the coma patient is a statistician. She might try to quantify the likelihood of each month given the observation of snow. Here’s a reasonable try:\n\n\n\n\n\n\n\n\nMonth\nProbability of seeing snow when looking out the window for the first time each day\nNotation\n\n\n\n\nJanuary\n2.3%\n\\(p(\\text{snow}{\\ |\\!\\!|\\  } \\text{January})\\)\n\n\nFebruary\n3.5%\n\\(p(\\text{snow}{\\ |\\!\\!|\\  } \\text{February})\\)\n\n\nMarch\n2.1%\n\\(p(\\text{snow}{\\ |\\!\\!|\\  } \\text{March})\\)\n\n\nApril\n1.2%\n\\(p(\\text{snow}{\\ |\\!\\!|\\  } \\text{April})\\)\n\n\nMay\n0.5%\n… and so on …\n\n\nJune\n0.1%\n\n\n\nJuly\n0\n\n\n\nAugust\n0\n\n\n\nSeptember\n0.2%\n\n\n\nOctober\n0.6%\n\n\n\nNovember\n0.9%\n\n\n\nDecember\n1.4%\n\n\n\n\nThe table lists 12 probabilities, one for each month. For the coma patient, these probabilities let her look up which months it is likely to be. For this reason, the probabilities are called “likelihoods.”\nThe coma patient has 12 hypotheses for which month it is. The table as a whole is a “likelihood function” describing how the likelihood varies from one hypothesis to another. Think of the entries in the table as having been radioed back to Earth from the 12 hypothetical planets \\({\\ |\\!\\!|\\  } \\text{January})\\) through \\({\\ |\\!\\!|\\  } \\text{December})\\).\nIt is helpful, I think, to have a notation that reminds us when we are dealing with a likelihood and a likelihood function. We will use the fancy \\({\\cal L}\\) to identify a quantity as a likelihood. The coma patient is interested in the likelihood of snow, which we will write \\({\\cal L}_\\text{snow}\\). From the table we can see that the likelihood of snow is a function of the month, that is \\({\\cal L}_\\text{snow}(\\text{month})\\), where month can be any of January through December.\nThis likelihood function has a valuable purpose: It will allow the coma patient to calculate the probability of it being any of the twelve months given her observation of snow, that is \\(p(\\text{month} {\\ |\\!\\!|\\  } \\text{snow})\\).\nIn general, likelihoods are useful for converting knowledge like \\({\\cal L}_a(b)\\) into the form \\(p(b {\\ |\\!\\!|\\  } a)\\). The formula for doing the conversion is called “Bayes’ Rule.”\nThe form of Bayes’ rule appropriate to the coma patient allows her to calculate the probability of it being any given month from the likelihoods. We also need to account for February, with only 28 days, being shorter than the other months. So we will define a probability function, \\(p(\\text{month}) = \\frac{\\text{number of days in month}}{365}\\)\n\nBayes’ Rule \\[p(\\text{month} {\\ |\\!\\!|\\  } \\text{snow}) = \\frac{{\\cal L}_\\text{snow}(\\text{month}) \\cdot p(\\text{month})}{{\\cal L}_\\text{snow}(\\text{Jan}) \\cdot p(\\text{Jan}) +\n  {\\cal L}_\\text{snow}(\\text{Feb}) \\cdot p(\\text{Feb}) + \\cdots +\n  {\\cal L}_\\text{snow}(\\text{Dec}) \\cdot p(\\text{Dec})}\\]"
  },
  {
    "objectID": "Reading-notes-lesson-35.html#sec-bayes-rule",
    "href": "Reading-notes-lesson-35.html#sec-bayes-rule",
    "title": "35  Accounting for prevalence",
    "section": "How serious is it, Doc?",
    "text": "How serious is it, Doc?\nImagine a patient getting a \\(\\mathbb{P}\\) test result and wondering what the probability is of his having condition C. That is, he wants to know \\(p(C {\\ |\\!\\!|\\  } \\mathbb{P})\\). This is equivalent to asking, “How serious is it, Doc?”\nThe doctor could point to Figure 35.2 as her answer. That figure was generated by creating a population with the relevant prevalence, using the sensitivity and specificity to determine the fraction of the C and H groups with \\(\\mathbb{P}\\) or \\(\\mathbb{H}\\) respectively, the re-organizing into new groups: the \\(\\mathbb{P}\\) group and the \\(\\mathbb{H}\\) group.\nAlternatively, we can do the calculation in the same way we did for the coma patient seeing snow. There, the observation of snow was the clue. Now, the test result \\(\\mathbb{P}\\) is the clue. One of the relevant likelihoods to interpret \\(\\mathbb{P}\\) is \\({\\cal L}_{\\mathbb{P}}(C)\\): the likelihood for a person who genuinely has condition C of getting a \\(\\mathbb{P}\\) result. Of course, this is just another way of writing the sensitivity.\nSimilarly, the specificity is \\({\\cal L}_{\\mathbb{H}}(H)\\). But since our person got a \\(\\mathbb{P}\\) result, the likelihood \\({\\cal L}_{\\mathbb{H}}(H)\\) is not directly relevant. (It would be relevant only to a person with a \\(\\mathbb{H}\\) result.) Fortunately, there is a simple relationship between \\({\\cal L}_{\\mathbb{P}}(H)\\) and \\({\\cal L}_{\\mathbb{H}}(H)\\). If we know the probability of an H person getting a \\(\\mathbb{H}\\) result we can figure out the probability of an H person getting a \\(\\mathbb{P}\\) result. \\[{\\cal L}_{\\mathbb{P}}(H) = 1 - {\\cal L}_{\\mathbb{H}}(H)\\]\nBayes’ Rule for the person with a \\(\\mathbb{P}\\) result is\n\\[p(C{\\ |\\!\\!|\\  } \\mathbb{P}) = \\frac{{\\cal L}_{\\mathbb{P}}(C) \\cdot p(C)}{{\\cal L}_{\\mathbb{P}}(C) \\cdot p(C) + {\\cal L}_{\\mathbb{P}}(H) \\cdot p(H)}\\]\n\n\n\n\n\n\nCalculating \\(p(C{\\ |\\!\\!|\\  } \\mathbb{P})\\)\n\n\n\nSuppose that \\(p(C) = 1\\%\\) for this age of patient. (Consequently, \\(p(H) = 99\\%\\).) And imagine that the test taken by the patient has a threshold score of 1. From Figure 34.7 we can look up the sensitivity (\\({\\cal L}_{\\mathbb{P}}(C) = 0.95\\)) and specificity (\\({\\cal L}_{\\mathbb{P}}(H) = 0.50)\\) for the test. Substituting these numerical values into Bayes’ Rule gives\n\\[p(C {\\ |\\!\\!|\\  } \\mathbb{P}) = \\frac{0.95\\times 0.01}{0.95\\times 0.01 + 0.50*0.99} = 1.9\\%\\] The \\(\\mathbb{P}\\) result has changed the probability that the patient has C from 1% to 1.9%. That’s big proportionally, but not so big in absolute terms.\n\n\nThe advantage of the Bayes’ Rule form of the calculation over the \\(\\mathbb{P}\\) group in Figure 35.2 is that it is very easy to do the Bayes’ Rule calculation for any value of prevalence \\(p(C)\\). Why would we be interested in doing this?\nTypically the prevalence of a condition is different for different groups in the population. For example, for an 80-year-old with a family history of C the prevalence might be 20% rather than the 1% that applied to the patient in the previous example. For the 80-year-old, the probability of having C given a \\(\\mathbb{P}\\) result is substantially different from the 1.9% found in the example:\n\\[p(C {\\ |\\!\\!|\\  } \\mathbb{P}) = \\frac{0.95\\times 0.2}{0.95\\times 0.2 + 0.50*0.8} = 32\\%\\]"
  },
  {
    "objectID": "Reading-notes-lesson-35.html#screening-tests",
    "href": "Reading-notes-lesson-35.html#screening-tests",
    "title": "35  Accounting for prevalence",
    "section": "Screening tests",
    "text": "Screening tests\nThe reliability of a \\(\\mathbb{P}\\) result differs depending on the prevalence of C. A consequence of this is that medical screening tests are recommended for one group of people but not for another.\nFor instance, the US Preventative Services Task Force (USPSTF) issues recommendations about a variety of medical screening tests. According to the Centers for Disease Control (CDC) summary:\n\nThe USPSTF recommends that women who are 50 to 74 years old and are at average risk for breast cancer get a mammogram every two years. Women who are 40 to 49 years old should talk to their doctor or other health care provider about when to start and how often to get a mammogram.\n\nRecommendations such as this can be baffling. Why recommend mammograms only for people 50 to 74? Why not for older women as well? And how come women 40-49 are only told to “talk to their doctor?”\nThe CDC summary needs decoding. For instance, the “talk to [your] doctor” recommendation really means, “We don’t think a mammogram is useful to you, but we’re not going to say that straight out because you’ll think we are denying you something. We’ll let your doctor take the heat, although typically if you ask for a mammogram, your doctor will order one for you. If you are a woman younger than 40, a mammogram is even less likely to give a useful result, so unlikely that we won’t even hint you should talk to a doctor.”\nThe reason mammograms are not recommended for women 40-49 is that the prevalence for breast cancer is much lower in that group of people than in the 50-74 group. The prevalence of breast cancer is even lower in women younger than 40.\nSo what about women 75+? The prevalence of breast cancer is high in this group, but at that age, non-treatment is likely to be the most sensible option. Cancers can take a long while to develop from the stage identified on a mammogram, and at age 75+ it’s not likely to be the cause of eventual death.\nThe USPSTF web site goes into some detail about the reasoning for their recommendations. It’s worthwhile reading to see what considerations went into their decision-making process.\n\nThe Loss Function\n\n\n\n\n\n\nIn Draft\n\n\n\nNEED TO FIX THIS. The prevalence wasn’t included in the calculation.\n\n\nIn order to set the threshold at an optimal level, it is important to measure the impact of the positive or negative test result. This impact of course will depend on whether the test is right or wrong about the person’s true condition. It is conventional to measure the impact as a “loss,” that is, the amount of harm that is done.\nIf the test result is right, there’s no loss. Of course, it’s not nice that a person is C, but a \\(\\mathbb{P}\\) test result will steer our actions to treat the condition appropriately: no loss in that.\nTypically, the loss stemming from a false negative is reckoned as more than the loss of a false positive. A false negative will lead to failure to treat the person for a condition that he or she actually has.\nIn contrast, a false-positive will lead to unnecessary treatment. This also is a loss that includes several components that would have been avoided if the test result had been right. The cost of the treatment itself is one part of the loss. The harm that a treatment might do is another part of the loss. And the anxiety that the person and his or her family go through is still another part of the loss. These losses are not necessarily small. The woman who gets a false positive breast-cancer diagnosis will suffer from the effects of chemotherapy and the loss of breast tissue. The man who gets a false-positive prostate-cancer diagnosis may end up with urinary incontinence and impotence.\nThe aim in setting the threshold is to minimize the total loss. This will be the loss incurred due to false negative times the number of false negatives plus the loss incurred from a false positive times the number of false positives.\n\n\n\n\n\n\nDemonstration: Setting the optimal threshold\n\n\n\nIn Lesson 34, we saw that the threshold for transforming a raw test score into a \\(\\mathbb{P}\\) or \\(\\mathbb{H}\\) result determined the sensitivity and specificity of the test. (See Figure 34.7.) Of course, its best if both sensitivity and specificity are as high as possible, but there is a trade-off between the two: increasing sensitivity by lowering the threshold will decrease specificity. Likewise, raising the threshold will improve specificity but lower sensitivity.\nThe “loss function” provides a way to set an optimal value for the threshold. It is a function, because the loss depends on whether the test result is a false-positive or a false-negative.\nSuppose that the\n\n\n\n\n\n\n\n\nFigure 35.4: Total loss as a function of test threshold for the test shown in Figure 34.7. In the blue curve, a false-negative is 3 times more costly than a false-positive. In the orange curve they are equally costly. In the red curve, a false-positive is 10 times more costly than a false-negative."
  },
  {
    "objectID": "Reading-notes-lesson-36.html#tests-generally",
    "href": "Reading-notes-lesson-36.html#tests-generally",
    "title": "36  Hypothesis testing",
    "section": "Tests, generally",
    "text": "Tests, generally\nIn Lessons 34 and 35, the classifiers we built had two possible outputs, \\({\\mathbb{P}}\\) or \\({\\mathbb{N}}\\). The classifier is only part of a bigger procedure which we called a “test.” To avoid unnecessary abstraction, our examples featured medical tests.Medical tests often have three or more outcomes such as “high,” “normal,” or “low.” This does not fundamentally change the situation from our \\({\\mathbb{P}}\\) or \\({\\mathbb{N}}\\) paradigm.\nIn a medical test, the first step in the procedure usually involves a measurement procedure for an individual, for instance, counting white blood cells or measuring the concentration of prostate-specific antigen (PSA).\nThe second step is purely arithmetical, comparing the measurement result to a threshold, thereby determining if the output should be \\({\\mathbb{P}}\\) or \\({\\mathbb{N}}\\). Lessons 34 and 35 were largely about how to set a good value for the threshold and involved stating a loss function and considering the prevalence of the condition involved.\nA more familiar kind of test is the one taken by students in school, for instance, an algebra test. As the reader knows, in an algebra test, the subject is made to answer questions, the number of correct answers counted, and that count applied to a threshold to determine whether the overall result is “pass” or “fail.” There is not an explicit underlying condition, say, “expert” or “dilettante.” Consequently, there is no such thing as a false-negative (an expert who fails the test) or a false-positive (a dilettante who passes the test). All that matters is the test result itself\nUnlike medical tests, academic tests are generally not the product of the sort of careful development phase described in Lesson 35. In an algebra test, there is no training data, that is, no sample of subjects who have been observed definitively to be “experts” or “dilettantes” and who take the test to be classified as “pass” or “fail.” There is no calculation of a sensitivity or specificity to characterize the test and no use of a loss function to bend sensitivity, specificity, and prevalence into a threshold dividing “pass” from “fail.” Academicians never tell their students what the false positive and false negative rates are.\nStudents and teachers think of an academic test as that part of the overall procedure where questions are asked and answered. Of course, there is a follow-up procedure that we call “grading,” where the correct answers are counted and the count converted to a pass/fail result. Sometimes the threshold between pass and fail is not fixed, but is set by the instructor to achieve a desired pass rate. This is called “grading on a curve,” the threshold depends on the observed counts.\nNHT is like “grading on a curve.” The data collection and summary of the data is not part of the procedure. Like “grading on a curve”, NHT starts at the point where the data have already been recorded and summarized. (Typically, the summary is the coefficient from a regression model or some other statistical measure like R2.)\nNHT is only about grading the summary, which is done on a curve. The grading is accomplished by examining a likelihood calculated from the summary. As notation for this likelihood, we will write \\({\\cal L}_{\\mathbb{S}}(\\text{Null hypothesis})\\).\nThe phrase “Null hypothesis” is too long to make for pleasant mathematical reading. Consequently, a shorter symbol, H_0_, is often used. The subscript 0 is a shorthand for “null.” The “H” indicates a hypothesis."
  },
  {
    "objectID": "Reading-notes-lesson-36.html#the-null-hypothesis",
    "href": "Reading-notes-lesson-36.html#the-null-hypothesis",
    "title": "36  Hypothesis testing",
    "section": "The Null hypothesis",
    "text": "The Null hypothesis\nThe key to understanding Null hypothesis testing is to know what the Null hypothesis is claiming.\nIn a technical sense, it would suffice to say that the Null hypothesis is a claim that the effect size is zero (or that R2 is zero) except for sampling variation. Making sense of this requires that one know what an “effect size” (or “R2”) is and what “sampling variation” means. At this point in these Lessons, you ought to know all of these things. But how to talk about hypothesis testing to a general audience?\nOne strategy is to describe the Null as the “absence of any effect” or “relationship.” Another common strategy is to avoid mentioning the Null at all and use alternates such as “the result is significant” or “not due to chance.”\nAnother way to think about the Null hypothesis is algorithmically. A Null-hypothesis relationship—really, a lack of relationship—can be created between two variables by shuffling one of them. Shuffling was introduced briefly in Lesson 29, where it was used to simulate an explanatory variable unrelated to the response.\nTo illustrate, consider the Galton data about the heights of adult children and their parents. We will make a simple model of height as a function of sex—everyday experience suggests a relationship between the two variables.\n\nlm(height ~ sex, data=Galton) %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    64.0 \n    64.0 \n  \n  \n    sexM \n    4.8 \n    5.4 \n  \n\n\n\n\nIf the Null hypothesis were true, that is, if sex were unrelated to height, the sexM coefficient ought to be close to zero and a confidence interval on the coefficient will usually include zero. But for height ~ sex the confidence interval does not include zero.\nNow consider what happens if we shuffle one or both of the variables.\n\nlm(height ~ shuffle(sex), data=Galton) %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    66.00 \n    67.00 \n  \n  \n    shuffle(sex)M \n    -0.35 \n    0.59 \n  \n\n\n\n\n\nlm(shuffle(height) ~ sex, data=Galton) %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    66.00 \n    67.00 \n  \n  \n    sexM \n    -0.47 \n    0.47 \n  \n\n\n\n\n\nlm(shuffle(height) ~ shuffle(sex), data=Galton) %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    67.00 \n    67.00 \n  \n  \n    shuffle(sex)M \n    -0.75 \n    0.19 \n  \n\n\n\n\nAll these confidence intervals include zero, as expected.\n\n\n\n\n\n\nFigure 36.1: Planet Null, known symbolically as \\(\\ |\\!\\!| H_0)\\). Any pattern on Planet Null is attributed to chance, in the form of sampling variation.\n\n\n\nIn terms of the planet metaphor for hypotheses, the Null hypothesis \\({\\ |\\!\\!|\\  } H_0)\\) is a planet. Variables on this planet are always unrelated to one another. The possible indication of a pattern is due to sampling variation, not a genuine relationship.\n\nRiddle: How do we get to Planet Null?\n\nTake the space shuffle.\nBefore the advent of ubiquitous computing, the Null hypothesis was implemented using algebra and probability theory. An example of such theory appeared in Lesson 29. The blue diagonal line in Figure 29.3 reflects what the average value of R2 would be if a large number of trials were run on the model y ~ x1 + x2 + ... + xk where the x’s are unrelated to the y. Another part of the theory has to do with the “distribution” of the F statistic from Lesson 29, which we will discuss in Lesson 37."
  },
  {
    "objectID": "Reading-notes-lesson-36.html#the-alternative-hypothesis",
    "href": "Reading-notes-lesson-36.html#the-alternative-hypothesis",
    "title": "36  Hypothesis testing",
    "section": "The Alternative hypothesis",
    "text": "The Alternative hypothesis\nIn Null hypothesis testing, there is only the one hypothesis—the Null—under consideration. Since the world \\({\\ |\\!\\!|\\  } H_0)\\) can be created by shuffling, the computations for NHT can be done pretty easily even without the probability theory just mentioned.\nThere has been a controversy since the 1930s about whether hypothesis testing—in the broad sense—should involve two (or more) competing hypotheses. One of these could be the Null hypothesis, the other, which we call the “Alternative hypothesis” (\\(H_a\\)) a statement of a specific non-null relationship.\n\n\n\n\n\n\nFigure 36.2: Planet Alt, that is, \\(\\ |\\!\\!|\\  H_a)\\) might look like this. We draw it as a cartoon planet, since any particular hypothesis is a product of the imagination.\n\n\n\nThe situation with two hypotheses would be very similar to that presented in Lessons 34 and 35. In those lessons, the two hypotheses were C and H. In developing a classifier, one starts by collecting a training sample which is a mixture of cases of C and H. But, in general, with a competition of hypothesis—\\(H_0\\) and \\(H_a\\)—we don’t have any real-world objects to sample that are known to be examples of the two hypotheses. Instead, we have to create them computationally. Instances of \\(H_0\\) can be made by data shuffling. But instances of \\(H_a\\) need to be generated by some other mechanism, perhaps one akin to the DAGs we have used in these lessons.\nWith mechanisms to generate data from both the Null and Alternative hypotheses, we would take the statistical summary \\(\\mathbb{S}\\) of the actual data, and compute the likelihoods for each hypothesis: \\({\\cal L}_{\\mathbb{S}}(H_0)\\) and \\({\\cal L}_{\\mathbb{S}}(H_a)\\). It should not be too controversial in a practical process to set the prior probability for each hypothesis at the same value: \\(p(H_0) = p(H_a) = {\\small \\frac{1}{2}}\\). Then, turn the crank of Bayes’ Rule (Section 35.4) to compute the posterior probabilities. If the posterior of one or the other hypothesis is much greater than \\({\\small \\frac{1}{2}}\\), we would have compelling evidence in favor of that hypothesis.1\nThere are specialized methods of Bayesian statistics and whole courses on the topic. An excellent online course is Statistical Rethinking.\nBefore the widespread acceptance of the Bayesian approach, statisticians Jerzy Neyman and Egon Pearson proposed a two-hypothesis framework in 1933. We will discuss this in ?sec-power.\n\n\n\n\n\n\nNot an alternative!\n\n\n\nIf you have studied statistics before, you likely have been exposed to NHT. Many textbook descriptions of NHT appear to make use of an “alternative hypothesis” within NHT. This style is traditional and so common in textbooks that it seems disrepectful to state plainly that it is wrong. There is only one hypothesis being tested in NHT: the Null.\nIn the textbook presentation of NHT, the “alternative” hypothesis is not a specific claim—for instance, “the drug reduces blood pressure by 10 mmHg”. Instead, the student is given a pointless choice of three versions of the alternative. These are usually written \\(H_a \\neq H_0\\) or as \\(H_a < H_0\\) or as \\(H_a > H_0\\), and amount to saying “the effect size is non-zero,” “the effect size is negative,” or “the effect size is positive.”\nOutside of textbooks, only \\(H_a \\neq H_0\\) is properly used. The other two textbook choices provide, at best, variations on exam questions. At worst, they are a way to put a thumb on the scale to disadvantage the Null."
  },
  {
    "objectID": "Reading-notes-lesson-36.html#sec-under-the-null",
    "href": "Reading-notes-lesson-36.html#sec-under-the-null",
    "title": "36  Hypothesis testing",
    "section": "“Under the Null”",
    "text": "“Under the Null”\nUsing the shuffling algorithm, the computation underlying NHT is very much like the technique introduced in Lesson 23: create a set of trials to represent the sampling distribution. The twist in NHT is that the sampling distribution is calculated on Planet Null. Or, in conventional statistical language, what’s computed is the “sampling distribution under the Null.”\nThe phrase “under the Null” is often described as being shorthand for “assuming the Null to be true.” When doing a calculation, “assuming the Null to be true” might be better expressed more emphatically: “arranging things so that the Null is true” or “enforcing the Null.”\nWe will use the Galton data, and the model height ~ mother, to illustrate computing the “sampling distribution under the Null.” The relationship being expressed by height ~ mother is that the adult child’s height is related at least in part to the mother’s height. As always, the effect size is a good way to quantify that relationship. Here it is:\n\nmod <- lm(height ~ mother, data=Galton) \nmod %>% coef()\n\n(Intercept)      mother \n 46.6907659   0.3131795 \n\nmod %>% conf_interval()\n\n\n\n \n  \n    term \n    .lwr \n    .upr \n  \n \n\n  \n    (Intercept) \n    40.2951224 \n    53.0864094 \n  \n  \n    mother \n    0.2134437 \n    0.4129153 \n  \n\n\n\n\nThese reports says that changing the model input by one inch will translate to about 0.3 inches in the output. The uncertainly due to sampling variation broadens the 0.3 into an interval, 0.2 to 0.4 inches of height per inch of mother.\nNow we rocket off to Planet Null. On Planet Null, children are not related to their mothers. Or, put another way, the unique relationship between a mother and her child is a Planet-Earth concept. On Planet Null, each mothers and children are paired at random.\nTo calculate the mother coefficient on Planet Null, use shuffle(). Here’s the result from one sample:\n\nlm(height ~ shuffle(mother), data=Galton) %>% coef_summary()\n\n\n\n \n  \n    term \n    coefficient \n  \n \n\n  \n    (Intercept) \n    67.1960085 \n  \n  \n    shuffle(mother) \n    -0.0067929 \n  \n\n\n\n\nRepeating the above generates another sample from Planet Null:\n\nlm(height ~ shuffle(mother), data=Galton) %>% coef_summary()\n\n\n\n \n  \n    term \n    coefficient \n  \n \n\n  \n    (Intercept) \n    62.7905224 \n  \n  \n    shuffle(mother) \n    0.0619522 \n  \n\n\n\n\nFrom these two Planet Null samples, it is already evident that the shuffle(mother) coefficient on Planet Null tends to be closer to zero than the mother coefficient from Planet Earth.\nWe can make a more compelling and precise statement if we look at a large number of Planet-Null samples:\n\nTrials <- do(1000) * {\n  lm(height ~ shuffle(mother), data=Galton) %>% coef()\n}\n\n\n\nCode\nTrials %>%\n  ggplot(aes(y = mother, x = \"All\")) +\n  geom_jitter(alpha = 0.2, width=.15) +\n  geom_violin(alpha = .2, fill = \"blue\", color = NA) +\n  geom_point(aes(y = 0.31, x = 1), color=\"red\") +\n  xlab(\"\")\n\n\n\n\n\n\nFigure 36.3: Values of the shuffle(mother) coefficient. Each gray dot is the result from one shuffling trial. The violin shows the sampling distribution under the null. The red dot is the mother coefficient found without shuffling.\n\n\n\nThe set of mother coefficients in these trials reflects us the “sampling distribution under the Null.” (Figure 36.3) The “sampling distribution under the Null” is represented by the gray dots. The violin gives an easier to read representation. For this model, the mother coefficient from the sample from Planet Earth is so far different that it cannot pretend to be an instance from Planet Null."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#the-p-value",
    "href": "Reading-notes-lesson-37.html#the-p-value",
    "title": "37  Calculating a p-value",
    "section": "The p-value",
    "text": "The p-value\nFigure 36.3 (reproduced in the margin) shows an example of the raw material: 1000 values of \\(\\mathbb{Snull}_i\\) and the single, unique value of \\(\\mathbb{Sreal}\\) from the model lm(height ~ mother, data=Galton).\n\nknitr::include_graphics(\"www/fig-shuffle-mother.png\")\n\n\n\n\nThe p-value comes from the fraction of the \\(\\mathbb{Snull}_i\\) that are larger in value than the \\(\\mathbb{Sreal}\\). In Figure 36.3, none of the 1000 \\(\\mathbb{Snull}\\) are larger than \\(\\mathbb{Sreal}\\). Therefore, we write \\(p < 1/1000\\) or \\(p < 0.001\\).\nThe p-value is the final result of an NHT. TALK ABOUT how to write about the p-value LATER ON.\nNHT is such a popular technique that statistical software will find the p-values from regression models for you. Consequently, in practice the shuffling technique is reserved for more specialized situations. For these Lessons, the specialized “situation” is pedagogical; we are trying to help you understand the concept of p-values and shuffling provides a concrete way to do this. In most modeling work, however, pedagogy is not an issue. So expect to use software to calculate p-values.\nThe p-value software we use in these Lessons is the two summary functions, regression_summary() and anova_summary().\n\nlm(height ~ mother, data = Galton) %>% regression_summary()\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    46.6907659 \n    3.2587355 \n    14.327879 \n    0 \n  \n  \n    mother \n    0.3131795 \n    0.0508178 \n    6.162793 \n    0 \n  \n\n\n\n\nThe regression model height ~ mother has two coefficients (the “estimate” column from regression_summary()). Insofar as our concern is the relationship between child’s height and mother, only the p-value for mother is of interest.\n\n\n\n\n\n\nSoftware for P-values\n\n\n\nIn these Lessons, we use the regression_summary() and anova_summary() R functions to calculate p-values from models. These two functions come from the {math300} package, which was written specially for these Lessons. In standard R, the equivalents are summary() and anova().\nIn fact, regression_summary() and anova_summary() are merely wrappers around summary() and anova(). The wrappers make sure that the output is in the form of a data frame and therefore suitable for data wrangling. The output from summary() and anova(), however, are not in a data-frame format."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#basic-interpretation-of-p-values",
    "href": "Reading-notes-lesson-37.html#basic-interpretation-of-p-values",
    "title": "37  Calculating a p-value",
    "section": "Basic interpretation of p-values",
    "text": "Basic interpretation of p-values\nWhen the p-value is small, the likelihood of \\(\\mathbb{Sreal}\\) under the Null hypothesis, that is, \\({\\cal L}_{\\mathbb{Sreal}}(\\text{Null hypothesis})\\) is also small. A small likelihood of a given hypothesis means that the hypothesis is not a compelling explanation for the observed \\(\\mathbb{Sreal}\\).\nThere is a formal vocabulary for NHT. Instead of saying, casually, “The Null hypothesis is not a compelling explanation for \\(\\mathbb{Sreal}\\), the formal NHT statement is,”The Null hypothesis is rejected.”\nCommonly, the threshold 0.05 is given as the numerical definition of “small” in “small likelihood.” For example, the p-value on the mother coefficient in the model lm(height ~ mother, data=Galton) is \\(p < 0.01\\). This is obviously less than 0.05, so the outcome of the NHT is to “reject the Null hypothesis.”\nSuppose, on the other hand, that the p-value had been “large,” that is \\(0.05 < p\\). What phrase should we use to summarize this situation. It is tempting—but wrong—to think that this leads to “accepting” the Null hypothesis. Instead the proper NHT phrase to use is “fail to reject the Null.”\nWe will return in Lesson 38 to the question of whether 0.05 is a good threshold to use. That’s part of a broader controversy.\nAnother part of the NHT formal vocabulary is the phrase “statistically significant.” In the everyday sense of the word, “significant” suggests “important,”, “worthy of attention,” or “noteworthy.” In NHT speak, “statistically significant” is a synonym for “reject the Null.” The NHT meaning of “statistically significant” has nothing at all to do with utility of the result.\nDifferent fields have different standards for defining small. For instance, it’s common in psychology to consider \\(p < 0.10\\) as fairly small, while in physics, “small” means perhaps \\(p < 0.001\\) or even \\(p < 0.000001\\).\nIt may seem odd that there is no universal agreement about “small.” The reason is that p-values are part of a standard operating procedure for evaluating research results to know if they are worthy of publication.\nIn physics, laws and models are meant to be exact or close to exact. Lord Rutherford (1871-1935), an important physicist who won the Nobel prize in 1908, famously disparaged the use of statistics, reportedly saying, “If your experiment needs statistics, you should have done a better experiment.” This was in an era where the p-value standard operating procedure had not yet been invented. Today, when p-values are common in most fields, Rutherford’s distaste for statistical method is reflected in p-value thresholds like \\(p < 0.000001\\).\nIn other fields such as economics or psychology or clinical medicine, models are sought that are useful but without any expectation that they be exact. (In the 19th and early 20th century, psychologists and economists sometimes used the vocabulary of “law” to describe their findings, but “model” is more appropriate, because, unlike physics, the laws are not strictly enforced!) Often, in economics or psychology or medicine, the size of a sample used to train a model is less than, say, \\(n=100\\). And the units of observation—people or countries, for instance—are different one from the other, quite unlike, say, electrons, which are all the same. Consequently, sampling variation is often an important source of noise, obscuring relationships or even suggesting relationships that are not really there. (See Lesson 31.) This situation—small sample size, variation in observational units, and large sampling variation—would cause many useful findings to go unreported, as would happen if \\(p < 0.000001\\) were the standard. So a less stringent threshold for publication is used, most commonly 0.05."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#p-values-for-coefficients",
    "href": "Reading-notes-lesson-37.html#p-values-for-coefficients",
    "title": "37  Calculating a p-value",
    "section": "P-values for coefficients",
    "text": "P-values for coefficients\nFor any regression model, the “regression report” contains one row for every coefficient for the model. Each of these rows will have a coefficient value (“estimate”) and a p-value. There are two additional columns: a standard error for the coefficient (“std.error”) and a value (labeled “statistic”) that is always just the estimate divided by the standard error.\nThe idea of the standard error was introduced in Lesson 23 (Section 23.1). The point of the standard error is to summarize the amount of sampling variation in the coefficient. But we standardized on the confidence interval format to summarize sampling variation.\nMany statisticians think there is little point to calculating a p-value on a model coefficient because the confidence interval contains all the information needed. Importantly, model coefficients and their confidence intervals come with units; the units are the connection between the number and the real world. P-values are without units, and their value depends strongly on the sample size. Thus, they mix together relevant information about the magnitude of the effect and incidental information about the size of the sample used for training."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#p-values-for-f",
    "href": "Reading-notes-lesson-37.html#p-values-for-f",
    "title": "37  Calculating a p-value",
    "section": "P-values for F",
    "text": "P-values for F\n[[Say how a model coefficient is different from a set of terms. The question is whether the new term adds information on top of the existing terms. ]]\nSometimes the interest is more general: Do any of these terms contribute to explaining variation in the response variable? In such situations, the appropriate p-value is one that compares one model to another. This style of p-value—not on the individual coefficients but on model terms—comes from a calculation called “analysis of variance.”"
  },
  {
    "objectID": "Reading-notes-lesson-37.html#traditional-names-for-tests",
    "href": "Reading-notes-lesson-37.html#traditional-names-for-tests",
    "title": "37  Calculating a p-value",
    "section": "Traditional names for tests",
    "text": "Traditional names for tests"
  },
  {
    "objectID": "Reading-notes-lesson-37.html#tests-in-textbooks",
    "href": "Reading-notes-lesson-37.html#tests-in-textbooks",
    "title": "37  Calculating a p-value",
    "section": "Tests in textbooks",
    "text": "Tests in textbooks\nStatistics textbooks usually include several different settings for “hypothesis tests.” I’ve just pulled a best-selling book off my shelf and find listed the following tests spread across eight chapters occupying about 250 pages.\n\nhypothesis test on a single proportion\nhypothesis test on the mean of a variable\nhypothesis test on the difference in mean between two groups (with 3 test varieties in this category)\nhypothesis test on the paired difference (meaning, for example, measurements made both before and after)\nhypothesis test on counts of a single categorical variable\nhypothesis test on independence between two categorical variables\nhypothesis test on the slope of a regression line\nhypothesis test on differences among several groups\nhypothesis test on R2\n\nAs statistics developed, early in the 20th century, distinct tests were developed for different kinds of situations. Each such test was given its own name, for example, a “t-test” or a “chi-squared test.” Honoring this history, statistics textbooks present hypothesis testing as if each test were a new and novel kind of animal.\nIn fact, almost all the different tests named in introductory statistics books are really just different manifestations of regression. Regression is to “animal” the way t-test is to “elephant.” An important theme in the history of statistics is that out of the diversity of statistical methods, almost all of them are encompassed by one method: regression modeling.\nIn these Lessons, we’ve focussed on that one method, rather than introducing all sorts of different formulas and calculations which, in the end, are just special cases of regression. Nonetheless, most people who are taught statistics were never told that the different methods fit into a single unified framework. Consequently, they use different names for the different methods. Communicating in a world where people learned the traditional names, you have to be able to recognize those names know which regression model they refer to. In the table below, we will use different letters to refer to different kinds of explanatory and response variables.\n\nx and y: quantitative variables\ngroup: a categorical variable with multiple (\\(\\geq 3\\)) levels.\nyesno: a categorical variable with exactly two levels (which can always be encoded as a zero-one quantitative variable)\n\n\n\n\n\n\n\n\nModel specification\ntraditional name\n\n\n\n\ny ~ 1\nt-test on a single mean\n\n\nyesno ~ 1\np-test on a single proportion.\n\n\ny ~ yesno\nt-test on the difference between two means\n\n\nyesno1 ~ yesno2\np-test on the difference between two proportions\n\n\ny ~ x\nt-test on a slope\n\n\ny ~ group\nANOVA test on the difference among the means of multiple groups\n\n\ny ~ group1 * group2\nTwo-way ANOVA\n\n\ny ~ x * yesno\nt-test on the difference between two slopes. (Note the *, indicating interaction)\n\n\n\nAnother named test, the z-test, is a special kind of t-test where you know the variance of a variable without having to calculate it from data. This situation hardly every arises in practice, and mostly it is used as a soft introduction to the t-test."
  },
  {
    "objectID": "Reading-notes-lesson-37.html#p-values-and-covariates",
    "href": "Reading-notes-lesson-37.html#p-values-and-covariates",
    "title": "37  Calculating a p-value",
    "section": "P-values and covariates",
    "text": "P-values and covariates\nUse cancer/grass-treatment example from Lesson 30 to illustrate how failing to think about covariates before the study analysis can lead to false discovery.\nUse age in marriage data.\nSo, standard operating procedures were based on the tools at hand. We will return to the mismatch between hypothesis testing and the contemporary world in Lesson 38.\n\n\\[\n\\begin{array}{cc|cc} & & \\textbf{Test Conclusion} &\\\\\n& & \\text{do not reject } H_0 &  \\text{reject } H_0 \\text{ in favor of }H_A  \\\\\n\\textbf{Truth} & \\hline H_0 \\text{ true} & \\text{Correct Decision} &  \\text{Type 1 Error}  \\\\\n& H_A \\text{true} & \\text{Type 2 Error} & \\text{Correct Decision}  \\\\\n\\end{array}\n\\]\n\n\nA Type 1 error, also called a false positive, is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination (from the Case Study) and the commercial length studies, it is possible that we made a Type 1 error in one or both of those studies. A Type 2 error, also called a false negative, is failing to reject the null hypothesis when the alternative is actually true. A Type 2 error was not possible in the gender discrimination or commercial length studies because we rejected the null hypothesis.\n\n\n\n\n\n\n\n\nThe chi-squared test\n\n\n\nMost statistics books include two versions of a test invented around 1900 that deals with counts at different levels of a categorical variable. This chi-squared test is genuinely different from regression. And, in theoretical statistics the chi-squared distribution has an important role to play.\nThe chi-squared test of independence could be written, in regression notation, as group1 ~ group2. But regression does not handle the case of a categorical variable with multiple levels.\nHowever, in practice the chi-squared test of independence is very hard to interpret except when one or both of the variables has two levels. This is because there is nothing analogous to model coefficients or effect size that comes from the chi-squared test.\nThe tendency in research, even when group1 has more than two levels, is to combine groups to produce a yesno variable. Chi-squared can be used with the response variable being yesno and almost all textbook examples are of this nature.\nBut for a yesno response variable, a superior, more flexible and more informative method is logistic regression.\n\n\nANOVA, which is always a comparison of two models, say y~1 versus y~group involves something called an F-test. For the simpler setting of the t-test, the model y~yesno, an F-test can also be done. Which to do, t or F? It turns out that t2 is exactly the same as F."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#avoid-bad-habits",
    "href": "Reading-notes-lesson-38.html#avoid-bad-habits",
    "title": "38  False discovery",
    "section": "Avoid bad habits",
    "text": "Avoid bad habits\nNEEDS RE-ORGANIZATION\nSometimes the interest is more general: Do any of these terms contribute to explaining variation in the response variable? In such situations, the appropriate p-value is one that compares one model to another. This style of p-value—not on the individual coefficients but on model terms—comes from a calculation called “analysis of variance.”\n\nNobody likes to summarize their work with the word “fail.” And so, when “fail to reject the Null hypothesis” is the correct conclusion, people express this in softer ways.\nIt’s very common for the conclusion “fail to reject the Null” simply not to be reported at all. Historically, and even today, some journals will not accept for publication a scientific article with the conclusion “fail to reject the Null.”\nConsider the situation of a researcher whose years-long project has led to a p-value of 0.07. To soften the blow of “fail to reject,” the researcher will report the p-value itself so that the reader can see how close it is to small. In some literatures, you will see language like “tending to significance” instead of “fail to reject.” In some fields, research publications will show the notation \\(p < 0.1\\). This also indicates failure to reject the null hypothesis.\nJournalists eager to publish reports about scientific work, but facing a p-value that is a little too large, will occasionally qualify their report with this phrase: “… although the work did not reach the rigorous scientific standard for statistical significance.”\nAll of these are dodges. There’s nothing “rigorous” about \\(p < 0.05\\) although seems unfair that a researcher who had a plausible idea and did the work to test it honestly does not get to publish that work and receive acknowledgement that they are a hard-working part of the overall scientific enterprise.\n\nAnother problem with p-values stems from misinterpretation of the admittedly difficult logic that underlies them. The misinterpretations are encouraged by the use of the term “tests of significance” to the p-value method. Particularly galling is the use of the description “statistically significant” to describe a result where p < 0.05. The everyday meaning of “significant” as something of importance is in no way justified by p < 0.05. Instead, the practical importance or not is more clearly signaled by examining an effect size. (It’s extremely disappointing that journalists, who are writing for an audience that for the most part has no understanding of p-value methodology, use “significant” when reporting on the statistics of research findings. It would be more honest to use a neutral term such as “null-validated” or “p-validated” which does not confuse the statistical result with actual practical importance.)\n\nThis example of a regression table shows that p-values can sometimes be very, very small. Such smallness is often mis-interpreted as indicating that a very powerful result has been found. This is simply nonsense, which is why the more dignified notation \\(p < 0.05\\) or * is to be preferred.\n\n\n\n\n\n\n“Significance” and significant digits\n\n\n\nIn the regression summary of the height ~ mother model, the p-value on the mother coefficient was reported as 1.079105e-09. This is a symptom of the choice by software designers to report more digits than are genuinely useful.\nStatistician Jeffrey Witmer, in an editorial in the Journal of Statistics Education, distinguishes between the “mathematical” information in a p-value and the “statistical” information. Mathematically, the p-value is the result of a calculation. Statistically, the p-value is used as a symbol to indicate whether the Null hypothesis is a plausible explanation for a statistical result.\nWitmer proposes a simple rule for printing p-values: Round to 1 significant digit. This means that a p-value computed to be 0.382 would be reported as 0.4. A p-value of 0.0079 would be reported as 0.008. The justification for this rule is that there is no information in the second non-zero digit of a p-value that can meaningfully guide a conclusion about whether the Null hypothesis is a plausible explanation for a statistical result. There may be a mathematical difference between 0.0079 and 0.008, but there is no meaningful statistical difference.\nWitmer also offers a simple solution to the problem of people misinterpreting “statistically significant” as related to the everyday meaning of “significant.” Replace the term “statistically significant” with “statistically discernible.” There is no difference between the everyday sense of “discernible”—able to be perceived—and the statistical implications. In conveying statistical information, “discernible” is more descriptive than “significant.” For example, it would be appropriate to describe the implications of a p-value \\(p < 0.03\\) as, “the relationship is barely discernible from the sampling variation.”\n\n\n\n\n\n\n\n\nUse the confidence interval instead\n\n\n\nNHT applied to an effect size is intended to demonstrate whether the effect size is sufficiently far from zero that we can reasonably conclude that it is non-zero. There is a simpler way to do this: look at the confidence interval on the effect size.\nBut, for those journals that (unwisely) require p-values, you’ll have to use NHT to generate them.\n\n\nSince “fail” and “reject” are unattractive words, in practice other expressions are used. One of the notations is \\(p < 0.05\\), another is to put a asterisk (\\(^\\star\\)) next to the value of the effect size or R2. Both of these correspond to “reject the Null.” The notation used for “fail to reject” is to put nothing next to the effect size or R2, but it would be more appropriate simply to list the effect size as “n.s.” to stand for “not significant.”\n\n\n\n\n\n\nIn Draft: Power\n\n\n\nMention the idea of power and why it’s helpful to look at power when interpreting a “failure to reject.”\n\n\n\nThere is, I think, a helpful analogy to be made between hypothesis testing and the familiar ways that we try to avoid information overload on the Internet.\n“Internet protocols” organize communication into standard format “packets” that are easily and rapidly transmitted, routed, and received. These packets make possible the vast web of connections that is the Internet. Anyone can put any digital content they like inside a packets; the protocols are neutral in this regard. The Internet protocols were not designed to determine what content is worth transmitting and what is worth receiving. We rely on other systems for that, mostly at the receiving end. There are spam filters to avoid email accounts being flooded with worthless or harmful messages. There are recommender systems that compare your history of music or movie streaming to that of others in order to identify what new content you might like. Search engines look inside web pages to identify connections and rank highly those pages that are linked to by other highly ranked pages. These systems leave creators free to follow their interests, ideas, and imaginations, while providing a little guidance to people who want to access some content but avoid being overcrowded by other content that is not worthwhile.\nHistorically, there were earlier waves of technology that increased the ability to communicate. Printing and postal systems emerged in the 13th and following systems. Before those innovations, communication was outrageously expensive, requiring hand-copying of manuscripts, couriers, and camel trains. Content was controlled to some extent by authorities: government censorship; church “indices” and spritual authorities; and often the authorities of those famous classical philosophers and poets whose work and thought was promulgated by early universities.\nAbout four centuries ago, such authorities were being challenged. It slowly became accepted to make judgements based on observations and to disregard antique authorities. Enlightened “scientists” communicated their discoveries in hand-written letters to one another.1 In the late 1600s, another, possibly more efficient means of communication was developed: scientific societies where members met and read aloud their work to an audience, and the journals of such societies which enabled mass communication to those scientists distant from the society’s meetings in time or space.\nEarly scientific journals are delightful collations on diverse and miscellaneous subjects. Everything seems to have been of interest to everyone. Publication was regulated by the recommendations of “members” of the society; new members were admitted by the consensus of earlier members.\nOver the centuries, the growth of scientific content and the specialization of methodology called for research findings to be sorted by area. But there was still need to regulate publication, to avoid distracting readers with worthless information.\nHand-in-hand with the scientific revolution’s reliance on observation and data rather than authority came the need to standardize methods for summarizing data. This might be called a “statistical protocol” by analogy to Internet protocols, but there is no wise governing body, only consensus and “accepted practice.”\nThe data from a bench-top experiment might consist of, say, six numbers: three from the treatment and three from controls. The arithmetic means of these two groups is practically certain to be non-zero, even if the the treatment had no effect. This meant that a means was needed to establish when the difference in means was large enough to suggest the two groups might be genuinely different and that the treatment did have an effect. The statistical protocol to decide such things needed to be simple: computers weren’t available and there were no courses to teach statistical method until the 1960s. In the 1930s, prominent statistical pioneer Ronald Fisher published a slim volume, Statistical Methods for Research Workers which laid out methods for managing and standardizing the calculations. Fisher’s authority was substantial but not absolute. Differing philosophical views also came to influence “accepted practice.”\nEarly statistics books and courses codified “accepted practice.” What emerged is the system of calculations that we call “hypothesis testing” and the ubiquitous p-value. Still, this was rooted in the need to avoid journals wasting library-shelf-space and reader time with experiments that produced arithmetic differences between groups that were accidental and not genuinely “significant.’\nThe now-codified accepted practice was in many ways similar to the protocols used by search engines and social media to direct our eyes and ears to content that might, possibly, be worthwhile. These systems are far from perfect, sometimes hiding good content or promoting worthless content. And, of course, the worth of content is a matter of personal interests and values, something that computer algorithms can mimic only imperfectly.\n“Hypothesis testing” is an ad hoc set of not always consistent concepts cobbled together by a unorganized community of independent researchers, steered perhaps by the perceived authority of one statistical celebrity or another. It is not a mathematically derived, highly optimized calculation of objective worth, just a simple means to deal with the fact that arithmetic differences are influenced by sampling variation and noise, and that a detected difference might not reliably point to a genuine difference between groups.\nIt is simply not possible to understand hypothesis testing in the same way you can understand differentiation or data wrangling.\nHypothesis testing emerged in an era of bench-top and agricultural experiments conducted by a small community of self-identified scientists working without central control. It might have been a practicable solution to the problem of information overload in that era of small data. But the protocol has been frozen in place by textbooks; each generation passing it along to the next as received wisdom, in much the same way as the views of classical philosophers and poets were passed down to later generations as authoritative and unchallengeable.\nSo lets step back from this frozen statistical protocol of hypothesis testing and point out inconsistencies and peculiarities that make it hard to make sense of and perhaps unsuited to the needs of handling information overload in todays world of big data and huge scientific enterprise.\np-value is an inseparable tangle of the amount of data available and the effect size. With enough data, practically everything has a small p-value.\nHundreds of thousands (perhaps millions) of scientists churning out research results. A filter that eliminates 95% of the nonsense still lets through an unfathomable mass of content.\nSo many choices in research and analysis methods—which covariates to include, whether to exclude an inconvenient point as an outlier, multiple choices for the response variable, all combined with a professional priority to “publish or perish.”"
  },
  {
    "objectID": "Reading-notes-lesson-38.html#false-discovery",
    "href": "Reading-notes-lesson-38.html#false-discovery",
    "title": "38  False discovery",
    "section": "False discovery",
    "text": "False discovery\nSIMPLIFY THIS. Make a DAG with hundreds of explanatory variables, none of which is connected to the response variable."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#sources-of-false-discovery",
    "href": "Reading-notes-lesson-38.html#sources-of-false-discovery",
    "title": "38  False discovery",
    "section": "Sources of false discovery",
    "text": "Sources of false discovery\n[NEEDS STREAMLINING and take out references to examples like Potomac/Austin]\nHow did the coupon classifier system identify so many accidental patterns, patterns that existed in the training data but not in the testing data?\nOne source of false discovery stems from having multiple potential response variables. In the Potomac/Austin example, there were ten different classifiers at work, one for each of the ten Austin products. Even if the probability of finding an accidental pattern in one classifier is small, looking in ten different places dramatically increases the odds of finding something.\nSimilarly, having a large number of explanatory variables – we had 100 in the coupon classifier – provides many opportunities for false discovery. The probability of an accidental pattern between one outcome and one explanatory variable is small, but with many explanatory variables each being considered it’s much more likely to find something.\nA third source of false discovery at work in the coupon classifier relates to the family of models selected to implement the classifier. We used a tree model classifier capable of searching through the (many) explanatory variables to find ones that are associated with the response outcome. Unbridled, the tree model is capable of very fine stratification. Each coupon classifiers stratified the customers into about 200 levels. On average, then, there were about 50 customers in each strata. But there is variation, so many of the strata are much smaller, with ten or fewer customers. The small groups were constructed by the tree-building algorithm to have similar outcomes among the members, so it’s not surprising to see a very strong pattern in each group. For each classifier, about 15% of all customers fall into a strata with 20 or fewer customers."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#identifying-false-discovery",
    "href": "Reading-notes-lesson-38.html#identifying-false-discovery",
    "title": "38  False discovery",
    "section": "Identifying false discovery",
    "text": "Identifying false discovery\nWe use data to build statistical models and systems such as the coupon-assignment machine. False discovery occurs when a pattern or model performance seen with one set of data does not generalize to other potential data sets.\nThe basic technique to avoid false discovery is called cross validation. One simple approach to cross validation splits the data frame into two randomly selected non-overlapping sets of rows: one for training and the other for testing. Use the training data to build the system. Use the testing data to evaluate the system’s performance.\nMost often, cross validation is used to test model prediction performance such as the root-mean-square error or the sensitivity and specificity of a classifier. This can be accomplished by taking the trained model and providing as input the explanatory variables from the testing data, then comparing the model output to the actual response variable values in the testing data. Note that using testing data in this way does not involve retraining the model on the testing data.\nHow big should the training set be compared to the testing set? For now, we’ll keep things simple and encourage use of a 50:50 split or something very close to that.\nThis is a simple and reliable approach that should always be used."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#false-discovery-and-multiple-testing",
    "href": "Reading-notes-lesson-38.html#false-discovery-and-multiple-testing",
    "title": "38  False discovery",
    "section": "False discovery and multiple testing",
    "text": "False discovery and multiple testing\nWhen the main interest is in an effect size, standard procedure calls for calculating a confidence interval on the effect. For example, a 2008 study examined the possible relationship between a woman’s diet before conception and the sex of the conceived child. The popular press was particularly taken by this result from the study:\n\nWomen producing male infants consumed more breakfast cereal than those with female infants. The odds ratio for a male infant was 1.87 (95% CI 1.31, 2.65) for women who consumed at least one bowl of breakfast cereal daily compared with those who ate less than or equal to one bowlful per week. (fetal-sex-2008?)\n\nThe model here is a classifier of the sex of the baby based on the amount of breakfast cereal eaten. The effect size tells the change in the odds of a male when the explanatory variable changes from one bowlful of cereal per week to one bowl per day (or more). This effect size is sensibly reported as a ratio of the two odds. A ratio bigger than one means that boys are more likely outcomes for the one-bowl-a-day potential mother than the one-bowl-a-week potential mother. The 95% confidence interval is given as 1.31 to 2.65. This confidence interval does not contain 1. In a conventional interpretation, this provides compelling evidence that the relationship between cereal consumption and sex is not a false pattern.\nBut the confidence interval is not the complete story. The authors are clear in stating their methodology: “Data of the 133 food items from our food frequency questionnaire were analysed, and we also performed additional analyses using broader food groups.” In other words, the authors had available more than 133 potential explanatory variables. For each of these explanatory variables, the study’s authors constructed a confidence interval on the odds ratio. Most of the confidence intervals included 1, providing no compelling evidence of a relationship between that food item and the sex of the conceived child. As it happens, breakfast cereal produced the confidence interval that was the most distant from an odds ratio of 1.\nLet’s look at the range of confidence intervals that can be found from studying 100 potential random variables that are each unrelated to the response variable. We’ll simulate a response randomly generated “sex” G and B where the odds of G is 1. Similarly, each explanatory variable will be a randomly generated “consumption” high or low where the odds of high is 1. A simple stratification of sex by consumption will generate the odds of G for those cases with consumption Y and also the odds of G for those cases with consumption N. Taking the ratio of these odds gives, naturally enough, the odds ratio. We can also calculate from the stratified data a 95% confidence interval on the odds ratio.\nSo that the results will be somewhat comparable to the results in (fetal-sex-2008?), we’ll use a similar sample size, that is, n = 740. Table @ref(tab:sex-consumption-1) shows one trial of the simulation.\n\n\n\n\n\n\n(ref:sex-consumption-1-cap)\n\n\n\n\n\n\nhigh\n\n\nlow\n\n\n\n\n\n\nB\n\n\n165\n\n\n182\n\n\n\n\nG\n\n\n211\n\n\n182\n\n\n\n\n\n(ref:sex-consumption-1-cap) A stratification of sex outcome (B or G) on consumption (high or low) for one trial of the simulation described in the text.\nReferring to Table @ref(tab:sex-consumption-1), you can see that the odds of G when consumption is low is 182 / 182 = 1. The odds of G when consumption is high is 211/165 = 1.28. The 95% confidence interval on the odds ratio can be calculated. It is 0.95 to 1.73. Since that includes 1, the data underlying Table @ref(tab:sex-consumption-1) provide little or no evidence for a relationship between sex and consumption. This is exactly what we expect, since the simulation involves entirely random data.\nFigure 38.1 shows the 95% confidence interval on the odds ratio for 133 trials like that in Table @ref(tab:sex-consumption-1). The confidence interval from each trial is shown as a horizontal line. The large majority of them include 1. That’s to be expected because the data have been generated so that sex and consumption have no relationship except those arising by chance.\n\n\nWarning: `geom_vline()`: Ignoring `mapping` because `xintercept` was provided.\n\n\n\n\n\nFigure 38.1: Confidence intervals on the odds ratio comparing female and male birth rates for many trials of simulated data with no genuine relationship between the explanatory and response variables.\n\n\n\n\nNonetheless, out of 133 simulations there are six where the confidence interval does not include 1. These are shown in red. By necessity, one of the intervals will be the most extreme. If instead of numbering the simulations, we had labelled them with food items – e.g. grapefruit, breakfast cereal, toast – we would have a situation very similar to what seems to have happened in the sex-vs-food study. (For a more detailed analysis of the impact of multiple testing in (fetal-sex-2008?), see (young-2009?).)\nSuppose now that half of the data used in (fetal-sex-2008?) had been held back as testing data. Using the training data, it would be an entirely legitimate practice to generate hypotheses about which specific food items might be related to the sex of the baby. The validity of any one selected hypothesis could then be established using the testing data without the ambiguity introduced by multiple testing. The testing data confidence interval can be taken at face value; the training data confidence interval cannot."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#example-organic-discovery",
    "href": "Reading-notes-lesson-38.html#example-organic-discovery",
    "title": "38  False discovery",
    "section": "Example: Organic discovery?",
    "text": "Example: Organic discovery?\nIt’s easy to find organic foods in many large grocery stores. Advocates of an organic diet are attracted by a view that it is sustainable, promotes small farms, and helps avoid contact with pesticides. There are also nay-sayers who make valid points, but that is not our purpose here. Informally, I find that many people and news reports point to the health benefits of an organic diet. Usually they believe that these benefits are an established fact.\nA 2018 New York Times article observed:\n\nPeople who buy organic food are usually convinced it’s better for their health, and they’re willing to pay dearly for it. But until now, evidence of the benefits of eating organic has been lacking. (NYT-2018-10-23-Rabin?)\n\nThe new evidence of health benefits is reported in an article in the Journal of the American Medical Association: Internal Medicine (baudry-2018?)\nDescribing the findings of the research, the Times article continued:\n\nEven after these adjustments [for covariates], the most frequent consumers of organic food had 76 percent fewer lymphomas, with 86 percent fewer non-Hodgkin’s lymphomas, and a 34 percent reduction in breast cancers that develop after menopause.\n\nThe study warrants being taken seriously: it involved about 70,000 French adults among whom 1340 cancers were noted. The summary of organic foot consumption was a scale from 0 to 32 and included 16 labeled products including dairy, meat and fish, eggs, coffee and tea, wine, vegetable oils, and sweets such as chocolate. Adjustment was made for a substantial number of covariates: age, sex, educational level, marital status, income, physical activity, smoking, alcohol intake, family history of cancer, body mass index, hormonal treatment for menopause, and others.\nYet … the research displays many of the features that can lead to false discovery. For instance, results were reported for four different types of cancer: breast, prostate, skin, lymphomas. The study reports p-values and hazard ratios2 comparing cancer rates among the four quartiles of the organic consumption index.\nComparing the most organic (average organic index 19.36/32) and the least organic (average index 0.72/32) groups, the 95% confidence interval on the relative risk and p-values given in the study’s Table 4 are:\n\nBreast cancer: 0.66 - 1.16 (p = 0.38)\nProstate cancer: 0.61- 1.73 (p = 0.39)\nSkin cancer: 0.49 - 1.28 (p = 0.11)\nLymphomas: 0.07 - 0.69 (p = 0.05)\n\nYou might be surprised to see that the confidence interval on the relative risk for breast cancer includes 1.0, which suggests no evidence for an effect. As clearly stated in the report, the risk reduction for breast cancer is seen only in a subgroup of study participants: those who are postmenopausal. And even then, the confidence intervals continue to include 1.0:\n\nBreast cancer pre-menopausal: 0.67 - 1.52 (p = 0.85)\nBreast cancer post-menopausal: 0.53 - 1.18 (p = 0.18)\n\nSo where is the claimed 34% reduction in breast cancer cited in the New York Times article. It turns out the the study used two different indices of organic food consumption. The 0 to 32 scale which includes many items for which the amount consumed is very small (e.g., coffee, chocolate) and a “simplified, plant derived organic food score.” It’s only when you look at the full 0 to 32 scale that you see the reduction in post-menopausal breast cancer: the confidence interval is 0.45 to 0.96 (p = 0.03).\nWhat about cancer rates overall? For the 0 to 32 scale the risk ratio was 0.58 - 1.01 (p = 0.10). To see the claimed reduction clearly you need to look at the simplified food score which gives 0.63 - 0.89 (p < 0.005). And it’s only in comparing the highest-index quarter of participants with the low\n\n\nWarning: `geom_hline()`: Ignoring `mapping` because `yintercept` was provided.\n\n\n\n\n\nFigure 38.2: The p-value as a function of sample size n when the test statistic R-squared has the trivial value 0.001. The horizontal line shows the usual threshold for “significance” of p < 0.05."
  },
  {
    "objectID": "Reading-notes-lesson-38.html#notes-in-draft",
    "href": "Reading-notes-lesson-38.html#notes-in-draft",
    "title": "38  False discovery",
    "section": "NOTES IN DRAFT",
    "text": "NOTES IN DRAFT\n“Statistical crisis” in science\nhttps://www.americanscientist.org/article/the-statistical-crisis-in-science\nGarden of the Forking Paths\nIonedes"
  },
  {
    "objectID": "QR2.html",
    "href": "QR2.html",
    "title": "39  Review of Lessons 28-38",
    "section": "",
    "text": "Warning\n\n\n\nI’ll put learning challenges here. The class day will be given over to the QR."
  }
]
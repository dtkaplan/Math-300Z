{
  "hash": "460efd10b513f6fe77954a224bdbcbd7",
  "result": {
    "markdown": "---\ntitle: \"Math 300R Lesson 34 Reading Notes\"\nsubtitle: \"Constructing a classifier\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"November 04, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\nWe all face many yes/no situations. A patient has a disease or does not. A credit card transaction is genuine or fraudulent. A **classifier** is a statistical model designed to *predict* the unknown outcome of a yes/no situation from information that is already available. \n\nConsider a credit-card company might building a classifier to predict at the time of the transaction whether a purchase of gasoline is fraudulent. The company knows how often and how much gasoline the individual cardholders buys, where the cardholder lives, whether the cardholder travels extensively, typical times of day for a purchase, and so on. **Feature engineering** is the process of using existing data---including, in our example, whether the purchase turned out to be fraudulent---to develop potential markers or signals of the outcome. For simplicity, imagine the features selected are the number of days since the last gasoline purchase and the distance from the last place of purchase. \n\nOnce potential features have been proposed, the engineers building the classifier assemble training and testing data sets. Suppose, for the purpose of illustration, that the training data has 2000 fraudulent transactions and 4000 non-fraudulent ones, and the testing set is about the same. \n\nThe word \"assemble\" was used intentionally to describe how the testing and training data were collected: a **case-control study**. Since the objective is to detect fraud, it is reasonable to have a lot of \"yes\" cases in the data. The \"no\" cases serve as a kind of control; they were included specifically to have balance in the data. If data had been collected as a simple random sample of credit card transactions, there would have been many, many more \"no\" cases than \"yes.\"\n\nWith such training data it is easy to build a statistical model with `Fraud` as the response variable. That model can then be evaluated on the testing data to produce a model output for each row:\n\nFraud | Feature 1 | Feature 2 | Model output\n------|-----------|-----------|-------------\nno    |  6 days   | 5 miles   |  -\nno    |  1 day    | 250 miles |  +\nyes   | 120 days  | 75 miles  |  -\nno    | 5 days    | 0 miles   |  -\nyes   | 0.2 days  | 90 miles  |  +\n\nIt's understandable that a classifier may not have perfect performance. After all, it iss trying to make a prediction based on limited data, and randomness may play a role. \n\nThere are different ways of making a mistake, and these different ways  have very different consequences. One kind of mistake, called a \"**false positive**\", involves a classifier output that's positive (i.e. the classifier indicates fraud) but which is wrong. The consequence of this sort of mistake in the present example is a customer who has to find another way to pay for gasoline.\n\nThe other kind of mistake is called a \"**false negative**\". Here, the classifer output is that the transaction is not fraudulent, but in actuality it was. The consequence of this kind of mistake is different: a successful theft.\n\nThe nomenclature signals that a mistake has been made with the word \"false.\" The kind of mistake is either \"positive\" or \"negative\", corresponding to the output of the classifier.\n\nWhen the classifier gets things right, that is a \"true\" result. As with the false results, a true result is possible both for a \"positive\" and a \"negative\" classifier output. So the two ways of getting things right are called \"**true positive**\" and \"**true negative**\". \n\nTabulating all 6000 rows of the testing data might produce something like this:\n\n.   | +    |  -\n----|------|------\nyes | 1900 | 100      \nno  |   50 | 3950\n\n\n## Incidence\n\n## Sensitivity and specificity\n\n\n\n::: {.callout-note}\n## Example: Accuracy of airport security screening \n\nAirplane passengers have, for decades, gone through a security screening process involving identity checks, \"no fly\" lists, metal detection, imaging of baggage, random pat-downs, and such. How accurate is such screening? Almost certainly, the accuracy is not as good as an extremely simple, no-input, alternative process: automatically identify every passenger as \"not a security problem.\" We can estimate the accuracy of the \"not a security problem\" classifier by guessing what fraction of airplane passengers are indeed a threat to aircraft. In the US alone, there are about 2.5 million airplane passengers each day and security problems of any sort rarely happen. So the accuracy of the no-input classifier is something like 99.999%. \n\nThe actual screening system, using metal detectors, baggage x-rays, etc. will have a lower accuracy. We know this since it regularly mis-identifies innocent people as security problems.\n\nThe problem here is not with airport security screening, but with the flawed use of *accuracy* as a measure of performance. Indeed, achieving super-high accuracy is not the objective of the security screening process. Instead, the objective is to *deter* security problems by convincing potential terrorists that they are likely to get caught before they can get on a plane. This has to do with the *sensitivity* of the system. The *specificity* of the system, although important to the everyday traveller, is not what deters the terrorist.\n\n:::\n\n\n",
    "supporting": [
      "Reading-notes-lesson-34_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
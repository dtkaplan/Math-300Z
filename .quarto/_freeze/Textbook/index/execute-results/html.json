{
  "hash": "7e5a7412ce336dbc3bbc93fefd6da4b4",
  "result": {
    "markdown": "# Preface {.unnumbered}\n\n\n---\nstatus: proofed Dec 26, 2022\n---\n\n\n\n\n::: {.callout-note}\n## Note to students in Math 300\n\nUp to now, Math 300 has been following the [*OpenIntro*](http://openintro.com) textbook. For the remainder of the semester, however, we will continue with the lessons in this little book: *Lessons in Statistical Thinking*. \n\n:::\n\n*Lessons in Statistical Thinking* is an update and reconsideration of the concepts and methods needed to extract information from data. Such an update is needed because the canon of traditional introductory statistics texts has long been obsolescent and fails to address the needs of the contemporary data scientist and decision-maker. That canon stems from an influential 1925 book, Ronald Fisher's *Statistical Methods for Research Workers*. Research workers of that era typically ran small benchtop or field experiments with a dozen or fewer observations on each of two treatments. A first task with such small data is to rule out the possibility that calculated differences might reflect only the accidental arrangement of numbers into groups.\n\nPerhaps emblematic of the current dissatisfaction with small-data methods is the controversy over \"statistical significance.\" Although situated at the core of many statistics textbooks, significance testing has little to do with the meaning of \"significant\" as \"important\" or \"relevant.\" [This article](www/d41586-019-00857-9.pdf) in the prestigious science journal *Nature* details the controversy. @fig-nature-significance reproduces a cartoon from that article that puts the shortcomings of \"statistical significance\" in a historical context.\n\n\n::: {.cell .fig-cap-location-margin}\n::: {.cell-output-display}\n![A cartoon published along with an article in *Nature*, \"Retire statistical significance,\" showing this once-respected idea heading to the graveyard for outdated and misleading \"scientific\" concepts such as phlogiston and aether.](www/Significance-cartoon.png){#fig-nature-significance width=610}\n:::\n:::\n\n\n## Statistical thinking\n\nThe work of today's data scientists is often to discover novel connections among multiple variables and to guide decision-making. It is common for data to be available in large masses from *observations* rather than *experiments*. One common purpose is \"prediction,\" which might be as simple as the uses of medical screening tests or as breathtaking as machine-learning techniques of \"artificial intelligence.\" Another pressing need from data analysis is to understand possible causal connections between variables. \n\nThe twenty lessons that follow describe a way of thinking that is historically novel, unfamiliar to most otherwise well-educated people, and incredibly useful for making sense of the world and what data can tell us about the world. Learning a new way of thinking is genuinely hard. To start, it will help to describe \"statistical thinking\" concisely. A definition I find useful is this:\n\n> *Statistic thinking is the explanation or description of variation* in the context of *what remains unexplained or undescribed.*\n\nImplicit in this definition is a pathway for learning to think statistically: \n\n1. Learn how to use data to describe variation; \n2. See how to measure \"what remains undescribed\" and use that measurement as a context for interpretation; and \n3. Understand how to link ideas of the process underlying data with appropriate description and modeling techniques.\n\n## Important word pairs\nMany of the vocabulary terms used in statistical thinking come in pairs. We list several such pairs below, in roughly the order they first appear in the Lessons. The pairs can be a reference while reading, but it is also helpful to return to this list to sharpen your understanding of the distinctions.\n\n**Explanatory** vs **response** variables. Models (in these Lessons) always involve a *single* response variable*. In contrast, models can have zero or more explanatory variables.\n\n**Variable** vs **covariate**. \"Covariate\" is another word for an explanatory variable. The word \"covariate\" signals that the variable is not itself of direct interest to the modeler but puts another explanatory variable in a correct context.\n\n**Categorical** vs **quantitative** variables. Always be aware of whether a model's response variable is categorical or quantitative. When categorical, expect to use `zero_one()` to convert it to quantitative before modeling. In contrast, explanatory variables can be either categorical or quantitative.\n\n**Regression model** vs **classifier**. A regression model always has a *quantitative* response variable. A classifier has a *categorical* response variable. In these Lessons, as in much professional use of data, our categorical response variables will have *two levels* (e.g., healthy or sick, up or down, yes or no). In this situation, regression techniques suffice to build classifiers.\n\n**Model** vs **model function**. By \"model,\" we will almost always mean \"regression model.\" A regression model, typically constructed by the `lm()` function, contains various information useful to summarize the model. The \"model function\" provides the mechanism for one important task, calculating from values from the explanatory variables the corresponding model output.\n\n**Model coefficient** vs **effect size**. Model coefficients are numerical parameters. Training determines the appropriate values for the coefficients. In contrast, an effect size describes the relationship between the response variable and a selected explanatory variable.\n\n**Point estimate** vs **interval estimate**. A point estimate is a single number. For instance, a model coefficient is a point estimate, as is the output from a model function. In contrast, interval estimates involve *two* numbers; one specifies the lower end of the interval and the other number specifies the upper end.\n\n**Prediction interval** vs **confidence interval**. A prediction interval describes the anticipated range of the actual result for which we have made a prediction, e.g., \"tomorrow's wind will be between 5 and 10 mph.\" A **confidence interval** is often used to express the uncertainty in a coefficient or effect size.\n\n## Software guide\n\nThese Lessons use about a dozen new R functions. Some of these are used frequently in examples and exercises and are worth mastering. Others appear only in **demonstrations**.\n\n::: {.callout-warning}\n## Demonstrations\n\nThese lessons contain *demonstrations* illustrating statistical concepts or data analysis strategies. We will place these in a distinctive box, of which this is an example.\n\nThe demonstrations will often contain new computer commands that perform tasks used in teaching statistics. However, readers are **not** expected to be able to construct such commands on their own.\n:::\n\n\n* Training models with data\n    - **`lm()`** arguments: i. tilde expression, ii. `data=` data frame.\n    - Occasionally, you will be directed to use `glm()` or `model_train()`, which work similarly to `lm()` but are specialized for models whose output is a *probability*.\n    - **`zero_one()`** converts a two-level categorical variable to a 0/1 encoding.\n    \n* Summarizing models. These invariably take as input a model produced by `lm()` (or `glm()`) and generate a summary report about that model.\n    - **`coefficients()`**: displays model coefficients as a single number.\n    - **`confint()`**: displays model coefficients as an *interval* with a lower and upper value.\n    - **`rsquared()`** reduces a model to a single number, called R^2^.\n    - **`regression_summary()`**, like `confint()`, but with more detail.\n    \n* Evaluating a model on inputs\n    - **`model_eval()`** takes a trained model (as produced by `lm()`) and calculates the model output in both a point form and an interval form. `model_eval()` can also display the residuals from training or evaluation data.\n\n* Graphics\n    - `model_plot()` draws a graphic of a model's function optionally with prediction or confidence intervals.\n    - `geom_violin()` is a modern alternative to `geom_boxplot()`.\n    \n* DAGs (directed, acyclic graphs)\n    - **`sample()`** collects simulated data from a DAG\n    - `dag_draw()` draws a picture of a DAG showing how the variables are connected.\n    \n* Used within the `summarize()` data wrangling function: \n    - **`var()`** computes the variance of a single variable.\n    \n::: {.callout-warning}\n## Demonstration\n\nHere are some of the command structures that appear in demonstrations. These explanations give a general idea of the tasks they perform.\n\n- `do(10) * {` *command* `}` causes the *command* to be executed repeatedly the indicated number of times. Such repetitions are useful when the *command* is a trial of a random process such as sampling, resampling, or shuffling.\n- `function(`*arguments*`) {` *set of commands* `}` packages in a single unit a set of one or more commands. The packaging facilitates using them over and over again with specified arguments.\n- `geom_errorbar()` works much like `geom_point()` but draws vertical bars instead of dots. Bar-shaped glyphs depict *intervals* such as confidence or prediction intervals.\n- `geom_ribbon()` is like `geom_line()` but for *intervals*. \n- `effect_size()` calculates the strength and direction of the input-output relationship between the response variable of a model and a selected *one* of the explanatory variables.\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "3648717e2f67d6f484c9a5e7112ca5a2",
  "result": {
    "markdown": "---\ntitle: \"Sampling and sampling variation\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\nstatus: \"Proofread Nov 29, 2022\"\n---\n\n\n\n\nWe use \"sample\" as a near synonym for \"data frame.\" However, a data frame may not be a sample but contains a row for every possible unit of observation. Such a complete enumeration---the inventory records of a merchant, the records kept of student grades by the school registrar---has a technical name: a \"**census**.\" Famously, many countries conduct a census of the population in which they try to record every resident of the country. For example, the US, UK, and China carry out a census every ten years.\n\nIn a typical setting, it is unfeasible to record every possible unit of observation.^[Even a population \"census\" inevitably leaves out some individuals.] Such incomplete records constitute a \"**sample**.\" One of the great successes of statistics is the means to draw useful information from a sample, at least when the sample is collected correctly.\n\nSampling is called for when we want to find out about a large group but lack time, energy, money, or the other resources needed to contact every group member. For instance, France collects samples at short intervals to collect up-to-date data while staying within a budget. The name used for the process---*recensement en continu* or \"rolling census\"---signals the intent. Over several years, the French rolling census contacts about 70% of the population.\n\nSometimes, as in quality control in manufacturing, the measurement process is destructive: the measurement process consumes the item. In a destructive measurement situation, it would be pointless to measure every single item. Instead, a sample will have to do.\n\n## Sampling bias\n\nCollecting a reliable sample is usually considerable work. An ideal is the \"simple random sample\" (SRS), where all of the items are available, but only some are selected---completely at random---for recording as data. Undertaking an SRS requires assembling a \"sampling frame,\" essentially a census. Then, with the sampling frame in hand, a computer or throws of the dice can accomplish the random selection for the sample.\n\nUnderstandably, if a census is unfeasible, constructing a perfect sampling frame is hardly less so. In practice, the sample is assembled by randomly dialing phone numbers or taking every 10th visitor to a clinic or similar means. Unlike genuinely random samples, the samples created by these practical methods are not necessarily representative of the larger group. For instance, many people will not answer a phone call from a stranger; such people are underrepresented in the sample. Similarly, the people who can get to the clinic may be healthier than those who cannot. Such unrepresentativeness is called \"**sampling bias**.\"\n\nProfessional work, such as collecting unemployment data, often requires government-level resources. Assembling representative samples uses specialized statistical techniques such as stratification and weighting of the results. We will not cover the specialized techniques in this introductory course, even though they are essential in creating representative samples. The table of contents of a classic text, William Cochran's [*Sampling techniques*](https://ia801409.us.archive.org/35/items/Cochran1977SamplingTechniques_201703/Cochran_1977_Sampling%20Techniques.pdf) shows what is involved.\n\nAll statistical thinkers, whether expert in sampling techniques or not, should be aware of factors that can bias a sample away from being representative. In political polls, many (most?) people will not respond to the questions. If this non-response stems from, for example, an expectation that the response will be unpopular, then the poll sample will not adequately reflect unpopular opinions. Such **non-response bias** can be significant, even overwhelming, in surveys. \n\n**Survival bias** plays a role in many settings. The `mosaicData::TenMileRace` data frame provides an example, recording the running times of 8636 participants in a 10-mile road race and including information about each runner's age. Can such data carry information about changes in running performance as people age? The data frame includes runners aged 10 to 87. Nevertheless, a model of running time as a function of age from this data frame is seriously biased. The reason? As people age, casual runners tend to drop out of such races. So the older runners are skewed toward higher performance. (We can see this by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.\n\n:::: {.callout-note}\n## Examples: Returned to base\n\nAn inspiring story about dealing with survival bias comes from a World War II study of the damage sustained by bombers due to enemy guns. The sample, by necessity, included only those bombers that survived the mission and returned to base. The holes in those surviving bombers tell a story of survival bias. Shell holes on the surviving planes were clustered in certain areas, as depicted in @fig-airplane-holes. The clustering stems from survivor bias. The unfortunate planes hit in the middle of the wings, cockpit, engines, and the back of the fuselage did not return to base. Shell hits in those areas never made it into the record. \n\n\n::: {.cell .column-margin}\n::: {.cell-output-display}\n![An illustration of shell-hole locations in planes that returned to base. [Source: Wikipedia](https://en.wikipedia.org/wiki/Survivorship_bias)](www/bomber-holes.png){#fig-airplane-holes width=600}\n:::\n:::\n\n::::\n\n## Measuring sampling variation\n\nSampling variation is a form of noise. Unlike some other forms of noise, modeling cannot filter out sampling variation or reduce its magnitude. Sampling variation is easiest to see by collecting multiple samples from the same source and summarizing each one. The summaries likely will vary from sample to sample: sampling variation.\n\nTypically, the data frame at hand is our only sample. With no other samples to compare it to, it may seem impossible to measure sampling variation. In this Lesson, we will use simulations from DAGs to study sampling variation. DAG simulations are suited to this because we can effortlessly collect as many samples as we wish from a DAG. In Lesson 23, we will use the knowledge gained from the simulations to see how to measure sampling variation even when there is only one sample.\n\nIn the spirit of starting simply, we return to `dag01`. This DAG is $\\mathtt{x}\\longrightarrow\\mathtt{y}$. The causal formula setting the value of `y` is `y ~ 4 + 1.5 * x + exo()`.\n\nIt is crucial to remember that sampling variation is not about the row-to-row variation in a single sample. Rather, it is about the variation in the summary from one sample to another. So our initial process for exploring sampling variation will be to carry out many trials, each trial being a summary of a sample. \n\n::: {.callout-warning}\n## Demonstration: Samples and specimens \n\nTo illustrate, here is one trial using a sample of $n=25$ and a simple model, `y ~ 1`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSample <- sample(dag01, size=25) \nSample %>% \n  lm(y ~ 1, data = .) %>%\n  coefficients()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n   4.248288 \n```\n:::\n:::\n\n\nWe cannot see sampling variation directly in the above result because there is only one trial. The sampling variation becomes evident when we run *many* trials. In each trial, a new sample (of size $n=25$ is taken and summarized.) \n\n\n::: {.cell}\n\n```{.r .cell-code}\nTrials <- do(100) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coefficients()\n}\n```\n:::\n\n::: {#tbl-sampling-trials .cell .column-margin tbl-cap='`Trials` for seeing sampling variation.'}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Intercept </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3.750687 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.552955 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.947941 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.008801 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.578861 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.006602 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.259823 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.869581 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.917671 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.932557 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.727117 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.094241 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.649158 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.017057 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.511036 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.278219 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.862383 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.835646 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.343176 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.094017 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.664953 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.586219 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.730871 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3.876046 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4.279536 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n`Trials` is a *sample of summaries*. (See Lesson 20). The row-to-row variation in`Trials` comes from sampling variation. We can *summarize* the variation in the *sample of summaries*. As always, our standard measure of variation is the standard deviation (or, equivalently, variance):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTrials %>%\n  summarize(se = sd(Intercept))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        se\n1 0.356543\n```\n:::\n:::\n\n\nThis summary quantity, which we have named `se`, has a technical name in statistics: the **standard error**. The standard error is an ordinary standard deviation in a particular context: the standard deviation of a sample of summaries. The words **standard error** should be followed by a description of the summary and the size of the individual samples involved. Here it would be, \"The standard error of the Intercept coefficient from a sample of size $n=25$ is around 0.36.\"\n\nIt is easy to confuse \"standard error\" with \"standard deviation.\" Adding to the potential confusion is another related term, the \"margin of error.\" To avoid this confusion, we will tend to use an *interval* description of the sampling variation called the \"**confidence interval**.\" However, for the present, we will continue with the standard error, sometimes written SE for short.\n\n## SE depends on the sample size\n\nWe found an SE of 0.36 on the Intercept in a sample of size $n=25$. We can see how the SE depends on sample size by repeating the trials for several different sizes, say, $n=25$, 100, 400, 1600, 6400, 25,000, and 100,000.\n\nThe following command estimates the SE a sample of size 400:\n\n::: {.cell}\n\n```{.r .cell-code}\nTrials <- do(1000) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coefficients()\n}\nTrials %>% summarize(se400 = sd(Intercept))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      se400\n1 0.3538538\n```\n:::\n:::\n\n\nWe repeated this process for each of the other sample sizes. @tbl-se-sizes reports the results.\n\n\n::: {.cell}\n\n:::\n\n::: {#tbl-se-sizes .cell .column-margin tbl-cap='Results of repeating the sampling variability trials for samples of varying sizes.'}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> se </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 25 </td>\n   <td style=\"text-align:right;\"> 0.3600 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 100 </td>\n   <td style=\"text-align:right;\"> 0.1900 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 400 </td>\n   <td style=\"text-align:right;\"> 0.0910 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1600 </td>\n   <td style=\"text-align:right;\"> 0.0430 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 6400 </td>\n   <td style=\"text-align:right;\"> 0.0230 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 25000 </td>\n   <td style=\"text-align:right;\"> 0.0110 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 100000 </td>\n   <td style=\"text-align:right;\"> 0.0056 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThere is a pattern in @tbl-se-sizes. Every time we double $n$, the standard error goes down by a factor of 2, that is, $\\sqrt{4}$. (The pattern is not exact because there is also sampling variation in the trials.)\n\n**Conclusion**: The larger the sample size, the smaller the SE. For a sample size of $n$, the SE will be proportional to $1/\\sqrt{\\strut n}$.\n\n## The confidence interval\n\nThe \"confidence interval\" is a more user-friendly format than SE for describing the amount of sampling variation. Being an interval, write it either as [lower, upper] or center$\\pm$half-width. These styles are equivalent; both styles are correct. (The preferred style can depend on the field or the journal publishing the report.)\n\nIn practice, confidence intervals are calculated using special-purpose software such as the `confint()` function, for instance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  2.5 %      97.5 %\n(Intercept) -533.432471 -406.521402\ndistance     246.387096  261.229494\nclimb          2.493307    2.726209\n```\n:::\n:::\n\nNotice that there is a separate confidence interval for each model coefficient. The sampling variation is essentially the same, but that variation appears different when translated to the various coefficients' units. \n\n::: {.callout-warning}\n## Demonstration: How many digits?\n\nThe confidence intervals on the model `time ~ distance + climb`, report the results to many digits. Such a report is appropriate for further calculations that might need doing, but it is usually not appropriate for a human reader.\n\nTo know how many digits are worth reporting to humans, look toward the standard error. The standard error is a part of a different kind of summary of a model: the \"regression report.\" We will only need to look at regression reports in the last few Lessons of the course. Here we want to point out how many digits are worth reporting to humans. That requires looking at the standard error itself.\n\nPreviously, we looked at the confidence intervals on coefficients from the `Hill_racing` model. Now we look at the regression summary, which contains the information on sampling variation in a different format.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  regression_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -470.     32.4        -14.5 9.92e- 46\n2 distance      254.      3.78        67.1 0        \n3 climb           2.61    0.0594      43.9 4.08e-304\n```\n:::\n:::\n\n\nEach coefficient's standard error appears in the `std.error` column of the regression summary.\n\nFor the human reader, only the first two significant digits of the standard error are worth reporting. (This is true regardless of the data and model design.) Here, the SE is 32 for the Intercept, 3.8 for the distance coefficient, and 0.059 for the climb coefficient. The confidence interval will be the coefficient (column labeled `estimate`) plus or minus \"twice\" the `std.error`. It is appropriate to round the confidence interval (for a human reader) to the first two significant digits of the standard error.\n\nFor example, the confidence interval on the distance coefficient will be $253.808295 \\pm 2 \\times 3.78433220$. Keep only the digits before the first two significant digits of the SE, so the reported interval can be $253.8 \\pm 3.8$. \n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "7cf8b78e1711e37774cd931078a31be8",
  "result": {
    "markdown": "---\ntitle: \"Covariates eat variance\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\nIn Lesson 28, we introduced covariates to set the relationship between an explanatory variable and the response variable in the correct context. In Lesson 30, we will return to this context-setting role to show that the appropriate choice of covariates to include in a model depends on the modeler's opinion about the relevant structure of a DAG. Here, we will treat covariates as commodity items to show a surprising property of models. This property is a boon to the modeler, helping to enable sound decisions about whether to include any given covariate. However, it is also a pitfall lieing in wait for the wishful thinker.\n\n## How much variation is explained\n\nWe start by returning to the definition of statistical thinking introduced at the start of these Lessons:\n\n> *Statistic thinking is the explanation or description of variation* in the context of *what remains unexplained or undescribed.*\n\nIn this Lesson, we will work with a straightforward measure of \"what remains unexplained or undescribed.\" The fitted model values represent the explained part of the variation. The residuals are what is left over, the difference between the actual values of the response variable and the fitted model values. \n\nAs a reminder, we will construct a simple model of the list price of books as a function of the number of pages and whether the book is a paperback or hardcover.^[If you seek to duplicate the results presented in this chapter, please note that we have deleted six rows from `amazon_books because the rows are either duplicates or have one of the variables missing. The deleted rows are 62, 103, 205, 211, 242, and 303.]\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nPrice_model <- lm(list_price ~ num_pages + hard_paper, \n                  data = amazon_books)\n```\n:::\n\n\nThe `model_eval()` function can extract the fitted model values and the residuals from the model. We show just a few rows here, but we will use the entire report from `model_eval()`. Remember that when `model_eval()` is not given input values, it uses the model *training* data as input.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nResults <- model_eval(Price_model)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> list_price </th>\n   <th style=\"text-align:right;\"> num_pages </th>\n   <th style=\"text-align:left;\"> hard_paper </th>\n   <th style=\"text-align:right;\"> .output </th>\n   <th style=\"text-align:right;\"> .resid </th>\n   <th style=\"text-align:right;\"> .lwr </th>\n   <th style=\"text-align:right;\"> .upr </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 12.95 </td>\n   <td style=\"text-align:right;\"> 304 </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:right;\"> 16.60 </td>\n   <td style=\"text-align:right;\"> -3.65 </td>\n   <td style=\"text-align:right;\"> -10.73 </td>\n   <td style=\"text-align:right;\"> 43.94 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 15.00 </td>\n   <td style=\"text-align:right;\"> 273 </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:right;\"> 15.98 </td>\n   <td style=\"text-align:right;\"> -0.98 </td>\n   <td style=\"text-align:right;\"> -11.36 </td>\n   <td style=\"text-align:right;\"> 43.32 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1.50 </td>\n   <td style=\"text-align:right;\"> 96 </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:right;\"> 12.45 </td>\n   <td style=\"text-align:right;\"> -10.95 </td>\n   <td style=\"text-align:right;\"> -14.97 </td>\n   <td style=\"text-align:right;\"> 39.88 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 15.99 </td>\n   <td style=\"text-align:right;\"> 672 </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:right;\"> 23.95 </td>\n   <td style=\"text-align:right;\"> -7.96 </td>\n   <td style=\"text-align:right;\"> -3.57 </td>\n   <td style=\"text-align:right;\"> 51.47 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 30.50 </td>\n   <td style=\"text-align:right;\"> 720 </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:right;\"> 24.91 </td>\n   <td style=\"text-align:right;\"> 5.59 </td>\n   <td style=\"text-align:right;\"> -2.67 </td>\n   <td style=\"text-align:right;\"> 52.49 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 28.95 </td>\n   <td style=\"text-align:right;\"> 460 </td>\n   <td style=\"text-align:left;\"> H </td>\n   <td style=\"text-align:right;\"> 24.57 </td>\n   <td style=\"text-align:right;\"> 4.38 </td>\n   <td style=\"text-align:right;\"> -2.89 </td>\n   <td style=\"text-align:right;\"> 52.03 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe first book in the training data is a 304-page paperback with a list price of $12.95. The fitted model value for that book is $16.60. (Ordinarily, we refer to the output of the model function simply as the \"output\" or the \"model output.\" However, the output of the model function, when applied to rows from the *training* data also called the *fitted model value*.)\n\nAt $16.60, the fitted model value is $3.65 *higher* than the list price. This difference is the residual for that book, the sign reflecting the definition \n$$\\text{residual} \\equiv \\text{response value} - \\text{fitted model value}\\ .$$\nWhen the residual is small in magnitude, the fitted model value is close to the response value. Conversely, a large residual means the model was way off target for that book. \n\nThe standard measure of the typical size of a residual is the standard deviation or, equivalently, the variance. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nResults %>% summarize(se_resids = sd(.resid), v_resids=var(.resid))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  se_resids v_resids\n1  13.81885 190.9606\n```\n:::\n:::\n\nAs always, the standard deviation is easier to read because it has sensible units, in this case, dollars. On the other hand, the variance has strange units (square dollars) because it is the square of the standard deviation. We will use the variance for measuring the typical size of a residual for the reasons described in Lesson 20; variances add nicely in a manner analogous to the Pythagorean Theorem.\n\nSimilarly, the total amount of variation in the list price is simply the variance of the list price:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nResults %>% summarize(v_response = var(list_price))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  v_response\n1   206.5129\n```\n:::\n:::\n\nA simple measure of how much of the variation in list price remains unexplained is the ratio of these variances $190.96/206.51 = 92.5\\%$.\nMore than 90% of the variation remains unexplained by the `Price_model`. This high fraction of unexplained variance suggests the model has little to tell us. In the spirit of putting a positive spin on things, statisticians typically work with the complement of the unexplained fraction. Since the unexplained fraction is 92.5%, the complement is 7.5%. This number is written R^2^ and pronounced \"R-squared.\" (It also has a formal name: the \"coefficient of determination.\" In Lesson 30, we will meet the inventor of the coefficient of determination, Sewall Wright, who is an early hero of causal reasoning.)\n\nR^2^ is such a widely used summary of how the explanatory variables account for the response variable that a software extractor calculates it and some related values. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nPrice_model %>% R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n k   Rsquared        F      adjR2\n1 317 2 0.07530934 12.78651 0.06941959\n```\n:::\n:::\n\n\nMany modelers act as if their goal is to build a model that makes R^2^ as big as possible. Their thinking is that large R^2^ means that the explanatory variables account for much of the response variable's variance. Unfortunately, it is a naive goal. Instead, always focus on the model's suitability for the purpose at hand. Often, shooting for a large R^2^ imposes costs that can undermine the purpose for the model. Furthermore, even models with the largest possible R^2^ sometimes have nothing to say about the response variable.\n\n## Getting to 1\n\nR^2^ can range from zero to one. Zero means that the model accounts for *none* of the variation in the response variable. We can construct such a model quickly enough: `list_price ~ 1` has no explanatory variables and, therefore, no ability to distinguish one book from another.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNull_model <- lm(list_price ~ 1, data = amazon_books)\nNull_model %>% R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n k Rsquared   F adjR2\n1 319 0        0 NaN     0\n```\n:::\n:::\n\n\nWe are using the word \"null\" to name this model. \"Null\" is part of the statistics tradition. The dictionary definition of \"null\" is \"having or associated with the value zero\" or \"lacking distinctive qualities; having no positive substance or content.\"^[Source: [Oxford Languages](https://languages.oup.com/dictionaries/)]\n\nIn the null model, the fitted model values are all the same; all the variation is in the residuals. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nNull_model %>% model_eval()\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> list_price </th>\n   <th style=\"text-align:right;\"> .output </th>\n   <th style=\"text-align:right;\"> .resid </th>\n   <th style=\"text-align:right;\"> .lwr </th>\n   <th style=\"text-align:right;\"> .upr </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 12.95 </td>\n   <td style=\"text-align:right;\"> 18.6 </td>\n   <td style=\"text-align:right;\"> -5.65 </td>\n   <td style=\"text-align:right;\"> -9.69 </td>\n   <td style=\"text-align:right;\"> 46.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 15.00 </td>\n   <td style=\"text-align:right;\"> 18.6 </td>\n   <td style=\"text-align:right;\"> -3.60 </td>\n   <td style=\"text-align:right;\"> -9.69 </td>\n   <td style=\"text-align:right;\"> 46.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1.50 </td>\n   <td style=\"text-align:right;\"> 18.6 </td>\n   <td style=\"text-align:right;\"> -17.10 </td>\n   <td style=\"text-align:right;\"> -9.69 </td>\n   <td style=\"text-align:right;\"> 46.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 15.99 </td>\n   <td style=\"text-align:right;\"> 18.6 </td>\n   <td style=\"text-align:right;\"> -2.61 </td>\n   <td style=\"text-align:right;\"> -9.69 </td>\n   <td style=\"text-align:right;\"> 46.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 30.50 </td>\n   <td style=\"text-align:right;\"> 18.6 </td>\n   <td style=\"text-align:right;\"> 11.90 </td>\n   <td style=\"text-align:right;\"> -9.69 </td>\n   <td style=\"text-align:right;\"> 46.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 28.95 </td>\n   <td style=\"text-align:right;\"> 18.6 </td>\n   <td style=\"text-align:right;\"> 10.35 </td>\n   <td style=\"text-align:right;\"> -9.69 </td>\n   <td style=\"text-align:right;\"> 46.89 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAt the other extreme, where R^2^ = 1, the explanatory variables account for every bit of variation in the response variable. We can try various combinations of explanatory variables to see if we can accomplish this. For example, `publisher` explains 67% of the variation in list price.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ publisher, data = amazon_books) %>% R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n   k  Rsquared        F     adjR2\n1 319 158 0.6749786 2.103008 0.3540199\n```\n:::\n:::\n\n\nWe can also check whether `author` has anything to say about the list price.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ author, data = amazon_books) %>% R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n   k  Rsquared        F     adjR2\n1 319 250 0.9434046 4.534044 0.7353333\n```\n:::\n:::\n\n\nIncredible! How about if we use *both* `publisher` and `author` as explanatory variables? We get very close to R^2^ = 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ publisher + author, data = amazon_books) %>%\n  R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n   k  Rsquared        F   adjR2\n1 319 281 0.9821609 7.249441 0.84668\n```\n:::\n:::\n\n\nThe modeler discovering this tremendous explanatory power of `publisher` and `author` can be forgiven for thinking he or she has found a meaningful explanation. But, unfortunately, the high R^2^ is an illusion in this case.\n\nTo see why, consider another possible explanatory variable, the International Standard Book Number (ISBN). The ISBN is a ten- or thirteen-digit number that marks each book with a unique number.\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code}\nknitr::include_graphics(\"www/SM2-ISBN.png\")\n```\n\n::: {.cell-output-display}\n![The ISBN number from one of the Project MOSAIC textbooks.](www/SM2-ISBN.png){#fig-isbn-sm2 width=241}\n:::\n:::\n\n\nThere is a system behind ISBNs, but despite the \"N\" standing for \"number,\" an ISBN is a character string or word (written using only digits). Consequently, the `isbn_10` variable in `amazon_books`is categorical.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nISBN_model <- lm(list_price ~ isbn_10, data = amazon_books)\nISBN_model %>% R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n   k Rsquared   F adjR2\n1 319 318        1 NaN   NaN\n```\n:::\n:::\n\n\nThe `isbn_10` explains all variation in the list price!\n\nGiven that the ISBN is, as we have said, an arbitrary sequence of characters, why does it do such a good job of accounting for the list price? The answer lies not in the content of the ISBN but in another fact: each book has a unique ISBN. As well, each book has a single price. So the ISBN identifies the price of each book. Cleverness is not involved; the list price could be anything, and the ISBN would still identify it precisely. The model coefficients store the whole set of ISBNs and the corresponding set of list prices.\n\nWe can substantiate the claim just made---that the list price could be anything at all---by synthesizing a data frame with random list prices:\n\n\n::: {.cell}\n\n```{.r .cell-code}\namazon_books %>% \n  mutate(random_list_price = rnorm(nrow(.))) %>%\n  lm(random_list_price ~ isbn_10, data = .) %>%\n  R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n   k Rsquared   F adjR2\n1 319 318        1 NaN   NaN\n```\n:::\n:::\n\n\nSimilar randomization can be accomplished by *shuffling* the `isbn_10` column of the data frame so that each ISBN points to a random book. Of course, such shuffling destroys the link between the ISBN and the list price. Even so, the R^2^ remains high.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ shuffle(isbn_10), data=amazon_books) %>% R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n   k Rsquared   F adjR2\n1 319 318        1 NaN   NaN\n```\n:::\n\n```{.r .cell-code}\nlm(shuffle(list_price) ~ isbn_10, data=amazon_books) %>% R2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n   k Rsquared   F adjR2\n1 319 318        1 NaN   NaN\n```\n:::\n:::\n\n\nStatistical nomenclature is obscure here. So we will make up a name for such incidental alignment with no true explanatory power: the \"**ISBN-effect**.\" \n\nStatistical thinkers know to be aware of situations where categorical variables have many levels and check whether the ISBN effect is in play.\n\n## The ISBN effect as a benchmark\n\nShuffling an explanatory variable (while keeping the response variable in the original order) voids any possible explanatory connection between the two. An R^2^=0, as we get from any model of the form `y ~ 1`, signals that the `1` cannot account for any variation. However, this does not mean shuffling will lead to R^2^ = 0. Instead, there is a systematic relationship between the number of model coefficients associated with the shuffled variable, the sample size $n$, and R^2^.\n\nWe can demonstrate this relationship by conducting many trials of modeling the `list_price` with a shuffled explanatory variable: either `publisher`, `author`, or `isbn_10`.\n\n::: {.callout-warning}\n## Demonstration: Counting coefficients\n\nThe `amazon_books` data frame has $n=319$ rows.^[The data frame in the `moderndive` package has six additional rows, which we have deleted as duplicates or because of missing data.] In the next computing chunk, we fit the model `list_price ~ publisher` and collect the coefficients for counting: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nPublisher_model <- lm(list_price ~ shuffle(publisher),\n                      data=amazon_books)\nCoefficients <- Publisher_model %>% coefficients() %>% data.frame()\nnrow(Coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 159\n```\n:::\n:::\n\n\nThere are 159 coefficients in the model, the first one being the \"Intercept.\" \n\n\n::: {.cell}\n\n```{.r .cell-code}\nCoefficients %>% head()\n```\n:::\n\n::: {#tbl-shuffle-publisher .cell .column-margin tbl-cap='Confidence intervals on the first six of 161 coefficients in `list_price ~ publisher`'}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 14.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shuffle(publisher)Adams Media </td>\n   <td style=\"text-align:right;\"> 0.05 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shuffle(publisher)Akashic Books </td>\n   <td style=\"text-align:right;\"> 13.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shuffle(publisher)Aladdin </td>\n   <td style=\"text-align:right;\"> 15.05 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shuffle(publisher)Albert Whitman &amp; Company </td>\n   <td style=\"text-align:right;\"> -0.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shuffle(publisher)Alfred A. Knopf </td>\n   <td style=\"text-align:right;\"> 0.05 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAltogether, there are $k=158$ coefficients relating to `shuffle(publisher)`.\n:::\n\nThe rule relating R^2^ to the number of coefficients associated is straightforward for shuffled explanatory variables: R^2^ will be random with mean value $\\frac{k}{n-1}$. For the `shuffle(publisher)` model, the mean across many trials will be R^2^ = 158/324 = 0.49. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPub_trials <- do(100) * {\n  lm(list_price ~ shuffle(publisher), data=amazon_books) %>%\n    R2()\n}\n```\n:::\n\n::: {.cell .column-margin}\n::: {.cell-output-display}\n![100 trials of R^2^ from `list_price ~ shuffled(publisher)`. The theoretical value $k/n=160/324=0.49$ is marked in red.](Reading-notes-lesson-29_files/figure-html/fig-pub-trials-1.png){#fig-pub-trials width=672}\n:::\n:::\n\n\n:::\n\nWe can carry out similar trials for the models `list_price ~ shuffle(author)` and `list_price ~ shuffle(isbn_10)`, which have $k=251$ and $k=319$ respectively. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![R^2^ from many trials of three models, `list_price ~ shuffle(publisher)` and `~ shuffle(author)` and `~shuffle(isbn_10)`.](Reading-notes-lesson-29_files/figure-html/fig-amazon-book-shuffle-1.png){#fig-amazon-book-shuffle width=672}\n:::\n:::\n\n\nThe blue diagonal line in @fig-amazon-book-shuffle shows the theoretical average R^2^ as a function of the number of model coefficients when the explanatory variable is randomized. R^2^ will always be 1.0 when $k=n$, that is, when the number of coefficients is the same as the sample size.\n\n@fig-amazon-book-shuffle suggests a way to distinguish between R^2^ resulting from the ISBN-effect and R^2^ that shows some true explanatory power: Check if R^2^ is substantially above the blue diagonal line, that is, check if R^2^$\\gg \\frac{k}{n-1}$ where $k$ is the number of model coefficients.\n\n\n## The F statistic\n\n$k$ and $n$ provide the necessary context for proper interpretation of R^2^; all three numbers are needed to establish whether R^2^ $\\gg \\frac{k}{n-1}$ to rule out the ISBN effect. The calculation is not difficult; the modeler always knows the size $n$ of the training data and can find $k$ as the number of coefficients in the model (not counting the Intercept term).\n\nPerhaps a little easier than interpreting R^2^ is the interpretation of another statistic, named F, which folds in the $k$, $n$, and R^2^ into a single number:\n$$F \\equiv \\frac{n-k-1}{k} \\frac{\\text{R}^2}{1 - \\text{R}^2}$$\n@fig-amazon-book-shuffle-F is a remake of @fig-amazon-book-shuffle but using F instead of R^2^. The blue line, which had the formula R^2^$= k/(n-1)$ in @fig-amazon-book-shuffle, gets translated to the constant value 1.0 in @fig-amazon-book-shuffle-F, regardless of $k$. To decide when a model points to a connection stronger than the ISBN effect, the threshold F $> 3$ is a good rule of thumb. (Lesson 37 introduces a more precise calculation for the F threshold, which is built into statistical software and presented as a \"**p-value**.\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(aes(x=k, y=F), data=All_trials) +\n  geom_jitter(width=15, alpha=.3) +\n  geom_abline(aes(intercept=1, slope=0), color=\"blue\") +\n  geom_point(aes(x=0, y=0), alpha=0) + \n  labs(subtitle=\"with shuffling\") +\n  ylab(expression(F)) + \n  scale_x_continuous(breaks=c(0,50, 100,158,200, 251,  318))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 100 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](Reading-notes-lesson-29_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-note}\n## Adjusted R^2^\n\nSome fields, notably economics, prefer an alternative to F called \"**adjusted R^2^**\" (or $R^2_\\text{adj}$).  The adjustment comes from moving the raw R^2^ downward and leftward, more-or-less in the direction of the blue line in @fig-amazon-book-shuffle. This movement adjusts a raw $R^2$ that lies on the blue line to $R^2_\\text{adj} = 0$.\n\nWe leave the debate on the relative merits of using F or $R^2_\\text{adj}$ their respective boosters. However, before getting wrapped up in such debates, it is worth pointing out that $R^2_\\text{adj}$ is just a rescaling of F.\n\n$$R^2_\\text{adj} = 1 - \\frac{n-1}{k} \\frac{R^2}{F}\\ .$$\n:::\n\n## Comparing models\n\nModelers are often in the position of having a model that they like but are contemplating adding one or more additional explanatory variables. To illustrate, consider the following models:\n\n- Model 1: `list_price ~ 1`\n- Model 2: `list_price ~ 1 + hard_paper`\n- Model 3: `list_price ~ 1 + hard_paper + num_pages`\n- Model 4: `list_price ~ 1 + hard_paper + num_pages + weight_oz`\n\n\n::: {.cell .column-margin}\n::: {.cell-output-display}\n![Nesting Russian dolls](www/Russian-Matroshka_no_bg.jpeg){#fig-russian-dolls width=230}\n:::\n:::\n\nAll the explanatory variables in the smaller models also apply to the bigger models. Such sets are said to be \"**nested**\" in much the same way as for Russian dolls.\n\nFor a nested set of models, R^2^ can never decrease when moving from a smaller model to a larger one---almost always, there is an increase in R^2^. To demonstrate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\namazon_books <- amazon_books %>% \n  select(list_price, weight_oz, num_pages, hard_paper) %>%\n  filter(complete.cases(.))\nmodel1 <- lm(list_price ~ 1, data=amazon_books)\nmodel2 <- lm(list_price ~ 1 + weight_oz, data = amazon_books)\nmodel3 <- lm(list_price ~ 1 + weight_oz + num_pages, data=amazon_books)\nmodel4 <- lm(list_price ~ 1 + weight_oz + num_pages + hard_paper, data=amazon_books)\n\nR2(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n k Rsquared   F adjR2\n1 309 0        0 NaN     0\n```\n:::\n\n```{.r .cell-code}\nR2(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n k  Rsquared        F     adjR2\n1 309 1 0.1558551 56.68166 0.1531055\n```\n:::\n\n```{.r .cell-code}\nR2(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n k  Rsquared        F     adjR2\n1 309 2 0.1662332 30.50456 0.1607838\n```\n:::\n\n```{.r .cell-code}\nR2(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    n k  Rsquared        F     adjR2\n1 309 3 0.1697025 20.77941 0.1615357\n```\n:::\n:::\n\n\nWhen adding explanatory variables to a model, a good question is whether the new variable(s) add to the ability to account for the variability in the response variable. R^2^ never goes down when moving from a smaller to a larger model, so we cannot rely on the increase in R^2^. A valuable technique called \"**Analysis of Variance**\" (ANOVA for short) looks at the incremental change in variance explained from a smaller model to a larger one. The increase can be presented as an F statistic. To illustrate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(model1, model2, model3, model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: list_price ~ 1\nModel 2: list_price ~ 1 + weight_oz\nModel 3: list_price ~ 1 + weight_oz + num_pages\nModel 4: list_price ~ 1 + weight_oz + num_pages + hard_paper\n  Res.Df   RSS Df Sum of Sq       F    Pr(>F)    \n1    308 54531                                   \n2    307 46032  1    8499.0 57.2516 4.565e-13 ***\n3    306 45466  1     565.9  3.8123   0.05179 .  \n4    305 45277  1     189.2  1.2744   0.25983    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nFocus on the F column of the report. The move from Model 1 to Model 2 produces F=57, well above the threshold described above and clearly indicating that the `weight_oz` variable accounts for some of the list price. Moving from Model 2 to Model 3 creates a much less impressive F of 3.8. It is as if the added explanatory variable, `num_pages`, is just barely pulling its own \"weight.\" Finally, moving from Model 3 to Model 4 produces a below-threshold F of 1.3. In other words, in the context of `weight_oz` and `num_pages,` the `hard_paper` variable does not carry additional information about the list price. \n\nThe last column of the report, labeled `Pr(>F)`, translates F into a universal 0 to 1 scale called a p-value. A large F produces a small p-value. The rule of thumb for reading p-values is that a value $p < 0.05$ indicates that the added variable brings new information about the response variable. We will return to p-values and the controversy they have entailed in Lessons 36 through 38.\n\n\n",
    "supporting": [
      "Reading-notes-lesson-29_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
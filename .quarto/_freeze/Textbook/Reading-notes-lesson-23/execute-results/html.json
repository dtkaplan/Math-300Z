{
  "hash": "8df7ffa2a4248ab10c5c7423e17068b4",
  "result": {
    "markdown": "---\ntitle: \"Estimating sampling variation from a single sample\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\nLesson 21 introduced separating data into components: signal and noise. The *signal* is a summary of the data that tells us something we want to know. Often, the signal will be one or more coefficients from a regression report, but it might be something as simple as the mean, median, or standard deviation of a variable in a data frame.\n\nThe *noise* comes into the data from various sources: e.g., error in measurement or a data-entry blunder. Another source of noise is omnipresent (except in a perfect census): sampling variation discussed in Lesson 22. Sampling variation arises because the particular sample we happen to be working with is, so far as sampling is concerned, the play of luck. If we had happened to select another sample, the results would be different.\n\nThe Greek philosopher Heraclitus (c. 500 BC) said, \"You can't step into the same river twice.\" A step into a river might be at the same place on the bank, but the water flowing by will be different. A data sample is like collecting water from a river or lake using a dipper. Imagine ten people standing side by side on the shore of a lake, each person dipping into the water to acquire a specimen and making one or more measurements from the specimen, for instance, the temperature, pH, and bacteria count. Each person collects a sample---that is, a series of specimens. These might be taken right after one another or by some protocol, say a weekly tracking of lake conditions over time.\n\nThe ten people are each doing the same thing in approximately the same place and time, but each person's sample will be different, even if only by a little bit. That sample-to-sample variation will be noise. \n\nIf the ten people were fishing, each specimen would be the catch from one cast of the rod. Typically this is just an empty hook, lake weeds, or a stick, but sometimes it will be a fish. Each fisherman will have a sample at the end of the fishing day. These samples will not be identical; the fishermen's varying skills (or luck) will produce different results. That is sampling variation. To fishermen, the question of interest, the signal they want to measure, might be, \"How good is the fishing today?\" The fishermen's varying catches are the sampling variation. \n\nIn Lesson 21, we repeated trials over and over again to gain some feeling for sampling variation. Each trial consisted of collecting a sample and summarizing it. The individual trial is a summary of a sample. Then, to quantify the sampling variation, we summarized the set of individual trial results using the standard measure of variation: the standard deviation. \n\nIt is time to take off the DAG simulation training wheels and measure sampling variation from a *single* data frame. Our first approach will be to turn the single sample into several smaller samples: subsampling. Later, we will turn to another technique, resampling, which draws a sample of full size from the data frame.\n\n## Subsampling\nTo \"subsample\" means to draw a smaller sample from a large one. \"Small\" and \"large\" are relative. For our example, we turn to the `TenMileRace` data frame containing the record of thousands of runners' times in a race, along with basic information about each runner. There are many ways we could summarize `TenMileRace.` Any summary would do for the example. We will summarize the relationship between the runners' ages and their start-to-finish times (variable `net`), that is, `net ~ age`. To avoid the complexity of a runner's improvement with age followed by a decline, we will limit the study to people over 40.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTenMileRace %>% filter(age > 40) %>%\n  lm(net ~ age, data = .) %>% coefficients()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         age \n 4278.21279    28.13517 \n```\n:::\n:::\n\n\nThe units of `net` are seconds, and the units of `age` are years. The model coefficient on `age` tells us how the `net` time changes for each additional year of `age`: seconds per year. Using the entire data frame, we see that the time to run the race gets longer by about 28 seconds per year. So a 45-year-old runner who completed this year's 10-mile race in 3900 seconds (about 9.2 mph, a pretty good pace!) might expect that, in ten years, when she is 55 years old, her time will be longer by 280 seconds.\n\nIt would be asinine to report the ten-year change as 281.3517 seconds. The runner's time ten years from now will be influenced by the weather, crowding, the course conditions, whether she finds a good pace runner, the training regime, improvements in shoe technology, injuries, and illnesses, among other factors. There is little or nothing we can say from the `TenMileRace` data about such factors.\n\nThere's also sampling variation. There are 2898 people older than 40 in the `TenMileRace` data frame. The way the data was collected (radio-frequency interrogation of a dongle on the runner's shoe) suggests that the data is a census of finishers. However, it is also fair to treat it as a sample of the kind of people who run such races. People might have been interested in running but had a schedule conflict, lived too far away, or missed their train to the start line in the city.\n\nWe see sampling variation by comparing multiple samples. To create those multiple samples from `TenMileRace`, we will draw, at random, subsamples of, say, one-tenth the size of the whole, that is, $n=290$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOver40 <- TenMileRace %>% filter(age > 40)\nlm(time ~ age, data = Over40 %>% sample(size=290)) %>% coefficients()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         age \n 4111.70587    34.99828 \n```\n:::\n\n```{.r .cell-code}\nlm(time ~ age, data = Over40 %>% sample(size=290)) %>% coefficients()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         age \n 4111.26953    33.21348 \n```\n:::\n:::\n\n\nThe age coefficients from these two subsampling trials differ one from the other by about 0.5 seconds. To get a more systematic view, run more trials:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# a sample of summaries\nTrials <- do(1000) * {\n  lm(time ~ age, data = sample(Over40, size=290)) %>% coefficients()\n}\n# a summary of the sample of summaries\nTrials %>%\n  summarize(se = sd(age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       se\n1 9.17641\n```\n:::\n:::\n\n\nWe used the name `se` for the summary of samples of summaries because what we have calculated is the standard error of the age coefficient in a sample of size $n=290$. \n\nIn Lesson 22 we saw that the standard error is proportional to $1/\\sqrt{\\strut n}$, where $n$ is the sample size. From the subsamples, know that the SE for $n=290$ is about 9.0 seconds. This tells us that the SE for the full $n=2898$ samples would be about $9.0 \\frac{\\sqrt{290}}{\\sqrt{2898}} = 2.85$. \n\nSo the interval summary of the `age` coefficient---the *confidence interval*--- is $$\\underbrace{28.1}_\\text{age coef.} \\pm 2\\times\\!\\!\\!\\!\\!\\!\\! \\underbrace{2.85}_\\text{standard error} =\\ \\ \\ \\  28.1 \\pm\\!\\!\\!\\!\\!\\!\\!\\! \\underbrace{5.6}_\\text{margin of error}\\ \\  \\text{or, equivalently, 22.6 to 33.6}$$\n\n\n## Bootstrapping\n\nThere is a trick, called \"**resampling**,\" to generate a random subsample of a data frame with the same $n$ as the data frame: draw the new sample randomly from the original sample **with replacement**. An example will suffice to show what the \"with replacement\" does: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample <- c(1,2,3,4,5)\n# without replacement\nsample(example)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 4 3 5 2\n```\n:::\n\n```{.r .cell-code}\n# now, with replacement\nsample(example, replace=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 4 3 3 5\n```\n:::\n\n```{.r .cell-code}\nsample(example, replace=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3 5 4 4 4\n```\n:::\n\n```{.r .cell-code}\nsample(example, replace=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 1 2 2 3\n```\n:::\n\n```{.r .cell-code}\nsample(example, replace=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 3 1 4 5\n```\n:::\n:::\n\nThe \"with replacement\" leads to the possibility that some values will be repeated two or more times and other values will be left out entirely.\n\nThe calculation of the SE using resampling looks like this: `r `set.seed(207)`\n\n\n# run many trials\nTrials <- do(1000) * {\n  lm(time ~ age, data = sample(Over40, replace=TRUE)) %>% \n       coefficients()\n}\n# summarize the trials to find the SE\nTrials %>% summarize(se = sd(age))\n# or let the comput\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
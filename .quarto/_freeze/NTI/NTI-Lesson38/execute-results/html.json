{
  "hash": "12f3427a18ceef6f97ec52ca25acc01f",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 38\"\nsubtitle: \"False discovery\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"November 29, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\n\n## Objectives\n\n\n\n\n\n\n\n\n\n## 38.1\n\n::: {.callout-note}\n## Solution\n\n:::\n\n--------\n\n\n\n## Reading\n\nOne or more of these articles:\n\n- [A review of false discovery](www/false-discovery-significance.pdf)\n- [Diet and sex determination](www/cereal_and_sex_determination.pdf)\n- [Most research findings false](www/most-publication-findings-false.pdf)\n\n\n## Lesson\n\nDiscussion of article(s).\n\n\n### What should the p-value become\n\nConsider `dag07`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](NTI-Lesson38_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nNode `d` is not connected to any of the other nodes. There should accordingly be a \"null\" relationship between `d` and the others. On the other hand, `b` and `c` are connected (although the connection is confounded with `a`).\n\nLet's model `d` by `b` and look at the p-value: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nSample <- sample(dag07, size=50)\nlm(d ~ b, data=Sample) %>% regression_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   0.0556    0.132      0.422   0.675\n2 b            -0.121     0.0985    -1.23    0.225\n```\n:::\n:::\n\n\nThe p-value on the `b` coefficient is large, greater than the usual threshold of 0.05.\n\nOn the other hand, `b` and `c` are connected and the p-value (with this much data) is tiny.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(c ~ b, data = Sample) %>% regression_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -0.106     0.151    -0.705 4.84e- 1\n2 b             -1.55      0.113   -13.8   2.52e-18\n```\n:::\n:::\n\n\nImagine a setting where a popular (but unproven!) hypothesis has emerged: that `b` and `d` are really related. 100 different research teams rush in to be the first to demonstrate, each generating their own experimental data. We'll simulate this and collect the summary of the `b` coefficient w.r.t. `d`. [First show the statement without the `do()` to show what each row looks like. Then run the 100 trials and look for small p-values]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAll_groups <- do(100) * {\n  lm(d ~ b, data=sample(dag07, size=50)) %>% \n  regression_summary() %>%\n  filter(term == 'b')\n  }\n```\n:::\n\n\nDid any of the groups get a \"significant\" result?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAll_groups %>% \n  filter(p.value < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 7\n  term  estimate std.error statistic  p.value  .row .index\n  <chr>    <dbl>     <dbl>     <dbl>    <dbl> <int>  <dbl>\n1 b        0.267    0.0858      3.12 0.00309      1     16\n2 b       -0.258    0.112      -2.29 0.0265       1     19\n3 b        0.260    0.105       2.47 0.0171       1     28\n4 b        0.200    0.0930      2.14 0.0371       1     43\n5 b       -0.236    0.104      -2.28 0.0274       1     49\n6 b       -0.243    0.101      -2.39 0.0206       1     65\n7 b        0.341    0.0933      3.66 0.000628     1     76\n```\n:::\n:::\n\n\nIn the context of 100 trials being done, it's understandable that some of the groups happened to get a p-value < 0.05. But suppose that only the groups with small p-values publish their results? Then it looks as if they found a \"significant\" result.\n\nHow can we guard against this accidental generation of significant results? The standard answer in scientific work is to **replicate** the result: the labs should try again to confirm the result they got in the first study. (In practice, there are strong social/financial/career pressures *against* conducting such replications. These need to be overcome to guard against false discovery.)\n\nHere's a simulation where each lab group runs the study twice. Do any get small p-values both times?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nReplicated_groups <- do(100) * {\n  do(2) * {\n    lm(d ~ b, data=sample(dag07, size=50)) %>% \n      regression_summary() %>%\n      filter(term == 'b')\n    } %>% .$p.value\n} \nPairs <- Replicated_groups %>% \n  tidyr::pivot_wider(names_from = .row, values_from = result)\nPairs %>% filter(`1` < 0.05, `2` < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 0 × 3\n# … with 3 variables: .index <dbl>, 1 <dbl>, 2 <dbl>\n```\n:::\n:::\n\n\nA better approach. As a rule of thumb, once you have a sample size $n$ that gives a genuine  p $\\approx 0.05$, doubling $n$ should reduce p by a factor of about 10. But if p is merely accidentally small, doubling the sample size won't have any effect. \n\nA demonstration when there is a genuine relationship:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(c ~ a, data = sample(dag07, size=5)) %>% regression_summary() %>%\n  filter(term == 'a')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 a         1.85     0.552      3.35  0.0442\n```\n:::\n\n```{.r .cell-code}\nlm(c ~ a, data = sample(dag07, size=10)) %>% regression_summary() %>%\n  filter(term == 'a')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 a         1.75     0.471      3.72 0.00585\n```\n:::\n:::\n\n\nLet's re-run the simulation with $n$ doubled, that is, `size=100` compared to the previous `size=50`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBigger_n <- do(100) * {\n  lm(d ~ b, data=sample(dag07, size=100)) %>% \n    regression_summary() %>%\n    filter(term == 'b')\n  }\n```\n:::\n\n\nAre these p-values smaller than in the trials with `size=50`?\n\n\n## Learning Checks\n\n\n\n\n\n\n\n\n\n\n## 38.1\n\n::: {.callout-note}\n## Solution\n\n:::\n\n--------\n\n\n## Documenting software\n\n  \n\n",
    "supporting": [
      "NTI-Lesson38_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
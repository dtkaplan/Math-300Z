{
  "hash": "9dc9a6d8064043c2be72f1868d5c9fdd",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 29\"\nsubtitle: \"Covariates eat variance\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"October 14, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\n## Objectives\n\n\n\n\n\n\n\n\n29.1 Correctly define \"covariate\".\n\n29.2 Understand why including covariates---even spurious ones---always improves the appearance of model performance in in-sample testing.\n\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n\n\n\n## Reading\n\nTBD\n\n\n## Lesson\n\n\n::: {.callout-note icon=false}\n## Summary\n\nIncluding covariates in a model can help, or can hurt. These are the conclusions we're working toward:\n\ni. In-sample, including covariates in a model always reduces the (in-sample) prediction error. The pattern is stronger the smaller the training data set.\n\nii. Out-of-sample, including covariates may or may not reduce prediction error. It depends on whether the covariates are genuinely connected to the response variable.\n\niii. Irrelevant covariates make (out-of-sample) prediction worse. This effect is strongest for small training data sets.\n\nRemember, since some of the conclusions depend on what variables are connected to what, we need to demonstrate the phenomena using a system where we know the structure.\n\nWe'll come back to this topic, as a basis for ANOVA, when we do hypothesis testing. There we'll look at the sum of squares, mean square, F and such.\n\n:::\n\n\nToday, we'll work mostly with **in-sample** modeling. This reflects the case in the real world, where you have a data set but not usually an easy way to collect more data for testing.^[There is a technique that let's you get many of the benefits of out-of-sample testing with only one dataset. It's called **cross-validation**. Perhaps later in the course, but we have bigger fish to fry right now.]\n\nLet's work with `dag04`,`dag05`, and `dag07` to illustrate some points about covariates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_draw(dag04)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson29_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndag_draw(dag05)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson29_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\ndag_draw(dag07)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson29_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n:::\n\n\nStart with `dag04`, where variables `a`, `b`, and `c` all contribute to the formation of `d`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_rms_error(dag04, d ~ c, d~ b + c, d ~ a + b + c, n=50, in_sample = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.396321 1.080053 0.954669\n```\n:::\n:::\n\n\nPrediction error gets smaller, the more covariates are included.\n\nThe situation can be different. In `dag05`, `a`, `b`, and `c` all contribute to `d`, but **not separately**. `a` and `b` communicate with `d`  only via `c`. If `c` is in the model, `a` and `b` contribute nothing to reducing prediction error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_rms_error(dag05, d ~ c, d~ b + c, d ~ a + b + c, n=50, in_sample = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.060188 1.060100 1.054646\n```\n:::\n:::\n\n\n`dag07` is a case where we have covariates, but they aren't actually connected to `d`. Will they reduce prediction error? We'll use a very small sample size, $n=4$, to make the situation obvious.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.331334e-01 6.211263e-01 3.273470e-01 6.401858e-16\n```\n:::\n:::\n\n\n::: {.callout-note icon=false}\n## Pattern shown by `dag07` model\n\nConfirm these by running many simulations.\n\n1. The prediction error gets smaller the more covariates are included in the model.\n2. The last prediction error, with 4 terms in the model (don't forget `1`!) is zero. A perfect model?\n\n:::\n\n### A change in accounting\n\nWe've been using RMS prediction error to quantify how well the response variable has een\n\n\n\n## Learning Checks\n\n\n\n\n\n\n\n\n\n\n## LC 29.1\n\nIn `dag04`, build models to predict `c` from the other variables. Does one of those variables \"block\" the others? \n\n- Explain how you know this from your models. Try to give an answer in everyday language as well.\n- Repeat but use a very small sample size, say $n=5$. Has your conclusion about blocking changed? Explain why.\n\n::: {.callout-note}\n## Solution\n::: {.cell}\n\n```{.r .cell-code}\ncompare_rms_error(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50, in_sample = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.0088062 0.8334812 0.7918114 0.6634426\n```\n:::\n:::\n\n`d` seems to block effect of `a` and `b` on `c`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_rms_error(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5, in_sample = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3958936 0.3273822 0.2771008 0.1784200\n```\n:::\n:::\n:::\n\n## LC 29.2\n\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-**using** stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\n\nFor out-of-sample, when working with new data, it's not just a matter of being tricked into thinking covariates are useful when they're not. Using irrelevant covariates can be genuinely harmful to the predictions.\n\nCompare these in-sample and out-of-sample results. \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.689275e-01 4.188891e-01 3.603896e-01 1.416962e-16\n```\n:::\n\n```{.r .cell-code}\nset.seed(101)\ncompare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.965495 1.434434 1.641881 1.591050\n```\n:::\n:::\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?\n\n\n\n\n--------\n\n\n\n\n## Documenting software\n\n  \n\n",
    "supporting": [
      "Math300R-Lesson29_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
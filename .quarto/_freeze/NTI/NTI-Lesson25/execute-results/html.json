{
  "hash": "f36fa88b08de49c0d7066d7b20f9141e",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 25\"\nsubtitle: \"Mechanics of prediction\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"November 04, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: true\n    toc_float: true\n    css: NTI.css\n  pdf_document:\n    toc: true\n---\n\n\n\n\n## Objectives\n\n\n\n\n\n\n\n\n25.1 Given a data frame, construct a predictor function for a specified response variable.\n\n25.2 Use the predictor function to estimate prediction error and summarize with root mean square (RMS) error. Relate this to a prediction interval.\n\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error. \n\n\n## Reading\n\nTBD\n\n\n## Lesson\n\nReview mathematical/computational notation for functions. \n\n* $f(x,y,z)$ has three inputs (\"arguments\") that are separated by commas inside the function parentheses.\n\n* You can create such a function using `makeFun()` (as in Math 141Z/142Z).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(358549)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nSamp <- sample(dag03, size=10)\nMod <- lm(y ~ x, data = Samp)\n```\n:::\n\n\nShow that `Mod` is not yet in the form of a function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nMod(x=1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in Mod(x = 1): supplied argument name 'x' does not match 'z'\n```\n:::\n:::\n\n\nBut we can turn it into one with `makeFun()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun(Mod)\nf(x=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n1.368519 \n```\n:::\n:::\n\n\nThe function notation was invented long before statistics was a field. It turns out not to be very convenient for working in statistics. The reason: We have many variables stored in **data frames**. We'd like the input to our model functions to be in the form of a data frame. Even better, we'd like the output also to be in that form.\n\nThe `mod_eval()` function let's us do this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOutput <- mod_eval(Mod, data = Samp)\n```\n:::\n\n\n* Note that we built the model with `g ~ x + y`, so the model outputs a `g`-like thing.\n* The inputs are `x` and `y` which are drawn from the `data=` frame.\n* The output calculated from the model is called `model_output`.\n\nThis output is often called a **prediction**, what the model tells us to expect for the response variable when the inputs are given.\n\n### Prediction error\n\nThe prediction made by the model is not perfect. We can calculate the **error**, that is the difference between model output and the actual output for the given set of inputs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOutput <- Output %>% \n  mutate(error = y - model_output)\ngf_density(~ error, data = Output)\n```\n\n::: {.cell-output-display}\n![](NTI-Lesson25_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngf_point(error ~ x, data = Output)\n```\n\n::: {.cell-output-display}\n![](NTI-Lesson25_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\n\n* The errors have a bell-shaped distribution.\n* Note that the error is centered on zero; sometimes the model is high and sometimes low. Only occasionally is it right on target.\n* I've plotted the error versus the actual value. In this case, there seems to be no systematic deviation from being centered on zero. \n\nWe can quantify the average size of the error with the **root mean square error**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOutput %>% summarize(rms_error = sqrt(mean(error^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  rms_error\n      <dbl>\n1     0.960\n```\n:::\n:::\n\n\nThis number is intended to be used to quantify the **uncertainty** in predictions from the model. \n\nBUT THERE IS A CATCH. Notice that something funny is going on in this pair of commands:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMod <- lm(y ~ x, data = Samp)\nOutput <- mod_eval(Mod, data = Samp)\n```\n:::\n\n\nThe prediction being made here is called an **in-sample prediction**; the same data are used to construct the model and to calculate the prediction error. \n\nIn contrast, here is an **out-of-sample prediction**, where \"new\" data is used for the data input to `mod_eval()`. The new data is called **testing** data, while the data used to construct the model is called **training** data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOOS <- mod_eval(Mod, data = sample(dag03, size=10000))\nOOS %>% \n  mutate(error = y - model_output) %>%\n  summarize(rms = sqrt(mean((error^2))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.47\n```\n:::\n:::\n\n\n::: {.callout_important icon=false}\n## Activity\n\nWhich of these is in-sample and which out-of-sample prediction error.\n:::\n\n### To be honest ...\n\nWe knew that the out-of-sample error will be bigger than the in-sample error. But that should really be \"bigger on average.\" Sometimes, just by luck the in-sample error will be bigger than the out-of-sample error.\n\nTo show \"on average,\" we need to run many trials of the insample and many trials of the out-of-sample errors. Doing this requires considerable technical skill. This is just to demonstrate what's going on. It's the conclusion, rather than the method, that we want you to understand.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrial <- function(n=100) {\n  # Get a random seed based on the fractional seconds of the clock\n  seed <- new_seed_from_time()\n  Samp <- sample(dag03, size=n, seed=seed)\n  Mod <- lm(y ~ x, data = Samp)\n  Res1 <- mod_eval(Mod, data = Samp) %>%\n    mutate(in_error = y - model_output) %>%\n    summarize(in_rms = sqrt(mean(in_error^2))) \n  Res2 <- mod_eval(Mod, data = sample(dag03, size=1000)) %>%\n    mutate(out_error = y - model_output) %>%\n    summarize(out_rms = sqrt(mean(out_error^2))) \n  bind_cols(Res1, Res2,data.frame(seed = seed))\n}\n```\n:::\n\n\nNote: We used `size=1000` for the out-of-sample prediction. We can get any amount of out-of-sample data we like from the simulation, so we used \"a lot\" in order to get more reliable estimates.\n\nNow repeat the trial many times and summarize the result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nExpt <- do(1000)* {trial(n = 50) %>%\n  mutate(perc_diff = 100*(out_rms - in_rms)/out_rms)}\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in nanotime::nanotime(Sys.time()) %>% as.character() %>% substr(21, :\nNAs introduced by coercion\n```\n:::\n\n```{.r .cell-code}\ngf_violin(perc_diff ~ 1, data = Expt)\n```\n\n::: {.cell-output-display}\n![](NTI-Lesson25_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngf_violin(out_rms ~ 1, data = Expt, color=\"blue\", alpha=0.5) %>%\n  gf_violin(in_rms ~ 2, data = Expt, color=\"red\", alpha=0.5) %>%\n  gf_refine(scale_y_log10())\n```\n\n::: {.cell-output-display}\n![](NTI-Lesson25_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\ndf_stats(~ out_rms, data = Expt, ci.mean())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  response    lower    upper\n1  out_rms 1.247667 1.252605\n```\n:::\n\n```{.r .cell-code}\ndf_stats(~ in_rms, data = Expt, ci.mean())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  response    lower    upper\n1   in_rms 1.175937 1.191047\n```\n:::\n:::\n\n\n\n\n## Learning Checks\n\n\n\n\n\n\n\n\n\n::: {.callout-warning}\n## Just while in draft\n\n25.1 Given a data frame, construct a predictor function for a specified response variable.\n\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error. Relate this to a predition interval.\n\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error. \n:::\n\n## 25.2\n\nThe data frame `moderndive::house_prices` lists the sales prices of 21,613 houses in King County, Washington (which includes Seattle) sold from May 2014 and May 2015. Often, with price or income data, economists work with the *logarithm* of the price or income or income-related quantity such as house living area.  We are going to do here, but this problem is not about logarithms, so once you create the \"logged\" data frame, you'll just be modeling the data using the usual methods.\n\nTo create the \"logged\" data, add these two new variables to the data frame, which we will call `Seattle`.\n\n::: {.cell}\n\n```{.r .cell-code}\nSeattle <- moderndive::house_prices %>% \n  mutate(log_price = log10(price),\n         log_area = log10(sqft_living))\n```\n:::\n\n1. Build a model of `log_price ~ log_area` using the `Seattle` data. Store the model under the name `pmod`.\n\n::: {.callout-note}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\npmod <- lm(log_price ~ log_area, data = Seattle)\n```\n:::\n:::\n\n2. Imagine that you are moving to Seattle in August 2014. Housing is expensive in the Seattle area, so you might decide to live in a small house, say 750 square feet. The `log_area` of such a house is 2.87. Using `mod_eval()`, predict the `log_price` of such a house. (Note that `mod_eval()` puts the model output in a column named `model_output`, not `log_price`.) In your Rmd file, give the command and show the output. If you're curious about what the predicted price is in dollars (rather than log-dollars), simply raise 10 to the log-dollar amount. For instance, if the `model_output` were 5, the dollar amount will be $10^5 = \\$100,000$.\n\n::: {.callout-note}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_eval(pmod, log_area = 2.87)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  log_area model_output\n1     2.87     5.324298\n```\n:::\n:::\nIn dollar terms, the predicted price is 10^5.324^ = 2.1086281\\times 10^{5}.\n:::\n\n3. Repeat (2), but for a house with lots of space: 1500 square feet. The `log_area` of such a house is 3.18. As in (2), give the command and show the output in your Rmd file.\n\n4. Your budget is $200,000. In log dollars this budget is `log10(200000)` = 5.3. The predicted price of a 750 square-foot house is somewhat beyond your budget. But you figure that some 750-square foot house will be within your budget. To see if this is likely, look at the **prediction interval** of the house price. You can do this by adding the `interval=\"prediction\"` to the `mod_eval()` command. Is your budget (5.3 log dollars) within the prediction interval? Show your command and the result in your Rmd file and also give a sentence stating your conclusion.\n\n::: {.callout-note}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_eval(pmod, log_area = 2.87, interval=\"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  log_area model_output    lower   upper\n1     2.87     5.324298 4.993416 5.65518\n```\n:::\n:::\nYeah! Your budget of 5.3 log dollars is near the center of the prediction interval. \n:::\n\n5. On a hunch, you decide to see whether you might find a 1500 square foot (`log_area` = 3.18) might also fall within the prediction interval. Will it? Show your command, the result, and a sentence interpreting the result.\n\n::: {.callout-note}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_eval(pmod, log_area = 3.18, interval=\"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  log_area model_output    lower    upper\n1     3.18     5.583697 5.252851 5.914543\n```\n:::\n:::\nStrictly speaking, your budget (5.3 log dollars) is within the prediction interval. But it is very close to the lower bound of 5.25 log dollars. So there will likely be few houses of 1500 square feet within your budget. So plan that the house you will end up purchasing will be somewhere in the range 750-1500 square feet.\n:::\n\n--------\n\n## 25.XX\n\nYou are a bus dispatcher in New York City. The Department of Education bus logistics office has called to say that a school bus has broken down and the students need to be offloaded onto a functioning bus to take them to school. Unfortunately, the DOE officer didn't tell you how many students are on the bus. You need to make a quick prediction in order to decide what kind and how many busses you will need for the pickup.\n\nYou go to the NYC OpenData site bus breakdown page to get the historical data on how many students are on the bus. There are more than 200,000 bus events listed, each one of them including the number of students. You make a jitter/violin plot of the number of students on each of the 200,000 busses.\n\n::: {.cell}\n::: {.cell-output-display}\n![](NTI-Lesson25_files/figure-html/cheetah-lose-saucer-1-1.png){width=672}\n:::\n:::\n\na. The violin plot looks like an upside-down T. Explain what's going on. (Hint: How many students fit on a school bus?) -A- As very often happens, the data file contains data-entry or other mistakes producing outliers. Almost all of the 200,000 bus incidents fall into the horizontal line near zero. There are only 164 with a number above 100 students. In the US, the legal maximum capacity for a school bus is 72 students.\n\n\nOne of the ways of handling outliers is to delete them from the data. A softer way is to trim the outliers, giving them a value that is distinct but not so far from the mass of values. The figure below shows a violin plot where any record where the number of students is greater than 20 is trimmed to 21. \n::: {.cell}\n::: {.cell-output-display}\n![](NTI-Lesson25_files/figure-html/cheetah-lose-saucer-2-1.png){width=672}\n:::\n:::\n\nb. If you sent a small school bus (capacity 14), what fraction of the time would you be able to handle all the students on the school bus? -A- Only about 5% of the area of the violin plot is above 14.\n\nc. If you sent one 14-passenger school bus with another on stand-by (just in case the first bus doesn't have sufficient capacity), what fraction of the time could you handle all the students?\n\n-A- It's tempting to say that the 2 x 14 = 28 passenger capacity could handle all the cases, but remember, the cases at 21 stand for \"21 or more passengers.\" We can't tell from the violin plot how many of those have more than 28 students on board.\n\nd. Notice that the violin plot is jagged. Explain why. -A- The number of passengers is an integer, e.g. 1, 2, 3, .... It can't be a number like 4.5. \n\n--------\n\n## 25.YYY\n\nAt a very large ballroom dance class, you are to be teamed up with a randomly selected partner. There are 200 potential partners. The figure below shows their heights.\n\nFrom the data plotted, calculate a 95% prediction interval on the height of your eventual partner. (Hint: You can do this by counting.)\n\n::: {.cell}\n::: {.cell-output-display}\n![](NTI-Lesson25_files/figure-html/fish-eat-vase-1-1.png){width=672}\n:::\n:::\n\n::: {.callout-note}\n## Solution\n59 to 74 inches.\n\nSince there are 200 points, a 95% interval should exclude the top five cases and the bottom five cases. So draw the bottom boundary of  the interval just above the bottom five points, and the top boundary just below the top five points.\n\n::: {.cell}\n::: {.cell-output-display}\n![](NTI-Lesson25_files/figure-html/fish-eat-vase-2-1.png){width=672}\n:::\n:::\n:::\n\n--------\n\n\n## Documenting software\n\n  \n\n",
    "supporting": [
      "NTI-Lesson25_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
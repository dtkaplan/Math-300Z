{
  "hash": "1e56f43044068520716e8c8da3ccfb33",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 29\"\nsubtitle: \"Covariates eat variance\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"November 04, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\n## Objectives\n\n\n\n\n\n\n\n\n29.1 Correctly define \"covariate\".\n\n29.2 Understand why including covariates---even spurious ones---always improves the appearance of model performance in in-sample testing.\n\n29.3 Read a DAG to anticipate when using spurious covariates will improve or will worsen model performance on out-of-sample prediction.\n\n29.4 Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)\n\n\n## Reading\n\nTBD\n\n\n## Lesson\n\n\n::: {.callout-note icon=false}\n## Summary\n\nIncluding covariates in a model can help, or can hurt. These are the conclusions we're working toward:\n\ni. In-sample, including covariates in a model always reduces the (in-sample) prediction error. The pattern is stronger the smaller the training data set.\n\nii. Out-of-sample, including covariates may or may not reduce prediction error. It depends on whether the covariates are genuinely connected to the response variable.\n\niii. Irrelevant covariates make (out-of-sample) prediction worse. This effect is strongest for small training data sets.\n\nRemember, since some of the conclusions depend on what variables are connected to what, we need to demonstrate the phenomena using a system where we know the structure.\n\nWe'll come back to this topic, as a basis for ANOVA, when we do hypothesis testing. There we'll look at the sum of squares, mean square, F and such.\n\n:::\n\n\nToday, we'll work mostly with **in-sample** modeling. This reflects the case in the real world, where you have a data set but not usually an easy way to collect more data for testing.^[There is a technique that let's you get many of the benefits of out-of-sample testing with only one dataset. It's called **cross-validation**. Perhaps later in the course, but we have bigger fish to fry right now.]\n\nLet's work with `dag04`,`dag05`, and `dag07` to illustrate some points about covariates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_draw(dag04)\n```\n\n::: {.cell-output-display}\n![](NTI-Lesson29_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndag_draw(dag05)\n```\n\n::: {.cell-output-display}\n![](NTI-Lesson29_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\ndag_draw(dag07)\n```\n\n::: {.cell-output-display}\n![](NTI-Lesson29_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n:::\n\n\nStart with `dag04`, where variables `a`, `b`, and `c` all contribute to the formation of `d`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_model_residuals(dag04, d ~ c, d~ b + c, d ~ a + b + c, n=50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.628050 1.375794 1.105072\n```\n:::\n:::\n\n\nPrediction error gets smaller, the more covariates are included.\n\nThe situation can be different. In `dag05`, `a`, `b`, and `c` all contribute to `d`, but **not separately**. `a` and `b` communicate with `d`  only via `c`. If `c` is in the model, `a` and `b` contribute nothing to reducing prediction error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_model_residuals(dag05, d ~ c, d~ b + c, d ~ a + b + c, n=50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.138071 1.132395 1.126588\n```\n:::\n:::\n\n\n`dag07` is a case where we have covariates, but they aren't actually connected to `d`. Will they reduce prediction error? We'll use a very small sample size, $n=4$, to make the situation obvious.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in qt((1 - level)/2, df): NaNs produced\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.387186 2.217092 3.216340 3.231776\n```\n:::\n:::\n\n\n::: {.callout-note icon=false}\n## Pattern shown by `dag07` model\n\nConfirm these by running many simulations.\n\n1. The prediction error gets smaller the more covariates are included in the model.\n2. The last prediction error, with 4 terms in the model (don't forget `1`!) is zero. A perfect model?\n\n:::\n\n\n\n\n## Learning Checks\n\n\n\n\n\n\n\n\n\n## 29.A {#sec-LC-29-A}\n\n\n\nThe `math300::Hill_racing\" data frame records 2236 winning times (in seconds) in Scottish hill racing competitions. Consider this model of the winning time as a function of the race distance (km) and the total climb (meters):\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(time ~ distance + climb, data=Hill_racing)\n```\n:::\n\nThe `model_eval()` function provides a convenient way to evaluate the model output (`.output`) for each of the rows in a data frame and, at the same time, calculates row-by-row residuals (`.resid`) and prediction errors (`.lwr` and `.upr`). *Make sure to take not of the starting periods on the names.*\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_eval(mod) %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  time distance climb  .output     .resid       .lwr     .upr\n1 1630        6   240 1679.215  -49.21475  -29.56279 3387.992\n2 1655        6   240 1679.215  -24.21475  -29.56279 3387.992\n3 2391        6   240 1679.215  711.78525  -29.56279 3387.992\n4 2351        6   240 1679.215  671.78525  -29.56279 3387.992\n5 4151       14   660 4805.779 -654.77947 3097.10184 6514.457\n6 3975       14   660 4805.779 -830.77947 3097.10184 6514.457\n```\n:::\n:::\n\n\n\n\nThe RMS residual from the model can be calculated this way:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_eval(mod) %>%\n  summarize(rms = sqrt(mean(.resid^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       rms\n1 870.4631\n```\n:::\n:::\n1. What are the units of the RMS residual?\n\n2. Modify the calculation to compute the sum-of-square residuals. Report the result numerically. Be sure to say what are the units.\n\n3. What are the units of the effect size on time with respect to climb? \n\n4. What are the units of the effect size on time with respect to climb? \n\n::: {.callout-note}\n## Solution\n\n1. RMS residual has the same units as the response variable. In this case, that's the time to run the race, with units \"seconds.\"\n\n2. SS residual has units that are the square of the respond variable, in this case \"square-seconds.\" \n\n3. Recall that the effect size on the response with respect to an explanatory variable has the units of the response variable divided by the units of the explanatory variable. The `climb` variable has units of meters, so the effect size has units \"seconds/meters.\"\n\n4. seconds/km\n\n:::\n\n---------\n\n## LC 29.B\n\n\nWhich of the following models are **not** nested within `time ~ distance + climb`?\n\na. `time ~ 1`\nb. `time ~ distance + sex`\nc. `time ~ distance`\nd. `time ~ climb`\n\n::: {.callout-note}\n## Solution\n\nThe model `time ~ distance + sex` is not nested in `time ~ distance + climb`. \n\nNote that `time ~ 1` is indeed nested in `time ~ distance + climb`. The `1` corresponds to the intercept.\n\n:::\n\n--------\n\n## 29.C \n\nIn LC -@sec-LC-29-A you calculated the RMS residuals and the sum-of-square residuals by wrangling the results from `mod_eval()`. That's a perfectly good way to do things, but the work becomes tedious when there are multiple models you want to compare.\n\nFor convenience, there is a `compare_model_residuals()` command, which can calculate the RMS residual or sum-of-square residual for each of a set of models. All the models must have the same response variable.\n\n::: {.cell}\n\n```{.r .cell-code}\nHill_racing %>% \n  compare_model_residuals(time ~ 1, \n                          time ~ distance + climb, \n                          time ~ distance + climb + sex,\n                          time ~ distance,\n                          measure = \"RMS\"\n                          )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3122.4821  870.4631  775.2962 1189.7148\n```\n:::\n:::\n\nIt happens that all of the models in the command are a nested set. Re-order the models so that each model nests inside the following model, that is, from smaller model to bigger model. \n\n1. Do the RMS residuals for the nested models increase or decrease when moving from a smaller model to a larger model?\n\n2. You can calculate the sum-of-square residual by using the argument `measure=\"SS\"`. Do the sum-of-square residuals for the nested models increas or decrease when moving from a smaller model to a larger model.\n\n3. You can calculate R^2^ by using the argument `measure=\"R2\"`. Do the R^2^ for the nested models increase or decrease when moving from a smaller model to a larger model.\n\n::: {.callout-note}\n## Solution\n\n1. Decrease\n2. Decrease\n3. Increase. R^2^ tells you how big the model output is compared to the response variable. 1-R^2^ tells you how big the residuals are compared to the response variable.\n:::\n\n--------\n\n## LC 29.D\n\n::: {.callout-warning}\n## Still a draft\n\nLook at `dag07`. Notice that `d` is not connected to any of the other variables.\n\nGenerate a sample of size $n=6$. Compare the sum of square residual (in sample) from the nested  models `c ~ 1`, `c ~ a` `c ~ a + b`, and `c ~ a + b + d`. (Use the `compare_model_residuals()` using the argument `method=\"SS\"`.\n\nWhich, if any, of the variables `a`, `b`, or `d` reduces the in-sample sum-of-squared residuals compared to the previous model.\n\n\n::: {.callout-note}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_model_residuals(dag07, c ~ 1, c ~ a, c ~ a + b, c ~ a + b + d, \n                        n=6, measure=\"R2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.000000 4.123805 5.041728 3.817478\n```\n:::\n:::\n:::\n\nOut of sample, the useless covariate often *increases* the SS error.\n\n--------\n\n## LC 29.1\n\nIn `dag04`, build models to predict `c` from the other variables. Does one of those variables \"block\" the others? \n\n- Explain how you know this from your models. Try to give an answer in everyday language as well.\n- Repeat but use a very small sample size, say $n=5$. Has your conclusion about blocking changed? Explain why.\n\n::: {.callout-note}\n## Solution\n::: {.cell}\n\n```{.r .cell-code}\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9584840 0.8647784 0.7687133 0.7156887\n```\n:::\n:::\n\n`d` seems to block effect of `a` and `b` on `c`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_model_residuals(dag04, c~ 1, c ~ d, c~ b + d, c ~ a + b + d, n=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5045527 0.9949466 1.2849421 1.3575294\n```\n:::\n:::\n:::\n\n## LC 29.2\n\nWe are using in-sample testing because that is often the case in the model-building stage. However, in the model-**using** stage, things are different. You will be making predictions of new cases, that is, out-of-sample.\n\nFor out-of-sample, when working with new data, it's not just a matter of being tricked into thinking covariates are useful when they're not. Using irrelevant covariates can be genuinely harmful to the predictions.\n\nCompare these in-sample and out-of-sample results. \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in qt((1 - level)/2, df): NaNs produced\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.965495 1.434434 1.641881 1.591050\n```\n:::\n\n```{.r .cell-code}\nset.seed(101)\ncompare_model_residuals(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, \n                        testing = \"out-of-sample\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in qt((1 - level)/2, df): NaNs produced\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.965495 1.434434 1.641881 1.591050\n```\n:::\n:::\n\nWhat do you see in the results that tells you that incorporating irrelevant covariates hurts the out-of-sample predictions?\n\n\n\n\n--------\n\n## LC 29.3\n\n::: {.callout-warning}\n## In draft\n\n`openintro::teacher`. What's the `base` pay difference between a teacher with an MA and a BA degree? What's a confidence interval on this effect size? How does the confidence interval change if you include `years` as a covariate.\n:::\n\n--------\n\n## LC 29.4\n\n::: {.callout-warning}\n## In draft\n\n`openintro::census` Predict log personal income based on other variables. Eat variance using the `total_family_income` variable.\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(log10(total_personal_income) ~ log10(age) + sex + marital_status + log10(total_family_income), data = openintro::census %>% filter(total_personal_income > 0, total_family_income > 0))\nanova(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: log10(total_personal_income)\n                            Df Sum Sq Mean Sq  F value    Pr(>F)    \nlog10(age)                   1  5.938  5.9383  35.6102 6.660e-09 ***\nsex                          1  5.976  5.9758  35.8351 6.006e-09 ***\nmarital_status               5  4.302  0.8604   5.1596 0.0001464 ***\nlog10(total_family_income)   1 17.620 17.6198 105.6615 < 2.2e-16 ***\nResiduals                  306 51.028  0.1668                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\ngf_jitter(total_personal_income ~ total_family_income | sex, \n         data =openintro::census %>% filter(total_personal_income > 3000),\n         color=~marital_status, alpha=0.3) %>% \n  gf_refine(scale_y_log10(), scale_x_log10())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Transformation introduced infinite values in continuous x-axis\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 20 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](NTI-Lesson29_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n:::\n\n--------\n\n## 29.5\n\n::: {.callout-warning}\n## Still in draft\n\n`openintro::starbucks`  where do the calories come from?  Find effect size of, say, protein on calories. Then see what happens if you use carbohydrates as a covariate.\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n```\n:::\n\n```{.r .cell-code}\nlm( calories ~ fat + carb + fiber + protein, data = openintro::starbucks) %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                2.5 %    97.5 %\n(Intercept) -2.938079 13.605279\nfat          8.591250  9.315766\ncarb         3.686593  3.997527\nfiber       -1.418966  1.370022\nprotein      3.631695  4.364091\n```\n:::\n:::\n\n::: {.comment-warning}\n## In draft\n\nMaybe come back to this in confounding lesson. Look for components that tend to go together\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(openintro::starbucks, cor(fat, protein))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.22347\n```\n:::\n\n```{.r .cell-code}\nwith(openintro::starbucks, cor(fiber, protein))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.488564\n```\n:::\n\n```{.r .cell-code}\nlm( calories ~ fiber , data = openintro::starbucks) %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %    97.5 %\n(Intercept) 276.119106 343.80739\nfiber         1.923476  24.07453\n```\n:::\n\n```{.r .cell-code}\nlm( calories ~ protein, data = openintro::starbucks) %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %     97.5 %\n(Intercept) 254.107446 322.072380\nprotein       2.616542   8.087778\n```\n:::\n\n```{.r .cell-code}\nlm( calories ~ protein + fiber, data = openintro::starbucks) %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %     97.5 %\n(Intercept) 247.891487 320.333514\nprotein       1.700777   7.996897\nfiber        -8.099007  15.978382\n```\n:::\n:::\n:::\n\n--------\n\n\n\n\n\n\n\n\n## Documenting software\n\n  \n\n",
    "supporting": [
      "NTI-Lesson29_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "6abb880edbdee302d9553e7e8cc8df99",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 37\"\nsubtitle: \"False discovery\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"October 14, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\n\n## Objectives\n\n#. Identify signs of false discovery in a research paper.\n\n#. Estimate how overall p-value should change when study is replicated.\n\n\n## Reading\n\nOne or more of these articles:\n\n- [A review of false discovery](www/false-discovery-significance.pdf)\n- [Diet and sex determination](www/cereal_and_sex_determination.pdf)\n- [Most research findings false](www/most-publication-findings-false.pdf)\n\n\n## Lesson\n\nDiscussion of article(s).\n\n\n### What should the p-value become\n\nConsider `dag07`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Math300R-Lesson38_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNode `d` is not connected to any of the other nodes. There should accordingly be a \"null\" relationship between `d` and the others. On the other hand, `b` and `c` are connected (although the connection is confounded with `a`).\n\nLet's model `d` by `b` and look at the p-value: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nSample <- sample(dag07, size=50)\nlm(d ~ b, data=Sample) %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   0.0556    0.132      0.422   0.675\n2 b            -0.121     0.0985    -1.23    0.225\n```\n:::\n:::\n\n\nThe p-value on the `b` coefficient is large, greater than the usual threshold of 0.05.\n\nOn the other hand, `b` and `c` are connected and the p-value (with this much data) is tiny.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(c ~ b, data = Sample) %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -0.106     0.151    -0.705 4.84e- 1\n2 b             -1.55      0.113   -13.8   2.52e-18\n```\n:::\n:::\n\n\nImagine a setting where a popular (but unproven!) hypothesis has emerged: that `b` and `d` are really related. 100 different research teams rush in to be the first to demonstrate, each generating their own experimental data. We'll simulate this and collect the summary of the `b` coefficient w.r.t. `d`. [First show the statement without the `do()` to show what each row looks like. Then run the 100 trials and look for small p-values]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAll_groups <- do(100) * {\n  lm(d ~ b, data=sample(dag07, size=50)) %>% \n  broom::tidy() %>%\n  filter(term == 'b')\n  }\n```\n:::\n\n\nDid any of the groups get a \"significant\" result?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAll_groups %>% \n  filter(p.value < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 7\n  term  estimate std.error statistic p.value  .row .index\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl> <int>  <dbl>\n1 b        0.298    0.105       2.84 0.00651     1     42\n2 b       -0.199    0.0889     -2.24 0.0297      1     76\n3 b        0.232    0.0934      2.48 0.0166      1     78\n4 b       -0.211    0.0989     -2.14 0.0375      1     99\n```\n:::\n:::\n\n\nIn the context of 100 trials being done, it's understandable that some of the groups happened to get a p-value < 0.05. But suppose that only the groups with small p-values publish their results? Then it looks as if they found a \"significant\" result.\n\nHow can we guard against this accidental generation of significant results? The standard answer in scientific work is to **replicate** the result: the labs should try again to confirm the result they got in the first study. (In practice, there are strong social/financial/career pressures *against* conducting such replications. These need to be overcome to guard against false discovery.)\n\nHere's a simulation where each lab group runs the study twice. Do any get small p-values both times?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nReplicated_groups <- do(100) * {\n  do(2) * {\n    lm(d ~ b, data=sample(dag07, size=50)) %>% \n      broom::tidy() %>%\n      filter(term == 'b')\n    } %>% .$p.value\n} \nPairs <- Replicated_groups %>% \n  tidyr::pivot_wider(names_from = .row, values_from = result)\nPairs %>% filter(`1` < 0.05, `2` < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 0 × 3\n# … with 3 variables: .index <dbl>, 1 <dbl>, 2 <dbl>\n```\n:::\n:::\n\n\nA better approach. As a rule of thumb, once you have a sample size $n$ that gives a genuine  p $\\approx 0.05$, doubling $n$ should reduce p by a factor of about 10. But if p is merely accidentally small, doubling the sample size won't have any effect. \n\nA demonstration when there is a genuine relationship:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(c ~ a, data = sample(dag07, size=5)) %>% broom::tidy() %>%\n  filter(term == 'a')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 a         1.63     0.968      1.68   0.191\n```\n:::\n\n```{.r .cell-code}\nlm(c ~ a, data = sample(dag07, size=10)) %>% broom::tidy() %>%\n  filter(term == 'a')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  <chr>    <dbl>     <dbl>     <dbl>   <dbl>\n1 a        0.957     0.585      1.64   0.141\n```\n:::\n:::\n\n\nLet's re-run the simulation with $n$ doubled, that is, `size=100` compared to the previous `size=50`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBigger_n <- do(100) * {\n  lm(d ~ b, data=sample(dag07, size=100)) %>% \n    broom::tidy() %>%\n    filter(term == 'b')\n  }\n```\n:::\n\n\nAre these p-values smaller than in the trials with `size=50`?\n\n\n## Learning Challenges\n\n\n\n## Documenting software\n\n  \n\n",
    "supporting": [
      "Math300R-Lesson38_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "7c48fce58a99a06ae71e60f797fafbe1",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 35\"\nsubtitle: \"Accounting for prevalence\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"October 14, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\n\n## Objectives\n\n\n\n\n\n\n\n\n#. Explain why case-control data may not give an proper measure of \"prevalence.\"\n\n#. Understand sensitivity and specificity as conditional probabilities.\n\n#. Calculate false-positive and false-negative rates for a given prevalence.\n\n\n\n## Reading\n\nTBD\n\n\n## Lesson\n\nIn [Lesson 33](Math300R-Lesson33.html) we built a classifier based on simulated data from `dag10`. \n\n- A classifier is a function of the variables deemed relevant by the designer and that produces a yes/no output.\n- Discovering a suitable function requires insight and usually trial and error.  \n\n\n\nHere's a classifier relevant to `dag10`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC1 <- make_classifier(c1 ~ b < 0)\n```\n:::\n\n\n`C1` is a function taking a data frame as input. The calculation is whether `b` is less than 0. If so, the function output is a 1. The output will be added as column `c1` to the input data\n\nTo evaluate it, create a dataframe from `dag10` and run the classifier on it. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nSample <- sample(dag10, size=1000) %>% C1()\nwith(Sample, table(y, c1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   c1\ny     -   +\n  0 368 125\n  1 131 376\n```\n:::\n:::\n\n\nThe false-positive rate is 125/1000, the false-negative rate is 131/1000. The \"accuracy\" is (368+376)/1000, about 74%.\n\nWe designed classifier `C1()` using where the disease state is 1 about half the time, as appropriate for a **case-control** design.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSample %>% summarize(yesses = mean(y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 1\n  yesses\n   <dbl>\n1  0.507\n```\n:::\n:::\n\n\nNow we want to deploy the classifier in a realistic setting, where the **base rate** is only, say, 10%. To simulate this, we'll keep all of the 0s from the simulation, but only 11% of the 1s. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nField_results <- sample(dag10, \n                        size=1000, \n                        survive=~ifelse(y==1, unif() < 0.11, TRUE)) %>% C1()\nwith(Field_results, table(y, c1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   c1\ny     -   +\n  0 683 225\n  1  23  69\n```\n:::\n:::\n\n\nThe false negative rate is now close to zero: 19/1000. But the false positive rate is 31%, much higher than with the case-control data used to design the classifier. \n\nWhat's gone wrong? Why did the false positive rate change? \n\nThe reason is that the \"**base rate**\" (or **prevalence**) for the disease is 10.1% in `dag10b` compared to the 50% in `dag10`. Same classifier, but a different false positive rate.\n\nWe would like to have a way to characterize a classifier that is independent of the base rate.\n\nThe false positive rate is a probability: p(0 & +). Similarly, the false negative rate is p(1 & -). We can read these off the table. \n\nThe quantities of interest to the patient and doctor are different probabilities: \n\n- p(0 | -) --- probability you don't have the disease given a negative test result\n- p(1 | +) --- probability you have the disease given a positive test result\n\nCalculate these probabilities from the classifer test results for the two base rates we've looked at: 50% for `dag10` and 21% for `dag10b`.\n\n\n- Case/control --- p(0 | -) is 368/(368 + 131) = 73%\n- Field -- p(0 | -) is 759/(746 + 29) = 96%\n\n- Case/control --- p(1 | +) is 376/(376 + 125) = 75%\n- Field -- p(1 | +) is 88/(248+88) = 26%\n\nThese patient-centered probabilities change as the base rate changes. It turns out that the proper way to characterize the classifier is with two different probabilities:\n\n- Sensitivity: Probability of a + test if you have the disease: p(+ | 1)\n- Specificity: Probability of a - test if you do not have the disease: p(- | 1)\n\nNote that these are not probabilities of direct interest to the patient or doctor. But they do come out the same regardless of the base rate of the disease.\n\n- Case/control --- p(+ | 1) is 376/(376+131) = 74%\n- Field -- p(+ | 1) is 88/(29+88) = 75%\n\n- Case/control --- p(- | 0) is 368/(368 + 125) = 75%\n- Field -- p(- | 0) is 746/(746+248)= 75%\n\nGiven the sensitivity/specificity and the base rate, we can calculate the probability of interest to the patient\n\np(1 | +) = p(+ | 1) p(1) / (p(+ | 1)p(1) + p(-|0)p(0))\n\nIn statistics, the sensitivity/specificity is called the \"likelihood function\", the probability of the observed data under the actual values in the world. THIS IS WEAK.\n\n## Learning Challenges\n\n\n\n\n\n\n\n\n\n## 35.1\n\nGiven some classifier summaries, calculate the false-positive and false-negative rates as well as the sensitivity and specificity\n\n::: {.callout-note}\n## Solution\n\n:::\n\n--------\n\n\n\n\n\n\n## Documenting software\n\n  \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
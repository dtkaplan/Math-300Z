{
  "hash": "f36fa88b08de49c0d7066d7b20f9141e",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 25\"\nsubtitle: \"Mechanics of prediction\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"October 14, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: true\n    toc_float: true\n    css: NTI.css\n  pdf_document:\n    toc: true\n---\n\n\n\n\n## Objectives\n\n\n\n\n\n\n\n\n25.1 Given a sample from a  DAG simulation, construct a predictor function for a specified response variable.\n\n25.2 Use the predictor function to estimate prediction error on a given DAG sample and summarize with root mean square (RMS) error.\n\n25.3 Distinguish between in-sample and out-of-sample prediction estimates of prediction error. \n\n\n## Reading\n\nTBD\n\n\n## Lesson\n\nReview mathematical/computational notation for functions. \n\n* $f(x,y,z)$ has three inputs (\"arguments\") that are separated by commas inside the function parentheses.\n\n* You can create such a function using `makeFun()` (as in Math 141Z/142Z).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(358549)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nSamp <- sample(dag03, size=10)\nMod <- lm(y ~ x, data = Samp)\n```\n:::\n\n\nShow that `Mod` is not yet in the form of a function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nMod(x=1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in Mod(x = 1): supplied argument name 'x' does not match 'z'\n```\n:::\n:::\n\n\nBut we can turn it into one with `makeFun()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- makeFun(Mod)\nf(x=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n1.368519 \n```\n:::\n:::\n\n\nThe function notation was invented long before statistics was a field. It turns out not to be very convenient for working in statistics. The reason: We have many variables stored in **data frames**. We'd like the input to our model functions to be in the form of a data frame. Even better, we'd like the output also to be in that form.\n\nThe `mod_eval()` function let's us do this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOutput <- mod_eval(Mod, data = Samp)\n```\n:::\n\n\n* Note that we built the model with `g ~ x + y`, so the model outputs a `g`-like thing.\n* The inputs are `x` and `y` which are drawn from the `data=` frame.\n* The output calculated from the model is called `model_output`.\n\nThis output is often called a **prediction**, what the model tells us to expect for the response variable when the inputs are given.\n\n### Prediction error\n\nThe prediction made by the model is not perfect. We can calculate the **error**, that is the difference between model output and the actual output for the given set of inputs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOutput <- Output %>% \n  mutate(error = y - model_output)\ngf_density(~ error, data = Output)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson25_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngf_point(error ~ x, data = Output)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson25_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\n\n* The errors have a bell-shaped distribution.\n* Note that the error is centered on zero; sometimes the model is high and sometimes low. Only occasionally is it right on target.\n* I've plotted the error versus the actual value. In this case, there seems to be no systematic deviation from being centered on zero. \n\nWe can quantify the average size of the error with the **root mean square error**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOutput %>% summarize(rms_error = sqrt(mean(error^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  rms_error\n      <dbl>\n1     0.960\n```\n:::\n:::\n\n\nThis number is intended to be used to quantify the **uncertainty** in predictions from the model. \n\nBUT THERE IS A CATCH. Notice that something funny is going on in this pair of commands:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMod <- lm(y ~ x, data = Samp)\nOutput <- mod_eval(Mod, data = Samp)\n```\n:::\n\n\nThe prediction being made here is called an **in-sample prediction**; the same data are used to construct the model and to calculate the prediction error. \n\nIn contrast, here is an **out-of-sample prediction**, where \"new\" data is used for the data input to `mod_eval()`. The new data is called **testing** data, while the data used to construct the model is called **training** data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOOS <- mod_eval(Mod, data = sample(dag03, size=10000))\nOOS %>% \n  mutate(error = y - model_output) %>%\n  summarize(rms = sqrt(mean((error^2))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.47\n```\n:::\n:::\n\n\n::: {.callout_important icon=false}\n## Activity\n\nWhich of these is in-sample and which out-of-sample prediction error.\n:::\n\n### To be honest ...\n\nWe knew that the out-of-sample error will be bigger than the in-sample error. But that should really be \"bigger on average.\" Sometimes, just by luck the in-sample error will be bigger than the out-of-sample error.\n\nTo show \"on average,\" we need to run many trials of the insample and many trials of the out-of-sample errors. Doing this requires considerable technical skill. This is just to demonstrate what's going on. It's the conclusion, rather than the method, that we want you to understand.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrial <- function(n=100) {\n  # Get a random seed based on the fractional seconds of the clock\n  seed <- new_seed_from_time()\n  Samp <- sample(dag03, size=n, seed=seed)\n  Mod <- lm(y ~ x, data = Samp)\n  Res1 <- mod_eval(Mod, data = Samp) %>%\n    mutate(in_error = y - model_output) %>%\n    summarize(in_rms = sqrt(mean(in_error^2))) \n  Res2 <- mod_eval(Mod, data = sample(dag03, size=1000)) %>%\n    mutate(out_error = y - model_output) %>%\n    summarize(out_rms = sqrt(mean(out_error^2))) \n  bind_cols(Res1, Res2,data.frame(seed = seed))\n}\n```\n:::\n\n\nNote: We used `size=1000` for the out-of-sample prediction. We can get any amount of out-of-sample data we like from the simulation, so we used \"a lot\" in order to get more reliable estimates.\n\nNow repeat the trial many times and summarize the result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nExpt <- do(1000)* {trial(n = 50) %>%\n  mutate(perc_diff = 100*(out_rms - in_rms)/out_rms)}\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in nanotime::nanotime(Sys.time()) %>% as.character() %>% substr(21, :\nNAs introduced by coercion\n```\n:::\n\n```{.r .cell-code}\ngf_violin(perc_diff ~ 1, data = Expt)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson25_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngf_violin(out_rms ~ 1, data = Expt, color=\"blue\", alpha=0.5) %>%\n  gf_violin(in_rms ~ 2, data = Expt, color=\"red\", alpha=0.5) %>%\n  gf_refine(scale_y_log10())\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson25_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\ndf_stats(~ out_rms, data = Expt, ci.mean())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  response    lower   upper\n1  out_rms 1.249791 1.25464\n```\n:::\n\n```{.r .cell-code}\ndf_stats(~ in_rms, data = Expt, ci.mean())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  response    lower    upper\n1   in_rms 1.188371 1.203595\n```\n:::\n:::\n\n\n\n\n## Learning Checks\n\n\n\n\n\n\n\n\n\n## 25.1\n\n::: {.callout-note}\n## Solution\n\n:::\n\n--------\n\n\n## Documenting software\n\n  \n\n",
    "supporting": [
      "Math300R-Lesson25_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "fdbb5c894382e942a5775abcbd3ff37d",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 27\"\nsubtitle: \"Covariates\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"October 14, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\n## Objectives\n\n#. Read a DAG to determine which covariates to include in a model to reduce (out-of-sample) prediction error.\n\n#. Calculate amount of in-sample mean square error reduction to be expected with a useless (random) covariate. (Residual sum of squares divided by residual degrees of freedom.)\n\n\n## Reading\n\nTBD\n\n\n## Lesson\n\nWe've talked about explanatory variables and the response variable. Sometimes, we have one or a few explanatory variables that we care about, but recognize that others may be playing a role in the formation of the outcome. The explanatory variables that we **don't** care about are called covariates. A covariate is nothing more than an explanatory variable in which we don't have a direct interest.\n\nToday's lesson is about whether using covariates can change the prediction error, either for better (a smaller prediction error) or for worse (a bigger prediction error).\n\nTo illustrate, consider `dag04` in which multiple variables contribute to an outcome:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_draw(dag04)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson27_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIt might seem evident that, to predict `d`, using `a`, `b`, and `c` as explantory variables will produce narrower prediction intervals than using just one or two of the variables. We can confirm this intuition---we'll do it with out-of-sample RMS error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTraining <- sample(dag04, size=500)\nmod1 <- lm(d ~ b, data = Training)\nmod2 <- lm(d ~ a + b + c, data = Training)\nTesting <- sample(dag04, size=1000)\nmod_eval(mod1, data = Testing) %>%\n  summarize(rms = sqrt(mean((d - model_output)^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1  1.70\n```\n:::\n\n```{.r .cell-code}\nmod_eval(mod2, data = Testing) %>%\n  summarize(rms = sqrt(mean((d - model_output)^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n    rms\n  <dbl>\n1 0.961\n```\n:::\n:::\n\n\nUsing the covariates reduces prediction error.\n\n::: {.callout-note icon=false}\n## Automating model comparison\n\nRather than having to go through the same commands over and over again, let's write a function that will compare the prediction error of different models. `compare_rms_error()` is currently defined in the `_startup.R` file.\n\n\n\nHow about in a situation like `dag05`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_draw(dag05)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson27_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_rms_error(dag05, n=500, d ~ c, d ~ b, d ~ a, d ~ a + b + c, in_sample=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9964960 1.3613627 1.7447563 0.9948077\n```\n:::\n:::\n\n\n::: {.callout-note icon=false}\n## Discussion\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_draw(dag06)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson27_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n1. In `dag06`, which are the best explanatory variables for predicting `d`?\n\n2. Can `d` and `b` help in predicting `a`?\n\n:::\n\n\nDo these principles hold for in-sample prediction error?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_rms_error(dag05, n=500, d ~ c, d ~ b, d ~ a, d ~ a + b + c, in_sample=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9906537 1.4092149 1.6916262 0.9912996\n```\n:::\n:::\n\n## Learning Challenges\n\n::: {.callout-note icon=false}\n## LX 27.1\n\nConsider `dag01`, which shows a simple causal relationship between two variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_draw(dag01)\n```\n\n::: {.cell-output-display}\n![](Math300R-Lesson27_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nSo far as the size of prediction error is concerned, does it matter whether `x` is used to predict `y` or vice versa? Show the models and the results you use to come to your conclusion.\n:::\n\n\n\n\n## Documenting software\n\n  \n\n",
    "supporting": [
      "Math300R-Lesson27_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
---
title: "Student notes lesson 23"
editor: visual
---

\## Margin of error

\`\`\`{r}

one_trial \<- function(n=2) {

vals \<- rnorm(n)

tibble(m = mean(vals), s = sd(vals))

}

\`\`\`

The confidence interval from each trial will be \$m \\pm \\beta s\$, where \$\\beta\$ is a number yet to be determined. How to do so, we want to select \$\\beta\$ so that, across all trials, 95% will include the mean of the distribution from which the data values were drawn.

\`\`\`{r}

\# vary beta until 95% of the trials have a left value smaller than zero.

n \<- 10000

beta \<- 0.02

Trials \<- do(1000) \* one_trial(n=n) %\>%

mutate(left = m - beta\*s, right = m + beta\*s)

Trials %\>%

summarize(coverage = sum(sign(left\*right) \< 0)/n())

\`\`\`

For sample size \$n=10\$, \$\\beta\$ needs to be 0.72, while for \$n=100\$, \$\\beta\$ needs to be 0.20. For \$n=1000\$, the multiplier needs to be 0.062, and so on. For \$n=10000\$, the multiplier needs to be 0.02

n \| \$\\beta\$ \| \$t = \\beta / \\sqrt{\\strut n}\$

\-\--\|\-\-\-\-\-\-\-\--\|\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

10 \| 0.72 \| 2.26

15 \| 0.55 \| 2.14

20 \| 0.47 \| 2.09

50 \| 0.28 \| 2.01

100 \| 0.20 \| 1.98

500 \| 0.088 \| 1.96

1000 \| 0.062 \| 1.96

10000 \| 0.20 \| 1.96

Notice that as \$n\$ gets bigger, the size of \$\\beta\$ to cover 95% of the trials gets smaller. More than a century ago, it was known that the multiplier for any sample size \$n\$ is effectively \$2/\\sqrt{n}\$. Consequently, the confidence interval for the mean of \$n\$ values is approximately

\$\$\\mathtt{CI} = \\mathtt{mean(x)}\\pm \\underbrace{\\frac{2}{\\sqrt{n}} \\mathtt{sd(x)}}\_\\text{margin of error}\$\$

The quantity following the \$\\pm\$ is called the "\*\*margin of error\*\*." Because of the \$\\pm\$, the overall length of the confidence interval is twice the margin of error.

It is much easier to remember \$2/\\sqrt{n}\$ than a list of \$\\beta\$ values that change from one \$n\$ to the next. Another ubiquitous memory aid involves another technical term, the \*\*standard error\*\*. This involves a simple re-arrangement of the equation for the confidence interval:

\$\$\\mathtt{CI} = \\mathtt{mean(x)}\\pm 2\\underbrace{\\frac{\\mathtt{sd(x)}}{\\sqrt{n}}} \_\\text{standard error}\$\$

It's standard in statistical software to report the standard error of a coefficient. Usually abbreviated \`se\` or \`std.error\` or something similar. The software is doing the divide-by-\$\\sqrt{n}\$ for you, so all you need to construct the margin of error is multiply the standard error by 2. That's convenient, but it comes at the cost of yet another use of the words "standard" and "error," which can be confusing.

Here's an example of a typical software output summarizing a model in the format called a "\*\*regression report\*\*." Here's an example, looking at the fuel economy of cars (\`mpg\`) as a function of the car's weight (\`wt\`) and horsepower (\`hp\`).

\`\`\`{r}

lm(mpg \~ wt + hp, data = mtcars) %\>%

regression_summary()

\`\`\`

According to this report, each additional 1000 lbs of weight decreases fuel economy by an estimated 3.9 miles per gallon. But since the model is based on a sample of data, it's important to report the \*precision\* of that number in the face of sampling variation. The confidence interval is the standard format for that precision. It will be the estimate plus-or-minus two times the standard error, that is: \$-3.88 \\pm 2\\times0.633\$, that is, -5.15 to -2.61 mpg per 1000 lbs. Similarly, each addition horsepower (\`hp\`) lowers fuel economy by \$-0.032 \\pm 2 \\times 0.009\$, that is, -0.05 to 0.013 mpg per horsepower.

Even more convenient is to calculate the confidence interval with \`confint()\` which handles all the computations, including the ones for tiny \$n\$ described in \@sec-tiny-n.

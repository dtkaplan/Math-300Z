[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lessons in Statistical Thinking",
    "section": "",
    "text": "Note to students in Math 300\n\n\n\nUp to now, you have been using the OpenIntro textbook. We will not be continuing into Block III of OpenIntro, but will replace it with the lessons in this little book.\nMany of the topics in OpenIntro Block III are covered in the following chapters. But they are introduced in a fundamentally different way with a fundamentally different orientation. OpenIntro Block III is entitled “Statistical inference with infer” and shows how to compute various traditional statistical summaries. Those statistical summaries were developed during a specific era, roughly 1900 to 1950, and oriented toward the interpretation of bench-top lab experiments with a handful of observations. The purpose of the summaries was to indicate whether the experiment collected enough data to draw a definite conclusion and, later, as a bit of quality control for scientific journals.\nBecause of the orientation to laboratory experiments, the statistical summaries never had to deal with the common settings faced by today’s data scientists. Today, it is common for data to be collected in large masses from observations rather than experiments. The analysis of data is often done for utterly different purposes. One common purpose is “prediction,” which might be as simple as the uses of medical screening tests or as breathtaking as machine-learning techniques of “artificial intelligence.” Another important purpose of data analysis is to understand possible causal connections between variables.\nThe work of today’s data scientists is often to discover novel connections and to guide decision-making. That is far cry from the analysis of small, laboratory experiments.\nIt turns out that some of the methods designed for the interpretation of experimental data are also useful in data science. But some of them are not so useful, such as classical “hypothesis testing.” And every statistics book devoted to the traditional methods carries the warnings, “Correlation is not causation,” and “No causation without experimentation.” But in today’s world, such a dogmatic attitude toward establishing causal connections does not reflect modern developments in statistical methods which have been designed to meet the broader needs of guiding decision-making and intervention in the world.\n\n\nInstead of focusing exclusively on statistical inference, we are going to work with a broader idea called “statistical thinking.” Statistical inference is a small part of statistical thinking, and hardly the most important part. Indeed, many statisticians and statistically-savvy scientists believe that statistical inference can be harmful and misleading. We will discuss the good reasons behind this belief in Lesson 38. If you can’t wait, take a look at this article in the prestigious science journal Nature. Figure 1 reproduces a cartoon from that article that puts the shortcomings of “statistical significance” in a historical context.\n\n\n\n\n\nFigure 1: A cartoon published along with an article in Nature, “Retire statistical significance”, showing this once-respected idea being relegated to the graveyard for outdated and misleading “scientific” concepts such as phlogiston and aether.\n\n\n\n\n\n\nOver the next dozen lessons, you are going to be learning a way of thinking that is historically novel, unfamiliar to most otherwise well-educated people, and incredibly useful for making sense of the world and what data can tell us about the world. Learning a new way of thinking is genuinely hard. One reason is that you will have to suspend some of the familiar, go-to concepts that you’ve learned in school or through your reading.\nTo get you started with statistical thinking, it will help to have a concise definition of “statistical thinking.” Here’s one I like:\n\nStatistic thinking is the explanation or description of measured variation in the context of what remains unexplained or undescribed.\n\nImplicit in this definition is a pathway for learning to think statistically: first, you need to learn how to use data to describe variation; second, you need to know how to measure “what remains undescribed” and to use that as a context for interpretation; third, you’ll need to understand how “explanation” differs from “description.” The lessons that follow will take you down this path.\n\n\n\nUp to now, you’ve studied three main topics using the OpenIntro textbooks.\n\nLessons 2-4 covered making graphics (with ggplot()) and drew on Chapter 2 of OpenIntro.\nLessons 5-8 covered data wrangling (with mutate(), inner_join() and its cousins, group_by() summarize(), filter(), arrange() and so on. These lessons drew on Chapters 3 & 4.\nLessons 11-17 covered regression and related topics. There was a huge amount of material in these lessons, covered in Chapters 5 & 6, including\n\nexploring and summarizing data with graphics and “statistics” such as the mean, median, proportion, and standard deviation.\nbuilding models of data using the lm() function. Such models are called “regression models” for historical reasons. (The story is fascinating because it comes from a blunder in interpretation of the models, a blunder that is still regrettably common today.)\ninterpreting models using residuals and coefficients.\nthe distinction between “correlation” and “causation.”\n\n\nChapters 7 through 10 of OpenIntro cover a topic known broadly as “statistical inference.” If you have studied statistics previously—say in an advanced high-school course—you have learned some of the term of statistical inference, such as “confidence interval,” “p-value,” “statistical significance”, and the “t-test.”"
  },
  {
    "objectID": "Reading-notes-lesson-19.html",
    "href": "Reading-notes-lesson-19.html",
    "title": "1  Preliminaries to Statistical thinking",
    "section": "",
    "text": "This lesson introduces a few basic tools that you will be using throughout the remaining lessons.\nSince this is an introductory course, we will treat only categorical response variables that have two levels, for instance, Alive/Dead, Promoted/Not, Win/Loss, and so on. We will call these types of categorical response variables as “binomial” variables (that is, bi (two) nomial (names)) or “yes/no” variables, or zero/one variables. All of these terms refer to the same idea: a categorical variable with two levels.\nStatistical techniques for handling categorical response variables with three or many more levels require more book-keeping and more intricate computer programming. The models used by the machine-learning community are called “classifiers” rather than “regression models.” But limiting ourselves in this course to binomial response variables means that classifiers are indeed regression models."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#statistical-thinking",
    "href": "Reading-notes-lesson-19.html#statistical-thinking",
    "title": "1  Statistical thinking",
    "section": "1.1 Statistical thinking",
    "text": "1.1 Statistical thinking\nOver the next dozen lessons, you are going to be learning a way of thinking that is historically novel, unfamiliar to most otherwise well-educated people, and incredibly useful for making sense of the world and what data can tell us about the world. Learning a new way of thinking is genuinely hard. One reason is that you will have to suspend some of the familiar, go-to concepts that you’ve learned in school or through your reading.\nTo get you started with statistical thinking, it will help to have a concise definition of “statistical thinking.” Here’s one I like:\n\nStatistic thinking is the explanation or description of measured variation in the context of what remains unexplained or undescribed.\n\nImplicit in this definition is a pathway for learning to think statistically: first, you need to learn how to use data to describe variation; second, you need to know how to measure “what remains undescribed” and to use that as a context for interpretation; third, you’ll need to understand how “explanation” differs from “description.” The lessons that follow will take you down this path.\nThis lesson covers a few basic tools that you will be using throughout the remaining lessons.\n\nA standard, unified format for data graphics that simplifies both the construction and the interpretation of graphics and permits layers of descriptions to be laid on top of a data layer.\nThe presentation of descriptions using intervals rather than a number like the mean or proportion.\nA modern mode of displaying one type of description—the “density” (also called the “distribution”) of data—that is compatible with the unified data-graphics format.\nHow extend regression modeling, which in Chapters 11-17 of OpenIntro always required a quantitative response variable, to be useful for modeling categorical response variables.\n\nSince this is an introductory course, we will treat only categorical response variables that have two levels, for instance, Alive/Dead, Promoted/Not, Win/Loss, and so on. We will call these types of categorical response variables as “binomial” variables (that is, bi (two) nomial (names)) or “yes/no” variables, or zero/one variables. All of these terms refer to the same idea: a categorical variable with two levels.\nStatistical techniques for handling categorical response variables with three or many more levels require more book-keeping and more intricate computer programming. The models used by the machine-learning community are called “classifiers” rather than “regression models.” But limiting ourselves in this course to binomial response variables means that classifiers are indeed regression models."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#unified-format-for-data-graphics",
    "href": "Reading-notes-lesson-19.html#unified-format-for-data-graphics",
    "title": "1  Preliminaries to Statistical thinking",
    "section": "1.1 Unified format for data graphics",
    "text": "1.1 Unified format for data graphics\nThe core descriptive technique we will be using is based on regression models. And, as you know, a key paradigm for building regression models is the choice of a response variable and the choice of one or more explanatory variables. (Actually, the previous sentence would be more complete if it said, “the choice of zero or more explanatory variables. You’ll see why a zero explanatory variable model is a useful concept as we move through the rest of the course. It is one of the main ways of”establishing context” for “what remains unexplained or undescribed. But we will cross that bridge when we come to it.)\nSince our descriptions will be grounded in regression models, and since we want to be able to generate graphics that show in different layers in the same graphics frame both the raw data and the description, it makes sense to structure data graphics so that there is a response variable displayed as well as one or more explanatory variables. Following convention, we will always display the response variable on the vertical (y) axis, and an explanatory variable on the horizontal (x) axis. If there are other explanatory variables to be displayed, we will use color and faceting.\nAnother aspect of our unified data graphic format is that it will always be a point plot or, closely related, a jitter plot.\nTo illustrate the construction of standard-format data graphics, consider the mosaicData::Gestation data frame. You can read about this data frame with the R command ?Gestation. Suppose we want to address the question, “Do experienced mothers have systematically different gestation periods than inexperienced mothers?” For this question, an appropriate response variable is the length of gestation. The explanatory variable needs to measure “experience,” which is a vague idea. We will make it concrete by taking it to mean the number of the mother’s pregnancies prior to the one reported in the data. This is the variable parity and ranges from zero to thirteen.\nNow that we know the response and explanatory variable, we can generate the data graphic simply enough:\n\nGestation %>% ggplot(aes(x=parity, y=gestation)) + geom_point() \n\nWarning: Removed 13 rows containing missing values (geom_point).\n\n\n\n\n\nThis graph tells you some things at a glance. A typical gestation period is about 275 days, that is, about 9 months. And you can see that it’s much more common to have a low parity than a high one. But perhaps there is some overplotting that’s hiding the number of low-parity cases. We can easily resolve this by using geom_jitter(), perhaps with some transparency. At the same time, noting that there are very few cases with, say, parity greater than 5, we will focus on the part of the data with parity of zero to five:\n\nGestation %>% \n  filter(parity <= 5) %>%\n  #mutate(parity = as.character(parity)) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) \n\n\n\n\nFigure 1.1: A jitter plot showing gestational period for pregnancies where the mother had five or fewer previous pregnancies. The width=0.2 controls the amount of horizontal jittering. We chose it to make the columns of data clear. Also, there’s no need to jitter in the vertical direction, so we set height=0"
  },
  {
    "objectID": "Reading-notes-lesson-19.html#displaying-density",
    "href": "Reading-notes-lesson-19.html#displaying-density",
    "title": "1  Preliminaries to Statistical thinking",
    "section": "1.2 Displaying density",
    "text": "1.2 Displaying density\nIt is easy to see a pattern in Figure 1.1: It looks like mothers with high parity tend to have gestation periods that are more reliably close to 280 days than for mothers with low parity. Or, maybe this pattern is an illusion. There are so few pregnancies with parity 3, 4, or 5 that we don’t expect to see as many uncommonly short or long gestational periods as for the parities with lots of cases.\nOne way to explore this idea is to plot the density of the dots as a function of gestation for each of the parity levels individually. A “violin” layer will make it easier to compare the distributions in the different columns, despite the unevenness in the case count. Figure 1.2 gives an example.\n\nGestation %>% \n  filter(parity <= 5) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) +\n  geom_violin(aes(group=parity), fill=\"blue\", alpha=0.2, color=NA)\n\nWarning: Removed 11 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 11 rows containing missing values (geom_point).\n\n\n\n\n\nFigure 1.2: A violin plot. The long axis of the violin-like shape is oriented along the response-variable axis (that is, the vertical axis in our standard format). The width of the violin for each possible value of the response variable is proportional to the density of data near that value.\n\n\n\n\nThe violin plot is a more flexible display of the distribution of gestation period that would be a histogram. The histogram has all those bars that clutter up the display. Even worse, one of the axes in the frame of a histogram plot is “count” or maybe “density.” Such a frame is not consistent with the unified response/explanatory format we will be using. The violin is drawn in the no-mans-land between the different levels of parity, just as the jittering moves data away from a single vertical line into that same no-mans-land.\nThis idea of using the graphical no-mans-land between levels of a categorical explanatory variable is not new. You encountered it earlier when you drew box plots. ?fig-density-box adds a box-plot annotation layer on top of the violin-plot layer.\n\nGestation %>% \n  filter(parity <= 5) %>%\n  ggplot(aes(x=parity, y=gestation)) + \n  geom_jitter(alpha=0.2, width=0.2, height=0) +\n  geom_violin(aes(group=parity), fill=\"blue\", alpha=0.2, color=NA) +\n  geom_boxplot(aes(group=parity), color=\"blue\", fill=NA, alpha=.5)\n\n\n\n\nFigure 1.3: A box and whisker plot uses the no-mans-land between levels of a categorical explanatory variable.\n\n\n\n\n:::: {.callout-note} ## Violins versus boxes\nAll of the graphical statistical annotations are human inventions. Each invention attempts to meet a need, but usually the invention is a compromise between the statistical objective and the computational and graphical resources available. The box plot format is a case in point. The statistical goal of a box plot is to display the distribution of values of a variable. It was invented in a time when graphics were mostly drawn by hand and computers were not widely available. The computations behind a box plot produce a five-number summary: min, first quartile, median, third quartile, max. It’s straightforward (but tedious!) to do these by hand since they are based on sorting and counting.The drawing itself uses only straight lines, which are easy to draw by hand with only a pencil and a straightedge.\nA violin plot requires hundreds or thousands of evaluations of the gaussian function along with post-processing. They are not feasible for a human; a computer is required. Similarly, drawing the detailed shape of the violin (Figure Figure 1.2) requires a computer.\nThe box plot has important deficiencies. It is appropriate only for uni-modal distributions and doesn’t give even a hint of possible bi-modality. The sharp boundaries of the box and endpoints of the whiskers suggest that even smooth density shapes have abrupt transitions. Points are marked as “outliers” in order to keep the whiskers from becoming absurdly long, but box-plots of data with a normal (gaussian) distribution will produce such “outliers” whenever the sample size is large.\nWhen it comes to computing power, we are today unimaginably rich compared to the generation that introduced box plots. In a sense, we are so rich we can use expensive, well made products such as a violin. The box-plot generation was living in computational poverty. Not having the (computational) funds to buy a violin, they had to make do with primitive instruments they had to make do with the materials at hand, just as early blues mucisians, coming out of poverty, often had to build instruments such as a cigar-box guitar.\n\n\n\n\n\nCigar box guitar\n\n\nFigure 1.4: A cigar-box guitar."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#describing-with-intervals",
    "href": "Reading-notes-lesson-19.html#describing-with-intervals",
    "title": "1  Preliminaries to Statistical thinking",
    "section": "1.3 Describing with intervals",
    "text": "1.3 Describing with intervals\nStatistical thinking often involves quantifying uncertainty. One manifestation of this is moving away from single-number “point” summaries such as the mean or median to “interval” summaries. As you will see as we progress through the future lessons, there are many kinds of such intervals, each of which is designed to deal to address a specific question. So you’ll see prediction intervals, confidence intervals, confidence bands, and so on.\nFor this lesson, we’re concerned only with what interval summaries look like. So let’s generate some, without worrying yet about the computer commands and mathematical underpinnings involved.\n\n\n\n\n\n\nGround rules for “demonstrations”\n\n\n\nThere will be many occasions in these lessons where we want you illustrate a statistical technique or phenomenon, but we don’t expect the reader to master the commands involved. We will call these demonstrations: something we don’t expect you to do at home. A good way to think about these demonstrations is that you should focus on the outputs from the calculations, rather than the calculation steps themselves. We’ll show you the calculations since some readers might be interested, but focus your attention on the output.\n\n\n\n\n\n\n\n\nDemonstration: Food at Starbucks\n\n\n\nStarbucks is a famous coffee-shop franchise, with more than 30,000 branches (as of 2021) across the world. People go to Starbucks for the coffee, but they often buy something to eat as well. Let’s look at the calorie content of Starbucks’ food offerings. As always, when conducting a statistical analysis, it’s helpful to have in mind the purpose for the task. We’ll imagine, tongue in cheek, that we want to make food recommendations for the calorie conscious consumer.\nFirst, a point summary of the calories in the different types of food products available at Starbucks:\n\npoint_summary <- df_stats(calories ~ type, data = openintro::starbucks, mean)\npoint_summary\n\n  response          type     mean\n1 calories        bakery 368.7805\n2 calories    bistro box 377.5000\n3 calories hot breakfast 325.0000\n4 calories       parfait 300.0000\n5 calories        petite 177.7778\n6 calories         salad  80.0000\n7 calories      sandwich 395.7143\n\n\nThis summary supports the common-sense advise that to avoid calories, focus your choices on salads or on smaller portions (type “petite”). You might be tempted to go further, for example concluding that a sandwich is a bad choice (in terms of calorie content) so lean toward parfaits or hot breakfasts. You can even imagine someone concluding from this summary that a bistro box is a better calorie-conscious choice than a sandwich.\nA graphic showing both the point summary and the raw data can put things in a useful context.\n\nopenintro::starbucks %>% \n  ggplot(aes(x=type, y=calories)) +\n  geom_jitter(width=0.2, alpha=0.5) +\n  geom_errorbar(data=point_summary, aes(ymin=mean, ymax=mean), y=NA, color=\"blue\") +\n  geom_point(data=point_summary, aes(y=mean), color=\"red\")\n\n\n\n\nWe’ve shown the point summary as red dots, one for each food type. A somewhat stronger visual impression is given by drawing the point summary not as points, but as lines that extend into the no-mans-land between food types. These are drawn in blue and they make the red dots superfluous; you don’t need both.\nPlotting the point summary in the context of the raw data shows at a glance that the point summary is not of any use beyond the common sense advice to eat salads and small portions if you are trying to avoid calories. With the point summary on its own, we were tempted to conclude that, say, hot breakfasts are a better choice than sandwiches, but the data display suggests otherwise; there’s just one low-calorie breakfast. The others are much like sandwiches.\nA point summary is compact, but it fails to take into account the variation within each food type.\nAn interval summary does take into account this variation. This is an important aspect of statistical thinking. Recall the definition of statistical thinking given earlier:\n\nThe explanation or description of measured variation in the context of what remains unexplained or undescribed.\n\nThere are several kinds of interval summaries. You’re not yet in a position to know which kind is the appropriate one for the task at hand—giving advice about food choices based on food type—so we’ll tell you: a prediction interval.\n\ninterval_summary <- df_stats(calories ~ type, data = openintro::starbucks, coverage(.95))\ninterval_summary\n\n  response          type  lower  upper\n1 calories        bakery 130.00 490.00\n2 calories    bistro box 284.00 469.50\n3 calories hot breakfast 166.25 473.75\n4 calories       parfait 300.00 300.00\n5 calories        petite 162.00 190.00\n6 calories         salad  80.00  80.00\n7 calories      sandwich 351.50 454.00\n\n\n\n\nOr in graphical form:\n\nopenintro::starbucks %>% \n  ggplot(aes(x=type, y=calories)) +\n  geom_jitter(width=0.2, alpha=0.5, height=0) +\n  geom_errorbar(data=interval_summary, aes(ymin=lower, ymax=upper), y=NA, color=\"blue\") \n\n\n\n\nUnlike point summaries, interval summaries can overlap. Such overlap is an indication that the groups being summarized are not all that different. Here, an appropriate conclusion indicated by the interval summary is, “Don’t make your diet choices based on food type. Look at the calorie content of individual items before making your choice.”\nAdmittedly, in this simple setting the data themselves would lead to the conclusion. But as we move into more complicated settings, it will become infeasible to quickly see patterns straight from the data. In these complicated settings, summaries are an important tool for displaying and quantifying patterns. The statistical thinker knows to prefer interval summaries."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#categorical-response-variables.",
    "href": "Reading-notes-lesson-19.html#categorical-response-variables.",
    "title": "1  Preliminaries to Statistical thinking",
    "section": "1.4 Categorical response variables.",
    "text": "1.4 Categorical response variables.\nOur last topic in this lesson is relatively simple: the zero-one transformation of categorical variables which allows regression and related techniques to be used for categorical response variables.\nTo illustrate, let’s use data collected in the 1970s to study the relationship between smoking and mortality. The data we’ll use, mosaicData::Whickham, recorded for one-thousand nurses whether or not they smoked at the time of the initial interview and whether or not they were still alive twenty years after the initial interview.\nHere’s a graph of the data in our standard response-vs-explanatory graphic frame:\n\nWhickham %>%\n  ggplot(aes(x = smoker, y = outcome)) +\n  geom_jitter(width=0.2, height=0.2, alpha=0.5)\n\n\n\n\nThe graph suggests that non-smokers were more likely than smokers to be dead at the follow-up interview. But it’s hard to calculate proportions from such a graph. It’s reasonable to argue that for the purpose of showing the fraction of smokers and of non-smokers who died, a bar chart would be better.\n\nWhickham %>%\n  ggplot(aes(x=smoker, fill=outcome)) +\n  geom_bar()\nWhickham %>%\n  ggplot(aes(x=smoker, fill=outcome)) +\n  geom_bar(position = \"fill\") +\n  ylab(\"Proportions\")\n\n\n\n\n\n\n\n(a) counts\n\n\n\n\n\n\n\n(b) proportions\n\n\n\n\nFigure 1.5: Barplots of the Whickham data.\n\n\n\nThe left barplot, showing counts, suggests that a higher proportion of non-smokers died than of smokers. But its easy to instruct the geom_bar() to graph proportions rather than counts, as done in the left plot. This makes it easy to conclude at a glance that a higher proportion of non-smokers have died.\nThe important question here, “Does smoking affect mortality?” translates well into the response/explanatory paradigm: outcome is the response variable while smoker is the explanatory variable. In the jitter-plot presentation of the data, these assignments are clearly indicated in the computer commands, which set x=smoker, y=outcome. In the barplot, a different notation is used: x=smoker, fill=outcome.\nUnfortunately, neither of the graphic styles—jitter or boxplot—answers the important question. At best they provide a description of the nurses in the Whickham data frame.\nTo answer the important question, we need to invoke statistical thinking. In particular, we need an interval summary of the proportion who died, not the point summary produced by the barplot.\nThis doesn’t mean that we can’t easily calculate the proportions from the categorical response variable: we just have to use the right commands. for instance:\n\nWhickham %>%\n  df_stats(outcome ~ smoker, prop, ci.prop)\n\n  response smoker prop_Alive     lower    center     upper\n1  outcome     No  0.6857923 0.6507824 0.6857923 0.7192969\n2  outcome    Yes  0.7611684 0.7243939 0.7611684 0.7952677\n\n\nThe point summary—the prop_Alive column—suggests an obvious difference between the smokers and non-smokers. The interval summary—columns lower and upper—tempers this conclusion a little: the intervals almost touch.\nAlthough regression is our go-to technique for modeling relationships between variables, we can’t use it directly on a categorical response variable. Here’s what happens if we try:\n\nlm(outcome ~ smoker, data = Whickham) %>% confint()\n\nWarning in model.response(mf, \"numeric\"): using type = \"numeric\" with a factor\nresponse will be ignored\n\n\nWarning in Ops.factor(y, z$residuals): '-' not meaningful for factors\n\n\nWarning in Ops.factor(r, 2): '^' not meaningful for factors\n\n\n            2.5 % 97.5 %\n(Intercept)    NA     NA\nsmokerYes      NA     NA\n\n\nThe computer’s warning message is a reminder that the response variable is categorical. (The message uses the phrase “factor response,” which is just computerese for “categorical response.”)\nWe can fix things with a simple trick: trasforming the response variable to a zero-one encoding. In the following, we’ll use 1 to represent \"Alive\" and 0 to represent \"Dead\", although we can equally well do things the other way around.\n\nlm(zero_one(outcome, one=\"Alive\") ~ smoker, data = Whickham) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) 0.65329520 0.7182895\nsmokerYes   0.02654662 0.1242054\n\n\nYou don’t yet know enough to interpret this interval summary. That will have to wait until Lesson 24. The significant1 feature of the interval on smokerYes is that it does not include zero. In everyday terms, the interval means, “Smokers are 3 to 12 percentage points more likely to survive for 20 years than are non-smokers.”\nUsing interval summaries instead of point summaries is an important aspect of statistical thinking, but there are other aspects that need to be taken into account. A simple, but important, question is whether the nurses recorded in the Whickham data frame are good representatives of all smokers. (It turns out that the nurses in Whickham are all women interviewed in the 1970s. At that moment of history, women were very different than men when it comes to smoking, and the Whickham smokers were also very different from today’s female smokers. We’ll say more about this in the demonstration below.)\nStatistical thinking also leads one to ask another sort of question: What else might be going on other than smoking? In technical language, the other-goings-on are called “covariates,” the topic of Lessons 28 & 29.\nFor instance, you might wonder about the overall result from our brief examination of the Whickham data. Is it really the case that the smokers were more likely to survive than the non-smokers? The answer is “yes,” as we have demonstrated from the previous analysis. But this answer is completely misleading. Tobacco companies worked hard to mislead people into thinking that smoking was not dangerous. They knew full well the negative health consequence of smoking, but they used statistical-sounding claims to hide this knowledge from the public.\nIn the following demonstration, we’ll look at the Whickham data again using the power of regression models to incorporate covariates.\n\n\n\n\n\n\nDemonstration: Smoking with covariates\n\n\n\nRemember that you are not expected to master the calculations in these demonstrations. Focus your attention on the output from the calculations.\nIt goes without saying that smoking is not the only thing that kills people. There are other risky behaviors such as heavy drinking, there’s environmental exposure to pollutants, and there’s disease (other than the smoking induced ones of lung cancer, emphasema, and high blood pressure). But there’s one risk factor for death that everyone knows about but nobody is doing anything about: getting old.\nIn virtually every public health or clinical study, the participants’ age is taken into account. Not doing so can produce a completely misleading view of the situation. This is also the case with smoking and mortality in the Whickham study.\nRegression techniques enable us to take multiple explanatory variables into account. In this demonstration, we’ll use regression to study outcome as a function of smoker and, importantly, age.\nTo get started, we need to remember to convert the categorical outcome variable into a zero-one encoding. After that, building the model is not so hard.\n\nsurvival_model <- Whickham %>% \n  mutate(survived = zero_one(outcome, one=\"Alive\")) %>%\n  model_train(survived ~ age + smoker, data=.)\n\nFrom this model, we can read off an interval summary of the effect of smoking on survival:\n\nsurvival_model %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept)  6.7686824  8.5002535\nage         -0.1382922 -0.1101260\nsmokerYes   -0.5369777  0.1238805\n\n\nA full understanding of this interval summary will need to wait until Lessons 22 through 24. For the present, we’ll simply point out that the summary interval on smokerYes includes zero, so Whickham provides no support for the mistaken conclusion that smoking improves survival. But seeing this requires taking into account age. A graphic may help explain why:\n\nModel_output <- model_eval(survival_model, interval=\"confidence\")\nModel_output %>%\n  ggplot(aes(y = survived, ymin=.lwr, ymax=.upr, x=age, color=smoker, fill=smoker)) +\n  geom_jitter(height=.1, width=0, alpha=0.2) +\n  geom_ribbon(alpha=0.2) \n\n\n\n\nThe interval summary in the graph shows how the probability of survival changes for different ages. The intervals for non-smokers and smokers entirely overlap. For both groups, 20-year survival goes down with greater initial age. So why did the model outcome ~ smoker suggest that smokers have a higher survival? The reason relates to the proportion of smokers with initial age 70+. In the 1970s, life expectancy was such that people 70+ were unlikely to survive 20 years. This pulls down the survival rate at that age. Notice that the 70+ nurses were unlikely to have been smokers compared to younger nurses. The 70+ nurses grew up in an era when social conventions caused smoking to be uncommon for women (even though it was very common for men)."
  },
  {
    "objectID": "Reading-notes-lesson-20.html",
    "href": "Reading-notes-lesson-20.html",
    "title": "2  Measuring and simulating variation",
    "section": "",
    "text": "This Lesson introduces two ideas. The first is how to measure variation. This is important, as you can see from the definition of statistical thinking given in the previous Lesson:\nVariation is what we’re trying to explain/describe. To do this, it helps to be able to measure variation.\nThe second idea is also fundamental to statistical thinking. Often, but not always, our interest in studying data is to reveal the causal connections between variables. This is important, for instance, if we are planning to make an intervention in the world and want to anticipate the consequences. Interventions are things like “increase the dose of medicine,” “stop smoking!”, “lower the budget,” “add more cargo to a plane (which will increase fuel consumption and reduce the range).”\nHistorically, statisticians were hostile to the idea of using data to explore causal relationships. The one exception was experiment, where the data come from an actual intervention in the world. (See Lesson 32.) Statistics teachers encouraged students to use phrases like “associated with” or “correlated with” and reminded them that “correlation is not causation.”\nRegretably, this attitude made statistics irrelevant to that part of the real world where intervention was the matter of interest and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists were seeing evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, “smoking is associated with lung cancer.” He speculated that smoking and lung cancer might have a common cause, perhaps genetic. To establish causation, it would be necessary to run an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result.\nFortunately, around 1960 a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. This was one of the first developments in a field called “causal inference”"
  },
  {
    "objectID": "Reading-notes-lesson-20.html#sec-size-of-variable",
    "href": "Reading-notes-lesson-20.html#sec-size-of-variable",
    "title": "2  Measuring and simulating variation",
    "section": "2.1 Measuring variation",
    "text": "2.1 Measuring variation\n\nVariation itself is nature’s only irreducible essence. Variation is the hard reality, not a set of imperfect measures for a central tendency. Means and medians are the abstractions. —– Stephen Jay Gould (1941- 2002), paleontologist and historian of science\n\nA common task in statistical modeling is to break down a variable into components. For instance, a person’s height or intelligence or charm is presumably a combination of genetics and environment. In doing this breaking down, it’s convenient to be able to characterize the size of each component.\nThere are many possible ways to measure “size.” In this course, we will emphasize two, intimately related measures:\n\nvariance\nstandard deviation, which is simply the square root of variance.\n\nThe OpenIntro text introduced the standard deviation in Chapter 3 where it was described as a measure of “spread.” In Chapter 6, OpenIntro introduced the variance as the square of the variance. All this is right, so far as it goes, but it dramatically understates the importance of the two measures. These measures are as important to statistical thinking as the Pythagorean Theorem is to geometry.\nYou remember the Pythagorean Theorem: \\(A^2 + B^2 = C^2\\), where \\(C\\) is the length of the hypothenuse of a right triangle, and \\(A\\) and \\(B\\) are the lengths of the other two sides of the triangle. Surprisingly, the Pythagorean Theorem is highly relevant to statistical models.\nRecall from OpenIntro chapters 5 & 6 that the linear modeling technique produces two columns of numbers: the fitted values and the residuals. These columns have the same number of rows as the data frame used for training. The fitted values are the output from the model when the explanatory variables from the training data are given as inputs to the model. The residuals are the row-by-row numerical difference betwen the response variable and the fitted values.\nThese three columns of numbers—the response variable, the fitted model values, and the residuals—are exactly analogous to the three sides of a right triangle. (This is not an obvious fact, but it is an important one to keep in mind.) In particular, the following numerical relationship is as true for linear models as it is for triangles:\n\\[\\text{sd(fitted)}^2 + \\text{sd(residuals)}^2 = \\text{sd(response)}^2\\] where sd() refers to the standard deviation. Consequently, sd()^2 is the variance.\nThe variance and the standard deviation are defined mathematically in a special way that makes the Pythagorean relationship always describe models constructed the the lm() technique, that is “least squares” models.\nThe antique name “standard deviation” hardly hints at this, largely because it was invented before the least squares technique was invented. For thinking about data, it can be helpful to have in mind a modernization of “standard deviation.”\nStep 1 in the modernization is to make clear what “standard” means: $$ =   \n\n $$\nStep 2 in the modernization replaces the archaic word “deviation” with something more descriptive:\n\\[\\text{standard deviation} = \\text{accepted}\\ \\ \\textit{measure of} \\\n\\ \\text{variation in the variable}\\]\nWe won’t explain here why the standard deviation became the go-to, accepted, standard measure of variation, but it did and for excellent reasons.\n:::\nBoth variance and standard deviation are quantities, that is, a single number with associated units. The standard deviation of any variable has units that are exactly the same as the variable itself. For instance, the measured heights of a group of people is often measured in cm. So the units of the standard deviation of height will also be in cm.\nIn contrast, the variance, being the square of standard deviation, has units of the square of the units of the variable. The variance of height, for instance, will be measured in cm2. This will seem odd at first glance, but you have to get used to it: the variance of a variable has units that are the square of the units of the variable itself.\nKeep in mind also that variance and standard deviation are summaries of a variable. A variable in a data frame consists of multiple values, one for each row of the data frame. The variance or standard deviation of that variable will be just a single number (and units), summarizing all of the values in the variable.\n::: {.callout-note} ## Calculating variance\nAlmost always, people use software to do the calculations. The relevant R functions are sd() and var(). You can use these functions in a summarize() statement, for instance\n\nmtcars %>%\n  summarize(v = var(hp))\n\n         v\n1 4700.867\n\nmtcars %>% \n  summarize(s = sd(hp))\n\n         s\n1 68.56287\n\n\nRegrettably, the software does not indicate the units of the quantity. For that, you need to determine the units of the variable itself, typically by reading the documentation for the data frame.\nTo understand what is being calculated by var(), we will describe an algorithm. There are more efficient algorithms than the one described here, but this one is easy to understand.\nStarting material:\n\nA single column of numbers creating by pulling out from the data frame the variable whose variance is to be calculated.\nA long roll of paper on which you can write numbers, one after the other.\n\nBasic calculation: You are going to repeat a calculation for each and every row in the column of numbers. To illustrate, suppose you are doing the calculation for the kth row. Take the data value from the kth row, and call it the “reference value.” Then subtract the reference value from each and every other value in the column and square the results. Write those numbers, all of them, on the roll.\nUsing the same roll of paper for all, carry out the basic calculation starting at each of the rows in the single column of data. Now your roll of paper has many numbers on each, each of which is the square difference between the values from two rows of the table. If you are mathematically inclined, you might like to know that there will be exactly \\(n(n-1)\\) numbers written on the roll. If you are a statistical instructor, your ears might perk up when you notice the \\(n-1\\) in that count.\nThe final result—the variance—will be the mean of the numbers on the roll. Since each of the numbers on the roll is the square difference between two values of the variable, the mean will be the average square difference.\n\n\n\n\n\n\nFor the statistically experienced reader …\n\n\n\nWarning! This box contains mathematical formulas that are not needed for the course. The formulas might be interested to mathematically inclined statistics instructors. If that’s not you, skip this material.\nI realize that the algorithm described above is probably not used by any statistical software package. It’s really inefficient numerically.\nYou can see this by comparing the traditional formula for the variance to the formula version of the above algorithm:\n\\[\\underbrace{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}_\\text{traditional} \\ \\ \\text{versus} \\ \\ \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\left[\\frac{1}{n-1}\\sum_{j\\neq i} (x_i - x_j)^2\\right]}_\\text{our algorithm}\\]\nThe inefficiency of the algorithm stems from the double sum. The advantage of the algorithm is conceptual and two-fold:\n\nYou can see where the \\(n-1\\) in the formula for the variance comes from: the inner sum involves \\((n-1)\\) numbers. No hand waving needed to explain the \\(n-1\\). (What might need explaining is the \\(j \\neq i\\) in the inner sum. Why not \\(\\sum_j=1^n\\)? Because that would put \\(n\\) zeros on the roll and bias the result downward. We want to average the square distance between each value and every other value.)\nThere is no need to introduce the mean \\(\\bar{x}\\) of the values. Of course, \\(\\bar{x}\\) is easy and fast to calculate so there is no numerical reason to avoid using it in the calculation. There is, however, a philosophical reason based on Stephen J. Gould’s observation, quoted at the start of this lesson: “Variation is the hard reality. Means … are the abstractions.”\n\nHere’s a traditional-minded definition: “Variance is equal to the average squared deviations from the mean.” This definition makes it seem like the mean has some special status and that variations from it are “deviations.”"
  },
  {
    "objectID": "Reading-notes-lesson-20.html#causality",
    "href": "Reading-notes-lesson-20.html#causality",
    "title": "2  Measuring and simulating variation",
    "section": "2.2 Causality",
    "text": "2.2 Causality\nThe introduction to this chapter contained a very brief mention of a causal relationship: changing the dose of a medication to, say, lower a patient’s blood pressure. Assuming that the drug is effective, it’s common sense that the change in dose had a causal influence on the patients’ condition. A natural belief that one thing can cause another is the entire basis for medical and other interventions.\nThe historical statisticians who insisted that data alone cannot establish a causal connection would also, nonetheless, go to the doctor for treatment. Without an experiment, professional pride would lead them to stipulate that data can only establish “correlation” or “association” and that it’s impossible to say from data what causes what. But in their everyday lives they believed that medication has a causal influence. How could they justify this belief given their professional attitudes toward causality? Because they have common sense and know something about how the world works.\nThis section is about formal ways to say “something about how the world works” that can be used, along with data, to make responsible conclusions about causal relationships."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#directed-acyclic-graphs",
    "href": "Reading-notes-lesson-20.html#directed-acyclic-graphs",
    "title": "2  Measuring and simulating variation",
    "section": "2.3 Directed acyclic graphs",
    "text": "2.3 Directed acyclic graphs\nThe title of this section is a mouthful, but the mathematical structure of a “directed acyclic graph” (DAG, for short) is one of the most popular ways for statistical thinkers to express their ideas about what might be going on in the real world. Despite the long name, DAGs are very accessible to a broad audience. You may even have constructed one without knowing the formal name.\nStatistical graphics are so common and so often used in this course, that you may think that the “graph” in DAG refers to these. Not really, even though statistical thinkers often draw pictures of DAGs. The “graph” in DAG is a mathematical term of art; a good synonym is “network.” Mathematical graphs consist of a set of “nodes” and a set of “edges” connecting the nodes. ?fig-graphs shows three different graphs, each one having five nodes labelled A, B, C, D, and E.\n\n\n\n\n\n\nFigure 2.1: undirected graph\n\n\n\n\n\n\n\nFigure 2.2: directed but cyclic\n\n\n\n\n\n\n\nFigure 2.3: directed acyclic graph\n\n\n\n\n\nThe nodes are the same in all three graphs of ?fig-graphs, but each of the graphs is different from the others. That’s because it is not just the nodes that define a graph; the edges (drawn as lines) are part of the definition as well.\nThe left-most graph in ?fig-graphs is an “undirected” graph; there is no suggestion that the edges run one way or another. In contrast, the middle graph as the same nodes and edges, but the edges are directed. A nice way to think about a directed graph is that each node is a pool of water and each directed edge shows how the water flows between pools. This analogy is also helpful in thinking about causality: the causal influence flow like water.\nLook more carefully at the middle graph. There is a couple of loops; the graph is cyclic. In one loop, water is flowing from E to C to D and back again to E. The other loop runs B, C, D, E, and back to B. Such a flow pattern could not exist unless some of the edges involved pumps to push the water back uphill.\nIn the right-most graph, some of the edges have had their direction reversed. This graph has no cycles; it is acyclic. To use the flowing and pumped water analogy, in an acyclic graph no pumps are needed. You could arrange the pools of water at different heights to create the flow through gravity. The node-D pool would be the highest, E and C a little lower. But C has to be lower than E in order for gravity to pull water along the edge from E to C. The node-B pool has to be the lowest of all so that water can flow to it from E, C, and A.\nDirected acyclic graphs are used to represent causal influences; think of “A causes B” as meaning that causal “water” flows naturally from A to B.\nIn a DAG, a node can have multiple outputs, like D and E, and it might have multiple inputs, like B and C. In terms of causality, when a node—like B—has multiple inputs it means merely that more than one factor is responsible for the value of that node. A real-world example: the rising sun causes a rooster to crow, but so can another intruder to the coop.\nOften, nodes do not have any inputs. These are called “exogenous factors”—at least by economists. The “genous” beens “originates from,” the “exo” means “outside.” The value of an exogenous node is determined by something, just not something that we are interested in (or perhaps capable of) modeling. But we can be sure that the exogenous node is not determined by any of the other nodes in the DAG. Otherwise there would need to be an arrow drawn into the node. But then it wouldn’t be exogenous."
  },
  {
    "objectID": "Reading-notes-lesson-20.html#using-dags",
    "href": "Reading-notes-lesson-20.html#using-dags",
    "title": "2  Measuring and simulating variation",
    "section": "2.4 Using DAGs",
    "text": "2.4 Using DAGs\nThe point of a DAG is to make a clear statement of a hypothesis about causation. Drawing a DAG doesn’t mean that the hypothesis is right, just that we believe the hypothesis is in some sense a possibility. Different people might have different beliefs about what causes what in real world systems. Comparing their different DAGs can help, sometimes, to discuss and resolve the disagreement.\nWe are going to use DAGs for two distinctive purposes. One purpose is to inform responsible conclusions from data about what causes what. The data on its own is not sufficient to establish what are the causal connections. But data combined with a DAG can tell us something. Sometimes a DAG includes a causal connection that should create an association between variables. Not seeing that association in the data is evidence that the hypothesis behind the DAG is wrong. DAGs are also useful for building models; they can tell us which variables to include and which to exclude from a model in order to capture the hypothetical causal connections.\nThe second purpose is for learning modeling technique. We are going to generate simulated data from various DAGs. You can see exactly what is going on the the DAG and then see whether your statistical analysis of data is capturing the known mechanism of the DAG. This is useful for learning what can go right or wrong in building a model, just as a aircraft simulator is useful for training pilots to handle real-world situations in real aircraft. The DAGs that we will use for this second purpose consist of formulas that relate the value of each variable to the values of its inputs. The value of exogenous nodes is usually set randomly, without any input from the other nodes in the DAG.\nHere’s a simple example: a DAG with two exogenous nodes (a and b) and one another node, c, that gets input from both a and b.\n\ndag_draw(dag09)\n\n\n\nprint(dag09)\n\n[[1]]\na ~ eps()\n\n[[2]]\nb ~ eps()\n\n[[3]]\nc ~ binom(2 * a + 3 * b)\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nThe dag_draw() command draws a picture of the graph. Printing the value of the dag gives the formulas that set the values of the nodes. In dag09, the nodes a and b are both set at random and independently of one another. That’s what the eps() function does: set the value at random. In contrast, the formula for node c says that the value of c will be a linear combination of the values of a and b, translated into a zero-one format.\nYou will be generating simulated data from such dags using the sample() function. For instance,\n\nsample(dag09, size=5)\n\n# A tibble: 5 × 3\n       a      b     c\n   <dbl>  <dbl> <dbl>\n1 -0.326  1.17      1\n2  0.552  0.619     0\n3 -0.675 -0.113     0\n4  0.214  0.917     1\n5  0.311 -0.223     0\n\n\nEach of the rows in the sample is one trial in which the values of the nodes have been assigned, either by random numbers for exogenous nodes, or by a formula for nodes that receive inputs.\n\n\n\n\n\n\nWARNING!! DAGs are not reality\n\n\n\nSometimes students get so used to working with DAGs that they forget they are simulations. They start to think that they can generate data from a DAG that will help to understand some aspect of the real world.\nWRONG! If you want to know about the real world, you need to collect data from the real world, not a computer simulation. Where DAGs can be useful in practice is to guide model building from real data when you have a hypothesis about the causal connections in the world.\nIn this course, we also sample from DAGs to help learn about statistical technique. But never to make claims about real-world phenomena.\n\n\nAs the name suggests, sample() collects a sample of data. Typically, you will then summarize the data, often by fitting a model to the data and then summarizing the model. To illustrate, here we generate a sample of size \\(n=10,000\\), then fit the model c ~ a + b, and summarize by taking the coefficients.\n\nsample(dag09, size=10000) %>% \n  lm(c ~ a + b, data = .) %>%\n  coefficients()\n\n(Intercept)           a           b \n  0.5064368   0.1994570   0.3013939 \n\n\nYou might notice that the coefficients on a and b are not the same as the coefficients in the dag09 formulas. That’s because the lm() technique is not adequate to reveal the coefficients.\nThe simulated data from DAGs, along with a knowledge of the actual formulas used in the simulation, will help you learn about model building.\n\n\n\n\n\n\nDemonstration: Modeling binomial variables\n\n\n\nKeep in mind that this is just a demonstration. You’re not expected to master (or even understand) the calculations done in this box.\nRecall from the printed version of dag09 that the value of node c was set by a linear combination of a and b converted into a zero-one, binomial value. The linear modeling, lm(), technique is not well tuned to work with binomial data. Instead, another technique called “generalized linear modeling” (implemented by glm) is appropriate. When we use the right technique we can, in this case, recover the coefficients in the DAG formula: 2 for a and 3 for b.\n\nsample(dag09, size=10000) %>% \n  glm(c ~ a + b, data = ., family=\"binomial\") %>%\n  coefficients()\n\n(Intercept)           a           b \n 0.03286538  1.96356232  3.10024718"
  },
  {
    "objectID": "Reading-notes-lesson-20.html#samples-summaries-and-samples-of-summaries",
    "href": "Reading-notes-lesson-20.html#samples-summaries-and-samples-of-summaries",
    "title": "2  Measuring and simulating variation",
    "section": "2.5 Samples, summaries, and samples of summaries",
    "text": "2.5 Samples, summaries, and samples of summaries\nA “sample” (for the purposes of this course) is a set of rows in a data frame. The “sample size” is the number or rows. “Sampling” is the process of collecting the data to be put into the data frame.\nA “summary of a sample” is exactly that: a summary, not the sample itself. In Chapter 3 of the OpenIntro textbook, you were introduced to a data wrangling operator, summarize() and used it to construct some summaries of data frames, that is, “summaries of a sample.” For instance, you might decide to summarize the mtcars data frame by finding the mean and standard deviation of the mpg variable. In the following command, mtcars is the sample and the summary is produced by summarize().\n\nmtcars %>% \n  summarize(m = mean(mpg), s = sd(mpg))\n\n         m        s\n1 20.09062 6.026948\n\n\nIt can be very good style for the summary to be contained within a single row. The dplyr package for data wrangling is popular because it makes this happen automatically.\nWhen we use DAGs and sometimes even with real data (see Lesson 22), we may want to see whether the summary is always the same or whether it varies from trial to trial.\nThe following command is one trial of sampling and summarizing data from dag09.\n\nsample(dag09, size=10000) %>% \n  glm(c ~ a + b, data = ., family=\"binomial\") %>%\n  coefficients()\n\n(Intercept)           a           b \n-0.01222425  1.99487924  2.93153502 \n\n\nTo generate a sample of summaries, run many trials of the summary. The do() function is handy for this. The following command runs five trials of the dag09 summary. (Note that the command for the trial is placed inside curly braces.)\n\ndo(5) * {\n  sample(dag09, size=10000) %>% \n    glm(c ~ a + b, data = ., family=\"binomial\") %>%\n    coefficients()\n}\n\n    Intercept        a        b\n1 -0.01315189 2.041331 3.049575\n2 -0.01027258 1.987569 3.074509\n3  0.02787421 1.934953 3.044001\n4 -0.03509540 1.973983 2.969853\n5 -0.09887196 1.978861 2.971170\n\n\nEach trial produces one row of the data frame. The five trials are collected together by do() into the five rows of a single data frame. Such a data frame can be considered a “sample of summaries.”\nOne of the things we will do with a “sample of summaries” is to … wait for it … summarize it. For instance, in the following code chunk, a sample of 40 summaries is stored under the name Trials. Then we will summarize Trials, in this case to see how much the the values of the a and b coefficients vary from trial to trial.\n\nTrials <- do(40) * {\n  sample(dag09, size=10000) %>% \n    glm(c ~ a + b, data = ., family=\"binomial\") %>%\n    coefficients()\n} \nTrials %>% \n  summarize(mean_a = mean(a), spread_a = sd(a), \n            mean_b = mean(b), spread_b = sd(b))\n\n    mean_a   spread_a   mean_b   spread_b\n1 2.004514 0.04364563 3.012044 0.06410626\n\n\nThe result of summarizing the trials is a “summary of a sample of summaries.” This phrase is admittedly awkward, but we will be using this technique often: summarizing trials, where each trial is a summary of a sample. Often the clue will be the use of do(), which repeats trials as many times as you ask."
  },
  {
    "objectID": "Reading-notes-lesson-21.html",
    "href": "Reading-notes-lesson-21.html",
    "title": "3  Signal and noise",
    "section": "",
    "text": "Imagine being transported back to June 1940 . You and your family might be sitting around a radio receiver, having just switched on set and waited for it to warm up in time to hear a news broadcast. I’ve selected a newscast for you, recording 103. The recording covers the surrender of the French in the face of the German invasion. Press the play button in the box below and listen.\nThere are many other recordings on the site which are worth listening to. I’m directing you to #103 as an example of a radio phenomenon: noise (or, in slang, “static”). You can clearly make out the spoken words from the recording. But there is also a background sound, something like the sound made by the act of crumpling paper.\nModern radio engineering has more-or-less eliminated noise, mainly by the use of digital technology. (Many of the recordings on the radio archive site have been “cleaned” so the noise is not so evident.) But if you have ever talked to a friend at a sporting event, you have probably had to shout to get your message over the noise of the crowd. At the receiving end, your friend intuitively filters out the noise (unless it is too loud) and recovers your words.\nEngineers and others make a distinction between signal and noise. The signal is the spoken words of the 1940 broadcast, the noise is the hiss and clicks. You can intuitively separate the signal and the noise in this recording, focusing attention on the signal and ignoring the noise.\nSeparating signal from noise—or, at least trying to reduce the noise—is a common problem in all sorts of settings. Historically, statistics emerged from a confluence of two needs: i) the need to summarize the resources and activities of countries and states (whence comes the “stat” in “statistics”) and ii) the need to filter out noise so that the signal becomes clearer."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#sec-signal-and-noise",
    "href": "Reading-notes-lesson-21.html#sec-signal-and-noise",
    "title": "3  Signal and noise",
    "section": "3.1 Signal and noise",
    "text": "3.1 Signal and noise\nTo illustrate the statistical problem of signal and noise, let’s turn to a DAG simulation: dag01. Here’s a sample from dag01:\n\nsample(dag01, size=2)\n\n# A tibble: 2 × 2\n       x     y\n   <dbl> <dbl>\n1 -0.326  2.84\n2  0.552  5.04\n\n\nThe DAG simulation implements a relationship between x and y. In statistics, this relationship is the signal.\n\nLook at the 2-row sample from the simulation and make a guess about what the relationship is.\n\nYour guess will be exactly that: a guess. Any of an infinite number of possible relationships could account for the x and y data. The noise reduction problem of statistics is to make the guess as good as possible. For a sample of size \\(n=2\\), as good as possible is not very good!\nTo have a better shot at revealing the relationship hidden by the noise, we need more data, a bigger sample. Here’s a sample of size \\(n=10\\):\n\nsample(dag01, size=10)\n\n# A tibble: 10 × 2\n         x     y\n     <dbl> <dbl>\n 1 -0.786  1.89 \n 2  0.0547 4.12 \n 3 -1.17   2.36 \n 4 -0.167  6.33 \n 5 -1.87   0.933\n 6 -0.120  2.93 \n 7  0.826  5.70 \n 8  1.19   5.90 \n 9 -1.09   2.13 \n10 -0.375  4.23 \n\n\nLooking carefully at the two rows of data you may be able to see some patterns. x is never larger than, say, 2 in magnitude and can be positive or negative. y is always positive. And notice that when x is negative, the corresponding y value is relatively small compared to the y values for positive x.\nWith the bigger sample size, \\(n=10\\) versus \\(n=2\\), we can make a more informed guess about the relationship between variables x and y.\nHuman cognition is not well suited to looking a long columns of numbers. Often, we can make better use of our natural human talents by translating the sample into a graphic, like this:\n\n\n\n\n\nCollecting more data can make the relationship clearer. Here’s a graph of a sample of size \\(n=10,000\\) with the smaller \\(n=10\\) sample shown in orange:\n\n\n\n\n\nFigure 3.1: With \\(n=10,000\\) rows, the relationship between x and y is evident graphically.\n\n\n\n\nThere are many ways to describe the relationship between x and y indicated by the graph of the large sample. For instance, we can see that when x is positive, y is almost always greater than 4, but for negative x the value of y tends to be less than 4.\nIn this course, we prefer to make quantitative summaries of relationships. We do this by fitting models to the data. Here’s the relationship that’s shown by the original sample of 10:\n\nlm(y ~ x, data = original) %>% coefficients() # size 10 sample\n\n(Intercept)           x \n   4.262846    1.741758 \n\n\nThe coefficients in this model correspond to the mathematical relationship \\(y = 4.26 + 1.74 x\\). On it’s own, this formula doesn’t tell us the extent to which we have filtered out the noise in the simulation.\nWith more data, say, the larger sample of \\(n=10,000\\), the relationship becomes more evident:\n\nlm(y ~ x, data = larger) %>% coefficients() # size 10 sample\n\n(Intercept)           x \n   4.008928    1.495904 \n\n\nBecause these data come from a DAG simulation, we can look at the formulas to see exactly what relationship is behind the data:\n\nprint(dag01)\n\n[[1]]\nx ~ eps()\n\n[[2]]\ny ~ 1.5 * x + 4 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nComparing the two models to the DAG formula for y, we can see that the larger sample produced coefficients that are much closer to the formula than did the smaller sample. Closer, but not exactly the same. Even in the coefficients calculated from the large sample, there is still a legacy of the noise in the original relationship.\nThe lesson here is simple: More data can give a better view of the relationships.\nThe challenge we face when working with data generated in the real world is that it is not often possible to open the black box that generated the data; all we have is the data! So how can we tell whether the data we have at hand are sufficient for giving a faithful description of the actual relationships?\nThe general idea is to use the variation within the sample to accomplish two things at once: i. make a description of the relationship, and ii. estimate how much inherited noise there is in the description. The result of (ii) is important, since it can tell us whether or not our description (i) is good enough for the purpose we seek to serve.\nTo get started, let’s explore how to measure the amount of variation in the data. This can give us an idea of the size of the overall signal+noise, which we will do in the next section of this Lesson. In Lesson 22 we will use DAG simulations to get an idea of how to estimate the amount of inherited noise in the description of the relationship. The DAG simulation is useful because we have access to the internal mechanism of the DAG, so it is easy to see how close the description is to the actual relationship.\nIn Lesson 23, we will take off the DAG training wheels and learn how to estimate the size of the inherited noise in the description directly from data, without having to open the black box of the mechanism that generated the data. If you think about it, it is an amazing claim that we can estimate how close our data-driven description is to the actual mechanism, without having to know the actual mechanism!"
  },
  {
    "objectID": "Reading-notes-lesson-21.html#measuring-variation",
    "href": "Reading-notes-lesson-21.html#measuring-variation",
    "title": "3  Signal and noise",
    "section": "3.2 Measuring variation",
    "text": "3.2 Measuring variation\nWe already know the standard way to measure variation in a single variable: the variance or, equivalently, the standard deviation, which is simply the square root of the variance.\nPerhaps you are wondering why there are two standard ways to measure variation, when each can be calculated from the other? The variance can be found by squaring the standard deviation, the standard deviation by taking the square root of the variance. Either will do, so why both?\nThe answer to this question is illustrated by a bit of geometry: the mathematics of right triangles and the corresponding Pythagorean relationship among the sides of the triangle: \\(A^2 + B^2 = C^2\\). The quantities \\(A\\), \\(B\\), and \\(C\\) are the lengths of the edges of the right triangle. The quantities \\(A^2\\), \\(B^2\\) and \\(C^2\\) are the lengths-squared of those edges. In order to calculate the length of one edge given the lengths of the others, we need first to square the lengths. Having squared them, we can easily do the calculation of the length-squared of the unknown edge. Then, we take the square root of the length-squared to find the length of the edge.\nLengths are like standard deviations, lengths-squared are like the variance. Where does the right triangle fit in? The overall variation in the response variable is like the hypothenuse of a right triangle. One of the other two edges represents the noise in the relationship. The other edge represents the signal: the relationship itself. It’s easy to measure the overall variation in the response variable. We can also measure the noise, but indirectly. First, we fit a model connecting the response variable to the explanatory variable(s). Then the variation of the residuals for that model are the estimate for the noise.\nOur first illustration will use data from dag01. We will arbitrarily set the sample size to \\(n=10,000\\). (In Lesson 22, we will look at the impact of sample size on the results.)\n\nDag_data <- sample(dag01, size=10000)\n\nNow measure the variation in x and y in the standard way:\n\nDag_data %>%\n  summarize(sx = sd(x), sy = sd(y), vx = var(x), vy = var(y))\n\n# A tibble: 1 × 4\n     sx    sy    vx    vy\n  <dbl> <dbl> <dbl> <dbl>\n1 0.998  1.81 0.995  3.29\n\n\nThe size of the x variation is about 1. The size of the y variation is about 1.7. (We’re using the standard deviation to measure the size of the variation.\nLook again at the formulas that compose dag01:\n\nprint(dag01)\n\n[[1]]\nx ~ eps()\n\n[[2]]\ny ~ 1.5 * x + 4 + eps()\n\nattr(,\"class\")\n[1] \"list\"      \"dagsystem\"\n\n\nFrom the formula for x we can see that x comes from a random number generator, eps(). The eps() generator is designed to generate noise of size 1 by default.\nAs for y, the formula includes two sources of variation:\n\nThe part of y determined by x, that is \\(y = \\mathbf{1.5 x} + \\color{gray}{4 + \\text{eps()}}\\)\nThe noise added directly into y, that is \\(y = \\color{gray}{\\mathbf{1.5 x} + 4} + \\color{blank}{\\mathbf{eps(\\,)}}\\)\n\nThe 4 in the formula doesn’t add any variation to y; it’s just a number.\nLet’s measure variation using the standard deviation: We already know that eps() generates variation of size 1. So the amount of variation contributed by the + eps() term in the DAG formula is 1. The remaining variation is contributed by 1.5 * x. The amount of variation in x is 1, coming from the eps() in the formula for x. A reasonable guess is that 1.5 * x will have 1.5 times the variation in x. So, the variation contributed by the 1.5 * x component is 1.5. The overall variation in y is the sum of the variations contributed by the individual components. This suggests that the variation in y should be \\[\\underbrace{1}_\\text{from eps()} + \\underbrace{1.5}_\\text{from 1.5 x} = \\underbrace{2.5}_\\text{overall variation in y}.\\] Simple addition! Unfortunately, the result is wrong. In the previous summarize() of the Dag_data, we measured the overall variation in y as about 1.72.\nLet’s try again, this time using the variance as our measure of variation.\nSince eps() generates variation whose standard deviation is 1, the variance is simply \\(1^2 = 1\\). The variance of x is therefore 1, as is the variance of the eps() component of y.\nWhat’s the variance of 1.5 * x? It turns out to be \\(1.5^2\\, \\text{var(}\\mathit{x}\\text{)} = 2.25\\). Adding up the variances from the two components gives\n\\[\\text{var(}\\mathit{y}\\text{)} = \\underbrace{2.25}_\\text{from 1.5 eps()} + \\underbrace{1}_\\text{from eps()} = 3.25\\]\nThis result, that the variance of y is 3.25, is a close match to what we found in summarizing the y data generated by the DAG. And, of course, \\(\\sqrt{3.25} = 1.80\\), which is what we found by calculating the standard deviation of the y directly from the data.\nThe lesson here: When adding two sources of variation, the variances of the individual sources add to produce the overall variance of the sum. Just like \\(A^2 + B^2 = C^2\\)."
  },
  {
    "objectID": "Reading-notes-lesson-21.html#dags-from-data",
    "href": "Reading-notes-lesson-21.html#dags-from-data",
    "title": "3  Signal and noise",
    "section": "3.3 DAGs from data",
    "text": "3.3 DAGs from data\nIn modeling data from dag01 we could recover the DAGs formula for y.\n\nsample(dag01, size=10000) %>%\n  lm(y ~ x, data = .) %>%\n  coefficients()\n\n(Intercept)           x \n   3.996846    1.494939 \n\n\nIt is wrong to think that from data we can determine the DAG that generated the data. It’s only if we know the structure of the data-generation DAG that we can recover the mechanism inside that DAG. But another statistical thinker might claim that what’s behind the data is y causing x. Based on this assumption, she also can find the mechanism inside her hypothesized DAG:\n\nsample(dag01, size=10000) %>%\n  lm(x ~ y, data = .) %>%\n  coefficients()\n\n(Intercept)           y \n -1.8485902   0.4635813 \n\n\nA DAG is a hypothesis, a statement that might or might not be true. DAGs are part of the statistical apparatus for thinking responsibly about causality. You use a DAG—or, potentially, multiple DAGs—when the issue of what causes what is relevant to your work.\nWhen there are only two variables involved in the system under consideration—we’ll call them X and Y for simplicity—there are only two possible DAGs:\n\\[X \\rightarrow Y\\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\ X \\leftarrow Y\\]\nOften, but not always, our understanding of the world allows us to focus on one of these and not the other. Example: Does the rooster crowing cause the sun to rise, or does the rising sun cause the rooster to crow? That’s a pretty easy question if you know how things work.\nBut there are additional DAG possibilities that can account for the relationship between x and y. For instance, if we introduce another quantity, c in between x and y, four other DAGs need to be considered:\n\\[X \\rightarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\leftarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\leftarrow C \\rightarrow Y \\ \\ \\ \\ \\ \\text{and}\\ \\ \\ \\ \\\nX \\rightarrow C \\leftarrow Y\\]\nActually, there are many other configurations of DAGs involving three variables. To keep things simple, we’ll restrict things to DAGs where X might or might not cause Y, but Y never causes X. (We don’t lose anything from this restriction because you get to make the choice of what real-world variable correspond to X and which one to Y.) Figure 3.2 shows the 10 configurations of 3-variable DAGs where Y doesn’t cause X.\n\n\n\n\n\nFigure 3.2: Ten DAG configurations involving three variables X, Y, and C and where there is no causal path going from Y to X.\n\n\n\n\nThe statistical thinker, with the conceptual tool of DAGs at, can consider multiple possibilities for what might cause what. Sometimes she will be able to discard some of the possibilities based on common sense. (Think: roosters and the sun.) But other times there may be possibilities that she doesn’t favor but which nonetheless might be plausible to other people. In Lesson 28 we will explore how each configuration of DAG has implications for which model formulas can or cannot reveal the hypothesized causal mechanism."
  },
  {
    "objectID": "Reading-notes-lesson-22.html",
    "href": "Reading-notes-lesson-22.html",
    "title": "4  Sampling variation",
    "section": "",
    "text": "There are many sources of noise in data; every variable has its own story, part of which is noise from measurement error, recording blunders, etc. Economists use national statistics, like GDP, even though the definition is arbitrary (a Hurricane can raise GDP!) and early reports are invariably corrected a few months later. Historians go back to original documents, but inevitably many of the documents have been lost or destroyed: a source of noise. Even in elections, where you would think counting is straightforward, the voters’ intentions are measured imperfectly due to “hanging chads,” “butterfly ballots,” broken voting machines, spoiled ballots, and so on.\nThe statistical thinker is well advised to know about the sources of noise in the system she is studying. Your analysis of data will be better the more you know about how measurements are made and data collected."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#sampling-variation",
    "href": "Reading-notes-lesson-22.html#sampling-variation",
    "title": "4  Sampling variation",
    "section": "4.1 Sampling variation",
    "text": "4.1 Sampling variation\nThere is one source of noise that is so common across many settings that every statistical thinker needs to be intimately familiar with. This is called “sampling variation.”\nWe’ve been using “sample” as a near synonym for “data frame.” But that’s not completely fair. Often, data frames contain a row for each and every “item” of relevance. For instance, the Department of Labor never suggested that the contractor in the previous example had left out some of the applicants. Such a complete enumeration—the inventory records of a merchant, the records kept of student grades by the school registrar—has a technical name: a “census.” Famously, many countries have a regular census of the population—every 10 years in the US or the UK or China—in which (they try to) reach out to every resident.\nBut there are many settings where it is unfeasible to collect data in the form of a census. The records will be incomplete and therefore constitute a “sample.” Sampling is called for when we want to find out about a large group but don’t have the resources—time, energy, money—to contact every member of the group. France, in order to collect up-to-date data while staying within a budget, runs a “rolling census” where samples are made at short time intervals. It’s estimated that the French rolling census ultimately reaches 70% of the population.\nSometimes, as in quality control in manufacturing, the measurement process is destructive: the item is consumed in the process of measurement. Then, of course, it would be pointless to make a measurement of every single item. A sample will have to do.\nCollecting a reliable sample is usually a lot of work. One idealization is called a “simple random sample” (SRS) where all of the items are available, but only some are selected, at random, to be recorded as data. The work comes from having to assemble all of the items. Making a SRS calls for first assembling a “sampling frame,” which is essentially a census. If a census is unfeasible, the construction of a perfect sampling frame is hardly less so.\nProfessional work, such as the collection of unemployment data, often requires government-level resources and draws on specialized statistical techniques such as stratified sampling and weighting of the results. We won’t cover the specialized techniques in this introductory course, even though they are very important in creating representative samples. If you’re interested in seeing what’s involved, you can get an idea by scrolling through the table of contents of a classic text, William Cochran’s Sampling techniques\nAll statistical thinkers, whether expert in sampling techniques or not, should be aware of factors that can bias a sample away from being representative. Non-response bias can be significant, even overwhelming, in surveys. In political polls, many (most?) people will not respond to the questions. If this non-response stems from, for example, an expectation that the response will be unpopular, then the poll sample won’t adequately reflect unpopular opinions.\nSurvival bias is an important consideration in many settings. An example is given by the mosaicData::TenMileRace data. This records the running times of 8636 participants in a 10-mile road race held in 2005 and includes information about the runner, such as his or her age. You might think that such data could tell you about changes in running performance as people age: the data frame includes runners from age 10 to 87. But a model of running time as a function of age from this data frame is seriously biased. The reason? As people age, casual runners tend to drop out of such races. So the older runners are skewed toward higher performance. (We can see this by taking a different approach to the sample: collecting data over multiple years and tracking individual runners as they age.\n\n\n\n\n\n\nExamples: Returned to base\n\n\n\nAn inspiring story about dealing with survival bias comes from a World War II study of the damage sustained by bombers due to enemy guns. The sample, by necessity, included only those bombers that survived the mission and returned to base. The shell holes in those surviving bombers were not representative of where shells hit the planes, they were only representative of shell hits that did not prevent the plane from returning. The study report, available here, is a tribute the the work and ingenuity needed to deal with issues such as survival bias. The report itself doesn’t contain any diagram showing where shells it the bombers, but a hypothetical diagram on Wikipedia conveys the idea.\n\n\n\nThe shell holes on the surviving planes were clustered in certain areas. The clustering stems from survivor bias. Planes that were it in the areas, such as the middle of the wings, the cockpit, the engines, and the back of the fusalage did not return to base. Consequently, those shell hits were never recorded."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#measuring-sampling-variation",
    "href": "Reading-notes-lesson-22.html#measuring-sampling-variation",
    "title": "4  Sampling variation",
    "section": "4.2 Measuring sampling variation",
    "text": "4.2 Measuring sampling variation\nWe start the process of learning about sampling variation on the training ground. That is, we’ll use simulations from DAGs even though our ultimate goal is to work with real data. DAGs are a convenient training tool because the data generated is always a simple random sample and we can generate any number of samples of any size we wish. In the spirit of starting simply, we’ll return to dag01 which, you may remember, has the structure \\(\\mathtt{x}\\longrightarrow\\mathtt{y}\\) and the causal formula y ~ 4 + 1.5 * x + eps(x).\nIt’s crucial to remember that sampling variation is not about the row-to-row variation in a single sample, it is about the variation in the summary from one sample to another. So our initial process for exploring sampling variation will be to carry out many trials, each of which is a summary of a sample.\n::: {.callout-warning} ## Samples and specimens\nTo illustrate, here is one trial using a sample of size \\(n=25\\). There are many ways to summarize a sample, here we will use y ~ 1.\n\nSample <- sample(dag01, size=25) \nSample %>% \n  lm(y ~ 1, data = .) %>%\n  coefficients()\n\n(Intercept) \n   3.928023 \n\n\nWe can’t see sampling variation directly in the above result because there is only one trial. To see sampling variation directly, we need to run many trials. In each trial, a new sample (of size \\(n=25\\) is taken and summarized.)\n\nTrials <- do(100) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coefficients()\n}\nTrials\n\n    Intercept\n1    4.199875\n2    3.817843\n3    4.510713\n4    3.734723\n5    4.239279\n6    4.116032\n7    3.626492\n8    3.745319\n9    4.298532\n10   4.104179\n11   3.950145\n12   4.132146\n13   4.007944\n14   3.775330\n15   3.999737\n16   3.534516\n17   3.941521\n18   3.549225\n19   3.899203\n20   4.436699\n21   4.039248\n22   3.896074\n23   3.795834\n24   4.106192\n25   3.493857\n26   3.969599\n27   3.902611\n28   3.986890\n29   3.810399\n30   3.485495\n31   4.092823\n32   4.559235\n33   3.742683\n34   3.482938\n35   4.324149\n36   3.879057\n37   4.235532\n38   4.266414\n39   3.913059\n40   4.402609\n41   4.145027\n42   4.185490\n43   4.125238\n44   3.728135\n45   3.733025\n46   3.682608\n47   4.032067\n48   3.711723\n49   3.700979\n50   4.237847\n51   4.511106\n52   3.949393\n53   3.868491\n54   3.925727\n55   4.342317\n56   4.036528\n57   3.504173\n58   3.523165\n59   3.579379\n60   3.896941\n61   3.253011\n62   4.605904\n63   3.530294\n64   4.203356\n65   4.069098\n66   4.106359\n67   4.424720\n68   3.802443\n69   3.719274\n70   3.927286\n71   4.394132\n72   3.954937\n73   3.787975\n74   4.595992\n75   4.174882\n76   3.515858\n77   4.187859\n78   3.751202\n79   3.789112\n80   4.147821\n81   4.345657\n82   3.645454\n83   4.141770\n84   4.400095\n85   4.268064\n86   3.871160\n87   4.079240\n88   3.749730\n89   4.378494\n90   4.262415\n91   4.496611\n92   3.924040\n93   3.926052\n94   4.038813\n95   4.377791\n96   4.453000\n97   4.210344\n98   4.329442\n99   3.950655\n100  4.049530\n\n\nTrials is a sample of summaries. In Trials, the sampling variation can indeed be seen in the row-to-row variation in the data frame, but only because the data frame is a summary of samples. Since it is hard to read columns of numbers, we will summarize the variation in the sample of summaries.\nAs always, our standard measure of variation is the standard deviation (or, equivalently, variance):\n\nTrials %>%\n  summarize(sIntercept = sd(Intercept))\n\n  sIntercept\n1  0.2988593\n\n\nThis quantity, which is the standard deviation of a sample of summaries, has a technical name in statistics: the standard error. The words standard error should properly be followed by a description of the summary and the size of the individual samples involved. Here it would be, “0.348 is the standard error of the Intercept coefficient from a sample of size 25.\nThe standard error is an ordinary standard deviation, but in a particular context: the standard deviation of a sample of summaries. This can be confusing, since “error” and “deviation” are somewhat synonomous in everyday language. It can be hard to remember when to use “error” and when to use “deviation.” Fortunately, it’s more common to use another way to present the information about sampling variation."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#the-se-depends-on-sample-size",
    "href": "Reading-notes-lesson-22.html#the-se-depends-on-sample-size",
    "title": "4  Sampling variation",
    "section": "4.3 The SE depends on sample size",
    "text": "4.3 The SE depends on sample size\nWe found an SE of 0.348 on the Intercept in a sample of size \\(n=25\\). Does the SE depend on the sample size. We can find out by trying it for several different sample sizes, say, 25, 100, 400, 1600, 6400, 25600, 102400. We picked these particular numbers to be multiples of 4 times the previous sample size.\nHere’s the SE for a sample of size 400:\n\nTrials <- do(100) * {\n  Sample <- sample(dag01, size=25) \n  Sample %>% \n    lm(y ~ 1, data = .) %>%\n    coefficients()\n}\nTrials %>% summarize(se = sd(Intercept))\n\n         se\n1 0.3674418\n\n\nYou can try it yourself for the other sample sizes. Here’s what we got, running 1000 trials in each instance:\n\n\n\n\nSE  %>% knitr::kable()\n\n\n\n \n  \n    n \n    se \n  \n \n\n  \n    25 \n    0.3600 \n  \n  \n    100 \n    0.1900 \n  \n  \n    400 \n    0.0910 \n  \n  \n    1600 \n    0.0430 \n  \n  \n    6400 \n    0.0230 \n  \n  \n    25600 \n    0.0110 \n  \n  \n    102400 \n    0.0056 \n  \n\n\n\n\n\nThere’s a pattern here. Every time we double \\(n\\), the standard error goes down by a factor of 2, that is, \\(\\sqrt{4}\\). (The pattern isn’t exact because there is sampling variation in the trials themselves.)\nLesson: The standard error gets smaller the larger the sample size. For a sample size of \\(n\\), the SE will be proportional to \\(1/\\sqrt{\\strut n}\\)."
  },
  {
    "objectID": "Reading-notes-lesson-22.html#the-confidence-interval",
    "href": "Reading-notes-lesson-22.html#the-confidence-interval",
    "title": "4  Sampling variation",
    "section": "4.4 The confidence interval",
    "text": "4.4 The confidence interval\nThe “confidence interval” is a more user-friendly format for describing the amount of sampling variation. As an interval, it commonly written either as [lower, upper] or center\\(\\pm\\)half-width. These styles are completely equivalent and either style can be used. The preferred style can depend on the field or the journal in which a report is being published. Some journals like a different style, center (half-width).\n\n\n\n\n\n\nTechnical vocabulary\n\n\n\nThere is a technical name for the half-width: the “margin of error.” We will leave the confidence interval calculation to software, so we won’t have much need to refer to the margin of error, but it is a term commonly used by statisticians and scientists.\n\n\nThe margin of error is defined to be twice the SE. A lot of early statistical theory was given over to defining “twice.” For our purposes, twice means “multiply by 2.” Some people prefer the theoretically more precise “multiply by 1.96” which is appropriate for very large sample sizes. For small sample sizes “twice” is larger than 1.96 and depends on how many model coefficients there are. For instance, consider the simplest model y ~ 1. There is one coefficient and for a sample size of \\(n=20\\) twice would be 2.09 while a for a sample size of \\(n=5\\) “twice” would be 2.8.\n\n\n\n\n\n\nDemonstration: Confidence interval on the Intercept coefficient for \\(n=25\\)\n\n\n\nThe following command computes the confidence interval (that is, the two numbers [lower,upper]) for the trials we ran on samples of size \\(n=25\\) from dag01 and summarized by the intercept coefficient from y ~ 1. We show this just to make clear what that the margin of error is twice the standard error.\n\nTrials %>%\n  summarize(m=mean(Intercept), se=sd(Intercept)) %>%\n  mutate(lower = m - 2*se, upper = m + 2*se)\n\n         m        se    lower   upper\n1 3.996807 0.3674418 3.261923 4.73169\n\n\nNotice that we have used 2 for “twice.” But best to leave the detailed calculations to the software.\n\n\nStatistical software is written to use the correct value of “twice” for any given sample size and number of coefficients. But for everyday purposes, and samples larger than, say, \\(n=10\\), “twice” is roughly 2.\nIn calculations, finding the half-width of the confidence interval requires first finding the standard error, then multiplying by “twice.” In practice, it’s far easier to use software. In R, the confint() function reports the confidence interval for model coefficients:\n\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  confint()\n\n                  2.5 %      97.5 %\n(Intercept) -533.432471 -406.521402\ndistance     246.387096  261.229494\nclimb          2.493307    2.726209\n\n\n\n\n\n\n\n\nDemonstration: How many digits?\n\n\n\nIn the calculation of confidence intervals on the model time ~ distance + climb, the results were reported to many digits. Such a report is appropriate for whatever further calculations might need to be done on the results, but it is usually not appropriate for a human reader.\nTo know how many digits are worth reporting to humans, you can look at the standard error. The standard error is a part of a different kind of summary of a model: the “regression report.” We won’t need to look at regression reports until the end of the course. We show one here just to make the point about how many digits are worth reporting to humans.\nHere’s the regression report on the Hill_racing model\n\nHill_racing %>% \n  lm(time ~ distance + climb, data=.) %>% \n  regression_summary()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -470.     32.4        -14.5 9.92e- 46\n2 distance      254.      3.78        67.1 0        \n3 climb           2.61    0.0594      43.9 4.08e-304\n\n\nThe “standard error” for each coefficient is reported in the column labelled std.error.\nFor the human reader, only the first two significant digits of the standard error are worth reporting. In this case, that is 32 for the Intercept, 3.8 for the distance coefficient, and 0.059 for the climb coefficient. The confidence interval will be the coefficient itself (column labelled estimate) plus-or-minus “twice” the std.error. The report of the confidence interval (for a human reader) should be rounded to the place of the first two significant digits of the standard error.\nFor example, the confidence interval on the distance coefficient will be \\(253.808295 \\pm 2 \\times 3.78433220\\). Keep only the digits that come before the first two significant digits of the SE, so the reported interval can be \\(253.8 \\pm 3.8\\)."
  },
  {
    "objectID": "Reading-notes-lesson-23.html",
    "href": "Reading-notes-lesson-23.html",
    "title": "5  Estimating sampling variation from a single sample",
    "section": "",
    "text": "Lesson 21 introduced the idea of separating data into separate components: signal and noise. The signal is a summary of the data that tells us something we want to know. Often, the signal will be one or more coefficients from a regression report, but it might be something as simple as the mean or median or standard deviation of a variable in a data frame.\nThe noise comes into the data from any of a variety of sources: e.g. error in measurement or a data-entry blunder. Another source of noise is omnipresent (except in a perfect census): sampling variation. The idea behind sampling variation is that the particular data at hand is just one sample and is contingent on the time and situation in which the data were collected. Data collection at a different time or situation would presumably be somewhat different.\nThe Greek philosopher Heraclitus (c. 500 BC) said, “You can’t step into the same river twice.” Each time you step into a river, you might be at the same place on the bank but the water around you will be different. A data sample is like collecting water from a river or lake using a dipper. Imagine ten people standing side by side on the shore of a lake, each person dipping into the water acquire a specimen and making one or more measurements from the specimen, for instance the temperature, pH, and bacteria count. Each person collects a sample—that is, a series of specimens. These might be taken one right after the other or by some protocol, say a weekly tracking of lake conditions over time.\nThe ten people are each doing the same thing in approximately the same place and same time, but each person’s sample—say the collection of 52 weekly specimens over the course of a year—will be different. Perhaps only a little bit different. That sample-to-sample variation will be noise.\nIf the ten people were fishing, each specimen would be the result of one cast of the rod. Typically this is just an empty hook, lake weeds, or a stick, but sometimes it will be a fish. At the end of the fishing day, each fisherman will have a sample. With fishing luck (or skill), some of the specimens in the sample will be fish. Presumably, the luck (or skill) of the fishermen will differ one from the other. That’s the noise of sampling variation. To fishermen the question of interest, the signal they want to measure, might be, “How good is the fishing today?” Each answers that question by looking at how many fish he or she caught. The ten fishermen’s catch will differ: sampling variation. Consequently, each fisherman’s answer will be contaminated with some noise. The fisherman’s summary (the count of fish caught) will have some noise stemming from sampling variation.\nIn Lesson 21, to gain some feeling for sampling variation, we repeated trials over and over gain. Each trial consisted of collecting a sample (that is, multiple specimens) and summarizing it. The individual trial is a summary of a sample. We then summarized the whole set of trials with the standard deviation. This is how we quantified sampling variation.\nNow it is time to take off the DAG training wheels and measure sampling variation from actual data, from a single sample. This sounds like an impossible task. Sampling variation is about the variation in summaries between samples. With a single sample there is no “between” to be had.\nWhat we will need to estimate sampling variation from a single sample is a way to simulate drawing new samples from the single sample."
  },
  {
    "objectID": "Reading-notes-lesson-23.html#subsampling",
    "href": "Reading-notes-lesson-23.html#subsampling",
    "title": "5  Estimating sampling variation from a single sample",
    "section": "5.1 Subsampling",
    "text": "5.1 Subsampling\nAuthors of statistics books tend to choose examples based on their own interests rather than their students’, so in this section let’s look at athletic performance as people age. The TenMileRace data are readily to hand, so we’ll look at net race time (from start line to finish line) as a function of age. We’ll limit the study to people over 40.\n\nTenMileRace %>% filter(age > 40) %>%\n  lm(net ~ age, data = .) %>% coefficients()\n\n(Intercept)         age \n 4278.21279    28.13517 \n\n\nThe units of net are seconds, the units of age are years (as conventional). The model coefficient on age tells us how the net time changes for each additional year of age. This summary of the data tells us that the time to run the race gets longer by about 28 seconds per year. So a 45-year-old runner who completed this year’s 10-mile race in 3900 seconds (that’s about 9.2 mph, a pretty good pace!) might expect that, in ten years, when she is 55 years old her time will be longer by 280 seconds.\nIt would be asinine to report the ten-year change as 281.3517 seconds. The runner’s time ten years from now will be influenced by the weather, crowding, the course conditions, whether she finds a good pace runner, the training regime, improvements in shoe technology, injuries, illnesses, etc. There’s little or nothing we can say from the TenMileRace data about such factors.\nBut we should not forget sampling variation. TenMileRace has 2898 includes 2898. The way the data was collected (radio-frequency interogation of a dongle on the runner’s shoe) suggests that the data is a census of finishers, but really it is a sample of the kind of people who run such races. People might have been interested in running but had a schedule conflict, or lived too far away, or missed their train into the start line in the city. So we’ll treat the data as a sample.\nThis sample of 2898 runners is the only sample we have, so there is no option to compare multiple samples in order to look at sampling variation. That’s no excuse for not making an interval estimate on the age coefficient. We have to use some ingenuity.\nHere’s an idea. Instead of using samples of size 2898, let’s use sub-samples one-tenth the size: \\(n=290\\). We’ll select the subsamples at random:\n\nOver40 <- TenMileRace %>% filter(age > 40)\nlm(time ~ age, data = Over40 %>% sample(size=290)) %>% coefficients()\n\n(Intercept)         age \n 4694.73528    23.37827 \n\nlm(time ~ age, data = Over40 %>% sample(size=290)) %>% coefficients()\n\n(Intercept)         age \n 4669.07546    23.89813 \n\n\nThe age coefficients differ one from the other by about 0.5 seconds. Better, let’s select many subsamples of size 1449 at random, and find the age coefficient for each of them. We will run 100 trials\n\n# a sample of summaries\nTrials <- do(1000) * {\n  lm(time ~ age, data = sample(Over40, size=290)) %>% coefficients()\n}\n# a summary of the sample of summaries\nTrials %>%\n  summarize(se = sd(age))\n\n        se\n1 9.042024\n\n\nWe used the name se for the summary of samples of summaries because what we have calculated is the standard error of the age coefficient in a sample of size \\(n=290\\).\nIn Lesson 22 we saw that the standard error is proportional to \\(1/\\sqrt{\\strut n}\\), where \\(n\\) is the sample size. From the subsamples, know that the SE for \\(n=290\\) is about 9.0 seconds. This tells us that the SE for the full \\(n=2898\\) samples would be about \\(9.0 \\frac{\\sqrt{290}}{\\sqrt{2898}} = 2.85\\).\nSo the interval summary of the age coefficient—the so-called “confidence interval” is \\[\\underbrace{28.1}_\\text{age coef.} \\pm 2\\times\\!\\!\\!\\!\\!\\!\\! \\underbrace{2.85}_\\text{standard error} =\\ \\ \\ \\  28.1 \\pm\\!\\!\\!\\!\\!\\!\\!\\! \\underbrace{5.6}_\\text{margin of error}\\ \\  \\text{or, equivalently, 22.6 to 33.6}\\]"
  },
  {
    "objectID": "Reading-notes-lesson-23.html#bootstrapping",
    "href": "Reading-notes-lesson-23.html#bootstrapping",
    "title": "5  Estimating sampling variation from a single sample",
    "section": "5.2 Bootstrapping",
    "text": "5.2 Bootstrapping\n\nThere is a trick to generating a random subsample of a data frame with the same \\(n\\) as the data frame: draw the subsample from the original sample with replacement. An example will suffice to show what the “with replacement” does:\n\nexample <- c(1,2,3,4,5)\n# without replacement\nsample(example)\n\n[1] 1 4 3 5 2\n\n# now, with replacement\nsample(example, replace=TRUE)\n\n[1] 2 4 3 3 5\n\nsample(example, replace=TRUE)\n\n[1] 3 5 4 4 4\n\nsample(example, replace=TRUE)\n\n[1] 1 1 2 2 3\n\nsample(example, replace=TRUE)\n\n[1] 4 3 1 4 5\n\n\nThe “with replacement” leads to the possibility that some of the values will appear two or more times in the subsample and others of the values will be left out.\nThe calculation of the SE using sampling with replacement looks like this: rset.seed(207)`\n\n# run many trials\nTrials <- do(1000) * {\n  lm(time ~ age, data = sample(Over40, replace=TRUE)) %>% \n       coefficients()\n}\n# summarize the trials to find the SE\nTrials %>% summarize(se = sd(age))\n\n        se\n1 2.948786\n\n# or let the computer do the work of converting to a confidence interval\nTrials %>% confint()\n\n  name    lower   upper level     method estimate\n1  age 21.58031 33.3163  0.95 percentile  27.2782\n\n\nThis method is called “bootstrapping a confidence interval.” The same word, “bootstrapping” is used to describe how a computer turns itself on. It comes from the idea of a person raising herself from the ground by pulling upward on her own boots. An impossible task. And a suitable metaphor for generating many samples from a single sample."
  },
  {
    "objectID": "Reading-notes-lesson-23.html#using-the-residuals",
    "href": "Reading-notes-lesson-23.html#using-the-residuals",
    "title": "5  Estimating sampling variation from a single sample",
    "section": "5.3 Using the residuals",
    "text": "5.3 Using the residuals\nLesson 21 pointed to the idea of data consisting of two parts: signal plus noise. In Lesson 22 and thusfar in this lesson, we’ve tried to estimate the signal by summarizing the data. But we still had to account for sampling variation, which we did by generating many subsamples.\nAnother route to measuring sampling variation takes more literally the division of data into signal and noise. The idea is still to estimate the signal by a regression model summary. But now, we take the residuals from the model as the evidence for how much noise there is. We quantify the variation in the residuals in the same way that we have always done: their standard deviation. This quantity, the standard deviation of the residuals from a model, has its own technical name: the “residual standard error.” For some types of models, it’s possible to push the residual standard error through the model-fitting apparatus in order to construct standard errors and confidence intervals. The mathematics of this is a matter for specialists, but computers handle the calculations well.\nThe confint() function knows how to take lm()-fitted models and translate the residuals into confidence intervals. Like this:\n\nlm(time ~ age, data = Over40) %>% confint()\n\n                 2.5 %     97.5 %\n(Intercept) 4239.50891 4804.89396\nage           21.58833   32.96807\n\n\nContemporary statistics uses many different model types depending on the situation. In this course we will only use two: linear models and generalized linear models. The field of machine learning has introduced many other kinds of models, often with evocative names like “regression trees,” “random forests,” and “vector support machines.” Usually there is not a way to push the residual standard error through such calculations. But when a confidence interval is needed, the boostrapping method can always be used."
  },
  {
    "objectID": "Reading-notes-lesson-23.html#margin-of-error",
    "href": "Reading-notes-lesson-23.html#margin-of-error",
    "title": "5  Estimating sampling variation from a single sample",
    "section": "5.4 Margin of error",
    "text": "5.4 Margin of error\n\none_trial <- function(n=2) {\n  vals <- rnorm(n)\n  tibble(m = mean(vals), s = sd(vals))\n}\n\nThe confidence interval from each trial will be \\(m \\pm \\beta s\\), where \\(\\beta\\) is a number yet to be determined. How to do so, we want to select \\(\\beta\\) so that, across all trials, 95% will include the mean of the distribution from which the data values were drawn.\n\n# vary beta until 95% of the trials have a left value smaller than zero.\nn <- 10000\nbeta <- 0.02\nTrials <- do(1000) * one_trial(n=n) %>% \n  mutate(left = m - beta*s, right = m + beta*s) \nTrials %>% \n  summarize(coverage = sum(sign(left*right) < 0)/n())\n\n# A tibble: 1 × 1\n  coverage\n     <dbl>\n1    0.967\n\n\nFor sample size \\(n=10\\), \\(\\beta\\) needs to be 0.72, while for a sample size \\(n=100\\), \\(\\beta\\) needs to be 0.20. For \\(n=1000\\), the multiplier needs to be 0.062, and so on. For \\(n=10000\\), the multiplier needs to be 0.02\n\n\n\nn\n\\(\\beta\\)\n\\(t = \\beta / \\sqrt{\\strut n}\\)\n\n\n\n\n10\n0.72\n2.26\n\n\n15\n0.55\n2.14\n\n\n20\n0.47\n2.09\n\n\n50\n0.28\n2.01\n\n\n100\n0.20\n1.98\n\n\n500\n0.088\n1.96\n\n\n1000\n0.062\n1.96\n\n\n10000\n0.20\n1.96\n\n\n\nNotice that as \\(n\\) gets bigger, the size of \\(\\beta\\) to cover 95% of the trials gets smaller. More than a century ago, it was known that the multiplier for any sample size \\(n\\) is effectively \\(2/\\sqrt{n}\\). Consequently, the confidence interval for the mean of \\(n\\) values is approximately\n\\[\\mathtt{CI} = \\mathtt{mean(x)}\\pm \\underbrace{\\frac{2}{\\sqrt{n}} \\mathtt{sd(x)}}_\\text{margin of error}\\]\nThe quantity following the \\(\\pm\\) is called the “margin of error.” Because of the \\(\\pm\\), he overall length of the confidence interval is twice the margin of error.\nIt’s much easier to remember \\(2/\\sqrt{n}\\) than a list of \\(\\beta\\) values that change from one \\(n\\) to the next. Another ubiquitous memory aid involves another technical term, the standard error. This involves a simple re-arrangement of the equation for the confidence interval:\n\\[\\mathtt{CI} = \\mathtt{mean(x)}\\pm 2\\underbrace{\\frac{\\mathtt{sd(x)}}{\\sqrt{n}}} _\\text{standard error}\\]\nIt’s standard in statistical software to report the standard error of a coefficient. Usually abbreviated se or std.error or something similar. The software is doing the divide-by-\\(\\sqrt{n}\\) for you, so all you need to construct the margin of error is multiply the standard error by 2. That’s convenient, but it comes at the cost of yet another use of the words “standard” and “error,” which can be confusing.\nHere’s an example of a typical software output summarizing a model in the format called a “regression report.” Here’s an example, looking at the fuel economy of cars (mpg) as a function the car’s weight (wt) and horsepower (hp).\n\nlm(mpg ~ wt + hp, data = mtcars) %>% \n  regression_summary()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  37.2      1.60        23.3  2.57e-20\n2 wt           -3.88     0.633       -6.13 1.12e- 6\n3 hp           -0.0318   0.00903     -3.52 1.45e- 3\n\n\nAccording to this report, each additional 1000 lbs of weight decreases fuel economy by an estimated 3.9 miles per gallon. But since the model is based on a sample of data, it’s important to report the precision of that number in the face of sampling variation. The confidence interval is the standard format for that precision. It will be the estimate plus-or-minus two times the standard error, that is: \\(-3.88 \\pm 2\\times0.633\\), that is, -5.15 to -2.61 mpg per 1000 lbs. Similarly, each addition horsepower (hp) lowers fuel economy by \\(-0.032 \\pm 2 \\times 0.009\\), that is, -0.05 to 0.013 mpg per horsepower.\nEven more convenient is to calculate the confidence interval with confint() which handles all the computations, including the ones for tiny \\(n\\) described in ?sec-tiny-n.\n\n\n\n\n\n\nHow many digits?\n\n\n\nNotice that the estimate of the wt coefficient in the above regression report is -3.87783074. That seems like an awful lot of digits to report when the confidence interval is -5.15 to -2.61. Or, rather, an awful lot of digits for the human reader.\nIt is of course easy for the human to ignore the last several digits of the number. This makes reading more reliable; there are not as many digits to confuse. Even worse, the many digits suggest a level of precision that is belied by the width of the confidence interval. (When the number is going to be part of a continuing computation, that is, the “reader” is a computer, mis-interpretion or faulty reading is not an issue, which is why the software calculates so many digits .)\nSo how many digits ought to be reported for a human reader? There is an easy procedure to determine this.\n\nLook at the standard error in the regression report and multiply by 2 to get the margin of error. For example, for the hp coefficient, the margin of error is \\(2 \\times 0.63273349 = 1.265467\\).\nIt is always the case that no more than two digits of the margin of error have any meaning. (Even the second digit would suffer sampling variation.) So round the margin of error to two digits, that is 1.3 for the hp standard error.\nNotice the location of the second digit of the rounded standard error. For hp, the second digit is 3 and it’s located in the one-tenths place. Round the coefficient to this place. So, the hp coefficient -3.87783074 will round to -3.9.\nThe confidence interval, formatted for the human reader, will be the rounded coefficient plus-or-minus the rounded standard error. For hp, the confidence interval will be \\(-3.9 \\pm 1.3\\) or -5.2 to -2.6."
  },
  {
    "objectID": "Reading-notes-lesson-23.html#tiny-n-optional",
    "href": "Reading-notes-lesson-23.html#tiny-n-optional",
    "title": "5  Estimating sampling variation from a single sample",
    "section": "5.5 Tiny \\(n\\) (optional)",
    "text": "5.5 Tiny \\(n\\) (optional)\nWhen you have a very small sample size—say, \\(n=2\\)—the values may coincidentally be very close together. Around 1907, William Gosset, a scientist at Guinness, discovered that such coincidences force \\(\\beta\\) to be much larger than \\(2/\\sqrt{n}\\) in order to produce confidence intervals that cover the mean of the data-generating process. Gosset’s particular interest was in making sense of Guinness’s standard testing protocols, which involve averaging the results from three small batches of beer ingredients. Contacting the leading statisticians of the day, Gosset was told that such small \\(n\\) is “brewing, not statistics.” Nonetheless, Gosset had to work within Guinness’s testing protocols, which were indeed brewing but still needed statistical interpretation.\nGosset carried out trials by hand, a large number of measurements from a study of criminals’ hand sizes. (They did this kind of thing in 1900.) Each measurement was written on a card. A trial consisted of drawing \\(n\\) cards from the deck and calculating the mean and standard deviation of the measurements. Using computers, we can simulate the calculation of results from a Gosset-like trials using a simple function that calculates the mean and standard deviation of data from a Gaussian distribution.\n\none_trial <- function(n=2) {\n  vals <- rnorm(n)\n  tibble(m = mean(vals), s = sd(vals))\n}\n\nWe can pick a small \\(n\\) and running many trials using a candidate value for “twice.”\n::: {.callout-warning} ## IN DRAFT\nCONVERT the beta to something named twice.\n\nn=10\nbeta <- 2 / sqrt(n)\nTrials <- do(100) * one_trial(n=n) %>% \n  mutate(left = m - beta*s, right = m + beta*s) \ngf_errorbarh(.index ~ left + right, data = Trials, alpha=0.5) %>%\n  gf_errorbarh(.index ~ left + right, \n               data = Trials %>% filter(left > 0 | right < 0)) %>%\n  gf_vline(xintercept = ~ 0, color=\"blue\", inherit=FALSE)\n\n\n\n\nGosset effectively tabulated the \\(\\beta\\) multipliers\n\n\n\nn\n\\(\\beta\\)\n\\(t = \\beta / \\sqrt{\\strut n}\\)\n\n\n\n\n2\n8.98\n12.7\n\n\n3\n2.48\n4.30\n\n\n4\n1.59\n3.18\n\n\n5\n1.24\n2.78\n\n\n6\n1.04\n2.57\n\n\n7\n0.92\n2.44\n\n\n\\(\\vdots\\)\n\n\n\n\n10\n0.72\n2.26\n\n\n15\n0.55\n2.14\n\n\n20\n0.47\n2.09\n\n\n50\n0.28\n2.01\n\n\n100\n0.20\n1.98\n\n\n500\n0.088\n1.96\n\n\n1000\n0.062\n1.96\n\n\n\nYou can see that for \\(n\\) bigger than 10 or 20, the \\(t\\) multiplier is 2. But for very small \\(n\\), the t-multiplier can be considerably larger.\nYou can see the wisdom of brewers here. They made tests by averaging measurements from three small batches of beer. If they had used only two batches, the confidence interval would be almost three times larger than for \\(n=3\\), making it very hard to conclude anything about whether the tests show the ingredients to be within the quality-control standards.\nGosset’s work was published under the pseudonym “Student,” since Guinness forbade employees to publish under their own names. Statisticians, recognizing the value of the work (and knowing the name behind the pseudonym), came to use the name \\(t\\), perhaps because tea was considered more refined than “beer.” In many statistics texts, you will see the phrase “Student t” to refer to how Gosset’s work is used."
  },
  {
    "objectID": "Reading-notes-lesson-24.html",
    "href": "Reading-notes-lesson-24.html",
    "title": "6  Effect size",
    "section": "",
    "text": "You now have a substantial toolbox for for summarizing data in ways that support statistical thinnking. Time to move to the next step: extracting actionable information from such summarizing. Why the word “actionable” in the previous sentence? Because much of the time the goal of summarizing data is to guide decision making. The setting is that you or your organization have to make a decision: administer a medicine, change a budget, raise or lower a price, respond to an evolving situation, and so on. Decisions ought to be made on an informed basis. Often, the information needed is hidden in tables of data. The statistical thinker knows how to extract information in a form that is as useful as possible to the decision maker.\nSetting for decisions vary widely, but a useful simplification splits support for decision making into two broad categories.\nLess mundane: A patient comes to an urgent-care clinic with symptoms. A decision needs to be made about what disease or illness the patient has in order to guide choices of tests and, in turn, possible treatment. The inputs to the prediction are the symptoms—neck stiffness, a tremor, and so on—as well as facts about the person—age, sex, occupation, etc. The output of the prediction will assign a probability to each of medical conditions that could cause the symptom. As new tests or measurements are done—temperature, blood pressure, white-blood-cell count, blood oxygenation, and others—they become new inputs for the prediction and the probabilities change accordingly. The television drama House provides in every episode an example of such evolving predictions, which clinicians call “differential diagnosis.” The word “prediction” suggests the future, but many predictions have to do with the current or past state that is as yet unknown to greater or lesser extent. Synonyms for “prediction” include “classification” (Lessons 34 and 35), “conjecture”, “guess”, “bet”, …. The phrase “informed guess” points to the idea: using information to support decision making.\nThis lesson is focuses on two ideas that are useful for building and summarizing models of a system for the purposes of intervening in that system: effect size and interaction. We will need some additional concepts and tools in order to bring causality into the picture. This will have to wait until Lessons 28 through 31."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#effect-size-input-to-output",
    "href": "Reading-notes-lesson-24.html#effect-size-input-to-output",
    "title": "6  Effect size",
    "section": "6.1 Effect size: Input to output",
    "text": "6.1 Effect size: Input to output\nIn an intervention you change something about the world. That might be the budget for a program, the dose of a medicine, the fuel input to an engine. The thing you change is the input. In response, something else in the world changes: reading ability of students, the patient’s seratonin levels (a neurotransmitter), the power output from the engine. The thing that changes in response to the change in input is called the “output.” Systems such as education, mental state, or aircraft have many components. The context in which the modeler works dictates which of these components ought to be considered the input and which the output. Usually the input is something that you can directly change; the output is something that changes in response.\nThe effect size is merely a statement of the amount of change in the output with respect to the input. There are two fundamental types of inputs, just as there are two fundamental types of variables:\n\ncategorical: e.g., whether or not a person smokes.\nquantitative: e.g., how many cigarettes per day a person smokes\n\nSimilarly, there are two fundamental types of outputs: categorical or quantitative.\n\ncategorical: e.g. whether the person develops cancer\nquantitative: e.g. the lung capacity of the person\n\nHow you properly describe an effect size depends on types of both the input and the output.\n\n\n\n\n\n\n\n\ninput\noutput\neffect size\n\n\n\n\ncategorical\nquantitative\nthe **amount* by which the output changes when the input changes category\n\n\nquantitative\nquantitative\nthe rate of change in the output with respect to the input. Calculus students will recognize this rate as the partial derivative of the output with respect to the input.\n\n\ncategorical\ncategorical\nthe probability of being in each of the output categories when the input category is changed\n\n\nquantitative\ncategorical\nthe rate of probability of being in each of the output categories per unit of change in the input.\n\n\n\nTerms like “rate of probability” can be confusingly abstract. It helps to have some examples in mind to keep your thinking clear.\nExamples:\n\nSystem: an automobile\n\nSelected input: Gallons of gasoline put in a car’s tank. Quantitative.\nSelected output: How far the car can be driven.\nEffect size will be a rate: miles per gallon.\n\nSystem: an autombile\n\nSelected input: Whether to use a fuel additive the promises high fuel efficiency. Categorical.\nSelected output: Money spent on fuel (or, perhaps, tons of CO_2 emitted). Quantitative.\nEffect size is an amount: Dollars spent (or, tons of CO_2 emitted)\n\nSystem:\n\ninput categorical\noutput quantitative\n\nSystem:\n\ninput categorical\noutput quantitative"
  },
  {
    "objectID": "Reading-notes-lesson-24.html#calculating-an-effect-size",
    "href": "Reading-notes-lesson-24.html#calculating-an-effect-size",
    "title": "6  Effect size",
    "section": "6.2 Calculating an effect size",
    "text": "6.2 Calculating an effect size\nSo long as you keep track of which of the four combinations of input and out are applicable to your case, calculating an effect size is easy. You evaluate the model at two values for the input then collect the two corresponding output values. For instance, you can use the model_eval() function. It takes as arguments the model whose effect size you’re interested in and, optionally, values for some or all of the inputs.\n\nMod <- lm(mpg ~ hp, data=mtcars)\nmodel_eval(Mod, hp=c(100, 150))\n\n   hp  .output     .lwr     .upr\n1 100 23.27603 15.20660 31.34547\n2 150 19.86462 11.85278 27.87645\n\n\nThe column labeled .output shows the model output for the corresponding input values for hp. Here, both the input and the output are quantitative, so the effect size will be a ratio: change in output divided by change in input. In this case:\n\\[\\text{effect size:}\\ \\ \\frac{23.28-19.86}{100-150} = -0.0684\\]\nIt is wise to pay attention to the units of the effect size. Here, the output is mpg, which has units miles-per-gallon. The input has units horsepower, so the units of the effect size are miles gallon-1 horsepower-1. Admittedly, that’s a mouthful of units. But it tells us something simple: A car with 100 additional horsepower will get worse fuel economy, down by 6.8 miles per gallon.\nNotice that the report from model_eval() has additional columns: .lwr and .upr. That s a glue that it is giving both a single-number, “point” estimate (.output.) and a two number interval estimate. We’ll talk about the meaning of the interval in the following section and in Lesson 26.\n\n\n\n\n\n\nIs horsepower the cause?\n\n\n\nIt might seem from the negative sign on the effect size of engine horsepower on fuel economy that a more powerful engine is not as efficient than a less powerful engine at moving the car a given number of miles. That’s a reasonable conclusion. But the statistical thinker always keeps in mind other possibilities. For instance, another factor in fuel economy is the overall weight of the vehicle. A van designed to haul many passengers weighs more than a 2-passenger sporty vehicle. The van needs more horsepower because it is accelerating more weight.\n\n\n\n?fig-four-hp-mpg-dags shows four DAGs, each of which describe a plausible scenario.\n\n\n\n\n\nFigure 6.1: Dag A\n\n\n\n\n\n\n\nFigure 6.2: Dag B\n\n\n\n\n\n\n\nFigure 6.3: Dag C\n\n\n\n\n\n\n\nFigure 6.4: Dag D\n\n\n\n\nIn DAG A, the vehicle’s design weight determines that an engine with high horsepower will be part of the design. The weight is also responsible for the lower fuel economy.\nThe other DAGs describe other scenarios. In DAG C, for instance, the car designers decided to build a muscle car and put in a big engine. The engine itself adds to the vehicle’s weight, and the higher weight determines lower miles per gallon. DAG D expresses a slightly different belief: again the choice to build a muscle car (high hp) influences the weight. But in DAG B, the big engine also directly influences the fuel economy, perhaps because the fuel-to-air ratio of the car, in normal use, is not optimal.\nAs we will see in Lesson 28, to reveal the direct causal link between engine power and fuel economy requires different choices for the model formula depending on which DAG you think might be relevant.\n\n\n\n\n\n\n\n\nExample for LCs: Price of book versus its page count.\n\n\n\nAnother example: Are longer books more expensive? Intuition suggests so, because more editing, paper, printing and shipping goes into making a longer book. We have some data that might be informative, moderndive::amazon_books. We can build a model of, say, list_price versus num_pages. To look at the effect size, let’s compare a 200-page book to a 400-page book.\n\nMod <- lm(list_price ~ num_pages, data = moderndive::amazon_books)\nmodel_eval(Mod, num_pages = c(200, 400))\n\n  num_pages  .output       .lwr     .upr\n1       200 15.82014 -11.636987 43.27726\n2       400 19.79643  -7.637503 47.23037\n\n\nThe longer book costs about 4 dollars more. So the effect size, to judge from this model, is $4 dollars divided by 200 more pages, which comes to 2 cents per page.\n\n\nAnother example: Are hardcovers more expensive than paperbacks? The output is a quantitative variable: price. The input is categorical. In the moderndive::amazon_books data frame the variable hard_paper has levels “P” and “H.” A possible model:\n\nMod <- lm(list_price ~ hard_paper, data = amazon_books)\nmodel_eval(Mod, hard_paper = c(\"P\", \"H\"))\n\n  hard_paper  .output      .lwr     .upr\n1          P 17.13523 -10.62291 44.89338\n2          H 22.39393  -5.46052 50.24839"
  },
  {
    "objectID": "Reading-notes-lesson-24.html#multiple-explanatory-variables",
    "href": "Reading-notes-lesson-24.html#multiple-explanatory-variables",
    "title": "6  Effect size",
    "section": "6.3 Multiple explanatory variables",
    "text": "6.3 Multiple explanatory variables\nWhen a model has more than one explanatory variable, there is a separate effect size for each. To illustrate, let’s consider prices of houses as recorded in the mosaicData::SaratogaHouses data frame, based on house sales in Saratoga County, NY, USA in 2006. We’ll follow a question asked by then-student Candice Corvetti in her Stat 101 class at Williams College: “How much is a fireplace worth?” Response variable: price. Explanatory variable: fireplaces. Since a handful of the houses has multiple fireplaces, we will simplify by filtering out those houses to retain only the ones with a single fireplace or none.\n\nSimplified <- SaratogaHouses %>% \n  filter(fireplaces <= 1)\nMod <- lm(price ~ fireplaces, data = Simplified)\nMod_values <- model_eval(Mod, fireplaces = c(0,1))\nMod_values\n\n  fireplaces  .output       .lwr     .upr\n1          0 174653.3  -751.4332 350058.1\n2          1 235162.9 59783.5404 410542.3\n\nSimplified %>%\n  ggplot(aes(x=fireplaces, y = price)) +\n  geom_jitter() +\n  geom_errorbar(data=Mod_values, aes(ymin=.output, ymax=.output, x = fireplaces), y=NA,\n                color=\"blue\")\n\n\n\n\nFrom the graphic, you can see that houses with a fireplace tend to have higher prices. From the report of the evaluated model, you can calculate the effect size: $235K for a house with a fireplace, $175K for a house without one. This suggests the value of a fireplace is $60K.\nThere are, of course, many other things that determine the price of a house. Real-estate agents famously list the three most important factors as “location, location, and location.” Common sense brings in other explanatory variables: how big the house is, how luxurious, how many bathrooms, and so on. The statistical thinker knows to put any one explanatory variable into the context of other plausable factors.\nFor simplicity, let’s collect all the factors other than fireplaces into a hypothetical variable which we will call “fancy.” Here are three plausible DAGs that plausibly describe an affect of fireplace on price in the context of fancy.\n\n\n\n\n\nFigure 6.5: Dag A\n\n\n\n\n\n\n\nFigure 6.6: Dag B\n\n\n\n\n\n\n\nFigure 6.7: Dag C\n\n\n\n\nIn DAG A, fancy and fireplace both contribute to price, but independently. In DAG B, fireplace directly contributes to price, but whether or not a house has a fireplace depends on the level of fancy. In DAG D, fireplace has no direct affect on price, which is set entirely by fancy. The fireplace variable is just an indicator of fancy.\nWe can’t say from the data alone which of these three DAGs is the closest description of the situation. In Lessons 28, 30, and 31 we will consider how the choice of explanatory variables in a model leads to a faithful or misleading picture of the connections. There you will find out that DAGS A & B both imply that fancy should be an explanatory variable if we want the effect size from the model to represent the direct effect of a fireplace on price. Easy enough to fit that model, … except that we don’t have an actual variable fancy in the SaratogaHouses data frame. To keep things simple for the moment, we will use livingArea—the size of the house—as a rough approximation to the hypothetical fancy.\nThe effect size of fireplaces on price is found by comparing the model output for houses with and without a fireplace, holding the values of all the other explanatory variables constant.\n\nMod2 <- lm(price ~ fireplaces + livingArea, data = Simplified)\nmodel_eval(Mod2, fireplaces = c(0,1), livingArea = 2000)\n\n  fireplaces livingArea  .output     .lwr     .upr\n1          0       2000 234706.4 101212.9 368199.9\n2          1       2000 240420.8 106988.7 373852.8\n\n\nFor a house with living area 2000 feet2, the model output is $235K with no fireplace and $240K with a fireplace, putting the effect size of fireplace on price at $5K. That’s much smaller than the previous model, price ~ fireplace, gave for the effect size. The reason for the difference in results from the two models is that houses with fireplaces tend to be larger in area."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#confidence-intervals",
    "href": "Reading-notes-lesson-24.html#confidence-intervals",
    "title": "6  Effect size",
    "section": "6.4 Confidence intervals",
    "text": "6.4 Confidence intervals\nStatistical thinkers know that any estimate they make, including estimates of effect sizes, are subject to sampling variation. Consequently, an interval estimate should be given. This communicates to the decision maker the uncertainty in the quantity being estimated. Sophisticated decision makes take this uncertainty into account, considering the range of outcomes likely from whatever use they make of an effect size. Statistically naive decision makers—even highly educated descision makers can be statistically naive—look at the interval and will sometimes ask the modeler, “Just give me a number. I don’t know what to do with two numbers.” Such a request might elicit a frank response: “If you don’t know what to do with two numbers, you also won’t know what to do with one number.” That kind of frankness is not often well received; a reasonable alternative is: “The interval indicates the amount of uncertainty in the result. If you would like to reduce the uncertainty, we’ll need to collect more data.” (In Lesson 29 you’ll meet a not-always-available alternative to collecting more data: building a better model!)\nThe appropriate interval estimate for an effect size is called a “confidence interval.” It’s extremely important to keep this name in mind, since there is another kind of interval to quantify uncertainty, called a “prediction interval,” which will be introduced in Lesson’s 25 and 26. Confusing the two kinds of intervals is a serious blunder.\nConfidence interval can be constructed using the same sorts of techniques introduced in Lesson 23. For models that are constructed by adding together different terms, like the price ~ fireplaces + livingArea model of the previous example, the estimated effect size for a given term is the corresponding model coefficient. The confidence interval on that effect size is simply the confidence interval on the coefficient. For example, for fireplaces:\n\nlm(price ~ fireplaces + livingArea, data = Simplified) %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept)  6979.0960 27188.993\nfireplaces  -1521.3683 12950.131\nlivingArea    102.7093   114.913\n\n\nThus, the confidence interval for the effect of a fireplace ranges from negative $1500 to positive $13,000. Broad though this may seem at first, it does carry genuine information. You can be confident that a fireplace alone will not add as much as $50,000 to the price of the house, nor will it cause the house’s value to fall by $10,000.\nThe confidence interval on the livingArea is pretty narrow $103 to $115 per square foot. If you’re looking to save a bit of money by shopping for a slighly smaller house, say 200 square-feet smaller, you can adjust your budget downwards by something in the range of $206,000 to $230,000. The units here come from multiplying the area units (square feet) by the effect size units (dollars per square feet), producing a quantity denominated in dollars.\nIt’s important always to keep in mind that an estimate of an effect size will likely be misleading if your choice of model seriously misrepresents reality. For instance, a salesperson hawking add-on fireplaces might show you results from the “obvious” model price ~ fireplace, leading to an effect size of $52,000 to $69,000, calculated this way.\n\nlm(price ~ fireplaces, data = Simplified) %>% confint()\n\n                2.5 %    97.5 %\n(Intercept) 168209.69 181097.01\nfireplaces   51899.26  69119.92\n\n\nIt would be unfair to say that the $52,000 to $69,000 claim is a lie; it’s entirely consistent with the data. But it relies on a grossly implausible description of the factors that determine house price.\n\n\n\n\n\n\nNote in draft: For the confounding section\n\n\n\nAn idea …\nSuppose the DAG is that fireplaces cause living area (fancy) and that both of these cause price. That’s distinct from DAG C in the above, because the causal arrow from fancy to fireplace is reversed. Could we decide between DAG C and this new DAG. How about the models fireplace ~ livingArea versus fireplace versus fancy plus price."
  },
  {
    "objectID": "Reading-notes-lesson-24.html#interaction",
    "href": "Reading-notes-lesson-24.html#interaction",
    "title": "6  Effect size",
    "section": "6.5 Interaction",
    "text": "6.5 Interaction\nNot all effects are additive."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#wrangling-versus-modeling",
    "href": "Reading-notes-lesson-19.html#wrangling-versus-modeling",
    "title": "1  Preliminaries to Statistical thinking",
    "section": "1.5 Wrangling versus modeling",
    "text": "1.5 Wrangling versus modeling\nThe first half of this course emphasized data wrangling and visualization. When using data wrangling commands, summaries of data frames were computed using, naturally enough, the summarize() function. Typical summaries for quantitative variables include means, medians, standard deviations, etc., each of which applies to one variable at a time. For instance, this command calculates four summary statistics on the net running time recorded in the TenMileRace data frame:\n\nTenMileRace %>%\n  summarize(ave = mean(net), middle = median(net), sd = sd(net), n = n())\n\n       ave middle       sd    n\n1 5599.065   5555 969.6564 8636\n\n\nConstructing such summaries groupwise is a matter of using the group_by() modifier. Here, we calculate summaries broken down by the state of residence of the participant and arranged from fastest (average) running time downward.\n\nTenMileRace %>%\n  group_by(state) %>%\n  summarize(ave = mean(net), middle = median(net), sd = sd(net), n = n()) %>%\n  arrange(ave) %>%\n  head(10)\n\n# A tibble: 10 × 5\n   state       ave middle    sd     n\n   <fct>     <dbl>  <dbl> <dbl> <int>\n 1 Australia 2872   2872    NA      1\n 2 Kenya     2934.  2874.  141.    14\n 3 Lithuania 2961   2961    NA      1\n 4 Japan     2992   2992   132.     2\n 5 Colombia  2998   2998    NA      1\n 6 Ethiopia  3185   3185   356.     2\n 7 EN        3251   3251    NA      1\n 8 Ukraine   3256   3256    NA      1\n 9 Russia    3267   3197   272.     3\n10 Romania   3287.  3297   162.     3\n\n\nWrangling is essential for many statistical purposes, including making graphical displays, cleaning data, and assembling data that comes from multiple sources.\nIn Lessons 11-17, you were introduced to regression modeling. The computing tasks for regression, such as fitting a model with lm(), don’t fit into the wrangling framework. To illustrate:\n\nlm(net ~ age, TenMileRace) \n\n\nCall:\nlm(formula = net ~ age, data = TenMileRace)\n\nCoefficients:\n(Intercept)          age  \n    5297.22         8.19  \n\n\nRegression produces a summary of a data set, somewhat like summarize(). The output of summarize() is always a data frame. The output of lm() is a different kind of thing, as you can see above. In computing lingo, the “kind of thing” is often called the “object class” or “object type.”\nPerhaps the first thing that’s confusing about the object produced by lm() is that they contain much more than what’s printed out when you display them directly on the screen. The printed output is just a glimpse at the object and, almost always, you use another function to present the content of the object in a way that suits your current need. Let’s informally call such functions “extractors.” Here are examples of a few of the extractors that you will be using in the coming lessons; you are not expected at this point to know what each is doing.\n\nlm(net ~ age, TenMileRace) %>% coefficients()\n\n(Intercept)         age \n5297.219248    8.189886 \n\nlm(net ~ age, TenMileRace) %>% rsquared()\n\n[1] 0.008014284\n\nlm(net ~ age, TenMileRace) %>% regression_summary()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  5297.      37.6      141.   0       \n2 age             8.19     0.981      8.35 7.76e-17\n\nlm(net ~ age, TenMileRace) %>% model_eval(skeleton=TRUE)\n\n  age  .output     .lwr     .upr\n1  10 5379.118 3485.069 7273.167\n2  20 5461.017 3567.394 7354.640\n3  30 5542.916 3649.525 7436.307\n4  40 5624.815 3731.460 7518.170\n5  50 5706.714 3813.200 7600.227\n6  60 5788.612 3894.744 7682.480\n7  70 5870.511 3976.094 7764.928\n8  80 5952.410 4057.249 7847.571\n9  90 6034.309 4138.210 7930.408\n\n\nMuch of what you will be learning in the following Lessons concerns such regression extractors: why what they calculate is useful and how to use it. But let’s return to the command patterns you will be seeing.\nRegression and wrangling are allies. You will use wrangling, especially mutate() and filter() to pre-process data before carrying out the regression. For instance, in studying the relationship between age and net running time, you might want to focus on older people.\n\nTenMileRace %>%\n  filter(age > 40) %>%\n  lm(net ~ age, data=.) %>% confint()\n\n                2.5 %     97.5 %\n(Intercept) 4014.7081 4541.71744\nage           22.8315   33.43884\n\n\nIn this example, we’ve used confint() as the extractor, but, depending on our purpose, we might have used any of the others.\nLooking closely at the above command you will notice something new: the data=. argument being used inside lm() . The simple . is doing something important, carrying the output of the earlier stages of the pipeline into the data= argument of lm().\nThe dot (.) has always been available for use when wrangling, but we haven’t needed to use it. For instance, here is an earlier example of a wrangling command translated to use the . notation:\n\nTenMileRace %>%\n  group_by(., state) %>%\n  summarize(., ave = mean(net), middle = median(net), sd = sd(net), n = n()) %>%\n  arrange(., ave) %>%\n  head(., 10)\n\n# A tibble: 10 × 5\n   state       ave middle    sd     n\n   <fct>     <dbl>  <dbl> <dbl> <int>\n 1 Australia 2872   2872    NA      1\n 2 Kenya     2934.  2874.  141.    14\n 3 Lithuania 2961   2961    NA      1\n 4 Japan     2992   2992   132.     2\n 5 Colombia  2998   2998    NA      1\n 6 Ethiopia  3185   3185   356.     2\n 7 EN        3251   3251    NA      1\n 8 Ukraine   3256   3256    NA      1\n 9 Russia    3267   3197   272.     3\n10 Romania   3287.  3297   162.     3\n\n\nThe . always means, use the output of the preceding stages of the pipeline as this argument to the function. All of the data wrangling commands were designed so that the first argument is always a data frame. The phrase %>% group_by(., state) %>% is an explicit direction to place the data coming from the pipeline as the first argument. The pipeline connector, %>%, is arranged by default to put the content it is transmitting as the first argument to the following function. For this reason, . is not needed if the pipeline pumps results into the first argument.\nBut not all functions are designed with this convention in mind. In particular, in lm() the first argument should be the model formula, e.g. net ~ age. The data frame that will be used in fitting the model is the second argument. So, in lm(net ~ age, data=.) the dot is directing the pipeline to empty its data frame into the second argument. Otherwise, by default, the pipeline would be plumbed to force the first argument to be the data frame and demote the net ~ age to the second position."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#grouping",
    "href": "Reading-notes-lesson-19.html#grouping",
    "title": "1  Preliminaries to Statistical thinking",
    "section": "1.6 Grouping",
    "text": "1.6 Grouping\nThe group_by() wrangling function is used whenever you want to treat a data frame in a group-by-group manner. It’s natural to assume that group_by() is slicing up a data frame according to groups. The mutate() and summarize() functions were specifically designed to make it appear that group_by() is doing the slicing. But in reality, group_by() is just adding a kind of tag to the data frame. mutate() and summarize() know to interpret that tag as an instruction to slice up the data when calculating on it.\nMost R functions simply ignore the tag added by group_by(). Consider, for instance, the pipeline\n\nTenMileRace %>%\n  group_by(sex) %>%\n  lm(net ~ age, data=.) %>%\n  coefficients()\n\n(Intercept)         age \n5297.219248    8.189886 \n\n\nThe output is not what you might expect from your earlier experiences using group_by(). Particularly, there is not a separate row for each of the groups defined by sex. Nor is there a column listing the sex. The group_by() has had no effect.\nWhat do you do if you really want to compare groups using regression models? For instance, it might be that you believe that the F group ages differently than the M group. How do you reveal this with regression?\nRegression models have their own, internal system for comparing groups. You express your wish to compare groups by using the model formula. The simple formula net ~ age does not instruct lm() to compare the sexes. Instead, you would use the formula net ~ age*sex. For instance,\n\nTenMileRace %>%\n  lm(net ~ age * sex, data = .) %>%\n  coefficients()\n\n(Intercept)         age        sexM    age:sexM \n5370.999953   15.961661 -785.145163    1.606559 \n\n\nThe output of lm() already contains the comparison information. You don’t yet know how to read and interpret that comparison information, but you will learn.\nThere are extremely good reasons why lm() does things the way it does. It is not at all a matter of software incompatabiility between the wrangling family of commands and the regression family. The lm() paradigm can make much more efficient use of data than group_by(). It also offers much more flexibility. lm() can handle multiple “grouping” variables together and even lets you “group” by quantitative variables. These capabilities are extremely important for extracting relevant information from data, as you will see in the following lessons."
  },
  {
    "objectID": "Reading-notes-lesson-19.html#learning-challenges",
    "href": "Reading-notes-lesson-19.html#learning-challenges",
    "title": "1  Preliminaries to Statistical thinking",
    "section": "1.7 Learning challenges",
    "text": "1.7 Learning challenges\n\nOne of these pipeline commands will work and the other won’t. Which one will work? Explain why the other one doesn’t work.\n\nlm(net ~ age, data = TenMileRace)\nTenMileRace %>% lm(net ~ age)\n\nAn example from the OpenIntro book uses data on promotions. Some data wrangling commands that might be relevant are these:\n\npromotions %>% tally()\n\n# A tibble: 1 × 1\n      n\n  <int>\n1    48\n\npromotions %>% group_by(decision) %>% tally()\n\n# A tibble: 2 × 2\n  decision     n\n  <fct>    <int>\n1 not         13\n2 promoted    35\n\npromotions %>% group_by(gender) %>% tally()\n\n# A tibble: 2 × 2\n  gender     n\n  <fct>  <int>\n1 male      24\n2 female    24\n\npromotions %>% group_by(gender, decision) %>% tally()\n\n# A tibble: 4 × 3\n# Groups:   gender [2]\n  gender decision     n\n  <fct>  <fct>    <int>\n1 male   not          3\n2 male   promoted    21\n3 female not         10\n4 female promoted    14\n\n\n\nYou could use such wrangling to compare groups. For instance, you can use the results of the last command to calculate separately the proportion of males who were promoted and, similarly, the proportion of females.\na. What are those proportions?\nThe following wrangling command will calculate the proportions for you, but it is a bit complicated:\n\npromotions %>%\n  group_by(gender) %>%\n  summarize(prop_promoted = sum(decision==\"promoted\") / n())\n\nb. Use this command to check your calculations in (a).\nc. In the regression paradigm, the comparison of proportions between the two groups is done directly in lm(), like this:\n\npromotions %>%\n  mutate(promoted = zero_one(decision, one=\"promoted\")) %>%\n  lm(promoted ~ gender, data = .) %>%\n  coefficients()\n\n (Intercept) genderfemale \n   0.8750000   -0.2916667 \n\n\nWe’ll explain the purpose of zero_one() in Lesson 19, but putting that matter aside for a moment, compare the two coefficients in the regression model to the proportion results you got from wrangling.\n\nWhat does the value of the intercept coefficient correspond to in the wrangling results?\nWhat does the genderfemale coefficient correspond to in the wrangling results? (Hint: you will have to do a bit of arithmetic on the wrangling results.)"
  }
]
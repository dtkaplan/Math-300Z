---
title: "Calculating a p-value"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
lesson <- 37
source("../_startup.R")
```

Hypothesis tests are very easy to carry out. You don't need to choose a particular hypothesis to test: it's always the Null hypothesis. The test has only two possible conclusions: "rejecting the Null hypothesis" or "failing to reject the Null hypothesis." The conclusion indicated by the test is signalled by a number called a "**p-value**."  

In this Lesson, we will demonstrate a general way to calculate p-values that has a simple logic. Even simpler, model-building software such as `lm()` will do the calculation for you. Often, there will be more than one p-value, each describing a different aspect of the model, so you have to identify which of the p-values is relevant for your purpose. This lesson will show you how. 

Once you have the relevant p-value in hand, reaching the conclusion of the test is not hard, but it is confusing that there are different styles that are used in different journals or in different fields.

We'll start by talking about the conclusion, then move on to how to select the relevant p-value, and finish by showing you the logic(s) behind the calculation of the p-value.

## The conclusion

Remember that there are only two possible conclusions from a hypothesis test: reject or fail to reject the Null hypothesis. The translation between a p-value and a hypothesis-test conclusion can be stated very simply because the p-value is always a number between zero and one.

> *A small p-value points toward rejecting the Null hypothesis. A large p-value points to "failing to reject" the Null.*

The definition of "small" differs from one scientific field to another. Physics proudly declares "small" as something like 0.001. In most other fields, "small" is set at the threshold 0.05. Sometimes, as you'll see in Lesson 38, the threshold for small should be adjusted downward, but for this Lesson we will do what is most common in many fields and use 0.05 as the dividing point between "small" and "not small."

Common sense suggests that the result of a hypothesis test should be to show the p-value. After all, it's easy to see if a number if greater or less than 0.05. But scientific convention says otherwise in part because "hypothesis testing" is a merger of two different traditions with different styles.

In principle, the result of a hypothesis test could be reported very simply either as "reject" or "fail." Understandably, researchers prefer to use a notation that avoids the negative connotations of the everyday words "reject" and "fail." Indeed, "reject" is often a very positive conclusion for a researcher. And "fail to reject" in no way means that the researcher did something wrong.

Possibly the most common notation for the result of a hypothesis test is written $p < 0.05$. Researchers like to show off a small p-value. In order to avoid violating the convention that the number itself is not reported directly, they will hint at the smallness by writing $p < 0.01$ or $p < 0.001$, whichever happens to be justified. 

In some fields the result of the hypothesis test is reported using the `*` character. A single `*` is exactly the same as $p < 0.05$. Vanity being an emotion that scientists share with the rest of us, sometimes `**` is used to mean `p < 0.01` and `***` to mean `p < 0.001`.

Why not just report the p-value itself? One reason is that p-values can be minutely small (e.g. $< 10^{12}$) just because a lot of data is available. 

::: {.callout-note}
## Avoiding "fail"

Nobody likes to summarize their work with the word "fail." And so, when "fail to reject the Null hypothesis" is the correct conclusion, people express this in softer ways.

It's very common for the conclusion "fail to reject the Null" simply not to be reported at all. Historically, and even today, some journals will not accept  for publication a scientific article with the conclusion "fail to reject the Null." 

Consider the situation of a researcher whose years-long project has led to a p-value of 0.07. To soften the blow of "fail to reject," the researcher will report the p-value itself so that the reader can see how close it is to small. In some literatures, you will see language like "tending to significance" instead of "fail to reject." In some fields, research publications will show the notation $p < 0.1$. This also indicates failure to reject the null hypothesis.

Journalists eager to publish reports about scientific work, but facing a p-value that is a little too large, will occasionally qualify their report with this phrase: "... although the work did not reach the rigorous scientific standard for statistical significance."

All of these are dodges. There's nothing "rigorous" about $p < 0.05$ although seems unfair that a researcher who had a plausible idea and did the work to test it honestly does not get to publish that work and receive acknowledgement that they are a hard-working part of the overall scientific enterprise.

:::

## Which p-value is relevant?

The regression-table summary of a model conveniently presents a p-value for each and every coefficient in the model. For instance,

```{r}
lm(time ~ climb + distance + sex, data=Hill_racing) %>%
  regression_summary()
```

Each term in the model has a coefficient, called the "estimate." Like all estimates, that of the coefficient involves uncertainty due to sampling variation. This uncertainty is reported, as we've seen in Lessons 23 and 24, by the standard error. The column "statistic" in the regression table is an intermediate step in computing the final column: the p-value itself.

This example of a regression table shows that p-values can sometimes be very, very small. Such smallness is often mis-interpreted as indicating that a very powerful result has been found. This is simply nonsense, which is why the more dignified notation $p < 0.05$ or `*` is to be preferred.

Which of these p-values is relevant depends on what you are interested in. If your focus is on the effect of the vertical `climb` on the winning time, then you should look at the p-value on the `climb` coefficient. If you're interested in whether the race `distance` is a factor in determining the winning time, use the p-value on distance. The regression report gives p-values for every coefficient because it has no way of determining mathematically which variable is of interest and which variables are "covariates," that is, not of direct interest.

It's important to note that when looking at model coefficients, a simpler report of the confidence interval on the coefficient carries exactly the same information as the comparison $p < 0.05$. Here's that report on the hill racing model:

```{r}
lm(time ~ climb + distance + sex, data=Hill_racing) %>%
  confint()
```

When a confidence interval does not include zero---that's the case for all of the coefficients here---then the p-value is $p < 0.05$. Most statisticians argue that the confidence interval is a better kind of report, since it also is informative about effect size. For instance, the p-value on `climb`, reported as 0, is only about the Null hypothesis. But the confidence interval, here [2.5 to 2.7] seconds per meter, gives insight into how much an extra meter of climb will prolong the race.

Reporting p-values rather than confidence intervals on coefficients has no scientific merit. But it can be hard to change traditions. And, as you'll see in the next section, it's not only model coefficients that have p-values.

## Analysis of variance

Sometimes the interest is more general: Do any of these terms contribute to explaining variation in the response variable? In such situations, the appropriate p-value is one that compares one model to another. This style of p-value---not on the individual coefficients but on model terms---comes from a calculation called "analysis of variance."

::: {.callout-warning}
FILL THIS IN.
:::

## A simple, general-purpose way to calculate a p-value

Since p-values are always given in the regression table and the ANOVA table, it's rarely necessary (except in statistics classes!) to calculate one yourself.

Even so, there is a simple method to calculate a p-value that provides good insight into what a p-value means. We will demonstrate this method so that you can gain that insight, but only in very specialized and esoteric circumstances would anyone use this method in place of reading the p-value from a regression table or ANOVA report.


::: {.callout-warning}
Put shuffling here!
:::

## The hypothesis testing zoo

Zoos are fun and interesting because of the great diversity of animals on display. The person who, looking in on a huge elephant or a long-legged crane or a chimpanze, drones, "Oh, another animal. That's all." 

As statistics developed, early in the 20th century, distinct tests were developed for different kinds of situations. Each such test was given its own name, for example, a "t-test" or a "chi-squared test." Honoring this history, statistics textbooks present hypothesis testing as if each test were a new and novel kind of animal.

In fact, almost all the different tests named in introductory statistics books are really just different manifestations of regression. Regression is to "animal" the way t-test is to "elephant." An important theme in the history of statistics is that out of the diversity of statistical methods, almost all of them are encompassed by one method: regression modeling.

In these Lessons, we've focussed on that one method, rather than introducing all sorts of different formulas and calculations which, in the end, are just special cases of regression. Even so, most people who are taught statistics were never told that all the different methods fit into a single unified framework. Consequently, they use different names for the different methods. To communicate in a world where people learned the old-fashioned names, you have to be able to recognize those names know which regression model they refer to. In the table below, we will use different letters to refer to different kinds of explanatory and response variables.

x and y - an ordinary quantitative variable

group - a categorical variable with multiple ($\geq 3$)  levels.

yesno - a categorical variable with exactly two levels (which can always be encoded as a zero-one quantitative variable)

Model formula | traditional name
--------------|-------------------
`y ~ 1`         | **t-test** on a single mean
`yesno ~ 1`     | **p-test** on a single proportion.
`y ~ yesno`     | **t-test** on the difference between two means
`yesno1 ~ yesno2` | **p-test** on the difference between two proportions
`y ~ x`         | **t-test** on a slope
`y ~ group`     | **ANOVA** test on the difference among the means of multiple groups
`y ~ group1 * group2` | **Two-way ANOVA**
`y ~ x * yesno` | **t-test** on the difference between two slopes. (Note the `*`, indicating interaction)

Another named test, the **z-test**, is a special kind of t-test where you know the variance of a variable without having to calculate it from data. This situation hardly every arises in practice, and mostly it is used as a soft introduction to the t-test.

::: {.callout-note}
## The chi-squared test

Most statistics books include two versions of a test invented around 1900 that deals with counts at different levels of a categorical variable. This chi-squared test is genuinely different from regression. And, in theoretical statistics the chi-squared distribution has an important role to play.

The chi-squared test of independence could be written, in regression notation, as `group1 ~ group2`. But regression does not handle the case of a categorical variable with multiple levels. 

However, in practice the chi-squared test of independence is very hard to interpret except when one or both of the variables has two levels. This is because there is nothing analogous to model coefficients or effect size that comes from the chi-squared test.

The tendency in research, even when `group1` has more than two levels, is to combine groups to produce a `yesno` variable. Chi-squared can be used with the response variable being `yesno` and almost all textbook examples are of this nature.

But for a `yesno` response variable, a superior, more flexible and more informative method is logistic regression. 
:::

ANOVA, which is always a comparison of two models, say `y~1` versus `y~group` involves something called an F-test. For the simpler setting of the t-test, the model `y~yesno`, an F-test can also be done. Which to do, t or F? It turns out that t^2^ is exactly the same as F. 



Statistics textbooks usually include several different settings for "hypothesis tests." I've just pulled a best-selling book off my shelf and find listed the following tests spread across eight chapters occupying about 250 pages.

- hypothesis test on a single proportion
- hypothesis test on the mean of a variable
- hypothesis test on the difference in mean between two groups (with 3 test varieties in this category)
- hypothesis test on the paired difference (meaning, for example, measurements made both before and after)
- hypothesis test on counts of a single categorical variable
- hypothesis test on independence between two categorical variables
- hypothesis test on the slope of a regression line
- hypothesis test on differences among several groups
- hypothesis test on R^2^

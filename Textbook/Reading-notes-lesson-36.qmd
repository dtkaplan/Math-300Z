---
title: "Hypothesis testing"


output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
lesson <- 36
source("../_startup.R")
```

It's important, first of all, to know that a sensible non-technical interpretation of the phrase "hypothesis testing" describes something that is very different from its actual technical meaning. Consider the hypothesis expressed by this sentence: "Birds are the evolutionary descendants of dinosaurs." Like all hypotheses, this one is a statement that might or might not be true. Unlike many hypotheses, it is an interesting and surprising statement. For many people, the reference examples of dinosaurs are heavy, armor plated and bespiked, land-dwelling fighting giants, so different from sweetly singing, gently dashing, lightweight, feathered airborne creatures found in our backyards. Even the names suggest an irreconcilable gap: compare Tyrannosaurus Rex, Stegosaurus, and Triceratops, to jay, finch, lark, swallow, and wren. In general, we test a hypothesis by making a list of things that should be true if the hypothesis were right, for instance, similar bone layout (check), reproduction via eggs (check), nest-building (check), hollow bones (check). A hypothesis test might involve also similar lists of possible facts that contradict the hypothesis; observing such facts is evidence against the hypothesis. Framing and testing hypotheses is a central part of scientific method.

In statistics, a "hypothesis test" has a completely different flavor. First of all, the hypothesis being tested is almost always *uninteresting*, something that we would *not* be surprised at. Second, we don't look for evidence to support the hypothesis or establish its truth. Instead, the conclusion we reach from statistical hypothesis testing can never be that the hypothesis is true or even likely. The allowed conclusions come in only two possibilities: *rejecting* the hypothesis or *failing to reject* the hypothesis. Also strange about a statistical hypothesis test: one person might collect data that lead correctly to the conclusion of *failing to reject* the hypothesis; another person might collect data that are completely compatible with the first person's, yet correctly lead to the opposite conclusion.

With such a strange and counter-intuitive structure, it's understandable that the consensus among statisticians is that few users of statistical hypothesis tests understand the correct interpretation of them. Many statisticians argue that the employment of statistical hypothesis testing in the usual ways has led to a crisis in science and a justifiable lack of trust in published scientific findings.

The three previous paragraphs might suggest that what's called "statistical hypothesis testing" is an odd topic that shouldn't be included in statistics textbooks. Yet, statistical hypothesis testing has for generations been at the center of statistics courses. Using it---even if it's not understood---is a practical necessity for scientists who want to publish their results or apply for research grants based on preliminary research.

Hypothesis testing is a difficult topic for many introductory students. Partly that's because of the large amount of terminology that's involved: Null hypothesis, test statistic, p-value, type-I and type-II errors, significance level. Partly that's because everyday language is being used to mean something that is nothing like the everyday meaning. The word "significant" is particularly abused by statistical hypothesis testing. In contemporary usage, "significant" is synonymous with "important," "consequential," "meaningful," "substantial," "momentous," and so on. Your "significant other" is a person of great importance to you. It's very good news when your doctor reports a "significant improvement" in your friend or relative's condition. In statistical hypothesis testing, "significant" can be entirely consistent with "trivial," "useless," "of no practical importance." It's unclear whether the choice of "significant" in statistics was intended to deceive, but it very often has that effect.

## A standard operating procedure

## What people want to know

Another major reason why statistical hypothesis testing can be difficult to get your head around is that many people have an intuitive idea about what they want to know when testing a hypothesis: whether the stated hypothesis is likely to be true. But statistical hypothesis is expressly designed to avoid making any statement about the probability that a hypothesis might be true or false. There is good reason for this since "truth" is a shakey concept philosophically.

Consider this example of a hypothesis: "Drug X lowers blood pressure." People being so different one from another, a hypothesis like this would not be refuted just because X raised the blood pressure of a person. A better statement might be, "Drug X typically lowers blood pressure." Still better would be a more definite statement, "Drug X typically lowers blood pressure by around 10 mmHg." Whether such a statement is true or not depends on the meaning of "typically" and "by around."

Rather than looking for the truth or falsity of a hypotheses, statistical thinkers focus on a question in this form, "Is the statement that 'Drug X typically lowers blood pressure by around 10 mmHg' consistent with the observed facts?" Suppose, to illustrate, that the facts are the recorded change in blood pressure in 10 patients given drug X.

Consider these measurements of change in blood pressure before and after administration of the drug: -5, -1, +7, -15, -3, -6, +1, -8, +2, 0

We're seeing a reduction (a negative number) in most of the patients. The numbers are near -10, even if they are not exactly -10 all the time. When there's an increase in blood pressure, it's small.

In contrast, suppose the numbers were 5, 1, -7, 15, 3, 6, -1, 8, -2, 0. These number are inconsistent with the claim that the typical change is -10. Most of them are positive, sometimes by a lot. Of the negative numbers, none of them even reaches -10.

It would be better to have a quantitative way to measure "consistency with the observed facts." Two changes to the way we frame hypotheses will help.

1.  Be more specific about "typical" and "by around." For instance, here's a very definite hypothesis: "In a group of patients with such-and-such condition, drug X lowers blood pressure by an average of -10 mmHg with a standard deviation of 7mmHg."

::: callout-note
## Demonstration: Likelihood of the facts given the hypothesis

With such a statement we can actually do some arithmetic to find a numeric measure of consistency: something very much like the probability of seeing the observed data under the assumption that the stated hypothesis is true. For instance, the relative probability of seeing -5 from a normal distribution with mean -10 and standard deviation 7 is easily calculated in R:

```{r}
dnorm(-5, mean = -10, sd = 7)
```

We can do a similar calculation for each of the "facts."

```{r}
facts <- c(-5, -1, +7, -15, -3, -6, +1, -8, +2, 0)
dnorm(facts, mean = -10, sd = 7)
```

But what we really want is the probability not of each of the facts but of all of them put together. This is the product of the individual relative probabilities:

```{r}
dnorm(facts, mean = -10, sd = 7) %>% prod()
```

This number, $5.9 \times 10^{-17}$, is called the "likelihood" of the observed facts. Obviously it is very small, but that is not an indication that the facts are unlikely under the assumption that the hypothesis is true. The smallness is a consequence of the fact that *any* particular number is an unlikely outcome of a draw from a continuous probability density. (For those familiar with calculus, the numbers calculated by `dnorm()` are not actually probabilities, they are probability densities, which we've casually called "relative probabilities.")
:::

How to interpret a likelihood number like $5.9 \times 10^{-17}$? In order to make sense of it, we need to add another component to our measurement of "consistency with the observed facts."

2.  Have one or more additional hypotheses so that we can compare likelihoods one to another. For instance, we might use the hypothesis, "In a group of patients with such-and-such condition, drug X lowers blood pressure by an average of 0 mmHg with a standard deviation of 7mmHg."

    A hypothesis like this, that stipulates zero change (on average), is called a "null hypothesis," the word "null" meaning "nothing" or "zero."

    The null hypothesis may not be of direct interest, but it provides a way for us to interpret likelihoods from the hypotheses of actual interest. We do this by comparing the numerical value of the likelihoods, often as a ratio.

::: callout-note
## The likelihood of the null hypothesis

Calculating the likelihood of the observed facts under the assumption that the null is true is done in the same way as we did for our original hypotheses. The only change is to use a mean of zero with `dnorm()`.

```{r}
dnorm(facts, mean = 0, sd = 7) %>% prod()
```
:::

The likelihoods of the two hypotheses are:

-   Hypothesis: Mean change of -10 mmHg: $5.9 \times 10^{-17}$

-   Null hypothesis: Mean change of 0 mmHg: $5.3 \times 10^{-15}$

Taking the ratio of likelihoods gives 0.01. From this, we conclude that the original hypothesis is only 1% as likely as the null hypotheses. In other words, the original hypothesis is not very likely given the observed facts.

## Digression: Likelihood, sensitivity, and specificity

Recall our discussion of classifiers in Lessons 34 and 35. There we defined two terms:

-   Sensitivity: The probability of a + test given that the patient has the disease.

-   Specificity: The probability of a - test given that the patient does not have the disease.

Both of these are actually likelihoods: the probability of a possible observation for a given condition of the world.

$p(D | +) = p(+ | D) p(D)$ Probability is sensitivity times prevalence

Or do probability ratio: $\frac{p(D|+)}{p(H|+)} = \frac{p(+|D) p(D)}{p(+|H)p(H)}$ the probability of the state of the world given the observed data (a + or - test). Here, $p(H) = 1-p(D)$ and $p(+|H) = 1 - p(-|H)$

We could turn the likelihood ratio into a probability ratio, but we need a "prior" to do so.

## What makes hypothesis testing different?

1.  No prior is allowed. Your subjective beliefs shouldn't influence your result.
2.  There's no definite hypothesis other than the null hypothesis. This means that all calculations need to be done based on the likelihood of the null hypothesis. But with nothing to compare the likelihood to, we have a problem. Established solution: Compare the total likelihood over all facts that are "more extreme" than the observed facts.

## The Null hypothesis as a DAG

Zero out any inputs to the node in question.

A DAG is a kind of hypothesis, a statement about the world that might or might not be true.

So far, we've used only the first three columns of the regression report: the name of the term to which the remaining entries belong, the estimate of the coefficient on that term, and the standard error of that estimate.

```{r}
lm(mpg ~ wt + hp, data = mtcars) %>% 
  regression_summary()
```

There are two more columns to go. The fourth column is labelled "**statistic**" (short for "**test statistic**") and the fifth column is the "**p-value**."

It's the p-value that concerns us here, the "statistic" is just an intermediate on the way to calculating the p-value. Both are reported because, in some fields people are accustomed to reading the statistic to draw quick conclusions. But in every field, the p-value is used.

The p-value is at once incredibly simple to interpret and impossibly difficult to make sense of. This contradiction is the reason many statisticians have called for moving away from the p-value as a summary of a result. We will discuss the reasons for the controversy in Lesson 38. In Lesson 37, we'll show how the p-value is computed from the test statistic and introduce another report summarizing a model, the "**ANOVA report**," which is useful for many purposes.

In this Lesson, we'll explain the "incredibly simple" interpretation of the p-value and the subtle logic behind it. This is important because frequently (pretty close to "always") people mistake the p-value as addressing a completely different question than the question it actually pertains to. It's useful to know about this misconception, because it points to a different question that often more directly addresses the needs of researchers and decision makers.

## "Incredibly simple" interpretation

As you will see, the p-value is always a number between zero and one. When the p-value is small, the conclusion is that the corresponding explanatory variable is contributing to explaining the variation in the response variable. That is, a p-value that's small is justification for believing that there is a connection of some sort between the explanatory variable and the response variable.

"Small" in the phrase "when the p-value is small" is most usually taken to mean $p < 0.05$. But different fields have different standards for defining small. For instance, it's common in psychology to consider $p < 0.10$ as fairly small, while in physics, "small" means perhaps $p < 0.001$ or even $p < 0.000001$.

It may seem odd that there is no universal agreement about "small." The reason is that p-values are part of a *standard operating procedure* for evaluating research results to know if they are worthy of publication.

In physics, laws and models are meant to be exact or close to exact. Lord Rutherford (1871-1935), an important physicist who won the Nobel prize in 1908, famously disparaged the use of statistics, reportedly saying, "If your experiment needs statistics, you should have done a better experiment." This was in an era where the p-value *standard operating procedure* had not yet been invented. Today, when p-values are common in most fields, Rutherford's distaste for statistical method is reflected in p-value thresholds like $p < 0.000001$.

In other fields such as economics or psychology or clinical medicine, models are sought that are *useful* but without any expectation that they be exact. (In the 19th and early 20th century, psychologists and economists sometimes used the vocabulary of "law" to describe their findings, but "model" is more appropriate, because, unlike physics, the laws are not strictly enforced!) Often, in economics or psychology or medicine, the size of a sample used to train a model is less than, say, $n=100$. And the units of observation---people or countries, for instance---are different one from the other, quite unlike, say, electrons, which are all the same. Consequently, sampling variation is often an important source of noise, obscuring relationships or even suggesting relationships that are not really there. (See Lesson 31.) This situation---small sample size, variation in observational units, and large sampling variation---would cause many useful findings to go unreported, as would happen if $p < 0.000001$ were the standard. So a less stringent threshold for publication is used, most commonly 0.05.

## What is a p-value?

A p-value is the result of a calculation based on data, but also involving a special hypothesis, called the "Null hypothesis." The Null hypothesis is almost always a statement in line with the claim that "there is no relationship between these variables" or "nothing is going on." For example, in a study about the effectiveness of a new drug, the Null hypothesis will be that the drug has no effect at all. Another example: In an economics study about the possible relationship between a country's "corruption index" and interest rates, the Null hypothesis would be "corruption is unrelated to interest rates."

Perhaps it is helpful to envision the Null hypothesis as the belief of a skeptical devil, standing on the researcher's shoulder and constantly whispering to the research that, "this study is useless, a waste of time, the result purely of sampling variation." Note that in a world where researchers always took the devil's advice, no study would be done. What motivates a researcher is a belief that the study will indeed produce results that are useful and represent something about the real world other than sampling variation, e.g. a relationship between two variables.

The calculation that results in a p-value is done under the assumption that the devil is right. The point is to see if the data are consistent with the devil's skeptical position. If they are consistent, then the p-value will be large. If not, the p-value will be "small."

The format of a p-value is that of a **conditional probability**. The condition is that the devil is right. The probability is that of seeing what the data analysis shows---typically summarized as a model coefficient---if the devil were right.

Actually, the probability reported in the p-value is not that of seeing the exact value of the model coefficient shown in the regression report. The probability also includes the events where the coefficient was larger in magnitude than the coefficient. Why? Because larger coefficients are stronger evidence that the devil is not right.

## The world of the Null hypothesis

Recall that the Null hypothesis is the claim that "nothing is going on." For a regression model, this amounts to saying that "there is no relationship between an explanatory variable and the response variable." In order to help clarify the description in the previous section, let's do an example calculation of a p-value. We will use for the example the possible relationship between a car's fuel economy (`mpg`) and the maximum horsepower (`hp`) of the engine.

A skeptic, such as the imaginary devil from the previous section, might argue this way: "The maximum horsepower is hardly ever used by a car. Instead, the driver throttles the engine so that it generates only that power needed to move the car along under the current conditions: acceleration, speed, wind, slope of the road. The maximum horsepower just affects the range of conditions under which the car can operate. But the fuel economy is based on a standard set of conditions which is the same for every car, regardless of the horsepower."

We will pick up the action at the point where the study has been designed and the design implemented to produce data. For the example, we have the `mtcars` data frame in hand. As should be familiar at this point, the data are modeled and the model coefficient on the explanatory variable of interest is recorded. Looking at the regression report presented at the beginning of this Lesson, that coefficient is -0.0318 mpg/horsepower.

The data were collected in the real world, but that is not the world that's relevant to the Null hypothesis. The world of the Null hypothesis is one where fuel economy is utterly unrelated to horsepower. To calculate the p-value, we construct a simulation of the Null-hypothesis world. But it is not sufficient for the simulation to generate Null-hypothesis data out of the blue. We want the simulation to be as much like the actual data as possible, except that there is no relationship between `mpg` and `hp`.

Perhaps surprisingly, there is a very simple device for accomplishing this. It involves creating a new variable to use in place of `hp` in the model, but which is unrelated to `mpg`. Let's call this new variable `hp_null`. We can generate `hp_null` by taking the values in `hp` and shuffling them. This randomized version of `hp` has no relationship to `mpg` because it is being dealt out to each row of the data frame at random.

Here's what the shuffling looks like, pretending for readability that there wee only ten rows in `mtcars`. `r set.seed(111)`

```{r}
Samp <- mtcars %>% 
  select(mpg, wt, hp) %>%
  sample_n(size=10) %>%
  mutate(hp_null = shuffle(hp))
Samp
```

We'll use such data, replacing the actual `hp` with the shuffled `hp`, to find the model coefficient on `hp`. This can be done concisely:

```{r}
set.seed(112)
lm(mpg ~ wt + shuffle(hp), data = mtcars) %>%
  regression_summary()
```

In this trial, the coefficient on the shuffled `hp` is 0.0120. Of course the coefficient might well be different if the trial were repeated. Let's run 1000 trials, from each of which we'll extract the coefficient on the shuffled `hp`.

```{r}
Trials <- do(1000) * { 
  lm(mpg ~ wt + shuffle(hp), data = mtcars) %>%
  regression_summary() %>%
  filter(term == "shuffle(hp)") %>%
  select(estimate)
}
```

@fig-mpg-null-trials shows the distribution of the shuffled `hp` coefficient, compared to the coefficient we found from the original, unshuffled data.

```{r}
#| label: fig-mpg-null-trials
#| fig-cap: "The sampling distribution of the shuffled `hp` coefficient"
gf_jitter(estimate ~ 1, data = Trials, alpha=0.3, width=0.3) %>%
  gf_violin(color=NA, fill="blue", alpha=0.5, width=0.1) %>%
  gf_lims(x=c(0,2)) %>%
  gf_hline(yintercept = ~ -0.03177295, color="red")
```

You can see in @fig-mpg-null-trials that the coefficient on the shuffled `hp` is near zero, as would be expected since we enforced `hp_null` to be unrelated to `mpg`. Almost always, the coefficients on `hp_null` are in the interval $\pm 0.02$. That is to say, even though `hp_null` is unrelated to `mpg`, sampling variation will spread out the estimated coefficient away from the ideal of zero. The amount of spread due to sampling variation is $\pm 0.02$.

The estimated coefficient on `hp` in the original, unshuffled data is shown as a red horizontal line. You can see that this is farther from zero than any of the null-hypothesis trials. Since there were 1000 trials, the extreme nature of the coefficient from the original data let's us eyeball the probability of that coefficient (or larger) coming out of the Null hypothesis simulation is something on the order of one-in-a-thousand. A detailed calculation---refer to the regression table at the start of this Lesson---puts that probability at $p = 0.0015$.

## What to conclude?

Remember always that the p-value is a probability calculated in a **hypothetical world** the world of the Null hypothesis. In the calculation, we are able to place the data in this hypothetical world by shuffling the explanatory variable.

No calculation done in the Null hypothesis world is going to tell us whether that hypothesis is correct or not. Nonetheless, that the Null hypothesis simulation did not generate a coefficient as large as that in the actual data suggests that the data are inconsistent with the Null hypothesis, that we can in the case of the `hp` coefficient regard the Null hypothesis as an implausible candidate to account for the data.

Almost always, newcomers to this p-value based scheme of hypothesis testing misinterpret the p-value to be the probability that the Null hypothesis is right. Small p-value would thus mean a small probability that the Null is right.

But suppose we want to do a calculation to produce something in the format "the probability that the Null is right?" The probability that decision-makers are usually interested in is the relative conditional probabilities for each of a set of hypotheses of interest. The "condition" under which these probabilities are calculated is, "*given the data at hand*." Returning to the notation of Lesson 34, this is $p(H | \text{data})$, where $H$ stands for each of the hypotheses of interest, say, that a drug has a large effect, a medium effect, no effect at all, or even a negative effect. The framework for calculation is called "**Bayesian**" statistics, the ideas of which date from the very beginnings of the emergence of statistical method.

To illustrate the Bayesian approach, return to Lesson 35 when we were evaluating the performance of classifiers. There, we had two hypotheses that were relevant. In the context of health, these might be $H_\text{sick}$ and $H_\text{healthy}$. The quantity of ultimate interest to the patient is p(sick given the test result). To calculate this probability we need to take a round-about route. We first find two completely different probabilities: p(test result given sick) and p(test result given healthy). In practice, these two probabilities are accessible: take a group of sick patients and see what fraction of them have positive tests, and take a different group of people who are healthy and see what fraction of that group have positive tests. With those two probabilities in hand, we take an estimate of the **prevalence** of sickness: p(sick). Then the probability of clinical interest, p(sick given test result) can be calculated using the Bayesian formula, just as we did in Lesson 35.

In contrast, the p-value is a probability in a different format: $p(\text{summary(data)} | H_0)$. Here, $H_0$, the Null hypothesis, is indeed a specific hypothesis, but not any hypothesis that motivates the research. The quantity "summary(data)" is a particular summary computed from the data, say the sample mean or a regression coefficient.

The p-value probability is bound to be confusing on first sight (and, for most people, on second, third, and later sightings). After all, we know exactly what is the "summary(data)"; we just calculated it from the data! The probability of "summary(data)" is therefore 1, at least until you understand what is the event being summarized by the p-value probability.

For the p-value, the random event that lies behind the probability is a number generated by a process: Go to the world of the Null hypothesis, that boring world of "nothing happening" or "no relationship between variables." @fig-four-planets lays out the different worlds involved in statistical inference using the metaphor of planets.

::: {#fig-four-planets layout-ncol="4"}
![Planet Alt](www/planet-alt.png){#fig-planet-alt}

![The real world](www/planet-earth.png){#fig-planet-earth}

![Planet Sample](www/planet-sample.png){#fig-planet-sample}

![Planet Null](www/planet-null.png){#fig-planet-null}

The four planets of the statistical solar system.
:::

What motivates the work of collecting and modeling data is a hypothesis about the world. Typically, such hypotheses are simplistic, cartoon-like ideas about the shape of things.

Naturally, our ultimate interest is in the real world. But we don't have the whole Earth at hand; we have only a sample from it. The sample is something like the real world, but being a sample it is somewhat patchy, assembled from the $n$ cases in our sample. Planet Sample lacks the detail of the real world, but each point on Planet Sample comes from a genuine place on Planet Earth.

The p-value is a probability computed on Planet Null, that boring world where nothing is going on and any perceived patterns are illusions, the appearance produced by random and shifting gusts of the winds of chance.

Almost all the work of calculating a p-value takes place on Planet Null. That work consists of simulation trials. Each trial involves taking a sample from Planet Null, summarizing it, and recording the result for later comparison the the summary calculated from Planet Sample.

It may seem perverse to base conclusions for real-world data on an imagined planet of no direct interest. And it is! At a minimum, we should put into competition at least two hypotheses: for instance Planet Alt and Planet Null. But in the world of the first half of the 20th century, when statistical analysis of data was just coming into the mainstream, it was impractical to compute the competing probabilities of the Bayesian style of reasoning. The reason: the computers and algorithms we use now had not been invented.

In addition, those early statisticians put a big premium on what they called "objectivity." They did not think the subjective beliefs of researchers---the cartoon alternative hypothesis---should play any role in data analysis. The method they ended up inventing, p-values, was based only on a hypothesis that everyone could agree might be in play: the Null hypothesis. Unfortunately, the only valid conclusions that can be drawn from p-values are 1) "reject the Null hypothesis" and 2) "fail to reject the Null hypothesis." These conclusions don't guide us to favor any other particular hypothesis and so are inadequate to support decision-making in the real world. But the p-value conclusions can be the basis for a standard operating procedure: If the conclusion is "fail to reject the Null hypothesis," don't allow the work to be published.

So, standard operating procedures were based on the tools at hand. We will return to the mismatch between hypothesis testing and the contemporary world in Lesson 38.

## More metaphors?

Use this from Section 19.4 of *Computational Probability and Statistics*?

> A US court considers two possible claims about a defendant: she is either innocent or guilty. Imagine you are the prosecutor. If we set these claims up in a hypothesis framework, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. Your job as the prosecutor is to use evidence to demonstrate to the jury that the alternative hypothesis is the reasonable conclusion.

> The jury considers whether the evidence under the null hypothesis, innocence, is so convincing (strong) that there is no reasonable doubt regarding the person's guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis).

> Jurors examine the evidence under the assumption of innocence to see whether the evidence is so unlikely that it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person's innocence. They are simply not convinced of the alternative that the person is guilty.

> This is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.

> There are two types of mistakes possible in this scenario, letting a guilty person go free and sending an innocent person to jail. The criteria for making the decision, reasonable doubt, establishes the likelihood of those errors.

> Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.

> There are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized below.

::: {.content-visible when-format="html"}

$$
\begin{array}{cc|cc} & & \textbf{Test Conclusion} &\\ 
& & \text{do not reject } H_0 &  \text{reject } H_0 \text{ in favor of }H_A  \\
\textbf{Truth} & \hline H_0 \text{ true} & \text{Correct Decision} &  \text{Type 1 Error}  \\
& H_A \text{true} & \text{Type 2 Error} & \text{Correct Decision}  \\
\end{array} 
$$
:::

::: {.content-visible when-format="pdf"}

Make this table nicer by constructing it in some other system.

. | do not reject H_0_ | reject H_0_ in favor of H_A_
------|-----------------|-----------------
H_0_ true | Correct decision | Type 1 error
H_A_ true | Type 2 error | Correct decision
 

:::

> A **Type 1 error**, also called a **false positive**, is rejecting the null hypothesis when $H_0$ is actually true. Since we rejected the null hypothesis in the gender discrimination (from the Case Study) and the commercial length studies, it is possible that we made a Type 1 error in one or both of those studies. A **Type 2 error**, also called a **false negative**, is failing to reject the null hypothesis when the alternative is actually true. A Type 2 error was not possible in the gender discrimination or commercial length studies because we rejected the null hypothesis.

::: callout-warning
## In DRAFT

Recast the previous paragraph to tie it to classifiers. Point out that in a hypothesis test, unlike a court, we never "accept the Null hypothesis." Neither is there any definite notion of "true," since neither the Null nor the Alternative are strictly speaking correct: they are both models of the world.
:::

---
title: "Constructing a classifier"
author: "Prof. Danny Kaplan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
lesson <- 34
source("../_startup.R")
```

We all face many yes/no situations. A patient has a disease or does not. A credit card transaction is genuine or fraudulent. A **classifier** is a statistical model designed to *predict* the unknown outcome of a yes/no situation from information that is already available. 


## Case-control study design

Consider this news report:

> Higher vitamin D intake has been associated with a significantly reduced risk of pancreatic cancer, according to a study released last week.
> Researchers combined data from two prospective studies that included 46,771 men ages 40 to 75 and 75,427 women ages 38 to 65.  They identified 365 cases of pancreatic cancer over 16 years.
> Before their cancer was detected, subjects filled out dietary questionnaires, including information on vitamin supplements, and researchers calculated vitamin D intake.  After statistically adjusting for [that is, holding constant]  age, smoking, level of physical activity, intake of calcium and retinol and other factors, the association between vitamin D intake and reduced risk of pancreatic cancer was still significant.
> Compared with people who consumed less than 150 units of vitamin D a day, those who consumed more than 600 units reduced their risk by 41 percent. - New York Times, 19 Sept. 2006, p. D6.

There are more than 125,000 cases in this study, but only 365 of them developed pancreatic cancer.  If those 365 cases had been scattered around dozens or hundreds of groups and analyzed separately, there would be so little data in each group that no pattern would be discernible.

## School spending example



## Example: Covariates and context in educational outcomes 

To illustrate how covariates set context, consider an issue of interest to public policy-makers in many societies: How much money to spend on children's education? In the United States, for instance, educational budget policy is set mainly on a state-by-state level. State lawmakers are understandably concerned with the quality of the public education provided, but they also have other concerns and constraints and constituencies who give budget priority to other matters. 

In evaluating the various trade-offs they face, lawmakers would be helped by knowing how increased educational spending will shape educational outcomes. What can available data tell us? Unfortunately, there are various political constraints that work against states adopting and publishing data on a common measure of genuine educational outcome. Instead, we have high-school graduation rates, student grades, etc. These have some genuine meaning but also can reflect the way the system is gamed by administrators and teachers and which cannot be easily compared across states. At a national level, we have college admissions tests such as the ACT and SAT. Perhaps because these tests are administered by private organizations and not state governments, it's possible to gather data on test-score outcomes on a state-by-state basis and collate these with public spending information.

@fig-sat-1 shows average SAT score in 2010 in each state versus expenditures per pupil in public elementary and secondary schools. Laid on top of the data is a flexible linear model (and its confidence band) of SAT score versus expenditure. The overall impression given by the model is that the relationship is negative, with lower expenditures corresponding to higher SAT scores. But the confidence bands are broad and it is possible to find a smooth path through the confidence band that has almost zero slope. Either way, the conventional wisdom that higher spending produces better school outcomes is not supported by this graph.

```{r echo = FALSE}
#| label: fig-sat-1
#| fig-cap: "State by state data (from 2010) on average score on the SAT college admissions test and expenditures for public education."
data(SAT_2010, package = "mdsr")
mod_1 <- lm(total ~ ns(expenditure, 2), data = SAT_2010)
mod_plot(mod_1, interval = "confidence") %>%
  gf_jitter(total ~ expenditure,  alpha = 0.5, data = SAT_2010, height = 0, width = 0.25) %>%
  gf_labs(y = "Average SAT score", x = "Public school expenditures per pupil ($1000s)")
```

There are other factors that play a role in shaping education outcomes: poverty levels, parental education, how the educational money is spent (higher pay for teachers or smaller class sizes? administrative bloat?), and so on. Modeling educational outcomes solely by expenditures ignores these other factors. 

At first glance, it's tempting to ignore these additional factors. We may not have data on them. And insofar as our interest is in understanding the relationship between expenditures and education outcomes, we are not directly concerned with the additional factors. This lack of direct concern, however, doesn't imply that we should totally ignore them but that we should do what we can to "hold them constant".

To illustrate, let's consider a factor on which we do have data: the fraction of eligible students (those in their last year of high school) who actually take the test. This varies widely from state to state. In a poor state where few students go to college the fraction can be very small (Alabama 8%, Arkansas 5%, Mississippi 4%, Louisiana 8%). In some states, the large majority of students take the SAT (Maine 93%, Massachusetts 89%, New York 89%). In states with low SAT participation rates, the students who do take the test are applying to schools with competitive admissions. Such strong students can be expected to be get high scores. In contrast, the scores in  states with high participation rates reflect both strong and weak students; they will be lower on average than in the low-participation states. 


Putting the relationship between expenditure and SAT scores in the context of the fraction taking the SAT can be done by using fraction as a co-variate, that is, building the model `SAT ~ expenditure + fraction` rather than just `SAT ~ expenditure`. @fig-sat-3) shows a model with fraction taken into account. 

```{r echo = FALSE}
#| label: fig-sat-3
#| fig-cap: "The model of SAT score versus expenditures, including as a covariate the fraction of eligible students in the state who take the SAT."
mod <- lm(total ~ ns(expenditure,2) * sat_pct, data = SAT_2010)
mod_plot(mod, interval = "confidence") %>%
  gf_labs(y = "Average SAT score", x = "Public school expenditures per pupil ($1000s)")
```

Note that the effect size of spending on SAT scores is positive when the expenditure level is less than $10,000 per pupil. And notice that when the fraction taking the SAT is near 0, the average scores don't depend on expenditure. This suggests that among elite students, expenditure doesn't make a discernable difference: it's the students, not the schools that matter.

The relationship shown in @fig-sat-1 is genuine. So is the very different relationship seen in @fig-sat-3. How can the same data be consistent with two utterly different displays? The answer, perhaps unexpectedly, has to do with the connections among the explanatory variables. Whatever the relationship between each individual explanatory variable and the response variable, the *appearance* of that relationship will depend on how explanatory variables are connected to each other.

## Connections among explanatory variables

To demonstrate that the apparent relationship between an explanatory variable and a response variable -- for instance, school expenditures and education outcomes -- depends on the connections of the explanatory variable with other explanatory variables, let's move away from the controversies of political issues and study some systems where everyone can agree exactly how the variables are connected. We'll look at data produced by simulations where we specify exactly what the connections are.

A simulation implements a hypothesis: a statement about that might or might not be true about the real world. As a starting point for our simulation, let's imagine that education outcomes increase with school expenditures in a very simple way: each $1000 increase in school expenditures per pupil results in an average increase of 10 points in the SAT score: an effect size of 0.01 points per dollar. Thus, the imagined relationship is:

$$\mbox{sat} = 1100 + 0.01 * \mbox{dollar expenditure}$$

Let's also imagine that the fraction of students taking the SAT test also influences the average test score with an effect size of -4 sat points per percentage point. Adding this effect into the simulation leads to an imagined relationship of 

$$\mbox{sat} = 1100 + 0.01 * \mbox{dollar expenditure} - 4 * \mbox{participation percentage} .$$

And, of course, there are other factors, but we'll treat their effect as random with a typical size of $\pm$ 50 points.

To complete the simulation, we'll need to set values for dollar expenditures and participation percentage. We'll let the dollar expenditures vary randomly from $7000 to $18,000 from one state to another and the participation percentage vary randomly from 1 to 100 percentage points.

Notice that in this simulation, both participation percentage and expenditures affect education outcomes, but there is no connection at all between the two explanatory variables. That is, the graphical causal network is that shown in Figure \@ref(fig:school-sim-1).

```{r}
#| label: fig-school-sim-1
#| fig-cap: "A graphical causal network relating expenditures, participation percentage, and education outcome, where there is no connection between expenditures and participation."
dag_school1
dag_draw(dag_school1)
```

We can generate simulated data and use the data to train models. @fig-school-data-1 shows the data and two different models.

```{r}
#| label: fig-school-data-1
#| fig-cap: "Data and models of the relationship between expenditures and education outcomes from a simulation in which expenditures and participation rate are unconnected as in @fig-school-sim-1.\n- (a) The model `outcome ~ expenditure` \n- (b) The model with participation as a covariate: `outcome ~ expenditure + participation` \nBoth models (a) and (b) show the same effect size for outcome with respect to expenditure."
Dat1 <- sample(dag_school1, size=500)
mod1_1 <- lm(outcome ~ ns(expenditure,2), data = Dat1)
mod1_2 <- lm(outcome ~ ns(expenditure,2) * participation, data = Dat1)
mod_plot(mod1_1, interval="prediction") %>%
  gf_point(outcome ~ expenditure, data = Dat1)
mod_plot(mod1_2, interval="prediction") %>%
  gf_point(outcome ~ expenditure, alpha=~participation, data = Dat1, inherit=FALSE)
```



The relationship between outcome and expenditure can be quantified by the effect size, which appears as the slope of the function. 
You can see that when the explanatory variables are unconnected, as in @fig-school-sim-1, the functions have the same slope.

Now consider a somewhat different simulation. Rather than expenditures and participation being unconnected (as in the causal diagram shown in @fig-school-sim-1), in this new situation we will posit a connection between the two explanatory variables. We'll image that there is some broad factor, labeled "culture" in @fig-school-sim-2, that influences both the amount of expenditure and the participation in the tests used to measure education outcome. For instance, "culture" might be the importance that the community places on education or the wealth of the community.

```{r}
#| label: fig-school-sim2
#| fig-cap: "A DAG for school outcomes that links `participation` and `expenditure` as a function of `culture`."
dag_school2
dag_draw(dag_school2)
```

Again, using data from this simulation, we can train models:

* (a) `outcome ~ expenditures`, which has no covariates.
* (b) `outcome ~ expenditures + participation`, which includes participation as a covariate. 

@fig-school-data-2 shows the data from the new simulation (which is the same in both subplots) and the form of the function trained on the data. Now model (a) shows a very different relationship between expenditures and outcome than model (b). 


```{r school-data2, echo = FALSE, fig.cap = "(ref:school-data-2-cap)", fig.fullwidth = TRUE, fig.show = "hold", out.width = "50%"}
#| label: fig-school-data2
#| fig-cap: "Similar to @fig-school-data-1 but using the simulation in which the explanatory variables -- expenditure and participation -- are connected by a common cause. The two models show very different relationships between outcomes and expenditures. Model (b) matches the mechanism used in the simulation, while that mechanism is obscured in model (a). "
Dat2 <- sample(dag_school2, size = 500) %>%
  mutate(.participation = ifelse(participation < 33, 0, 
         ifelse(participation < 67, 50, 100)))
mod2_1 <- lm(outcome ~ expenditure, data = Dat2)
mod2_2 <- lm(outcome ~ expenditure * participation, data = Dat2)
mod_plot(mod2_1) %>%
  gf_point(outcome ~ expenditure, data = Dat2)
mod_plot(mod2_2) %>%
  gf_point(outcome ~ expenditure | .participation, alpha=~participation, 
           data = Dat2, 
           inherit=FALSE)
```

Since we know the exact mechanism in the simulation---`outcome` increases with `expenditure`---we know that model (b) matches the workings of the simulation while model (a) does not.


For the simulation where expenditure and participation share a common cause, failing to stratify on `participation` -- that is, looking at the points in @fig:-school-data-2 (a) but ignoring color -- gives an utterly different result than if the stratification includes `participation`.





## Other stuff



Consider a credit-card company might building a classifier to predict at the time of the transaction whether a purchase of gasoline is fraudulent. The company knows how often and how much gasoline the individual cardholders buys, where the cardholder lives, whether the cardholder travels extensively, typical times of day for a purchase, and so on. **Feature engineering** is the process of using existing data---including, in our example, whether the purchase turned out to be fraudulent---to develop potential markers or signals of the outcome. For simplicity, imagine the features selected are the number of days since the last gasoline purchase and the distance from the last place of purchase. 

Once potential features have been proposed, the engineers building the classifier assemble training and testing data sets. Suppose, for the purpose of illustration, that the training data has 2000 fraudulent transactions and 4000 non-fraudulent ones, and the testing set is about the same. 

The word "assemble" was used intentionally to describe how the testing and training data were collected: a **case-control study**. Since the objective is to detect fraud, it is reasonable to have a lot of "yes" cases in the data. The "no" cases serve as a kind of control; they were included specifically to have balance in the data. If data had been collected as a simple random sample of credit card transactions, there would have been many, many more "no" cases than "yes."

With such training data it is easy to build a statistical model with `Fraud` as the response variable. That model can then be evaluated on the testing data to produce a model output for each row:

Fraud | Feature 1 | Feature 2 | Model output
------|-----------|-----------|-------------
no    |  6 days   | 5 miles   |  -
no    |  1 day    | 250 miles |  +
yes   | 120 days  | 75 miles  |  -
no    | 5 days    | 0 miles   |  -
yes   | 0.2 days  | 90 miles  |  +

It's understandable that a classifier may not have perfect performance. After all, it iss trying to make a prediction based on limited data, and randomness may play a role. 

There are different ways of making a mistake, and these different ways  have very different consequences. One kind of mistake, called a "**false positive**", involves a classifier output that's positive (i.e. the classifier indicates fraud) but which is wrong. The consequence of this sort of mistake in the present example is a customer who has to find another way to pay for gasoline.

The other kind of mistake is called a "**false negative**". Here, the classifer output is that the transaction is not fraudulent, but in actuality it was. The consequence of this kind of mistake is different: a successful theft.

The nomenclature signals that a mistake has been made with the word "false." The kind of mistake is either "positive" or "negative", corresponding to the output of the classifier.

When the classifier gets things right, that is a "true" result. As with the false results, a true result is possible both for a "positive" and a "negative" classifier output. So the two ways of getting things right are called "**true positive**" and "**true negative**". 

Tabulating all 6000 rows of the testing data might produce something like this:

.   | +    |  -
----|------|------
yes | 1900 | 100      
no  |   50 | 3950


## Incidence

## Sensitivity and specificity



::: {.callout-note}
## Example: Accuracy of airport security screening 

Airplane passengers have, for decades, gone through a security screening process involving identity checks, "no fly" lists, metal detection, imaging of baggage, random pat-downs, and such. How accurate is such screening? Almost certainly, the accuracy is not as good as an extremely simple, no-input, alternative process: automatically identify every passenger as "not a security problem." We can estimate the accuracy of the "not a security problem" classifier by guessing what fraction of airplane passengers are indeed a threat to aircraft. In the US alone, there are about 2.5 million airplane passengers each day and security problems of any sort rarely happen. So the accuracy of the no-input classifier is something like 99.999%. 

The actual screening system, using metal detectors, baggage x-rays, etc. will have a lower accuracy. We know this since it regularly mis-identifies innocent people as security problems.

The problem here is not with airport security screening, but with the flawed use of *accuracy* as a measure of performance. Indeed, achieving super-high accuracy is not the objective of the security screening process. Instead, the objective is to *deter* security problems by convincing potential terrorists that they are likely to get caught before they can get on a plane. This has to do with the *sensitivity* of the system. The *specificity* of the system, although important to the everyday traveller, is not what deters the terrorist.

:::



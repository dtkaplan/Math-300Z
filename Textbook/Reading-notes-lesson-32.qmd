---
title: "Experiment and random assignment"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
lesson <- 32
source("../_startup.R")
```

In its everyday meaning, the word "experiment" is similar in meaning to the word "experience." As a verb, to experiment means to "try out new concepts or ways of doing things." As a noun, an experiment is a "course of action tentatively adopted without being sure of the outcome: the farm is an ongoing experiment in sustainable living." Both quotes are from the [Oxford Languages](https://languages.oup.com/google-dictionary-en/), which provides examples of each: "the designers experimented with new ideas in lighting" or "the farm is an ongoing experiment in sustainable living." 

From movies and other experiences, people associate experiments with science. Indeed, one of the dictionary definitions of "experiment" is: "a scientific procedure undertaken to make a discovery, test a hypothesis, or demonstrate a known fact."

Almost all the knowledge needed to perform a scientific experiment relates to the science itself: what reagents to use, how to measure, say, the concentration of a neurotransmitter, how to administer a drug safely, and so on. This is why people who carry out scientific procedures are trained primarily in their area of science.

::: {.callout-note}
## Example: Malaria and bed nets

In many parts of the world, malaria is a major cause of disability and death. Economists who study ways to relieve poverty have a simple, plausible theory: reducing the effect of illnesses such as malaria will have an impact on poverty rates, since healthier people are more productive and reduced uncertainty can help them amass capital to invest to increase production further.

There are many possible ways to reduce the burden of malaria. Vaccination (although effective vaccines have been hard to develop), insect control using pesticides (which can cause environmental problems), etc. One simple intervention is the use of bed nets; screen nets deployed at night by draping over the bed and its occupant. Still, there are reasons why distributing bed nets may not be effective; people might use them incorrectly or for other purposes such as fishing. People might not be able to afford them, but giving them away might signal that they have no value.

To find out, try it: do an experiment. For instance, run a trial program where nets are given away to everyone in an area and observed whether and to what extent rates of malarial illness go down.

Such a trial is certainly and experiment. But it may not be the best way to get meaningful information.

:::

To understand the contribution that statistical thinking can make to experiment, recall our earlier definition:

> *Statistic thinking is the explanation/description of measured variation* in the context of *what remains unexplained/undescribed.*

A key concept that statistical thinking brings to experiment is the idea of **variation**. Simply put, a good experiment should involve some variation. The simplest way to create variation is to repeat each experimental trial multiple times. This is called "**replication**."

::: {.callout-note}
## Example: Replicated bed net trials

One way to improve the simple experiment bed net described above is to carry out many trials. One reason is that the results from any single trial might be shaped by accidental or particular circumstances: the weather in the trial area was less favorable to mosquito reproduction; another government agency decided to help out by spraying pesticides broadly, and so on. Setting up trials in different areas can help to balance out these influences.

Replicated trials also allow us to estimate the size of the variability caused by the accidental or particular factors. To illustrate, suppose a single trial is done and the rate of malarial illness goes down by 5 percentage points. What can we conclude? The result is promising but we can't rule out that it occurred because of accidental factors other than bed nets. Why not? Because we have no idea how much unexplained variation is in play.

In contrast, suppose four trials at different sites are done, showing reductions by 5, 8, 2, and -1 percentage points. (Reduction by a negative number, like -1, is an *increase*.) Now we know something about the amount of variation due to accidental, site-to-site factors. The replication introduces *observed* variation in results, the observed variation can be quantified and used to place the overall trend in context. 

Common sense correctly suggests summarizing the results of the four trials by their mean: a reduction in the rate of malarial diseases of $\frac{5 + 8 + 2 -1}{4} = 3.5$ percentage points. Statistical thinking tells us that this exact number, 3.5, is somewhat of an accident. This is because the four numbers tell us that there is site-to-site variation that remains unexplained by the bed nets: the residual variation. The residual variation gives us a handle on the amount of sampling variation to expect, the variation in results if we had collected a different sample of trials from different sites or at different times. 

As usual, we quantify the sampling variation by a "standard error," which in turn gets translated into a "margin of error," and transformed yet again into an interval estimate. Here, that interval is $3.5 \pm 4.5$. Generations of statistics students have learned how to carry out the standard-error/margin-of-error/confidence-interval calculations and how to interpret them. $3.5 \pm 4.5$ means that the results from our trials are entirely consistent with the bed nets having zero effect on rates of malarial illness. 

Put another way, the replication of trials---the $n=4$ trials in this example---provides us a way to quantify the amount of noise in our results. Here, the observed 3.5 percentage point reduction cannot be distinquished from noise so we have no confidence that we have seen a signal.
:::

Replication is a comparatively modern idea. Experiments go back into pre-history. For instance, a biblical example is [DAVID's experiment as described by Judea Pearl.]

Galileo, taken by many historians to mark the appearance of "science," didn't replicate his trials. He had no clear knowledge of the idea of "signal" versus "noise," let alone ......

GALILEO didn't do replication.





Statistics contributes to experiment in two, very different ways. The first is that statistical methods are used to summarize the measurements made during the experiment. For instance, if your experiment involves 

Go back to definition of statistics: explanation/description of variation in the context of what remains unexplained.

Purpose of a "scientif







## From SM2

One of the most important ideas in science is "**experiment**". In a simple, ideal form of an experiment, you cause one explanatory factor to vary, hold all the other conditions constant, and observe the response.  A famous story of such an experiment involves Galileo Galilei (1564-1642) `r index_entry('C','Galileo Galilei')` `r index_entry('C', 'Pisa, Leaning Tower')`  dropping balls of different masses but equal diameter from the Leaning Tower of Pisa.^[The picturesque story of balls dropped from the Tower of Pisa may not be true. Galileo did record experiments done by rolling balls down ramps.] Would a heavy ball fall faster than a light ball, as theorized by Aristotle 2000 years previously?  The quantity that Galileo varied was the weight of the ball, the quantity he observed was how fast the balls fell, the conditions he held constant were the height of the fall and the diameter of the balls.  The experimental method of dropping balls side by side also holds constant the atmospheric conditions: temperature, humidity, wind, air density, etc.

Of course, Galileo had no control over the atmospheric conditions.  By carrying out the experiment in a short period, while atmospheric conditions were steady, he effectively held them constant.

Today, Galileo's experiment seems obvious.  But not at the time.  In the history of science, Galileo's work was a landmark: he put *observation* at the fore, rather than the beliefs passed down from authority.  Aristotle's ancient theory, still considered authoritative in Galileo's time, was that heavier objects fall faster. 

`r index_entry('C', 'experiment! vs observation')` 
`r index_entry('C', 'relationship!partial vs total')` 
`r index_entry('C', 'partial relationship')` 
`r index_entry('C', 'total relationship')` 
`r index_entry('C', 'aprotinin (drug)')` 
`r index_entry('C', 'heart surgery')` 

The ideal of "holding all other conditions constant" is not always so simple as with dropping balls from a tower in steady weather.  Consider an experiment to test the effect of a blood-pressure drug. Take two groups of people, give the people in one group the drug and give nothing to the other group.  Observe how blood pressure changes in the two groups.  The factor being caused to vary is whether or not a person gets the drug. But what is being held constant?  Presumably the researcher took care to make the two groups as similar as possible: similar medical conditions and histories, similar weights, similar ages.  But "similar" is not "constant." 



## DAG interpretation of experiment

Albert Einstein is reputed to have said:

> *A theory is something nobody believes, except the person who made it. An experiment is something everybody believes, except the person who made it.*

A graphical causal network is a kind of theory. As a theory, it's natural for people to be skeptical about results stem from the theory. Experiments are more persuasive. Let's consider what an experiment looks like when represented by a graphical causal networks.

In an experiment, you have some real-world system and a means to intervene physically on at least one of the variables in that system and to read out the response of the system to the intervention. You don't necessarily know much about the actual structure of the real world system. In @fig-experiment-system the real-world system is shown in the rounded box. The intervention is on X and the output is Y.

```{r echo = FALSE}
#| label: fig-experiment-system
#| fig-cap: "An experiment is a system in which there is an intervention and an output."
knitr::include_graphics("www/experiment-system.png")
```


Note that in @fig-experiment-system X is, potentially, affected by other variables in the system.

Ideally, the experiment is set up to eliminate all other effects on X except the intervention as in @fig-experiment-pure. And the intervention is done in a way that none of the variables in the system can have any effect on it, for instance by assigning the intervention using a **computer random-number generator**. The lovely thing about this configuration is that the correct model to capture the effect of `X` on `Y` is simply `Y ~ X`. Whatever different people might believe about the real-world mechanism doesn't matter. The correct model is always `Y ~ X`. This is why Einstein's statement, "An experiment is something everybody believes," is justified.

```{r echo = FALSE}
#| label: fig-experiment-pure
#| fig-cap: "An ideal experiment is one where the *only* influence on X is the intervention. Any effect on X or the intervention of the other variables in the system has been eliminated. The input paths to X from C and D that appear in @fig-experiment-system have been deleted by the experimenter. This is not always possible in practice."
knitr::include_graphics("www/experiment-pure.png")
```


But there is another part to Einstein's statement: "... except the person who made it." Why shouldn't the experimenter believe her own experiment? The experimenter might know that she didn't or couldn't conduct an ideal experiment. She wasn't actually able to eliminate the arrows D $\rightarrow$ X and C $\rightarrow$ X. The other variables in the system might also be influencing X as in @fig-experiment-system. In this situation, the right model may not be Y `~` X. In fact, for the particular system shown in @fig-experiment-system the correct model would be Y `~` X + C + D. But how could the experimenter know this for sure if she didn't know all about the real-world mechanism?

It turns out that for either of the causal systems in [Figures @fig-experiment-system or @fig-experiment-pure] there is always a correct model to show the link between the intervention and output: Output `~` Intervention. Rather than modeling the output by the physical quantity X, model the output by the random numbers generated by the computer that were used to set the intervention. This modeling approach is called **intent to treat**.

Typically, experiments are done using a specially constructed system that is thought to resemble the system on which the intervention will actually be done. Insofar as the experimental system does resemble the real-world system, the experimental results will anticipate the effect of the real-world intervention. But often it's hard to establish that the experimental system is a match to the system on which the real-world intervention will be applied. As such, subjective belief is still a factor in accepting that the experiment will be informative about the real-world systems we work with.
 
--------

It's appropriate to show some humility about models and recognize that they can be no better than the assumptions that go into them.  Useful object lessons are given by the episodes where conclusions from modeling (with careful adjustment for covariates) can be compared to experimental results.  Some examples (from [@freedman-editorial-2008]):

* Does it help to use telephone canvassing to get out the vote? Models suggest it does, but experiments indicate otherwise.
* Is a diet rich in vitamins, fruits, vegetables and low in fat protective against cancer, heart disease or cognitive decline? Models suggest yes, but experiments generally do not.

The divergence between models and experiment suggests that an important covariate has been left out of the models. `r index_entry('C', 'covariate!leaving out')`  `r index_entry('C', 'adjustment!faulty')` 



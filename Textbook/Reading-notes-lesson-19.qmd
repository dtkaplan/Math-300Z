---
title: "Preliminaries to Statistical thinking"
author: "Prof. Danny Kaplan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
lesson <- 19
source("../_startup.R")
```




This lesson introduces a few basic tools that you will be using throughout the remaining lessons. 

1. A standard, unified format for data graphics that simplifies both the construction and the interpretation of graphics and permits layers of descriptions to be laid on top of a data layer.
2. The presentation of descriptions using **intervals** rather than a number like the mean or proportion.
3. A modern mode of displaying one type of description---the "**density**" (also called the "**distribution**") of data---that is compatible with the unified data-graphics format.
4. How extend regression modeling, which in Chapters 11-17 of *OpenIntro* always required a *quantitative* response variable, to be useful for modeling *categorical* response variables.

Since this is an introductory course, we will treat only categorical response variables that have two levels, for instance, Alive/Dead, Promoted/Not, Win/Loss, and so on. We will call these types of categorical response variables as "binomial" variables (that is, bi (two) nomial (names)) or "yes/no" variables, or zero/one variables. All of these terms refer to the same idea: a categorical variable with two levels. 

Statistical techniques for handling categorical response variables with three or many more levels require more book-keeping and more intricate computer programming. The models used by the machine-learning community are called "classifiers" rather than "regression models." But limiting ourselves in this course to binomial response variables means that classifiers are indeed regression models.

## Unified format for data graphics

The core descriptive technique we will be using is based on regression models. And, as you know, a key paradigm for building regression models is the choice of a response variable and the choice of one or more explanatory variables. (Actually, the previous sentence would be more complete if it said, "the choice of **zero** or more explanatory variables. You'll see why a *zero explanatory variable* model is a useful concept as we move through the rest of the course. It is one of the main ways of "establishing context" for "what remains unexplained or undescribed. But we will cross that bridge when we come to it.) 

Since our descriptions will be grounded in regression models, and since we want to be able to generate graphics that show in different layers in the same graphics frame both the raw data and the description, it makes sense to structure data graphics so that there is a response variable displayed as well as one or more explanatory variables. Following convention, we will *always* display the response variable on the vertical (y) axis, and an explanatory variable on the horizontal (x) axis. If there are other explanatory variables to be displayed, we will use color and faceting.

Another aspect of our unified data graphic format is that it will *always* be a point plot or, closely related, a jitter plot.

To illustrate the construction of standard-format data graphics, consider the `mosaicData::Gestation` data frame. You can read about this data frame with the R command `?Gestation`. Suppose we want to address the question, "Do experienced mothers have systematically different gestation periods than inexperienced mothers?" For this question, an appropriate response variable is the length of `gestation`. The explanatory variable needs to measure "experience," which is a vague idea. We will make it concrete by taking it to mean the number of the mother's pregnancies prior to the one reported in the data. This is the variable `parity` and ranges from zero to thirteen. 

Now that we know the response and explanatory variable, we can generate the data graphic simply enough:

```{r}
Gestation %>% ggplot(aes(x=parity, y=gestation)) + geom_point() 
```

This graph tells you some things at a glance. A typical gestation period is about 275 days, that is, about 9 months. And you can see that it's much more common to have a low parity than a high one. But perhaps there is some overplotting that's hiding the number of low-parity cases. We can easily resolve this by using `geom_jitter()`, perhaps with some transparency. At the same time, noting that there are very few cases with, say, parity greater than 5, we will focus on the part of the data with parity of zero to five:

```{r fig-gestation-jitter, warning=FALSE}
#| fig-cap: "A jitter plot showing gestational period for pregnancies where the mother had five or fewer previous pregnancies. The `width=0.2` controls the amount of horizontal jittering. We chose it to make the columns of data clear. Also, there's no need to jitter in the vertical direction, so we set `height=0`"
Gestation %>% 
  filter(parity <= 5) %>%
  #mutate(parity = as.character(parity)) %>%
  ggplot(aes(x=parity, y=gestation)) + 
  geom_jitter(alpha=0.2, width=0.2, height=0) 
```

## Displaying density

It is easy to see a pattern in @fig-gestation-jitter: It looks like mothers with high parity tend to have gestation periods that are more reliably close to 280 days than for mothers with low parity. Or, maybe this pattern is an illusion. There are so few pregnancies with parity 3, 4, or 5 that we don't expect to see as many uncommonly short or long gestational periods as for the parities with lots of cases.

One way to explore this idea is to plot the density of the dots as a function of gestation for each of the parity levels individually. A "violin" layer will make it easier to compare the distributions in the different columns, despite the unevenness in the case count. @fig-violin-intro gives an example.

```{r}
#| label: fig-violin-intro
#| fig-cap: "A violin plot. The long axis of the violin-like shape is oriented along the response-variable axis (that is, the vertical axis in our standard format). The width of the violin for each possible value of the response variable is proportional to the density of data near that value."
Gestation %>% 
  filter(parity <= 5) %>%
  ggplot(aes(x=parity, y=gestation)) + 
  geom_jitter(alpha=0.2, width=0.2, height=0) +
  geom_violin(aes(group=parity), fill="blue", alpha=0.2, color=NA)
```

The violin plot is a more flexible display of the distribution of gestation period that would be a histogram. The histogram has all those bars that clutter up the display. Even worse, one of the axes in the frame of a histogram plot is "count" or maybe "density." Such a frame is not consistent with the unified response/explanatory format we will be using. The violin is drawn in the no-mans-land between the different levels of parity, just as the jittering moves data away from a single vertical line into that same no-mans-land. 

This idea of using the graphical no-mans-land between levels of a categorical explanatory variable is not new. You encountered it earlier when you drew box plots. @fig-density-box adds a box-plot annotation layer on top of the violin-plot layer.

```{r warning=FALSE}
#| label: fig-density-plot
#| fig-cap: "A box and whisker plot uses the no-mans-land between levels of a categorical explanatory variable."
Gestation %>% 
  filter(parity <= 5) %>%
  ggplot(aes(x=parity, y=gestation)) + 
  geom_jitter(alpha=0.2, width=0.2, height=0) +
  geom_violin(aes(group=parity), fill="blue", alpha=0.2, color=NA) +
  geom_boxplot(aes(group=parity), color="blue", fill=NA, alpha=.5)
```

:::: {.callout-note}
## Violins versus boxes

All of the graphical statistical annotations are human inventions. Each invention attempts to meet a need, but usually the invention is a compromise between the statistical objective and the computational and graphical resources available. The **box plot** format is a case in point. The statistical goal of a box plot is to display the distribution of values of a variable. It was invented in a time when graphics were mostly drawn by hand and computers were not widely available. The computations behind a box plot produce a five-number summary: min, first quartile, median, third quartile, max. It's straightforward (but tedious!) to do these by hand since they are based on sorting and counting.The drawing itself uses only straight lines, which are easy to draw by hand with only a pencil and a straightedge.

A violin plot requires hundreds or thousands of evaluations of the gaussian function along with post-processing. They are not feasible for a human; a computer is required. Similarly, drawing the detailed shape of the violin (Figure @fig-violin-intro) requires a computer. 

The box plot has important deficiencies. It is appropriate only for uni-modal distributions and doesn't give even a hint of possible bi-modality. The sharp boundaries of the box and endpoints of the whiskers suggest that even smooth density shapes have abrupt transitions. Points are marked as "outliers" in order to keep the whiskers from becoming absurdly long, but box-plots of data with a normal (gaussian) distribution will produce such "outliers" whenever the sample size is large. 

When it comes to computing power, we are today unimaginably rich compared to the generation that introduced box plots. In a sense, we are so rich we can use expensive, well made products such as a violin. The box-plot generation was living in computational poverty. Not having the (computational) funds to buy a violin, they had to make do with primitive instruments they had to make do with the materials at hand, just as early blues mucisians, coming out of poverty, often had to build instruments such as a cigar-box guitar. 

::: {#fig-cigar-box-guitar}
![Cigar box guitar](www/cigar-box-guitar.png)

A cigar-box guitar.

::::


## Describing with intervals

Statistical thinking often involves quantifying uncertainty. One manifestation of this is moving away from single-number "**point**" summaries such as the mean or median to "**interval**" summaries. As you will see as we progress through the future lessons, there are many kinds of such intervals, each of which is designed to deal to address a specific question. So you'll see **prediction intervals**, **confidence intervals**, **confidence bands**, and so on.

For this lesson, we're concerned only with what interval summaries look like. So let's generate some, *without worrying* yet about the computer commands and mathematical underpinnings involved. 

::: {.callout-warning}
## Ground rules for "demonstrations"

There will be many occasions in these lessons where we want you illustrate a statistical technique or phenomenon, but we don't expect the reader to master the commands involved. We will call these **demonstrations**: something we don't expect you to do at home. A good way to think about these demonstrations is that you should focus on the *outputs* from the calculations, rather than the calculation steps themselves. We'll show you the calculations since some readers might be interested, but focus your attention on the output.
:::

::: {.callout-note}
## Demonstration: Food at Starbucks

Starbucks is a famous coffee-shop franchise, with more than 30,000 branches (as of 2021) across the world. People go to Starbucks for the coffee, but they often buy something to eat as well. Let's look at the calorie content of Starbucks' food offerings. As always, when conducting a statistical analysis, it's helpful to have in mind the purpose for the task. We'll imagine, tongue in cheek, that we want to make food recommendations for the calorie conscious consumer.

First, a **point summary** of the calories in the different types of food products available at Starbucks:

```{r}
point_summary <- df_stats(calories ~ type, data = openintro::starbucks, mean)
point_summary
```

This summary supports the common-sense advise that to avoid calories, focus your choices on salads or on smaller portions (type "petite"). You might be tempted to go further, for example concluding that a sandwich is a bad choice (in terms of calorie content) so lean toward parfaits or hot breakfasts. You can even imagine someone concluding from this summary that a bistro box is a better calorie-conscious choice than a sandwich.

A graphic showing both the point summary and the raw data can put things in a useful context.

```{r}
openintro::starbucks %>% 
  ggplot(aes(x=type, y=calories)) +
  geom_jitter(width=0.2, alpha=0.5) +
  geom_errorbar(data=point_summary, aes(ymin=mean, ymax=mean), y=NA, color="blue") +
  geom_point(data=point_summary, aes(y=mean), color="red")
```

We've shown the point summary as red dots, one for each food type. A somewhat stronger visual impression is given by drawing the point summary not as points, but as lines that extend into the no-mans-land between food types. These are drawn in blue and they make the red dots superfluous; you don't need both. 

Plotting the point summary in the context of the raw data shows at a glance that the point summary is not of any use beyond the common sense advice to eat salads and small portions if you are trying to avoid calories. With the point summary on its own, we were tempted to conclude that, say, hot breakfasts are a better choice than sandwiches, but the data display suggests otherwise; there's just one low-calorie breakfast. The others are much like sandwiches.

A point summary is compact, but it fails to take into account the *variation* within each food type.

An interval summary does take into account this variation. This is an important aspect of statistical thinking. Recall the definition of statistical thinking given earlier:

> *The explanation or description of measured variation* in the context of *what remains unexplained or undescribed.*

There are several kinds of interval summaries. You're not yet in a position to know which kind is the appropriate one for the task at hand---giving advice about food choices based on food type---so we'll tell you: a **prediction interval**.

```{r}
interval_summary <- df_stats(calories ~ type, data = openintro::starbucks, coverage(.95))
interval_summary
```
:::

Or in graphical form:

```{r}
openintro::starbucks %>% 
  ggplot(aes(x=type, y=calories)) +
  geom_jitter(width=0.2, alpha=0.5, height=0) +
  geom_errorbar(data=interval_summary, aes(ymin=lower, ymax=upper), y=NA, color="blue") 
```

Unlike point summaries, interval summaries can overlap. Such overlap is an indication that the groups being summarized are not all that different. Here, an appropriate conclusion indicated by the interval summary is, "Don't make your diet choices based on food type. Look at the calorie content of individual items before making your choice."

Admittedly, in this simple setting the data themselves would lead to the conclusion. But as we move into more complicated settings, it will become infeasible to quickly see patterns straight from the data. In these complicated settings, summaries are an important tool for displaying and quantifying patterns. *The statistical thinker knows to prefer interval summaries*.

## Categorical response variables.

Our last topic in this lesson is relatively simple: the zero-one transformation of categorical variables which allows regression and related techniques to be used for categorical response variables. 

To illustrate, let's use data collected in the 1970s to study the relationship between smoking and mortality. The data we'll use, `mosaicData::Whickham`, recorded for one-thousand nurses whether or not they smoked at the time of the initial interview and whether or not they were still alive twenty years after the initial interview.

Here's a graph of the data in our standard response-vs-explanatory graphic frame:

```{r}
Whickham %>%
  ggplot(aes(x = smoker, y = outcome)) +
  geom_jitter(width=0.2, height=0.2, alpha=0.5)
```

The graph suggests that non-smokers were more likely than smokers to be dead at the follow-up interview. But it's hard to calculate proportions from such a graph. It's reasonable to argue that for the purpose of showing the fraction of smokers and of non-smokers who died, a bar chart would be better.

```{r}
#| label: fig-smoker-barplot
#| fig-cap: "Barplots of the `Whickham` data."
#| fig-subcap:
#|   - "counts"
#|   - "proportions"
#| layout-ncol: 2
Whickham %>%
  ggplot(aes(x=smoker, fill=outcome)) +
  geom_bar()
Whickham %>%
  ggplot(aes(x=smoker, fill=outcome)) +
  geom_bar(position = "fill") +
  ylab("Proportions")
```

The left barplot, showing counts, suggests that a higher proportion of non-smokers died than of smokers. But its easy to instruct the `geom_bar()` to graph proportions rather than counts, as done in the left plot. This makes it easy to conclude at a glance that a higher proportion of non-smokers have died.

The important question here, "Does smoking affect mortality?" translates well into the response/explanatory paradigm: `outcome` is the response variable while `smoker` is the explanatory variable. In the jitter-plot presentation of the data, these assignments are clearly indicated in the computer commands, which set `x=smoker, y=outcome`. In the barplot, a different notation is used: `x=smoker, fill=outcome`.

Unfortunately, neither of the graphic styles---jitter or boxplot---answers the important question. At best they provide a description of the nurses in the `Whickham` data frame.

To answer the important question, we need to invoke statistical thinking. In particular, we need an *interval summary* of the proportion who died, not the point summary produced by the barplot. 

This doesn't mean that we can't easily calculate the proportions from the categorical response variable: we just have to use the right commands. for instance:

```{r}
Whickham %>%
  df_stats(outcome ~ smoker, prop, ci.prop)
```

The point summary---the `prop_Alive` column---suggests an obvious difference between the smokers and non-smokers. The interval summary---columns `lower` and `upper`---tempers this conclusion a little: the intervals almost touch.

Although regression is our go-to technique for modeling relationships between variables, we can't use it directly on a categorical response variable. Here's what happens if we try:

```{r}
lm(outcome ~ smoker, data = Whickham) %>% confint()
```

The computer's warning message is a reminder that the response variable is categorical. (The message uses the phrase "factor response," which is just computerese for "categorical response.")

We can fix things with a simple trick: trasforming the response variable to a zero-one encoding. In the following, we'll use 1 to represent `"Alive"` and 0 to represent `"Dead"`, although we can equally well do things the other way around.

```{r}
lm(zero_one(outcome, one="Alive") ~ smoker, data = Whickham) %>% confint()
```

You don't yet know enough to interpret this interval summary. That will have to wait until Lesson 24. The significant^[In lesson 38 you'll learn to be wary whenever a statistician uses the word "significant."] feature of the interval on `smokerYes` is that it does not include zero. In everyday terms, the interval means, "Smokers are 3 to 12 percentage points more likely to survive for 20 years than are non-smokers."

Using interval summaries instead of point summaries is an important aspect of statistical thinking, but there are other aspects that need to be taken into account. A simple, but important, question is whether the nurses recorded in the `Whickham` data frame are good representatives of all smokers. (It turns out that the nurses in `Whickham` are all women interviewed in the 1970s. At that moment of history, women were very different than men when it comes to smoking, and the Whickham smokers were also very different from today's female smokers. We'll say more about this in the demonstration below.)

Statistical thinking also leads one to ask another sort of question: What else might be going on other than smoking? In technical language, the other-goings-on are called "**covariates**," the topic of Lessons 28 & 29.

For instance, you might wonder about the overall result from our brief examination of the `Whickham` data. Is it really the case that the smokers were more likely to survive than the non-smokers? The answer is "yes," as we have demonstrated from the previous analysis. But this answer is completely misleading. Tobacco companies worked hard to mislead people into thinking that smoking was not dangerous. They knew full well the negative health consequence of smoking, but they used statistical-sounding claims to hide this knowledge from the public.

In the following demonstration, we'll look at the `Whickham` data again using the power of regression models to incorporate covariates.

::: {.callout-note}
## Demonstration: Smoking with covariates

*Remember that you are not expected to master the calculations in these demonstrations. Focus your attention on the output from the calculations.*

It goes without saying that smoking is not the only thing that kills people. There are other risky behaviors such as heavy drinking, there's environmental exposure to pollutants, and there's disease (other than the smoking induced ones of lung cancer, emphasema, and high blood pressure). But there's one risk factor for death that everyone knows about but nobody is doing anything about: getting old.

In virtually every public health or clinical study, the participants' age is taken into account. Not doing so can produce a completely misleading view of the situation. This is also the case with smoking and mortality in the `Whickham` study.

Regression techniques enable us to take multiple explanatory variables into account. In this demonstration, we'll use regression to study `outcome` as a function of `smoker` and, importantly, `age`.

To get started, we need to remember to convert the categorical `outcome` variable into a zero-one encoding. After that, building the model is not so hard.

```{r}
survival_model <- Whickham %>% 
  mutate(survived = zero_one(outcome, one="Alive")) %>%
  model_train(survived ~ age + smoker, data=.)
```

From this model, we can read off an interval summary of the effect of smoking on survival:

```{r message=FALSE}
survival_model %>% confint()
```

A full understanding of this interval summary will need to wait until Lessons 22 through 24. For the present, we'll simply point out that the summary interval on `smokerYes` includes zero, so `Whickham` provides no support for the mistaken conclusion that smoking improves survival. But seeing this requires taking into account `age`. A graphic may help explain why:

```{r}
Model_output <- model_eval(survival_model, interval="confidence")
Model_output %>%
  ggplot(aes(y = survived, ymin=.lwr, ymax=.upr, x=age, color=smoker, fill=smoker)) +
  geom_jitter(height=.1, width=0, alpha=0.2) +
  geom_ribbon(alpha=0.2) 
```

The interval summary in the graph shows how the probability of survival changes for different ages. The intervals for non-smokers and smokers entirely overlap. For both groups, 20-year survival goes down with greater initial age. So why did the model `outcome ~ smoker` suggest that smokers have a higher survival? The reason relates to the proportion of smokers with initial age 70+. In the 1970s, life expectancy was such that people 70+ were unlikely to survive 20 years. This pulls down the survival rate at that age. Notice that the 70+ nurses were unlikely to have been smokers compared to younger nurses. The 70+ nurses grew up in an era when social conventions caused smoking to be uncommon for women (even though it was very common for men).

:::

## Wrangling versus modeling

The first half of this course emphasized data wrangling and visualization. When using data wrangling commands, summaries of data frames were computed using, naturally enough, the `summarize()` function. Typical summaries for quantitative variables include means, medians, standard deviations, etc., each of which applies to one variable at a time. For instance, this command calculates four summary statistics on the `net` running time recorded in the `TenMileRace` data frame:

```{r}
TenMileRace %>%
  summarize(ave = mean(net), middle = median(net), sd = sd(net), n = n())

```

Constructing such summaries groupwise is a matter of using the `group_by()` modifier. Here, we calculate summaries broken down by the state of residence of the participant and arranged from fastest (average) running time downward.

```{r}
TenMileRace %>%
  group_by(state) %>%
  summarize(ave = mean(net), middle = median(net), sd = sd(net), n = n()) %>%
  arrange(ave) %>%
  head(10)
```

Wrangling is essential for many statistical purposes, including making graphical displays, cleaning data, and assembling data that comes from multiple sources.

In Lessons 11-17, you were introduced to regression modeling. The computing tasks for regression, such as fitting a model with `lm()`, don't fit into the wrangling framework. To illustrate:

```{r}
lm(net ~ age, TenMileRace) 
```

Regression produces a summary of a data set, somewhat like `summarize()`. The output of `summarize()` is always a data frame. The output of `lm()` is a different kind of thing, as you can see above. In computing lingo, the "kind of thing" is often called the "object class" or "object type."

Perhaps the first thing that's confusing about the object produced by `lm()` is that they contain much more than what's printed out when you display them directly on the screen. The printed output is just a glimpse at the object and, almost always, you use another function to present the content of the object in a way that suits your current need. Let's informally call such functions "extractors." Here are examples of a few of the extractors that you will be using in the coming lessons; you are not expected at this point to know what each is doing.

```{r}
lm(net ~ age, TenMileRace) %>% coefficients()
lm(net ~ age, TenMileRace) %>% rsquared()
lm(net ~ age, TenMileRace) %>% regression_summary()
lm(net ~ age, TenMileRace) %>% model_eval(skeleton=TRUE)
```

Much of what you will be learning in the following Lessons concerns such regression extractors: why what they calculate is useful and how to use it. But let's return to the command patterns you will be seeing.

Regression and wrangling are allies. You will use wrangling, especially `mutate()` and `filter()` to pre-process data before carrying out the regression. For instance, in studying the relationship between `age` and `net` running time, you might want to focus on older people.

```{r}
TenMileRace %>%
  filter(age > 40) %>%
  lm(net ~ age, data=.) %>% confint()
```

In this example, we've used `confint()` as the extractor, but, depending on our purpose, we might have used any of the others.

Looking closely at the above command you will notice something new: the `data=.` argument being used inside `lm()` . The simple `.` is doing something important, carrying the output of the earlier stages of the pipeline into the `data=` argument of `lm()`.

The dot (`.`) has always been available for use when wrangling, but we haven't needed to use it. For instance, here is an earlier example of a wrangling command translated to use the `.` notation:

```{r}
TenMileRace %>%
  group_by(., state) %>%
  summarize(., ave = mean(net), middle = median(net), sd = sd(net), n = n()) %>%
  arrange(., ave) %>%
  head(., 10)
```

The `.` always means, use the output of the preceding stages of the pipeline as *this argument* to the function. All of the data wrangling commands were designed so that the first argument is always a data frame. The phrase `%>% group_by(., state) %>%` is an explicit direction to place the data coming from the pipeline as the first argument. The pipeline connector, `%>%`, is arranged by default to put the content it is transmitting as the first argument to the following function. For this reason, `.` is not needed if the pipeline pumps results into the first argument.

But not all functions are designed with this convention in mind. In particular, in `lm()` the first argument should be the model formula, e.g. `net ~ age`. The data frame that will be used in fitting the model is the second argument. So, in `lm(net ~ age, data=.)` the dot is directing the pipeline to empty its data frame into the second argument. Otherwise, by default, the pipeline would be plumbed to force the first argument to be the data frame and demote the `net ~ age` to the second position.

## Grouping

The `group_by()` wrangling function is used whenever you want to treat a data frame in a group-by-group manner. It's natural to assume that `group_by()` is slicing up a data frame according to groups. The `mutate()` and `summarize()` functions were specifically designed to make it appear that `group_by()` is doing the slicing. But in reality, `group_by()` is just adding a kind of tag to the data frame. `mutate()` and `summarize()` know to interpret that tag as an instruction to slice up the data when calculating on it.

Most R functions simply ignore the tag added by `group_by()`. Consider, for instance, the pipeline

```{r}
TenMileRace %>%
  group_by(sex) %>%
  lm(net ~ age, data=.) %>%
  coefficients()
```

The output is not what you might expect from your earlier experiences using `group_by()`. Particularly, there is *not* a separate row for each of the groups defined by `sex`. Nor is there a column listing the sex. The `group_by()` has had no effect.

What do you do if you really want to compare groups using regression models? For instance, it might be that you believe that the `F` group ages differently than the `M` group. How do you reveal this with regression?

Regression models have their own, internal system for comparing groups. You express your wish to compare groups by using the model formula. The simple formula `net ~ age` does not instruct `lm()` to compare the sexes. Instead, you would use the formula `net ~ age*sex`. For instance,

```{r}
TenMileRace %>%
  lm(net ~ age * sex, data = .) %>%
  coefficients()
```

The output of `lm()` already contains the comparison information. You don't yet know how to read and interpret that comparison information, but you will learn.

There are extremely good reasons why `lm()` does things the way it does. It is not at all a matter of software incompatabiility between the wrangling family of commands and the regression family. The `lm()` paradigm can make much more efficient use of data than `group_by()`. It also offers much more flexibility. `lm()` can handle multiple "grouping" variables together and even lets you "group" by quantitative variables. These capabilities are extremely important for extracting relevant information from data, as you will see in the following lessons.

## Learning challenges

1.  One of these pipeline commands will work and the other won't. Which one will work? Explain why the other one doesn't work.

    ```{r eval=FALSE}
    lm(net ~ age, data = TenMileRace)
    TenMileRace %>% lm(net ~ age)
    ```

2.  An example from the *OpenIntro* book uses data on promotions. Some data wrangling commands that might be relevant are these:

    ```{r}
    promotions %>% tally()
    promotions %>% group_by(decision) %>% tally()
    promotions %>% group_by(gender) %>% tally()
    promotions %>% group_by(gender, decision) %>% tally()
    ```

You could use such wrangling to compare groups. For instance, you can use the results of the last command to calculate separately the proportion of males who were promoted and, similarly, the proportion of females.

a\. **What are those proportions?**

The following wrangling command will calculate the proportions for you, but it is a bit complicated:

```{r eval=FALSE}
promotions %>%
  group_by(gender) %>%
  summarize(prop_promoted = sum(decision=="promoted") / n())
```

b\. **Use this command to check your calculations in (a).**

c\. In the regression paradigm, the comparison of proportions between the two groups is done directly in `lm()`, like this:

```{r}
promotions %>%
  mutate(promoted = zero_one(decision, one="promoted")) %>%
  lm(promoted ~ gender, data = .) %>%
  coefficients()
```

We'll explain the purpose of `zero_one()` in Lesson 19, but putting that matter aside for a moment, compare the two coefficients in the regression model to the proportion results you got from wrangling.

i.  **What does the value of the intercept coefficient correspond to in the wrangling results?**

ii. **What does the `genderfemale` coefficient correspond to in the wrangling results?** (Hint: you will have to do a bit of arithmetic on the wrangling results.)



# Mechanics of prediction {#sec-lesson-25}

---
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r include=FALSE}
(lesson <- 25)
source("../_startup.R")
```

An effect size describes the relationship between two variables in an input/output format. [Lesson -@sec-lesson-24] introduced effect size in the context of causal connections as if turning a knob to change the input will produce a change in the output. Such mechanistic connections make for a nice mental image for those considering intervening in the world but can be misleading. 

First, the mere calculation of an effect size does not establish a causal connection. The statistical thinker has more work to do to justify a causal claim, as we will see in [Lesson -@sec-lesson-30]. 

Second, owing to noise, the input/output relationship quantified by an effect size may not be evident in a single intervention, say, increasing a drug dose for any given individual patient. Instead, effect sizes are descriptions of *average* effects---trends---across a large group of individuals. 

This Lesson is about *prediction*: what a model can properly say about the outcome of an individual case. Often, the setting is that we know values for some aspects of the individual but have yet to learn some other aspect of interest. 

The word "prediction" suggests the future but also applies to saying what we can about an unknown current or past state. Synonyms for "prediction" include "classification" (Lessons 34 and 35), "conjecture," "guess," and "bet." The phrase "informed guess" is a good description of prediction: using available information to support decision-making about the unknown.

::: {.callout-note}
## Example: Differential diagnosis

A patient comes to an urgent-care clinic with symptoms. The healthcare professional tries to diagnose what disease or illness the patient has. A diagnosis is a prediction. The inputs to the prediction are the symptoms---neck stiffness, a tremor, and so on---as well as facts about the person, such as age, sex, occupation, and family history. The prediction output is a set of probabilities, one for each medical condition that could cause the symptoms. 

Doctors learn to perform a *differential diagnosis*, where the current set of probabilities informs the choices of additional tests and treatments. The probabilities are updated based on the information gained from the tests and treatments. This update may suggest new tests or treatments, the results of which may drive a new update. The television drama *House* provides an example of the evolving predictions of differential diagnosis in every episode.
:::

Differential diagnosis is a cycle of prediction and action. This Lesson, however, is about the mechanics of prediction: taking what we know about an individual and producing an informed guess about what we do not yet know.

## The prediction machine

A statistical prediction is the output of a kind of special-purpose machine. The inputs given to the machine are values for what we already know; the output is a value (or interval) for the as-yet-unknown aspects of the system. 

There are always two phases involved in making a prediction. The first is building the prediction machine. The second phase is providing the machine with inputs for the individual case, turning the machine crank, and receiving the prediction as output.

These two phases require different sorts of data. Building the machine requires a "historical" data set that includes records from many instances where we already know two things: the values of the inputs and the observed output. The word "historical" emphasizes that the machine-building data must already have known values for each of the inputs and outputs of interest.

The evaluation phase---turning the crank of the machine---is simple. Take the input values for the individual to be predicted, put those inputs into the machine, and receive a predicted value as output. Those input values may come from pure speculation or the measured values from a specific case of interest.

## Building and using the machine

To illustrate building a prediction machine, we turn to a problem first considered quantitatively in the 1880s: the relationship between parents' heights and their children's heights at adulthood. The `Galton` data frame records the heights of about 900 children, along with their parents' heights.  Suppose we want to predict a child's adult height (variable name: `height`) from his or her parents' heights (`mother` and `father`). An appropriate model specification is `height ~ mother + father`. We use the model-training function`lm()` to transform the model specification and the data into a model.

```{r}
Mod1 <- lm(height ~ mother + father, data = Galton)
```
As the output of an R function, `Mod1` is a computer object. It incorporates a variety of information organized in a somewhat complex way. There are several often-used ways to extract this information in ways that serve specific purposes.

One of the most common ways to see what is in a computer object like `Mod1` is by printing:

```{r}
print(Mod1)
``` 
Newcomers to technical computing tend to confuse the printed form of an object with the object itself. For example, the `Mod1` object contains many components, but the printed form displays only two: the model coefficients and the command used to construct the object. 

We have already used some other functions to extract information from a model object. For instance,

```{r}
Mod1 %>% coef()
Mod1 %>% confint()
Mod1 %>% regression_summary()
```

Another extractor, `model_eval()`, is particularly convenient for prediction. Perhaps the most common use is to provide new input values to the model function, with `model_eval()` producing a data frame showing the output of the model function. To illustrate, here is how to calculate the predicted height of the child of a 63-inch-tall mother and a 68-inch father.

```{r}
Mod1 %>% model_eval(mother = 63, father=68)
```
The data frame includes the input values along with a point value for the prediction (`.output`) and a prediction interval (`.lwr` to `.upr`).

Naturally, the predictions depend on the explanatory variables used in the model. For example, here is a model that uses only `sex` to predict the child's height:

```{r}
Mod2 <- lm(height ~ sex, data = Galton)
Mod2 %>% model_eval(sex=c("F", "M"))
```

This model includes three explanatory variables:

```{r}
Mod3 <- lm(height ~ mother + father + sex, data = Galton)
Mod3 %>% model_eval(mother=63, father=68, sex=c("F", "M"))
```

In [Lesson -@sec-lesson-26], we will look at the components that make up the prediction interval and some ways to use it.

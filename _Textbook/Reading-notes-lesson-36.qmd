---
title: "Hypothesis testing"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
lesson <- 36
source("../_startup.R")
```

We are nearing the end of our journey through the world of statistical thinking. But I think it's time to get off the well-travelled road and take a detour of a few paragraphs. Detours are usually not the straightest path between two points, but they often have some advantage: safety, a scenic view or inspiring experience, and such.

Our detour starts with a turn onto a lane marked with the word "orthodox." There are, as you know, religions denominated by "orthodox." But the word has a more general meaning that can be seen by translating the Greek origin words "ortho" and "doxa" into a familiar language: "ortho" means "straight" or "right"; "doxa" means "belief".  Synonyms for "belief" include creed, dogma, teaching, doctrine (which stems from the Latin for "to teach"), conviction, and article of faith. A line is straight and the phrase "the party line" indicates attitudes that fall into line with the party leadership. "Ortho" appears in "orthodonture" (straightening the teeth), "orthography" (correct spelling), and "orthogonal" (being at a right angle).^[The link between words for "straight" and thinking according to the rule book has ancient origins.  The word "canon," meaning the set of officially accepted writings, comes from the Sumerian word for a straight reed.]

To "true a wheel" means to set it straight, and the phrase "true believer" does not refer to someone whose beliefs are correct but rather to someone who stays in line with to a particular system of belief. There can be many different systems of belief, many different orthodoxies, and disagreeing parties can each have their distinct line.

There are two major orthodoxies of statistical thought, "frequentists" and "Bayesians,"  a fact that's important to keeping straight the variety of statistical nomenclature. The key distinction between the frequentist orthodoxy and that of the Bayesians, is their attitude toward the meaning of "probability." One sect, the "frequentists," bases their methodology in a supreme being they call the "population." Coin flips are an example of a population; an abstractly infinite supply of events. The probability of "heads" is the *frequency* of a head turning up in $n$ trials, where $n \rightarrow\infty$. Flipping coins an infinite number of times is still a work in progress. Until it is complete, we need to work with the probability of heads as the proportion of a large but finite number of trials in which the outcome is "heads."

In contrast, the "Bayesian" sect holds that a probability is a statement about belief. Different people, depending on their experiences, will rightly come to different conclusions about the probability of the outcome of an event. Bayesians will happily deal with a statement like, "the probability of rain tomorrow is 60%," while frequentists might respond (while packing an umbrella in their knapsack for tomorrow's weather) by pointing out that there is no "population" of "tomorrows" from which we can draw a sample to estimate the frequency of rain. 

For a frequentist, the hypothesis "April showers bring May flowers" is an assumption. There is no actual population from which to draw many trials, so a probability cannot be assigned to the hypothesis. In the frequentist liturgy, to *test the hypothesis* means to make a *simulation* of the world. Typically the simulation implements a world in which it is hard coded that the connection described by the hypothesis *does not exist*. This no-connection hypothesis is generically called the "**Null hypothesis**." In this case, the Null hypothesis states that there is no association between April showers and May flowers. 

Constructing a simulation that generates data from the Null hypothesis is easy. Take the real-world data recording the observations of April precipitation and May floration. Randomly shuffle the entries in the April column while holding the May column fixed. The shuffling destroys any systematic association between the April and May columns. Any measured association in the shuffled data is incidental and accidental. 

Run many trials of the shuffling simulation. In each trial, record the measured association. It's reasonable to expect that the measured association across the trials will be close to zero. Indeed, you use the trial results to *define* what "close to zero" means. Then look back at the association found in the real-world, unshuffled data. If that observed association falls within the definition of "close to zero," then it is not fair to insist that the real-data is inconsistent with the Null hypothesis. That is, you "**fail to reject**" the Null hypothesis. On the other hand, if the observed association falls outside the bounds of "close to zero," then you are entitled to **reject** the Null hypothesis.

Notice that the simulation mechanism provides a population--- a "hypothetical planet" to use the language of Lesson 34---from which many samples could be drawn. Thus, it's straight thinking for the frequentists to assign a probability to the event "a simulation trial will generate a result at least as extreme as what was observed in the real world." This probability, in the lingo of hypothesis testing, is called a "**p-value**".

In my opinion, it would have been better for everyone to rename the p-value as the ${\cal L}$-value. After all, it's a **likelihood**, a proportion calculated on a hypothetical planet.

For many people, the orthodoxy that "hypothesis testing" can properly lead only to one of two conclusions---"reject" or "fail to reject" the Null hypothesis---seems stilted and overly rigid. Like others who are not aligned with orthodoxy, they will translate the orthodox result into language that they find reasonable and more comfortable. For instance, they will say that the p-value is the probability that the Null hypothesis is true. Or, they interpret a small probability for the Null hypothesis---what should properly lead to "rejecting" the Null hypothesis---as an indication that the original hypothesis is to be "accepted" as true. And "failing to reject" often gets (mis-)interpreted as meaning the original hypothesis is not true.

The ministers of orthodoxy---statistics professors, for example---will point out that such (mis-)statements are a sign of wrong thinking. The ministers exert such power as they have to set straight the strays among the flock, for instance by giving low scores on a course final examination. In practice, limited as it is to the examination room, this punishment rarely leads to a repentant and binding return to frequentist orthodoxy.

The Bayesian orthodoxy is more permissive. It accepts as legitimate statements involving probability, such as "Hypothesis A is less likely than Hypothesis B." 

In Lesson 35 we used a Bayesian approach to address two hypotheses that were relevant to a disease C. Once the data are in---say, a test result of + in the Lesson 35 example---we can use the likelihoods (sensitivity and specificity) and the disease prevalence to calculate the probability that, for a + test result, the patient actually has C. 

The frequentists don't disaggree with the legitimacy of a **likelihood**. That is just a probability under the assumption that a given hypothesis is true. Where they depart from the Bayesians is in accepting the "probability of a hypothesis." For, in order to calculate the relative probability of two hypotheses, you need to combine the likelihoods with a prior idea of how likely the hypotheses were in the first place. Frequentists assert that any such prior ideas are subjective. 

Few people will disagree with the idea that good science ought to be objective. Yet allowing some scope for subjectivity can lead to better informed decision-making. To illustrate, let's tell a story that illustrates the benefits of assigning a probability, subjective though it may be, to a hypothesis.

Imagine you are a doctor. A long-time patient comes to your clinic. His recent symptoms have him thinking that there is a strong chance he might have cancer. You, the doctor, ask questions about the patient's symptoms and do a brief physical examination. 

A standard medical practice is to construct a "**differential diagnosis**". In doing this, you construct a mental list of the medical disorders that might account for the symptoms and examination results. Since you have extensive training and experience, this list might be long and contain disorders that are relatively common and some that are rare. 

To keep the story simple, let's make the list of plausible disorders unrealistically short: the patient either has cancer or has the flu. Each of these is a hypothesis. Continuing the process of differential diagnosis, you, the doctor, consider what medical steps might point in favor of one hypothesis or the other. For instance, you could order a tissue biopsy. Less invasive and less costly, you could decide on ordering a magnetic resonance imaging (MRI) or, simpler, using a medical screening test for cancer. Or you might even direct the patient to take some over-the-counter flu treatment and report back in two weeks if the symptoms haven't resolved.

In medical school, you learned a mantra: "When you hear hoofbeats, think horses, not zebras." Zebras are very rare in most parts of the world, horses much less so. So horses are a much more likely source of hoofbeats than are zebras.

To make an appropriate decision, you consider what you know about the patient. Had he just finished a round of chemo-therapy six months ago? If so, an MRI might be a good choice. Is the patient elderly? The elderly as a group have a greater risk of developing cancer than the young. So for an elderly patient, you might decide on ordering a screening test. Does the patient have a history of good health and an active imagination? Then the flu medication might be the right road to go down.

A Bayesian would interpret this decision-making process in terms of probabilities. You assign, based on your observations, a probability to each hypothesis. If the probability of the cancer hypothesis is much smaller than the probability of flu, and if the cost of making a mistake (as measured by a "loss function," see Lesson 34) is small, the flu medication is a good next step. But if the probability of cancer is not so small, and the cost of a delay is high, then ordering the screening test seems appropriate.


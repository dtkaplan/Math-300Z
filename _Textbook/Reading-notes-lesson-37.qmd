---
title: "Calculating a p-value"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
lesson <- 37
source("../_startup.R")
```


This Lesson continues the discussion of Null hypothesis testing (NHP) started in Lesson 36. Recall from that lesson that the Null hypothesis is a statement in line with the claim that "there is no relationship between these variables" or "nothing is going on." For example, in a study about the effectiveness of a new drug, the Null hypothesis will be that the drug has no effect at all. Another example: In an economics study about the possible relationship between a country's "corruption index" and interest rates, the Null hypothesis would be "corruption is unrelated to interest rates."

The work that *precedes* an NHP involves acquiring data, modeling it in a way that illuminates the *relationship of interest*, then summarizing that model. Often, the summary takes the form of a model coefficient, but it might be the model's R^2^ or the incremental R^2^ from a nested set of models. (See @sec-partial-R2.) Whatever the details, we will call the summary $\mathbb{Sreal}$.

The primary calculation of an NHP, described in @sec-under-the-null, is to generate simulated data frames, each involving shuffling to annul the *relationship of interest*. Then, model and summarize the simulated data frames in exactly the same way as for the actual data. The result for the first simulated data frame is $\mathbb{Snull}_1$, for the second $\mathbb{Snull}_2$, and up to, say, $\mathbb{Snull}_{1000}$. [The exact number of simulated data frames does not matter. We will use 1000 to simplify the arithmetic.]{.aside}

At this point, there are 1001 summaries: one $\mathbb{Sreal}$ and 1000 $\mathbb{Snull}_i$, where $i=1, 2, \ldots, 1000$. These become the raw material for calculating numerical culmination of the NHT: the **p-value**. 


## The p-value

@fig-shuffle-mother (reproduced in the margin) shows an example of the raw material: 1000 values of $\mathbb{Snull}_i$ and the single, unique value of $\mathbb{Sreal}$ from the model `lm(height ~ mother, data=Galton)`.

```{r}
#| column: margin
knitr::include_graphics("www/fig-shuffle-mother.png")
```

The p-value comes from the fraction of the $\mathbb{Snull}_i$ that are larger in value than the $\mathbb{Sreal}$. In @fig-shuffle-mother, *none* of the 1000 $\mathbb{Snull}$ are larger than $\mathbb{Sreal}$. Therefore, we write $p < 1/1000$ or $p < 0.001$.

The p-value is the final result of an NHT. TALK ABOUT how to write about the p-value LATER ON.

NHT is such a popular technique that statistical software will find the p-values from regression models for you. Consequently, in practice the shuffling technique is reserved for more specialized situations. 
For these Lessons, the specialized "situation" is pedagogical; we are trying to help you understand the *concept* of p-values and shuffling provides a concrete way to do this. In most modeling work, however, pedagogy is not an issue. So expect to use software to calculate p-values. 


The p-value software we use in these Lessons is the two summary functions, `regression_summary()` and `anova_summary()`.

```{r}
lm(height ~ mother, data = Galton) %>% regression_summary()
```

The regression model `height ~ mother` has two coefficients (the "estimate" column from `regression_summary()`). Insofar as our concern is the relationship between child's `height` and `mother`, only the p-value for `mother` is of interest. 

::: {.callout-warning}
## Software for P-values 

In these Lessons, we use the `regression_summary()` and `anova_summary()` R functions to calculate p-values from models. These two functions come from the `{math300}` package, which was written specially for these Lessons. In standard R, the equivalents are `summary()` and `anova()`. 

In fact, `regression_summary()` and `anova_summary()` are merely *wrappers* around `summary()` and `anova()`. The wrappers make sure that the output is in the form of a data frame and therefore suitable for data wrangling. The output from `summary()` and `anova()`, however, are not in a data-frame format.
:::

## Basic interpretation of p-values

When the p-value is small, the likelihood of $\mathbb{Sreal}$ under the Null hypothesis, that is, ${\cal L}_\mathbb{Sreal}(\text{Null hypothesis})$ is also small. A small likelihood of a given hypothesis means that the hypothesis is not a compelling explanation for the observed $\mathbb{Sreal}$.

There is a formal vocabulary for NHT. Instead of saying, casually, "The Null hypothesis is not a compelling explanation for $\mathbb{Sreal}$, the formal NHT statement is, "The Null hypothesis is **rejected**."

Commonly, the threshold 0.05 is given as the numerical definition of "small" in "small likelihood." For example, the p-value on the `mother` coefficient in the model `lm(height ~ mother, data=Galton)` is $p < 0.01$. This is obviously less than 0.05, so the outcome of the NHT is to "reject the Null hypothesis."

Suppose, on the other hand, that the p-value had been "large," that is $0.05 < p$. What phrase should we use to summarize this situation. It is tempting---but wrong---to think that this leads to "accepting" the Null hypothesis. Instead the proper NHT phrase to use is "fail to reject the Null." 

We will return in Lesson 38 to the question of whether 0.05 is a good threshold to use. That's part of a broader controversy. 

Another part of the NHT formal vocabulary is the phrase "statistically significant." In the everyday sense of the word, "significant" suggests "important,", "worthy of attention," or "noteworthy." In NHT speak, "statistically significant" is a synonym for "reject the Null." The NHT meaning of "statistically significant" has nothing at all to do with utility of the result.

::: {.callout-note}
## "Significance" and significant digits

In the regression summary of the `height ~ mother` model, the p-value on the `mother` coefficient was reported as `1.079105e-09`. This is a symptom of the choice by software designers to report more digits than are genuinely useful. 

Statistician Jeffrey Witmer, in an editorial in the [*Journal of Statistics Education*](https://www.tandfonline.com/doi/full/10.1080/10691898.2019.1702415), distinguishes between the "mathematical" information in a p-value and the "statistical" information. Mathematically, the p-value is the result of a calculation. Statistically, the p-value is used as a symbol to indicate whether the Null hypothesis is a plausible explanation for a statistical result. 

Witmer proposes a simple rule for printing p-values: Round to 1 significant digit. This means that a p-value computed to be 0.382 would be reported as 0.4. A p-value of 0.0079 would be reported as 0.008. The justification for this rule is that there is no information in the second non-zero digit of a p-value that can meaningfully guide a conclusion about whether the Null hypothesis is a plausible explanation for a statistical result. There may be a mathematical difference between 0.0079 and 0.008, but there is no meaningful statistical difference.

Witmer also offers a simple solution to the problem of people misinterpreting "statistically significant" as related to the everyday meaning of "significant." Replace the term "statistically significant" with "statistically discernible." There is no difference between the everyday sense of "discernible"---able to be perceived---and the statistical implications. In conveying statistical information, "discernible" is more descriptive than "significant." For example, it would be appropriate to describe the implications of a p-value $p < 0.03$ as, "the relationship is barely discernible from the sampling variation."
:::

## P-values for coefficients

For any regression model, the "regression report" contains one row for every coefficient for the model. Each of these rows will have a coefficient value ("`estimate`") and a p-value. There are two additional columns: a standard error for the coefficient ("`std.error`") and a value (labeled "`statistic`") that is *always* just the estimate divided by the standard error. 

The idea of the standard error was introduced in Lesson 23 (@sec-subsampling). The point of the standard error is to summarize the amount of sampling variation in the coefficient. But we standardized on the confidence interval format to summarize sampling variation.

Many statisticians think there is little point to calculating a p-value on a model coefficient because the confidence interval contains all the information needed.


t-tests 

p-tests

::: {.callout-note}
## p-values in context: covariates

Use age at marriage data.


:::


## P-values for F 

Say how a model coefficient is different from a set of terms. The question is whether the new term adds information on top of the existing terms. 




--------

In talking about the regression report, show that the confidence interval on a coefficient is constructed using exactly the same information as a hypothesis test. 

--------

Since "fail" and "reject" are unattractive words, in practice other expressions are used. One of the notations is $p < 0.05$, another is to put a asterisk ($^\star$) next to the value of the effect size or R^2^. Both of these correspond to "reject the Null." The notation used for "fail to reject" is to put nothing next to the effect size or R^2^, but it would be more appropriate simply to list the effect size as "n.s." to stand for "not significant."

----------

The "test" is not about whether the Null hypothesis is true, and certainly not about the probability that the Null hypothesis is true. Instead, the test, like the + or - tests we have already studied, has **two possible results**. We might encode these results as + or -, or as yes/no, but conventionally the two outcomes are properly encoded as "**reject the null hypothesis**" or "**fail to reject the null hypothesis**." Unlike the + or - testing framework, in hypothesis testing the result is not to be used to calculate a posterior probability, in the "hypothesis testing" process you either "reject" or "fail to reject" the Null hypothesis.

---------

If the simulation trials rarely or never produce an association as strong as that found in real-world observations, you can fairly conclude that the assumptions embedded in the simulation ought to be *rejected* as a proper description of the real world. On the other hand, if the simulation shows good accord with the real-world observations, you can ... what? It's not a good idea to claim the simulation is a correct description of the real world; it's just a simulation. Instead, the proper statement is that you "*fail to reject*" the hypothesis hard-coded into the simulation.




--------------




So far, we've used only the first three columns of the regression report: the name of the term to which the remaining entries belong, the estimate of the coefficient on that term, and the standard error of that estimate.

```{r}
lm(mpg ~ wt + hp, data = mtcars) %>% 
  regression_summary()
```

There are two more columns to go. The fourth column is labelled "**statistic**" (short for "**test statistic**") and the fifth column is the "**p-value**."

It's the p-value that concerns us here, the "statistic" is just an intermediate on the way to calculating the p-value. Both are reported because, in some fields people are accustomed to reading the statistic to draw quick conclusions. But in every field, the p-value is used.

The p-value is at once incredibly simple to interpret and impossibly difficult to make sense of. This contradiction is the reason many statisticians have called for moving away from the p-value as a summary of a result. We will discuss the reasons for the controversy in Lesson 38. In Lesson 37, we'll show how the p-value is computed from the test statistic and introduce another report summarizing a model, the "**ANOVA report**," which is useful for many purposes.

In this Lesson, we'll explain the "incredibly simple" interpretation of the p-value and the subtle logic behind it. This is important because frequently (pretty close to "always") people mistake the p-value as addressing a completely different question than the question it actually pertains to. It's useful to know about this misconception, because it points to a different question that often more directly addresses the needs of researchers and decision makers.

## "Incredibly simple" interpretation

As you will see, the p-value is always a number between zero and one. When the p-value is small, the conclusion is that the corresponding explanatory variable is contributing to explaining the variation in the response variable. That is, a p-value that's small is justification for believing that there is a connection of some sort between the explanatory variable and the response variable.

"Small" in the phrase "when the p-value is small" is most usually taken to mean $p < 0.05$. But different fields have different standards for defining small. For instance, it's common in psychology to consider $p < 0.10$ as fairly small, while in physics, "small" means perhaps $p < 0.001$ or even $p < 0.000001$.

It may seem odd that there is no universal agreement about "small." The reason is that p-values are part of a *standard operating procedure* for evaluating research results to know if they are worthy of publication.

In physics, laws and models are meant to be exact or close to exact. Lord Rutherford (1871-1935), an important physicist who won the Nobel prize in 1908, famously disparaged the use of statistics, reportedly saying, "If your experiment needs statistics, you should have done a better experiment." This was in an era where the p-value *standard operating procedure* had not yet been invented. Today, when p-values are common in most fields, Rutherford's distaste for statistical method is reflected in p-value thresholds like $p < 0.000001$.

In other fields such as economics or psychology or clinical medicine, models are sought that are *useful* but without any expectation that they be exact. (In the 19th and early 20th century, psychologists and economists sometimes used the vocabulary of "law" to describe their findings, but "model" is more appropriate, because, unlike physics, the laws are not strictly enforced!) Often, in economics or psychology or medicine, the size of a sample used to train a model is less than, say, $n=100$. And the units of observation---people or countries, for instance---are different one from the other, quite unlike, say, electrons, which are all the same. Consequently, sampling variation is often an important source of noise, obscuring relationships or even suggesting relationships that are not really there. (See Lesson 31.) This situation---small sample size, variation in observational units, and large sampling variation---would cause many useful findings to go unreported, as would happen if $p < 0.000001$ were the standard. So a less stringent threshold for publication is used, most commonly 0.05.

## What is a p-value?



The calculation that results in a p-value is done under the assumption that the devil is right. The point is to see if the data are consistent with the devil's skeptical position. If they are consistent, then the p-value will be large. If not, the p-value will be "small."



## The world of the Null hypothesis

Recall that the Null hypothesis is the claim that "nothing is going on." For a regression model, this amounts to saying that "there is no relationship between an explanatory variable and the response variable." In order to help clarify the description in the previous section, let's do an example calculation of a p-value. We will use for the example the possible relationship between a car's fuel economy (`mpg`) and the maximum horsepower (`hp`) of the engine.

A skeptic, such as the imaginary devil from the previous section, might argue this way: "The maximum horsepower is hardly ever used by a car. Instead, the driver throttles the engine so that it generates only that power needed to move the car along under the current conditions: acceleration, speed, wind, slope of the road. The maximum horsepower just affects the range of conditions under which the car can operate. But the fuel economy is based on a standard set of conditions which is the same for every car, regardless of the horsepower."

We will pick up the action at the point where the study has been designed and the design implemented to produce data. For the example, we have the `mtcars` data frame in hand. As should be familiar at this point, the data are modeled and the model coefficient on the explanatory variable of interest is recorded. Looking at the regression report presented at the beginning of this Lesson, that coefficient is -0.0318 mpg/horsepower.

The data were collected in the real world, but that is not the world that's relevant to the Null hypothesis. The world of the Null hypothesis is one where fuel economy is utterly unrelated to horsepower. To calculate the p-value, we construct a simulation of the Null-hypothesis world. But it is not sufficient for the simulation to generate Null-hypothesis data out of the blue. We want the simulation to be as much like the actual data as possible, except that there is no relationship between `mpg` and `hp`.

Perhaps surprisingly, there is a very simple device for accomplishing this. It involves creating a new variable to use in place of `hp` in the model, but which is unrelated to `mpg`. Let's call this new variable `hp_null`. We can generate `hp_null` by taking the values in `hp` and shuffling them. This randomized version of `hp` has no relationship to `mpg` because it is being dealt out to each row of the data frame at random.

Here's what the shuffling looks like, pretending for readability that there wee only ten rows in `mtcars`. `r set.seed(111)`

```{r}
Samp <- mtcars %>% 
  select(mpg, wt, hp) %>%
  sample_n(size=10) %>%
  mutate(hp_null = shuffle(hp))
Samp
```

We'll use such data, replacing the actual `hp` with the shuffled `hp`, to find the model coefficient on `hp`. This can be done concisely:

```{r}
set.seed(112)
lm(mpg ~ wt + shuffle(hp), data = mtcars) %>%
  regression_summary()
```

In this trial, the coefficient on the shuffled `hp` is 0.0120. Of course the coefficient might well be different if the trial were repeated. Let's run 1000 trials, from each of which we'll extract the coefficient on the shuffled `hp`.

```{r}
Trials <- do(1000) * { 
  lm(mpg ~ wt + shuffle(hp), data = mtcars) %>%
  regression_summary() %>%
  filter(term == "shuffle(hp)") %>%
  select(estimate)
}
```

@fig-mpg-null-trials shows the distribution of the shuffled `hp` coefficient, compared to the coefficient we found from the original, unshuffled data.

```{r}
#| label: fig-mpg-null-trials
#| fig-cap: "The sampling distribution of the shuffled `hp` coefficient"
gf_jitter(estimate ~ 1, data = Trials, alpha=0.3, width=0.3) %>%
  gf_violin(color=NA, fill="blue", alpha=0.5, width=0.1) %>%
  gf_lims(x=c(0,2)) %>%
  gf_hline(yintercept = ~ -0.03177295, color="red")
```

You can see in @fig-mpg-null-trials that the coefficient on the shuffled `hp` is near zero, as would be expected since we enforced `hp_null` to be unrelated to `mpg`. Almost always, the coefficients on `hp_null` are in the interval $\pm 0.02$. That is to say, even though `hp_null` is unrelated to `mpg`, sampling variation will spread out the estimated coefficient away from the ideal of zero. The amount of spread due to sampling variation is $\pm 0.02$.

The estimated coefficient on `hp` in the original, unshuffled data is shown as a red horizontal line. You can see that this is farther from zero than any of the null-hypothesis trials. Since there were 1000 trials, the extreme nature of the coefficient from the original data let's us eyeball the probability of that coefficient (or larger) coming out of the Null hypothesis simulation is something on the order of one-in-a-thousand. A detailed calculation---refer to the regression table at the start of this Lesson---puts that probability at $p = 0.0015$.

## What to conclude?

Remember always that the p-value is a probability calculated in a **hypothetical world** the world of the Null hypothesis. In the calculation, we are able to place the data in this hypothetical world by shuffling the explanatory variable.

No calculation done in the Null hypothesis world is going to tell us whether that hypothesis is correct or not. Nonetheless, that the Null hypothesis simulation did not generate a coefficient as large as that in the actual data suggests that the data are inconsistent with the Null hypothesis, that we can in the case of the `hp` coefficient regard the Null hypothesis as an implausible candidate to account for the data.

Almost always, newcomers to this p-value based scheme of hypothesis testing misinterpret the p-value to be the probability that the Null hypothesis is right. Small p-value would thus mean a small probability that the Null is right.

But suppose we want to do a calculation to produce something in the format "the probability that the Null is right?" The probability that decision-makers are usually interested in is the relative conditional probabilities for each of a set of hypotheses of interest. The "condition" under which these probabilities are calculated is, "*given the data at hand*." Returning to the notation of Lesson 34, this is $p(H | \text{data})$, where $H$ stands for each of the hypotheses of interest, say, that a drug has a large effect, a medium effect, no effect at all, or even a negative effect. The framework for calculation is called "**Bayesian**" statistics, the ideas of which date from the very beginnings of the emergence of statistical method.

To illustrate the Bayesian approach, return to Lesson 35 when we were evaluating the performance of classifiers. There, we had two hypotheses that were relevant. In the context of health, these might be $H_\text{sick}$ and $H_\text{healthy}$. The quantity of ultimate interest to the patient is p(sick given the test result). To calculate this probability we need to take a round-about route. We first find two completely different probabilities: p(test result given sick) and p(test result given healthy). In practice, these two probabilities are accessible: take a group of sick patients and see what fraction of them have positive tests, and take a different group of people who are healthy and see what fraction of that group have positive tests. With those two probabilities in hand, we take an estimate of the **prevalence** of sickness: p(sick). Then the probability of clinical interest, p(sick given test result) can be calculated using the Bayesian formula, just as we did in Lesson 35.

In contrast, the p-value is a probability in a different format: $p(\text{summary(data)} | H_0)$. Here, $H_0$, the Null hypothesis, is indeed a specific hypothesis, but not any hypothesis that motivates the research. The quantity "summary(data)" is a particular summary computed from the data, say the sample mean or a regression coefficient.

The p-value probability is bound to be confusing on first sight (and, for most people, on second, third, and later sightings). After all, we know exactly what is the "summary(data)"; we just calculated it from the data! The probability of "summary(data)" is therefore 1, at least until you understand what is the event being summarized by the p-value probability.

For the p-value, the random event that lies behind the probability is a number generated by a process: Go to the world of the Null hypothesis, that boring world of "nothing happening" or "no relationship between variables." @fig-four-planets lays out the different worlds involved in statistical inference using the metaphor of planets.

::: {#fig-four-planets layout-ncol="4"}
![Planet Alt](www/planet-alt.png){#fig-planet-alt}

![The real world](www/planet-earth.png){#fig-planet-earth}

![Planet Sample](www/planet-sample.png){#fig-planet-sample}

![Planet Null](www/planet-null.png){#fig-planet-null}

The four planets of the statistical solar system.
:::

What motivates the work of collecting and modeling data is a hypothesis about the world. Typically, such hypotheses are simplistic, cartoon-like ideas about the shape of things.

Naturally, our ultimate interest is in the real world. But we don't have the whole Earth at hand; we have only a sample from it. The sample is something like the real world, but being a sample it is somewhat patchy, assembled from the $n$ cases in our sample. Planet Sample lacks the detail of the real world, but each point on Planet Sample comes from a genuine place on Planet Earth.

The p-value is a probability computed on Planet Null, that boring world where nothing is going on and any perceived patterns are illusions, the appearance produced by random and shifting gusts of the winds of chance.

Almost all the work of calculating a p-value takes place on Planet Null. That work consists of simulation trials. Each trial involves taking a sample from Planet Null, summarizing it, and recording the result for later comparison the the summary calculated from Planet Sample.

It may seem perverse to base conclusions for real-world data on an imagined planet of no direct interest. And it is! At a minimum, we should put into competition at least two hypotheses: for instance Planet Alt and Planet Null. But in the world of the first half of the 20th century, when statistical analysis of data was just coming into the mainstream, it was impractical to compute the competing probabilities of the Bayesian style of reasoning. The reason: the computers and algorithms we use now had not been invented.

In addition, those early statisticians put a big premium on what they called "objectivity." They did not think the subjective beliefs of researchers---the cartoon alternative hypothesis---should play any role in data analysis. The method they ended up inventing, p-values, was based only on a hypothesis that everyone could agree might be in play: the Null hypothesis. Unfortunately, the only valid conclusions that can be drawn from p-values are 1) "reject the Null hypothesis" and 2) "fail to reject the Null hypothesis." These conclusions don't guide us to favor any other particular hypothesis and so are inadequate to support decision-making in the real world. But the p-value conclusions can be the basis for a standard operating procedure: If the conclusion is "fail to reject the Null hypothesis," don't allow the work to be published.

So, standard operating procedures were based on the tools at hand. We will return to the mismatch between hypothesis testing and the contemporary world in Lesson 38.

## More metaphors?

Use this from Section 19.4 of *Computational Probability and Statistics*?

> A US court considers two possible claims about a defendant: she is either innocent or guilty. Imagine you are the prosecutor. If we set these claims up in a hypothesis framework, the null hypothesis is that the defendant is innocent and the alternative hypothesis is that the defendant is guilty. Your job as the prosecutor is to use evidence to demonstrate to the jury that the alternative hypothesis is the reasonable conclusion.

> The jury considers whether the evidence under the null hypothesis, innocence, is so convincing (strong) that there is no reasonable doubt regarding the person's guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis).

> Jurors examine the evidence under the assumption of innocence to see whether the evidence is so unlikely that it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person's innocence. They are simply not convinced of the alternative that the person is guilty.

> This is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.

> There are two types of mistakes possible in this scenario, letting a guilty person go free and sending an innocent person to jail. The criteria for making the decision, reasonable doubt, establishes the likelihood of those errors.

> Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.

> There are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized below.

::: {.content-visible when-format="html"}

$$
\begin{array}{cc|cc} & & \textbf{Test Conclusion} &\\ 
& & \text{do not reject } H_0 &  \text{reject } H_0 \text{ in favor of }H_A  \\
\textbf{Truth} & \hline H_0 \text{ true} & \text{Correct Decision} &  \text{Type 1 Error}  \\
& H_A \text{true} & \text{Type 2 Error} & \text{Correct Decision}  \\
\end{array} 
$$
:::

::: {.content-visible when-format="pdf"}

Make this table nicer by constructing it in some other system.

. | do not reject H_0_ | reject H_0_ in favor of H_A_
------|-----------------|-----------------
H_0_ true | Correct decision | Type 1 error
H_A_ true | Type 2 error | Correct decision
 

:::

> A **Type 1 error**, also called a **false positive**, is rejecting the null hypothesis when $H_0$ is actually true. Since we rejected the null hypothesis in the gender discrimination (from the Case Study) and the commercial length studies, it is possible that we made a Type 1 error in one or both of those studies. A **Type 2 error**, also called a **false negative**, is failing to reject the null hypothesis when the alternative is actually true. A Type 2 error was not possible in the gender discrimination or commercial length studies because we rejected the null hypothesis.

---------

Hypothesis tests are very easy to carry out. You don't need to choose a particular hypothesis to test: it's always the Null hypothesis. The test has only two possible conclusions: "rejecting the Null hypothesis" or "failing to reject the Null hypothesis." The conclusion indicated by the test is signalled by a number called a "**p-value**."  

In this Lesson, we will demonstrate a general way to calculate p-values that has a simple logic. Even simpler, model-building software such as `lm()` will do the calculation for you. Often, there will be more than one p-value, each describing a different aspect of the model, so you have to identify which of the p-values is relevant for your purpose. This lesson will show you how. 

Once you have the relevant p-value in hand, reaching the conclusion of the test is not hard, but it is confusing that there are different styles that are used in different journals or in different fields.

We'll start by talking about the conclusion, then move on to how to select the relevant p-value, and finish by showing you the logic(s) behind the calculation of the p-value.

## The conclusion

Remember that there are only two possible conclusions from a hypothesis test: reject or fail to reject the Null hypothesis. The translation between a p-value and a hypothesis-test conclusion can be stated very simply because the p-value is always a number between zero and one.

> *A small p-value points toward rejecting the Null hypothesis. A large p-value points to "failing to reject" the Null.*

The definition of "small" differs from one scientific field to another. Physics proudly declares "small" as something like 0.001. In most other fields, "small" is set at the threshold 0.05. Sometimes, as you'll see in Lesson 38, the threshold for small should be adjusted downward, but for this Lesson we will do what is most common in many fields and use 0.05 as the dividing point between "small" and "not small."

Common sense suggests that the result of a hypothesis test should be to show the p-value. After all, it's easy to see if a number if greater or less than 0.05. But scientific convention says otherwise in part because "hypothesis testing" is a merger of two different traditions with different styles.

In principle, the result of a hypothesis test could be reported very simply either as "reject" or "fail." Understandably, researchers prefer to use a notation that avoids the negative connotations of the everyday words "reject" and "fail." Indeed, "reject" is often a very positive conclusion for a researcher. And "fail to reject" in no way means that the researcher did something wrong.

Possibly the most common notation for the result of a hypothesis test is written $p < 0.05$. Researchers like to show off a small p-value. In order to avoid violating the convention that the number itself is not reported directly, they will hint at the smallness by writing $p < 0.01$ or $p < 0.001$, whichever happens to be justified. 

In some fields the result of the hypothesis test is reported using the `*` character. A single `*` is exactly the same as $p < 0.05$. Vanity being an emotion that scientists share with the rest of us, sometimes `**` is used to mean `p < 0.01` and `***` to mean `p < 0.001`.

Why not just report the p-value itself? One reason is that p-values can be minutely small (e.g. $< 10^{12}$) just because a lot of data is available. 

::: {.callout-note}
## Avoiding "fail"

Nobody likes to summarize their work with the word "fail." And so, when "fail to reject the Null hypothesis" is the correct conclusion, people express this in softer ways.

It's very common for the conclusion "fail to reject the Null" simply not to be reported at all. Historically, and even today, some journals will not accept  for publication a scientific article with the conclusion "fail to reject the Null." 

Consider the situation of a researcher whose years-long project has led to a p-value of 0.07. To soften the blow of "fail to reject," the researcher will report the p-value itself so that the reader can see how close it is to small. In some literatures, you will see language like "tending to significance" instead of "fail to reject." In some fields, research publications will show the notation $p < 0.1$. This also indicates failure to reject the null hypothesis.

Journalists eager to publish reports about scientific work, but facing a p-value that is a little too large, will occasionally qualify their report with this phrase: "... although the work did not reach the rigorous scientific standard for statistical significance."

All of these are dodges. There's nothing "rigorous" about $p < 0.05$ although seems unfair that a researcher who had a plausible idea and did the work to test it honestly does not get to publish that work and receive acknowledgement that they are a hard-working part of the overall scientific enterprise.

:::

## Which p-value is relevant?

The regression-table summary of a model conveniently presents a p-value for each and every coefficient in the model. For instance,

```{r}
lm(time ~ climb + distance + sex, data=Hill_racing) %>%
  regression_summary()
```

Each term in the model has a coefficient, called the "estimate." Like all estimates, that of the coefficient involves uncertainty due to sampling variation. This uncertainty is reported, as we've seen in Lessons 23 and 24, by the standard error. The column "statistic" in the regression table is an intermediate step in computing the final column: the p-value itself.

This example of a regression table shows that p-values can sometimes be very, very small. Such smallness is often mis-interpreted as indicating that a very powerful result has been found. This is simply nonsense, which is why the more dignified notation $p < 0.05$ or `*` is to be preferred.

Which of these p-values is relevant depends on what you are interested in. If your focus is on the effect of the vertical `climb` on the winning time, then you should look at the p-value on the `climb` coefficient. If you're interested in whether the race `distance` is a factor in determining the winning time, use the p-value on distance. The regression report gives p-values for every coefficient because it has no way of determining mathematically which variable is of interest and which variables are "covariates," that is, not of direct interest.

It's important to note that when looking at model coefficients, a simpler report of the confidence interval on the coefficient carries exactly the same information as the comparison $p < 0.05$. Here's that report on the hill racing model:

```{r}
lm(time ~ climb + distance + sex, data=Hill_racing) %>%
  confint()
```

When a confidence interval does not include zero---that's the case for all of the coefficients here---then the p-value is $p < 0.05$. Most statisticians argue that the confidence interval is a better kind of report, since it also is informative about effect size. For instance, the p-value on `climb`, reported as 0, is only about the Null hypothesis. But the confidence interval, here [2.5 to 2.7] seconds per meter, gives insight into how much an extra meter of climb will prolong the race.

Reporting p-values rather than confidence intervals on coefficients has no scientific merit. But it can be hard to change traditions. And, as you'll see in the next section, it's not only model coefficients that have p-values.

## Analysis of variance

Sometimes the interest is more general: Do any of these terms contribute to explaining variation in the response variable? In such situations, the appropriate p-value is one that compares one model to another. This style of p-value---not on the individual coefficients but on model terms---comes from a calculation called "analysis of variance."

::: {.callout-warning}
FILL THIS IN.
:::

## A simple, general-purpose way to calculate a p-value

Since p-values are always given in the regression table and the ANOVA table, it's rarely necessary (except in statistics classes!) to calculate one yourself.

Even so, there is a simple method to calculate a p-value that provides good insight into what a p-value means. We will demonstrate this method so that you can gain that insight, but only in very specialized and esoteric circumstances would anyone use this method in place of reading the p-value from a regression table or ANOVA report.


::: {.callout-warning}
Put shuffling here!
:::

## The hypothesis testing zoo

Zoos are fun and interesting because of the great diversity of animals on display. The person who, looking in on a huge elephant or a long-legged crane or a chimpanze, drones, "Oh, another animal. That's all." 

As statistics developed, early in the 20th century, distinct tests were developed for different kinds of situations. Each such test was given its own name, for example, a "t-test" or a "chi-squared test." Honoring this history, statistics textbooks present hypothesis testing as if each test were a new and novel kind of animal.

In fact, almost all the different tests named in introductory statistics books are really just different manifestations of regression. Regression is to "animal" the way t-test is to "elephant." An important theme in the history of statistics is that out of the diversity of statistical methods, almost all of them are encompassed by one method: regression modeling.

In these Lessons, we've focussed on that one method, rather than introducing all sorts of different formulas and calculations which, in the end, are just special cases of regression. Even so, most people who are taught statistics were never told that all the different methods fit into a single unified framework. Consequently, they use different names for the different methods. To communicate in a world where people learned the old-fashioned names, you have to be able to recognize those names know which regression model they refer to. In the table below, we will use different letters to refer to different kinds of explanatory and response variables.

x and y - an ordinary quantitative variable

group - a categorical variable with multiple ($\geq 3$)  levels.

yesno - a categorical variable with exactly two levels (which can always be encoded as a zero-one quantitative variable)

Model formula | traditional name
--------------|-------------------
`y ~ 1`         | **t-test** on a single mean
`yesno ~ 1`     | **p-test** on a single proportion.
`y ~ yesno`     | **t-test** on the difference between two means
`yesno1 ~ yesno2` | **p-test** on the difference between two proportions
`y ~ x`         | **t-test** on a slope
`y ~ group`     | **ANOVA** test on the difference among the means of multiple groups
`y ~ group1 * group2` | **Two-way ANOVA**
`y ~ x * yesno` | **t-test** on the difference between two slopes. (Note the `*`, indicating interaction)

Another named test, the **z-test**, is a special kind of t-test where you know the variance of a variable without having to calculate it from data. This situation hardly every arises in practice, and mostly it is used as a soft introduction to the t-test.

::: {.callout-note}
## The chi-squared test

Most statistics books include two versions of a test invented around 1900 that deals with counts at different levels of a categorical variable. This chi-squared test is genuinely different from regression. And, in theoretical statistics the chi-squared distribution has an important role to play.

The chi-squared test of independence could be written, in regression notation, as `group1 ~ group2`. But regression does not handle the case of a categorical variable with multiple levels. 

However, in practice the chi-squared test of independence is very hard to interpret except when one or both of the variables has two levels. This is because there is nothing analogous to model coefficients or effect size that comes from the chi-squared test.

The tendency in research, even when `group1` has more than two levels, is to combine groups to produce a `yesno` variable. Chi-squared can be used with the response variable being `yesno` and almost all textbook examples are of this nature.

But for a `yesno` response variable, a superior, more flexible and more informative method is logistic regression. 
:::

ANOVA, which is always a comparison of two models, say `y~1` versus `y~group` involves something called an F-test. For the simpler setting of the t-test, the model `y~yesno`, an F-test can also be done. Which to do, t or F? It turns out that t^2^ is exactly the same as F. 



Statistics textbooks usually include several different settings for "hypothesis tests." I've just pulled a best-selling book off my shelf and find listed the following tests spread across eight chapters occupying about 250 pages.

- hypothesis test on a single proportion
- hypothesis test on the mean of a variable
- hypothesis test on the difference in mean between two groups (with 3 test varieties in this category)
- hypothesis test on the paired difference (meaning, for example, measurements made both before and after)
- hypothesis test on counts of a single categorical variable
- hypothesis test on independence between two categorical variables
- hypothesis test on the slope of a regression line
- hypothesis test on differences among several groups
- hypothesis test on R^2^

---
title: "Accounting for prevalence"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
lesson <- 35
source("../_startup.R")
```

Lesson 34 introduced classifiers, model functions where the output is one of two values which we called + and -. 

The designer of the classifier chooses what inputs to use and the architecture of the model. The model is fit to training data, thereby producing a specific classifier function.

::: {.callout-note}
## Example: Survival

To illustrate the construction of a classifier function, imagine yourself participating in the 1970s-1990s Whickham study of nurses in the UK. Your task in 1991 is to follow up on more of the nurses who were interviewed in the 1970s. You have already done some of the work and stored the results in the `Whickham`. To make your search for the remaining nurses more productive, you want to build a classifier whose output, + or -, will direct you to look first in the death records or, for those with a - output, in the employment and pension records.

The inputs to the classifier will be the data you have already assembled: the `age` and `smoker` status of the nurse when first interviewed. Since `Whickham` includes an `outcome` variable---you've already followed up on those nurses---you can train your classifer with that data frame. You have decided on the model `outcome ~ age*smoker`, but since `outcome` is categorical with levels `"Alive"` and `"Dead"`, you will use a zero-one encoding.

```{r warning=FALSE}
Whickham_model <- lm(zero_one(outcome, one="Alive") ~ age*smoker, data=Whickham)
Whickham_function <- makeFun(Whickham_model)
```

To get a sense for the range of outputs of the model, you try out the model function:

```{r}
Whickham_function(age = 75, smoker="Yes")
Whickham_function(age = 20, smoker="No")
```

::: {.callout-warning}
## Move to Learning Checks: LC 35.H

Explain how come the output of `Whickham_function()` can be bigger than one?

Use logistic regression to produce a model whose output can be interpreted directly as the probability of being alive at follow up.
:::

You are not quite done with the task of building a classifier. The final step is to choose a threshold to translate the numerical output of `Whickham_function()` into one of the classifer outputs, + or -.

Probably you are not sure what that threshold should be. A good step in such situations is to take a guess and see what you get by way of model performance. Then you can improve the guess if need be. In the following demonstration, we write an R function that takes the `age` and `smoker` inputs, uses `Whickham_function()` to translate them into a number, and applies a threshold to that number to translate it into a + or - output.

::: {.callout-warning}
## Demonstration: Turning a model function into a classifier function

```{r}
Whickham_classifier <- function(age, smoker, threshold) {
  number <- Whickham_function(age=age, smoker=smoker)
  ifelse(number >= threshold, "+", "-")
}
```

Try it out to make sure that the `Whickham_classifier()` takes appropriate inputs and returns an output in the right form. We'll use a threshold of 0.6---just a guess.

```{r}
Examples <- data.frame(age = c(75, 20), smoker=c("Yes", "No"))
Examples %>%
  mutate(test_result = Whickham_classifier(age, smoker, threshold=0.6))
```
:::



Having built a classifier, we are ready to check the performance of the classifier in terms of false positive and false negative rates:

```{r}
Whickham %>%
  mutate(test_result = Whickham_classifier(age, smoker, threshold=0.6)) %>%
  group_by(outcome, test_result) %>%
  tally() %>%
  mutate(proportion = n/nrow(Whickham))
```

Evidently, a + test result is a prediction that the person will be alive on follow-up. Consequently, the (Alive, -) row has the false negatives (a negative result that is wrong).
The (Dead, +) row is the false negatives. So the false-positive rate---false positives per test---is 5.2%. The false-negative rate is 13.6%.

We still have to explore which choice of the threshold will make the test as good as possible. Our ultimate measure is based on the consequences of the test being wrong. These consequences are encoded into the *loss function*. 

For our purposes here, where the classifier will guide how we search for each person, the loss function could be based on the difficulty of the different kinds of searching. It is comparatively easy to check the death records, they are public, stored in a central database, and searchable by the nurse's social insurance ID number. It's hard to go through employment and pension records; they are private and decentralized. So we'll define the loss function like this:

- loss_function(false positive) $\equiv$ 10 loss units
- loss_function(false negative) $\equiv$ 1 loss unit

With this loss function, the expected loss per test is 
$$10 \times 0.052 + 1 \times 0.136 = 6.56\  \text{loss units per test}$$
To find the *best* value for the threshold, try other possible values for the threshold. Here, we'll try 0.8, re-apply the test with that threshold, and tabulate the false-positive and false-negative rates.

```{r}
Whickham %>%
  mutate(test_result = Whickham_classifier(age, smoker, threshold=0.8)) %>%
  group_by(outcome, test_result) %>%
  tally() %>%
  mutate(proportion = n/nrow(Whickham))
```

The 0.8 threshold lowers the false positive rate to 1.6%, but raises the false negative rate to 30%. That's the typical behavior of classifiers, changing the threshold to lower the false positive rate will raise the false negative rate, and *vice versa*.

With a threshold of 0.8, the expected loss per test is
$$10 \times 0.16 + 1 \times 0.30 = 1.9\ \text{loss units per test}$$
Better. 

Before we continue on the find the *best* threshold, we need to do a *reality check.* The loss rates we just calculated are based on the training data. In that training data, there were 945 subjects who survived out of $n=1314$ altogether. That's a prevalence of $\frac{945}{1314} = 72\%$ in the training data. It might well be that the prevalence among the remaining nurses in the study is different. 

Accordingly, we need to adjust the loss calculations to reflect the actual prevalence. The first step in doing this is to find the *sensitivity* and *specificity*, as we did in Lesson 35.

Sensitivity and specificity are easy to calculate so long as you keep in mind that sensitivity is only relevant to that part of the population who actually have the condition intended to be marked by a + test. In the Whickham example, the sensitivity is the percentage of correct results **only** among the people whose outcome is "Alive."  There are 395 + 550 such people, of whom 550 had a correct result, so

sensitivity (using the theshold 0.8) is $550/(395+550) = 58\%$.

Similarly, specificity is only relevant to the people whose outcome is "Dead". There are 348 + 21 such people, of whom 348 got the correct test result.

specificity (using the threshold 0.8) is $348/(348 + 21) = 94\%$.

Sensitivity and specificity measure the test itself. False positive and false negative rates measure the test as applied to a specific population. The relevant fact of the population is the "**prevalence**" of the condition. 

## Prevalence in the population

It often happens that the data used to *train* a classifier function is not representative of the population in which it will be used. For example, classifiers are often trained on "**case-control**" data. People with a particular condition, say, having strong symptoms of COVID-19 are easily recruited at health clinics. These are the "cases." 

The "controls" are people who don't have any symptoms. These can be recruited by, say, door-to-door canvasing of a neighborhood. If we recruit 500 cases and 500 controls, then the prevalence of COVID-19 symptoms in our database will be 50%.

Assume for the sake of an example that the sensitivity of an inexpensive, over-the-counter COVID-19 test is 90%, and the specificity is 95%. From these numbers, we can calculate the false-positive and false-negative rates.

False negatives can only arise in the part of the population who have the condition. For the 500 cases in our database, the sensitivity of 90% will work out to 450 correct positive tests and 50 incorrect negative tests.

False positives can only arise among the people who do not have the condition. For the 500 controls in our database, the specificity of 95% will work out to 475 correct negative tests and 25 incorrect positive tests. 

There are 1000 people in the database, so the false-positive rate is 25/1000 = 2.5%. The false-negative rate is 50/1000 = 5%.

However, these rates are not necessarily representative of the rates we will find when the test is used in the general population. Fortunately, we can calculate the false-positive and false-negative rates for a population with any given prevalence.

Imagine, say, that we think the prevalence of COVID-19 in the general population is 4%. This puts the false-negative rate---remember, false-negative only applies to people with the condition---at $4\% \times (1-\text{sensitivity|) = 0.04 \times 0.10 = 0.004$.

In that population, 96% do not have COVID-19. This, combined with the specificity, allows calculation of the false-positive rate: $96\% \times (1-\text{specificity}) = 0.96 \times 0.05 = 0.48. 

## Sensitivity and specificity as likelihoods

In Lesson 34, we used training data to estimate the sensitivity and specificity of the test in the C vs H classifier. In that example, the sensitivity was 90% and the specificity was 85%. We can use those facts to construct the test results on a hypothetical population with whatever *prevalence* we think is relevant to the actual population we will be testing. The arithmetic is straightforward, but we will use graphics to illustrate the calculations.

First, we synthesis a data frame that has many rows, half corresponding to imaginary people with condition C and the other half with condition H. Next, we assign a *test outcome* for each of the people. We do this in a way that duplicates the sensitivity and specificity that we measured earlier. That is, for the C people, we randomly generate a + or - test result, setting the probability of a + result to be the measured *sensitivity*. We do similarly for the H people, the only difference being that the probability of a - result will be the *specificity*. Figure @fig-planets-of-C-and-H shows 2000 rows of simulated data, with 1000 C cases and 1000 H cases. 

```{r echo=FALSE}
#| label: fig-planets-of-C-and-H
#| fig-cap: "Knowing the specificity and sensitivity of the test, we can create hypothetical worlds to use in a simulation.)"
PeopleHyp <- bind_rows(
  patient_group(0, n=600, p=0.5),
  patient_group(1, n=800, p=0.5),
  patient_group(2, n=600, p=0.5)
)
pC <- plot_people_final(PeopleHyp %>% filter(cond=="C"))
pNC <- plot_people_final(PeopleHyp %>% filter(cond != "C"))
gridExtra::grid.arrange(pC + labs(title="Planet of the C"), pNC + labs(title="Planet of the H") , ncol=2)
```

@fig-planets-of-C-and-H shows the simulated data split up into two groups, one for the C cases and the other for the H cases.

Now we are going to push hard on a fanciful metaphor in order to introduce some new terminology that will be important for Lessons 36 and 37. The metaphor is that C people originate in a *planet* populated exclusively by C people. The H people live on another planet, inhabited only by the H. On both planets, the classifier has been applied to everyone so that we know their test results: + is shown in blue, - in gray, as in Lesson 34.

Needless to say, these planets are imaginary. Let's call them "**hypothetical**" planets, planets where a particular hypothesis is known to apply. The hypothesis on the C planet is that all the inhabitants are C. Likewise for the H planet. The proportion of + test results on Planet C has been set to match the actual *sensitivity* of the classifier. Similarly, the - test results on Planet H match the actual *specificity* of the classifer. 

A new bit of statistical vocabulary that will be important in Lessons 36 and 37. A 
"**likelihood**" is a special kind of probability. Like all probabilities, it's a number between zero and one. Likelihoods are always conditional probabilities, that is a probability calculated on the assumption that a specific assumption holds. In terms of our metaphor, a likelihood is a probability calculated on a hypothetical planet.

The sensitivity is a likelihood. The hypothetical planet is Planet C, the planet where all inhabitants have condition C. On that planet, the sensitivity is the probability that a randomly selected inhabitant will get a test result +. But, since the probability is being calculated on the condition that all inhabitants have condition C, it is a conditional probability: a likelihood. 

The specificity is similarly a likelihood. The hypothetical planet is H, where all inhabitants have condition H. The conditional probability is that of getting a - result on the hypothetical planet.

We will denote likelihoods using the letter ${\cal L}$. The fanciful script reminds us that the likelihood is a fanciful probability, a probability on a hypothetical world.

The sensitivity is ${\cal L}(+ \text{ given } C)$. The specificity is ${\cal L}(- \text{ given } H)$. The "given C" means "on hypothetical planet C. The "given H" means "on hypothetical planet H." It would be just as correct to write $p(+\text{ given }C)$ and $p(-\text{ given }H)$, but I want to remind you that the the sensitivity and specificity apply only to their respective conditions, C and H. Also, phrases like "assuming H, the probability of - is ..." or "under the assumption H, the probability of - is ...."

::: {.callout-note}
## Constructing a world with a given prevalence 

Now to use the hypothetical planets C and H to construct a realistic world. By "realistic" we mean that the sensitivity and specificity are right *and* that the prevalence matches that of the target population in which the classifier is going to be used.

As notation, let's write the prevalence as $p(C)$. This is a probability, one that we will make up in order to define the target population. Notice that $p(C)$ does not have any condition. We knew it from previous studies of the target population. For this reason, it's appropriate to call $p(C)$ by a special name: the **prior** probability. The twin probability, $p(H)$ is also a prior probability. Since C and H are the only possibilities it must be that $p(H) = 1 - p(C)$.

To construct a realistic world, we're going to import people from hypothetical planets C and H. Remember, on C everyone has condition C; on H, everyone has condition H. How many people to import from each hypothetical planet? To put $N$ people on the realistic world, we will take $N p(C)$ from planet C, and $N (1-p(C))$ from planet H. The world thus populated, with $N=500$ and $p(C)$ set to 30% is shown in @fig-prevalence-world.

```{r echo=FALSE}
#| label: fig-prevalence-world
#| fig-cap: "The world created to have prevalence $p(C) = 30\\%$ and $N=500$ inhabitants. This world has 150 immigrants from Planet C and 350 from Planet H."
New_world <- bind_rows(
  PeopleHyp %>% filter(cond=="C") %>% sample_n(150),
  PeopleHyp %>% filter(cond=="H") %>% sample_n(350),
)
plot_people_final(New_world) + labs(title="A realistic world")
```

While it's feasible to do the calculations as a simulation on a computer, the mathematics is not so tricky. For example, we can compute the false-positive and false-negative rates on the realistic world:

- False positive rate:  $\left(1 - \underbrace{{\cal L}(+ \text{ given } C)}_\text{sensitivity}\right)\cdot \underbrace{p(C)}_\text{prevalence}$

- False negative rate: $\left(1 - \underbrace{{\cal L}(- \text{ given } H)}_\text{specificity}\right) \cdot \left(1-\underbrace{p(C)}_\text{prevalence}\right)$

:::

## From the patient's point of view

Having created and populated the realistic world, we can compute quantities that are useful in communicating with patients and doctors. For example, suppose a patient has just gotten a positive test. How certain an indication is this of the patient's having the condition C?

On the realistic world, there are two groups who get a positive test. One group is the people who genuinely have C and who also have a positive test. These are the true positives. The relative size of this group is $P(+ \text{ given } C) \equiv \text{proportion of true positives} = {\cal L}(+ \text{ given } C)\cdot p(C)}$.

The other group is the false positives, the people who have condition H but nonetheless received a + test. The relative size of this group is $P(+ \text{ given } H) \equiv \text{proportion of false positives} = {\cal L}(+ \text{ given } H)\cdot p(H)$. We can calculate the size of this group since $p(H) = 1-p(C)$ and ${\cal L}(+ \text{ given } H) = 1 - {\cal L}(- \text{ given } H)$.

Consequently, the probability that a + test result will be true is
$$\frac{{\cal L}(+ \text{ given } C)\cdot  p(C)}{{\cal L}(+ \text{ given } C)\cdot p(C) + {\cal L}(+ \text{ given } H)\cdot p(H)}$$

## For the patient with a + test result

The above formula looks daunting, but we know each of the quantities and can do the arithmetic.

::: {.callout-warning}
## Demonstration
For instance, in the example in Lesson 34 we stipulated that the prevalence $p(C) = 4\%$. This, of course, implies that $p(H) = 96\%$. The sensitivity was stipulated at 90%, that is, ${\cal L}(+ \text{ given } C) = 90\%$. The specificity was 85%, meaning that ${\cal L}(+ \text{ given } H) = 15\%$. Putting these values together in the formula tells us that the probability that a person with a + test actually having the condition C is:

$$\frac{0.90 \cdot 0.04}{0.90 \cdot 0.04 + 0.15\cdot 0.96} = 20\%$$
That's good news to the Lesson 34 patient who gets a positive test result: He or she has only a 20% chance of having condition C.

Now consider the situation of a patient in the world of @fig-prevalence-world. There, the population has a prevalence of 30% rather than 4%. This changes the meaning of a + test. The probability of being C given a + test is:

$$\frac{0.90 \cdot 0.30}{0.90 \cdot 0.30 + 0.15\cdot 0.70} = 72\%$$

In a population where the prevalence is high, a + test is more likely to mean what it suggests.
:::

Since the reliability of a + result differs depending on the prevalence of C, it often happens that medical screening tests are recommended for one group of people but not for another.

For instance, the US Preventative Services Task Force (USPSTF) issues recommendations about a variety of medical screening tests. According to the Centers for Disease Control (CDC) summary:  

> *The USPSTF recommends that women who are 50 to 74 years old and are at average risk for breast cancer get a mammogram every two years. Women who are 40 to 49 years old should talk to their doctor or other health care provider about when to start and how often to get a mammogram.*

Recommendations such as this can be baffling. Why recommend mammograms only for people 50 to 74? Why not for older women as well? And how come women 40-49 are only told to "talk to their doctor?"

The CDC summary needs decoding. For instance, the "talk to [your] doctor" recommendation really means, "We don't think a mammogram is useful to you, but we're not going to say that straight out because you'll think we are denying you something. We'll let your doctor take the heat, although typically if you ask for a mammogram, your doctor will order one for you. If you are a woman younger than 40, a mammogram is even less likely to give a useful result, so unlikely that we won't even hint you should talk to a doctor."

The reason mammograms are not recommended for women 40-49 is that the prevalence for breast cancer is much lower in that group of people than in the 50-74 group. The prevalence of breast cancer is even lower in women younger than 40.

So what about women 75+? The prevalence of breast cancer is high in this group, but at that age non-treatment is likely to be the most sensible option. Cancers can take a long while to develop from the stage identified on a mammogram, and at age 75+ it's not likely to be the cause of eventual death.

The [USPSTF web site](https://www.uspreventiveservicestaskforce.org/uspstf/recommendation/breast-cancer-screening) goes into some detail about the reasoning for their recommendations. It's worthwhile reading to see what considerations went into their decision-making process.


---
title: "A little more detail"
---

```{r include=FALSE}
source("../_startup.R")
```


## Lesson 19

Regression can summarize relationships in more detailed ways as well. The following model looks at the trend with age separately for males and females:

`r options(digits=3)`

```{r}
TenMileRace %>% lm(net ~ sex * age, data = .) %>% coef() 
```

Here, the age trend for women is an increase in `net` running time of 16 seconds per year of age, while for men, that increase is bigger, an extra 1.6 seconds per year of age.

There are good reasons why `lm()` organizes summaries the way it does. The `lm()` paradigm can make much more efficient use of data than `group_by()`. It also offers much more flexibility. `lm()` can handle multiple "grouping" variables together and even lets you "group" by quantitative variables.

## More stuff


Put the odds and ends here, then sort them out.

## Standard errors, margins of error, and confidence intervals

## Tiny $n$ (optional)

When you have a very small sample size---say, $n=2$---the values may coincidentally be very close together. Around 1907, William Gosset, a scientist at Guinness, discovered that such coincidences force "twice" to be $> 2$ in order to produce confidence intervals that reliably cover the mean of the data-generating process. Gosset's particular interest was in making sense of Guinness's standard testing protocols, which involve averaging the results from three small batches of beer ingredients. Contacting the leading statisticians of the day, Gosset was told that such small $n$ is "brewing, not statistics." Nonetheless, Gosset had to work within Guinness's testing protocols, which were indeed brewing but still needed statistical interpretation. 

Gosset carried out trials by hand, a large number of measurements from a study of criminals' hand sizes. (They did this kind of thing in 1900.)  Each measurement was written on a card. A trial consisted of drawing $n$ cards from the deck and calculating the mean and standard deviation of the measurements. Using computers, we can simulate the calculation of results from a Gosset-like trials using a simple function that calculates the mean and standard deviation of data from a Gaussian distribution.

```{r}
one_trial <- function(n=2) {
  vals <- rnorm(n)
  tibble(m = mean(vals), s = sd(vals))
}
```

We can pick a small $n$ and running many trials using a candidate value for "twice."


::: {.callout-warning}
## IN DRAFT

CONVERT the `beta` to something named `twice`.

```{r}
n=10
beta <- 2 / sqrt(n)
Trials <- do(100) * one_trial(n=n) %>% 
  mutate(left = m - beta*s, right = m + beta*s) 
gf_errorbarh(.index ~ left + right, data = Trials, alpha=0.5) %>%
  gf_errorbarh(.index ~ left + right, 
               data = Trials %>% filter(left > 0 | right < 0)) %>%
  gf_vline(xintercept = ~ 0, color="blue", inherit=FALSE)
```
Gosset effectively tabulated the $\beta$ multipliers

n  | $\beta$ | $t = \beta / \sqrt{\strut n}$
---|---------|------------------
2  | 8.98    | 12.7
3  | 2.48    | 4.30
4  | 1.59    | 3.18
5  | 1.24    | 2.78
6  | 1.04    | 2.57
7  | 0.92    | 2.44
$\vdots$ |  |
10 | 0.72    | 2.26
15 | 0.55    | 2.14
20 | 0.47    | 2.09
50 | 0.28    | 2.01
100 | 0.20   | 1.98
500 | 0.088  | 1.96
1000 | 0.062 | 1.96

You can see that for $n$ bigger than 10 or 20, the $t$ multiplier is 2. But for very small $n$, the t-multiplier can be considerably larger. 

You can see the wisdom of brewers here. They made tests by averaging measurements from three small batches of beer. If they had used only two batches, the confidence interval would be almost three times larger than for $n=3$, making it very hard to conclude anything about whether the tests show the ingredients to be within the quality-control standards.

Gosset's work was published under the pseudonym "Student," since Guinness forbade employees to publish under their own names. Statisticians, recognizing the value of the work (and knowing the name behind the pseudonym), came to use the name $t$, perhaps because tea was considered more refined than "beer." In many statistics texts, you will see the phrase "Student t" to refer to how Gosset's work is used.

## Interaction

RETURN TO THIS MODEL, but add an interaction.

```{r}
Mod3 <- lm(list_price ~ hard_paper + num_pages, data = amazon_books)
model_eval(Mod3, hard_paper = c("P", "H"),  num_pages=c(200, 400))
```

## Prediction interval

::: {.callout-warning}
## Demonstration: Simple-minded data analysis shows why

We constructed our model of running time using the "linear least-squares" modeling methodology implemented by the `lm()` model-training function. For the purpose of demonstration, we'll show you a simple-minded method to make a prediction. This simple-minded method would give results more or less equivalent to the least squares method if we had an almost infinite amount of data. Since we don't, the simple-minded method is not as reliable as the least-squares method.

Remember our goal: to predict the running time for a 10km race with a 500m climb. All that we have to inform the prediction is the historical data contained in the `Hill_racing` data frame. The simple-minded method is ... well ... simple to understand. We will pull out from the `Hill_racing` data frame those rows where the race distance is close to 10km and the race climb is close to 500m. For example:

```{r}
close_rows <- Hill_racing %>%
  filter(9 <= distance, distance <= 11,
         450 <= climb, climb <= 550)
## the prediction
close_rows %>% summarize(sample_size=n(), pred = mean(time))
```
You might not agree with our definition of "close to 10km" as "between 9 and 11", and similarly for `climb`.

To get the confidence interval on this simple-minded prediction, we point out that the value of the mean time is the same as the coefficient from the model `time ~ 1`. Let's fit that model formula to the `close_rows` and look at the coefficient and confidence interval. 

```{r}
simple_mod <- lm(time ~ 1, data = close_rows)
simple_mod %>% coef()
simple_mod %>% confint()
```

To interpret this confidence interval, we can plot the actual running times and compare them to the interval, as in @fig-running-prediction-conf

```{r}
#| label: fig-running-prediction-conf
#| fig-cap: "The rows from `Hill_racing` with a distance close to 10km and climb close to 500m. The confidence interval on the mean time (shown in red) is narrow compared to the prediction interval (blue) calculated from the whole data frame and the model `time ~ distance + climb`."
ggplot(close_rows, aes(y=time, x=1)) +
  geom_jitter(width=0.2) + xlim(0,2) +
  geom_violin(fill="blue", alpha=0.2, color=NA) +
  geom_errorbar(aes(ymin=3364, ymax=3683), color="red") +
  geom_errorbar(aes(ymin=1665, ymax=5082, x=1.5), color="blue")
```
:::

::: {.callout-note}
## Example: A missed historical opportunity

Sewall Wright (1889-1988) was an American geneticist and statistical pioneer. One of his statistical inventions is the "coefficient of determination" now universally called R^2^ and a widely used, basic summary of statistical models. In 1921, he invented "path analysis." One of his "path diagrams" is shown in @fig-path-guinea-pig.

```{r echo=FALSE}
#| label: fig-path-guinea-pig
#| fig-cap: "Sewall Wright's path diagram describing the the mixing of environmental, developmental, and genetic factors contributing to the color patterns on guinea pigs."

"GET THIS FROM p.75 of the *Book of Why*"
```

His path diagrams are directed acyclic graphs, DAGs, augmented with coefficients representing the relative strength of each contributor to a node. He worked out the algebra of the correlation induced by the graph between any two nodes. Then, by measuring the R^2^ between pairs of nodes, he was able in some cases to work backwards to numerical values for the coefficients.

Wright's path diagrams are the historically earliest form of our DAGs. In his honor, we've constructed a DAG to represent one of his calculations, how much the body weight at birth of a guinea pig increases due to one day longer in the womb. The path diagram Wright imagined is drawn below, though we have left out the coefficients from the display.

```{r echo=FALSE}
dag_pigs <- dag_make(
  liter_size ~ 5 + as.numeric(exo() > 1),
  .gestation ~ 24 - liter_size + exo(),
  growth_rate ~ 10 - liter_size + exo(.3),
  weight ~ growth_rate*.gestation + liter_size + exo(),
  gestation ~ round(.gestation)
)
dag_pigs_for_drawing <- dag_make(
  Liter_size ~ 5 + as.numeric(exo() > 1),
  Gestation ~ 24 - Liter_size + exo(),
  growth_rate ~ 10 - Liter_size + exo(.3),
  Weight ~ growth_rate*Gestation + Liter_size + exo()
)
dag_draw(dag_pigs_for_drawing, vertex.label.cex=1,
         vertex.size=60, seed=103)
```

We can't measure the growth rate directly, but we can measure liter size, gestation length, and birth weight. But how can we estimate the direct effect of growth rate when it is confounded with the other causal pathways?

Sewall's breeding experiments would have provided data like this:

```{r echo=FALSE}
set.seed(104)
Pigs <- sample(dag_pigs, size=1000)
knitr::kable(head(round(Pigs %>% select(-gestation))))
```

You might think that weight gain per day of gestation can be simply calculated as `weight/gestation`, but this ignores the fact that weight gain is slow early in gestation and faster as the cubs develop. Instead, using a model `weight ~ gestation` lets us look at the marginal impact of an extra day of gestation. The coefficients from this model indicates that weight increases by 6.8 grams per extra day of gestation.

```{r}
lm(weight ~ gestation, data = Pigs) %>% confint()
```

But Wright knew that this number was misleading. Larger liters tend to have shorter gestation times. And larger liters produce cubs that weigh less. With more computational power available to us, we can use a simpler calculation to incorporate these facts into the estimation of weight gain per day of gestation:

```{r}
lm(weight ~ gestation + liter_size, data = Pigs) %>% confint()
```

This model pegs the growth rate at about 4.5 grams per day. 

Since we generated the data from a DAG, we have the luxury of measuring the actual growth rate used for each liter. 

```{r}
Pigs %>% summarize(rate = mean(growth_rate))
```

Covariates help us deal with confounding! Unfortunately, the statistical bigwigs of the 1920s through 1950s poo-pooed Wright's ideas about path analysis. They were rehabilitated only the the 1980s.

:::



## DAGs and covariates

WHAT CHAPTER DID THE SPENDING and aprotonin examples COME FROM?


The argument, "reduce spending by reducing spending" is very compelling, common sense even. It's harder to see how reducing spending in one area---the cash payment to people not on the insurance plan---can increase spending overall. I might have been more successful convincing the college budget committee not to eliminate the cash payment if they had understood the language of DAGs. @fig-insurance-two-dags shows two competing DAGs for the situation:

```{r echo=FALSE}
#| label: fig-insurance-two-dags
#| fig-cap: "Two different DAGs relevant to the debate about eliminating the cash payment to employees not on the college's health care plan."
#| layout-ncol: 2
#| fig-subcap: 
#|   - "One path from cash to expenditures"
#|   - "Two paths from cash to expenditures"
dagA <- dag_make(`Cash payment` ~ 1, 
                 Enrollment ~ 1,
                 `Expenditures` ~ `Cash payment` + Enrollment)
dagB <- dag_make(`Cash payment` ~ 1, 
                 `Enrollment` ~ `Cash payment`,
                 `Expenditures` ~ `Cash payment` + `Enrollment`)
set.seed(107); dag_draw(dagA, seed=107, vertex.label.cex=1);
dag_draw(dagB, vertex.label.cex=1, seed=109, vertex.size=60)
```

The people on the budget committee saw clearly the direct link between the cash payment and total expenditures and likely would not have disputed a direct link between enrollment and expenditures. But they didn't imagine a link between the cash payment and enrollment. I did, because I knew of several colleagues who used their spouse's companies insurance plan, even though it was identical to the college's plan.

The situation with the drug aprotinin is similar. 

```{r echo=FALSE}
#| label: fig-drugs-two-dags
#| fig-cap: "Two different DAGs relevant to the link between aprontinin and mortality."
#| layout-ncol: 2
#| fig-subcap: 
#|   - "One path from cash to expenditures"
#|   - "Two paths from cash to expenditures"
dagA <- dag_make(`Health condition` ~ 1, 
                 Aprontinin ~ 1,
                 Mortality ~ `Health condition` + Aprontinin)
dagB <- dag_make(`Health condition` ~ 1, 
                 Aprontinin ~ `Health condition`,
                 Mortality ~ `Health condition` + Aprontinin)
dag_draw(dagA, seed=107, vertex.label.cex=1); 
dag_draw(dagB, seed=107, vertex.label.cex=1)
```


------


::: {.callout-note}
## Example: A bookies' calculations [NEEDS FIXING]

The most familiar use of "odds" is in gambling. For instance, a famous song lyric puts the odds of Valentine winning the horse rate "at five to nine." Less musically, this odds is $5/9 = 0.5555$, but the two-number format makes particular sense for keeping track of bets. Five-to-nine describes a bet of one unit. The second number, 9, specifies the amount the gambler is staking on the outcome. On a loss, the gambler loses that stake. On a win, the gambler gets back the stake and, in addition, gets the amount specified by the first number. So a winner at five to nine would leave the racetrack with an extra $5. But on a loss, the gambler leaves $9 behind.

A "bookie" is someone who provides a service. You can go to a bookie to lay a bit. In drama, this might be done by telephone: "Lay $90 on Valentine" is all the gambler needs to communicate. No money has to change hands. On a win, the bookie will return $50 to the gambler. On a loss, the gambler has a debt of $90.

A bookie is not a gambler; he's an accountant who records numbers. The bookie arranges these numbers so that he makes money. To see this, imagine a horse race including Valentine, Paul Revere, and Epitaph. To start, the bookie specifies odds on each possible outcome, say 5:9 for Valentine, 1:3 for Paul Revere (a favorite!), and 1:2 on Epitaph. 

If the bookie has a good nose, about a third of the stakes will be bet on each outcome. If not, as new bets come in the bookie raises or lowers the odds to encourage or discourage bets so that the roughly one-third of stakes are placed on each outcome. Suppose at the end of the day that $500 is staked on each of the three outcomes. 

WRONG WRONG WRONG. It needs to work that the winning returned for Valentine has to be less than the stakes on the other horses, and similarly for all horses. So if $100 is bet on Valentine we need $100 staked on the other horses.  





Added up, these odds are $5+1+1=7$ on the top and $9+2+1=12$ on the bottom. It's important---for the bookie---that the odds are arranged so that the bottom number is larger than the top number: 12 is larger than 11. Note that this method of adding is simpler than combining fractions. To add the fractions $1/2$ and $1/3$ gives $5/6$. But to combine the odds $1:2$ and $1:3$ gives $2:5$. One more detail is needed for a real-life bookies, taking into account the size of each bet. For instance, a $5 bet at 5:9 would be recorded as 25:45.

Now the race is run. The winner is ... well ... from the bookie's point of view it doesn't matter who wins.
:::




## "Irrationality"

[From *The Model Thinker*, p. 52] 

**Gain Framing**: You have two options

Option A) Win $400 for certain

Option B) Win $1000 if a fair coin comes up heads and $0 if tails

**Loss Framing**: You are given $1000 and have two options:

Option a) Lose $600 for certain

Option b) Lose $0 if a fair coin comes up heads and lose $1000 if tails.


**Hyperbolic discounting**: see pp 52-43

"Prospect theory", Kahneman and Tversky (1979) "Prospect theory: an analysis of decisions under risk," *Econometrica* 47(2):263-291 [link to paper](www/kahneman-tversky-79.pdf)


::: {.callout-note}
## Example

A subtle modification to the linear model architecture allows the modeller to guarantee that the output will be between zero and one. The modified architecture, called "**logistic regression**", is therefore well suited to modeling categorical response variables, where the model output will be interpreted as a probability.

@fig-w-logistic shows a logistic model of survival as a function of age and smoking status. Notice that in the logistic model, the effect of smoking on survival is negative, particularly for people around age 50. The logistic architecture provides an intrinsic flexibility which avoids the undue influence of the very young and very old, for whom survival is close to 100% or 0 respectively *regardless* of smoking status.

::: {.content-visible when-format="html"}
```{r echo = FALSE, warning=FALSE}
#| label: fig-w-logistic
#| fig-cap: A simple logistic model of survival versus age and smoking status. 
wmod_glm1 <- glm(outcome=="Alive" ~ age + smoker, data = mosaicData::Whickham, family = binomial)
mod_plot(wmod_glm1) %>%
  gf_lims(y = c(-.1,1.1)) %>%
  gf_refine(scale_y_continuous(breaks=c(0,.2,.4,.6,.8,1.0))) %>%
  gf_labs(y = "Probability of survival for 20 years") %>%
  gf_jitter(as.numeric(outcome=="Alive") ~ age, color = ~ smoker, 
            data = mosaicData::Whickham, alpha = 0.2, height = 0.1)
```
:::


:::: {.content-visible when-format="pdf"}

::: {.callout-warning}
The figure fig-w-logistic is not compiling in PDF mode
:::
::::



-------

## Many hypotheses

Self-driving car safety here?

But there are also different situations. For instance, the observation might be the mean of a sampled variable, or it might be the difference in sample means  between two groups, or even the calculated effect size from a model fitted to a sample. In such situations, the possible hypotheses form a continuum, one hypothesis for each point on the number line.


## Orthodox

We are nearing the end of our journey through the world of statistical thinking. But I think it's time to get off the well-travelled road and take a detour of a few paragraphs. Detours are usually not the straightest path between two points, but they often have some advantage: safety, a scenic view or inspiring experience, and such.

Our detour starts with a turn onto a lane marked with the word "orthodox." There are, as you know, religions denominated by "orthodox." But the word has a more general meaning that can be seen by translating the Greek origin words "ortho" and "doxa" into a familiar language: "ortho" means "straight" or "right"; "doxa" means "belief".  Synonyms for "belief" include creed, dogma, teaching, doctrine (which stems from the Latin for "to teach"), conviction, and article of faith. A line is straight and the phrase "the party line" indicates attitudes that fall into line with the party leadership. "Ortho" appears in "orthodonture" (straightening the teeth), "orthography" (correct spelling), and "orthogonal" (being at a right angle).^[The link between words for "straight" and thinking according to the rule book has ancient origins.  The word "canon," meaning the set of officially accepted writings, comes from the Sumerian word for a straight reed.]

To "true a wheel" means to set it straight, and the phrase "true believer" does not refer to someone whose beliefs are correct but rather to someone who stays in line with to a particular system of belief. There can be many different systems of belief, many different orthodoxies, and disagreeing parties can each have their distinct line.

There are two major orthodoxies of statistical thought, "frequentists" and "Bayesians,"  a fact that's important to keeping straight the variety of statistical nomenclature. The key distinction between the frequentist orthodoxy and that of the Bayesians, is their attitude toward the meaning of "probability." One sect, the "frequentists," bases their methodology in a supreme being they call the "population." Coin flips are an example of a population; an abstractly infinite supply of events. The probability of "heads" is the *frequency* of a head turning up in $n$ trials, where $n \rightarrow\infty$. Flipping coins an infinite number of times is still a work in progress. Until it is complete, we need to work with the probability of heads as the proportion of a large but finite number of trials in which the outcome is "heads."

In contrast, the "Bayesian" sect holds that a probability is a statement about belief. Different people, depending on their experiences, will rightly come to different conclusions about the probability of the outcome of an event. Bayesians will happily deal with a statement like, "the probability of rain tomorrow is 60%," while frequentists might respond (while packing an umbrella in their knapsack for tomorrow's weather) by pointing out that there is no "population" of "tomorrows" from which we can draw a sample to estimate the frequency of rain. 

For a frequentist, the hypothesis "April showers bring May flowers" is an assumption. There is no actual population from which to draw many trials, so a probability cannot be assigned to the hypothesis. In the frequentist liturgy, to *test the hypothesis* means to make a *simulation* of the world. Typically the simulation implements a world in which it is hard coded that the connection described by the hypothesis *does not exist*. This no-connection hypothesis is generically called the "**Null hypothesis**." In this case, the Null hypothesis states that there is no association between April showers and May flowers. 

Constructing a simulation that generates data from the Null hypothesis is easy. Take the real-world data recording the observations of April precipitation and May floration. Randomly shuffle the entries in the April column while holding the May column fixed. The shuffling destroys any systematic association between the April and May columns. Any measured association in the shuffled data is incidental and accidental. 

Run many trials of the shuffling simulation. In each trial, record the measured association. It's reasonable to expect that the measured association across the trials will be close to zero. Indeed, you use the trial results to *define* what "close to zero" means. Then look back at the association found in the real-world, unshuffled data. If that observed association falls within the definition of "close to zero," then it is not fair to insist that the real-data is inconsistent with the Null hypothesis. That is, you "**fail to reject**" the Null hypothesis. On the other hand, if the observed association falls outside the bounds of "close to zero," then you are entitled to **reject** the Null hypothesis.

Notice that the simulation mechanism provides a population--- a "hypothetical planet" to use the language of Lesson 34---from which many samples could be drawn. Thus, it's straight thinking for the frequentists to assign a probability to the event "a simulation trial will generate a result at least as extreme as what was observed in the real world." This probability, in the lingo of hypothesis testing, is called a "**p-value**".

In my opinion, it would have been better for everyone to rename the p-value as the ${\cal L}$-value. After all, it's a **likelihood**, a proportion calculated on a hypothetical planet.

For many people, the orthodoxy that "hypothesis testing" can properly lead only to one of two conclusions---"reject" or "fail to reject" the Null hypothesis---seems stilted and overly rigid. Like others who are not aligned with orthodoxy, they will translate the orthodox result into language that they find reasonable and more comfortable. For instance, they will say that the p-value is the probability that the Null hypothesis is true. Or, they interpret a small probability for the Null hypothesis---what should properly lead to "rejecting" the Null hypothesis---as an indication that the original hypothesis is to be "accepted" as true. And "failing to reject" often gets (mis-)interpreted as meaning the original hypothesis is not true.

The ministers of orthodoxy---statistics professors, for example---will point out that such (mis-)statements are a sign of wrong thinking. The ministers exert such power as they have to set straight the strays among the flock, for instance by giving low scores on a course final examination. In practice, limited as it is to the examination room, this punishment rarely leads to a repentant and binding return to frequentist orthodoxy.

The Bayesian orthodoxy is more permissive. It accepts as legitimate statements involving probability, such as "Hypothesis A is less likely than Hypothesis B." 

In Lesson 35 we used a Bayesian approach to address two hypotheses that were relevant to a disease C. Once the data are in---say, a test result of + in the Lesson 35 example---we can use the likelihoods (sensitivity and specificity) and the disease prevalence to calculate the probability that, for a + test result, the patient actually has C. 

The frequentists don't disaggree with the legitimacy of a **likelihood**. That is just a probability under the assumption that a given hypothesis is true. Where they depart from the Bayesians is in accepting the "probability of a hypothesis." For, in order to calculate the relative probability of two hypotheses, you need to combine the likelihoods with a prior idea of how likely the hypotheses were in the first place. Frequentists assert that any such prior ideas are subjective. 

Few people will disagree with the idea that good science ought to be objective. Yet allowing some scope for subjectivity can lead to better informed decision-making. To illustrate, let's tell a story that illustrates the benefits of assigning a probability, subjective though it may be, to a hypothesis.

Imagine you are a doctor. A long-time patient comes to your clinic. His recent symptoms have him thinking that there is a strong chance he might have cancer. You, the doctor, ask questions about the patient's symptoms and do a brief physical examination. 

A standard medical practice is to construct a "**differential diagnosis**". In doing this, you construct a mental list of the medical disorders that might account for the symptoms and examination results. Since you have extensive training and experience, this list might be long and contain disorders that are relatively common and some that are rare. 

To keep the story simple, let's make the list of plausible disorders unrealistically short: the patient either has cancer or has the flu. Each of these is a hypothesis. Continuing the process of differential diagnosis, you, the doctor, consider what medical steps might point in favor of one hypothesis or the other. For instance, you could order a tissue biopsy. Less invasive and less costly, you could decide on ordering a magnetic resonance imaging (MRI) or, simpler, using a medical screening test for cancer. Or you might even direct the patient to take some over-the-counter flu treatment and report back in two weeks if the symptoms haven't resolved.

In medical school, you learned a mantra: "When you hear hoofbeats, think horses, not zebras." Zebras are very rare in most parts of the world, horses much less so. So horses are a much more likely source of hoofbeats than are zebras.

To make an appropriate decision, you consider what you know about the patient. Had he just finished a round of chemo-therapy six months ago? If so, an MRI might be a good choice. Is the patient elderly? The elderly as a group have a greater risk of developing cancer than the young. So for an elderly patient, you might decide on ordering a screening test. Does the patient have a history of good health and an active imagination? Then the flu medication might be the right road to go down.

A Bayesian would interpret this decision-making process in terms of probabilities. You assign, based on your observations, a probability to each hypothesis. If the probability of the cancer hypothesis is much smaller than the probability of flu, and if the cost of making a mistake (as measured by a "loss function," see Lesson 34) is small, the flu medication is a good next step. But if the probability of cancer is not so small, and the cost of a delay is high, then ordering the screening test seems appropriate.

------------------







## Feature engineering

Consider a credit-card company building a classifier to predict at the time of the transaction whether a purchase of gasoline is fraudulent. The company knows how often and how much gasoline the individual cardholders buys, where the cardholder lives, whether the cardholder travels extensively, typical times of day for a purchase, and so on. **Feature engineering** is the process of using existing data---including, in our example, whether the purchase turned out to be fraudulent---to develop potential markers or signals of the outcome. For simplicity, imagine the features selected are the number of days since the last gasoline purchase and the distance from the last place of purchase. 

Once potential features have been proposed, the engineers building the classifier assemble training and testing data sets. Suppose, for the purpose of illustration, that the training data has 2000 fraudulent transactions and 4000 non-fraudulent ones, and the testing set is about the same. 

---------------------


## Example: Accuracy of airport security screening 

Airplane passengers have, for decades, gone through a security screening process involving identity checks, "no fly" lists, metal detection, imaging of baggage, random pat-downs, and such. How accurate is such screening? Almost certainly, the accuracy is not as good as an extremely simple, no-input, alternative process: automatically identify every passenger as "not a security problem." We can estimate the accuracy of the "not a security problem" classifier by guessing what fraction of airplane passengers are indeed a threat to aircraft. In the US alone, there are about 2.5 million airplane passengers each day and security problems of any sort rarely happen. So the accuracy of the no-input classifier is something like 99.999%. 

The actual screening system, using metal detectors, baggage x-rays, etc. will have a lower accuracy. We know this since it regularly mis-identifies innocent people as security problems.

The problem here is not with airport security screening, but with the flawed use of *accuracy* as a measure of performance. Indeed, achieving super-high accuracy is not the objective of the security screening process. Instead, the objective is to *deter* security problems by convincing potential terrorists that they are likely to get caught before they can get on a plane. This has to do with the *sensitivity* of the system. The *specificity* of the system, although important to the everyday traveller, is not what deters the terrorist.

## From Lesson 36

BUT THIS IS ALREADY COVERED IN LESSONS 34 and 35.

## Example: The ${\cal L}_+()$ function

Suppose that the sensitivity of a test is 90% and the specificity is 85%. A person has a + test result. Since the observation is +, the likelihood function ${\cal L}_\mathbf{+}()$ is relevant The domain of ${\cal L}_\mathbf{+}()$ is simply the two possibilities for the subject's condition: C or H.


$${\cal L_{\mathbf +}}(X) \equiv\left\{\begin{array}{l}90\%\  \text{given}\ \  X=C\\15\%\ \text{given}\ \  X = H\end{array}\right.$$

If the observation had been different, a different likelihood function would be relevant. In testing, there are only the two possible observations, + and -. For a - observation, the relevant likelihood is ${\cal L}_{\mathbf -}()$

$${\cal L_\mathbf{-}}(X) \equiv\left\{\begin{array}{l}10\%\  \text{given}\ \  X=C\\85\%\ \text{given}\ \  X = H\end{array}\right.$$

Note that the argument to the likelihood functions, which we have written as $X$ is **not the result of the test**. Instead, $X$ is related to the condition of the person who underwent the test. 

It's essential to remember that a "hypothesis test" can be performed only *after* the observation has been made. In that situation, there is only one relevant likelihood function, the one corresponding to the actual observation.

In Lesson 35, we used the likelihood function for a particular purpose, to calculate the implications of a + result into terms relevant to the patient. The patient receiving a + result is naturally interested in what this means about the chances that she has the condition C. We will write this as a probability, but it is a probability that applies only for the patient with a + result. We will write it $p_\mathbf{+} (C)$. The probability is labeled with a + as a reminder that it is relevant only to a patient with a + result.

As described in Lesson 35, to calculate $p_\mathbf{+} (C)$, we need some more information. Specifically, we need to know the prevalence of the condition. Our notation for the prevalence of C is $p(C)$. Pay attention to two things about this notation. First, we have written it as a function that we are evaluating for the input C. The output of the evaluated function is a number. In Lesson 34, we showed an example where $p(C) = 4\%$, meaning that 4% of the population has condition C. That same function can be evaluated at input H, giving $p(H)$, producing another number. Given that $p(C) = 4\%$, and that C or H are the only possible states a patient can be in, then $p(H) = 1 - p(C) = 96\%$.

The second thing to pay attention to is the *absence* of any subscript on $p(C)$. That is because the prevalence of the disease is known *before* our particular patient has received her result. Such probabilities, being known *before* the test leading to the + result, are called "**prior**" probabilities. In contrast, $p_\mathbf{+}(C)$ is called a **posterior** probability. 

::: {.callout-warning}
## Demonstration: From prior to posterior *via* likelihood

Lesson 35 gives the formula, called "**Bayes' Rule**", for converting prior probabilities into posterior probabilities relevant to a particular observation +. This formula is where the likelihood function comes into play. Of course, only ${\cal L}_\mathbf{+}(\ )$ is relevant because the posterior being calculated is relevant to the + observation.



$$p_\mathbf{+}(C) = \frac{{\cal L}_{\mathbf +}(C)\cdot  p(C)}{{\cal L}_{\mathbf +}( C)\cdot p(C) + {\cal L}_\mathbf{+}(H)\cdot p(H)}$$

Had the result been -, the probability relevant to the patient would have been $p_\mathbf{-}(C)$, that is, the chances of having condition C even though the test result is -. This probability would also be calculated with Bayes' Rule, but the relevant likelihood function would be ${\cal L}_{\mathbf -}(\ )$ rather than ${\cal L}_{\mathbf +}(\ )$. 

$$p_\mathbf{-}(C) = \frac{{\cal L}_{\mathbf -}(C)\cdot  p(C)}{{\cal L}_{\mathbf -}( C)\cdot p(C) + {\cal L}_\mathbf{-}(H)\cdot p(H)}$$

The *prior* probabilities are the same in both formulas since the prevalence in the population is known *before* the test result.
:::

---------

## "Hypothesis testing"

In the everyday use of language, to "test a hypothesis" might reasonably mean "to perform a procedure to establish whether the hypothesis is true. A statistical thinker, being aware of the omnipresence of noise, would translate this everyday meaning just a little bit: "to perform a procedure to establish the probability that the hypothesis is true." The procedure will presumably include collecting relevant data, perhaps in the form of an experiment, perhaps in the form of observations made without experimental intervention. If the statistical thinker was entertaining just two hypothetical possibilities, $H_1$ and $H_2$, the theory behind the hypotheses would provide the likelihood function ${\cal L}_\textbf{data}(H_1)$. 

To give a historical example, Aristotle's physics stated that two balls of the same size, released from the same height at the same time, will hit the ground simultaneously. Galileo suspected that this Aristotelian claim might not be true. His hypothesis was that the balls would hit at different times if the balls had different weights. To test Aristotle's hypothesis, Galileo famously ascended the Tower of Pisa, conveniently tilted so that balls dropped from the top would go directly to the ground. Dropping the balls, one metal, one wooden, from the top of the Tower, it was found that the metal ball hit discernibly before the wooden ball. In our modern way of thinking, we can say that Galileo's observation *lowered* the probability that the Aristotelian hypothesis was true. (In historical actuality, Galileo predated the development of even simple notions of probability.)

Statistical thinkers who use data to calculate posterior probabilities using Bayes' Rule are called, sensibly enough, "Bayesians." The Bayes' Rule calculations are a routine part of many engineering disciplines, medicine, and so-called artificial intelligence and machine learning.

There is another group of statistical thinkers, called "Frequentists," who also accept Bayes' Rule as mathematically correct, but who define "hypothesis testing" in a very different way. Frequentists have dominated the academic discipline of statistics for a full century. This dominance is now beginning to fade, but generations of scientists have been taught the frequentist "hypothesis testing" framework and express their conclusions using Frequentist terminology, sometimes correctly and sometimes not.

There a fundamental aspect of the Bayesian procedure for testing hypotheses that the Frequentists reject. Frequentists point out that the prior probabilities---that is, probabilities before data has been observed---are subjective. Scientific procedures, according to the Frequentists, ought to be entirely objective, that is, should not depend on opinions that are not yet supported by data. Since most people associate science with objectivity, the Frequentist point of view is attractive. (We won't argue the point here except to point out that saying, "Science should be as objective as possible" warrants keeping track of the ways subjectivity does influence scientific results. There are ways to do this within the framework of Bayesian calculations by looking at many different priors and checking the sensitivity of the results to the choice of prior.)

Owing to this rejection of priors as subjective, the frequentist procedure for hypothesis testing has to be built without the use of priors. Looking back at Bayes' Rule, and eliminating any mention of priors, leaves only the likelihood function. Frequentists do make extensive use of likelihood functions and even compare likelihoods of different hypotheses,^[As in "maximum likelihood" estimation] but they don't accept the use of priors in a scientific context, nor the assignment of probabilities to competing hypotheses.

Frequentist "hypothesis testing" centers on a specific hypothesis that, surprisingly, has no scientific content. This is called the "**Null hypothesis**" and abbreviated H_0_. In a modeling context, the Null hypothesis typically is about an effect size, but often it is about a measure such as R^2^ that can combine multiple model terms. In either case, the Null hypothesis is that the effect size (or R^2^) is zero, but that sampling variation may push away from zero the quantity estimated from data. 

------------------

::: {.callout-note}
## Example: Studying the impact of smoking

Imagine being on the staff of a study of the effects of mortality on smoking. We will model the example on the kind of data recorded in `Whickham`, which we will pretend is just a small part of a broader study involving 10,000 participants. The participants recorded in `Whickham` are nurses. The remaining participants come from a variety of occupations.  

Much work went into completing the `Whickham` data, but a huge amount of work lies ahead of us. Following up on the 9000 non-Whickham participants will be difficult. Consequently, headquarters has directed the staff to streamline the remaining work using "artificial intelligence." The AI system will determine, for each person who still needs following-up, whether to look first at national death records or search the employment and pension records. 

The AI system might less romantically be called a "classifier." The inputs to the classifier will be the `age` and `smoker` status of the person when first interviewed. `Whickham`, the follow-up work you have already completed, will be used to train the classifier. The model will be `outcome ~ age + smoker`. The `outcome` variable is categorical and has levels "Alive" and "Dead." The level "Dead" indicates that the person could be found via the national death records.
:::

We build the model function relevant to the example just given using `Whickham` as training data:

```{r warning=FALSE}
Whickham_model <- lm(zero_one(outcome, one="Alive") ~ age*smoker, data=Whickham)
AI_function <- makeFun(Whickham_model)
```

To get a sense for `model_function()` , try it out:

```{r}
AI_function(age = 75, smoker="Yes")
AI_function(age = 20, smoker="No")
```

Notice that the output of `model_function()` is a *number*.

## Sensitivity and specificity

The model function will work in conjunction with a **threshold** to constitute the classifier. The classifier output---call it "employment" or "mortality"--- will be the result of comparing the model function output to the threshold. 

We will turn to the question of setting the threshold later on. To illustrate the calculation of sensitivity and specificity, assume we have decided to try a threshold of 0.6. 

::: {.callout-warning}
## Demonstration: Turning a model function into a classifier function

To support the choice of the threshold value, we will build a simple function that implements the classifier. The threshold is an argument to this function, allowing us to check the classifier performance for any given threshold.

```{r}
AI_classifier <- function(age, smoker, threshold=0.6) {
  number <- AI_function(age=age, smoker=smoker)
  ifelse(number >= threshold, "employment", "mortality")
}
```
To illustrate what `AI_classifier` is doing, we try it out on a few cases with typical inputs.  

```{r}
Examples <- data.frame(age = c(75, 20), smoker=c("Yes", "No"))
Examples %>%
  mutate(test_result = AI_classifier(age, smoker, threshold=0.6))
```
:::

By applying `AI_classifier()` to the training data, we can find the sensitivity and specificity of the test (at the assumed threshold of 0.6).
```{r}
Whickham %>%
  mutate(test_result = AI_classifier(age, smoker, threshold=0.6)) %>%
  group_by(outcome, test_result) %>%
  tally() %>%
  mutate(proportion = n/nrow(Whickham))
```

The sensitivity looks only at the cases where the classifier reported "employment." In 766 of those cases, the classifier output is correct, in 68 the classifier output is incorrect. So the sensitivity is $\frac{766}{766 + 68} = 92\%$.

Similarly, the specificity comes from those cases where the classifier reported "mortality." The specificity is $\frac{301}{301 + 179) = 63\%$.

--------

 ## False-positive and false-negative rates

The sensitivity and specificity describe the performance of the classifier function. To calculate the false-positive and false-negative rates, we need to know the **prevalence** of the condition in the population of interviewees. 

In the training data, there were 945 subjects who survived out of $n=1314$ altogether: a prevalence of $\frac{945}{1314} = 72\%$. In practice, the training data used to build a classifier is not a fair sample of the overall population. Instead, data for building classifiers are a sample constructed to provide many instances of both levels of `outcome`.  

The `Whickham` training data is about nurses. Nurses tend to have better health outcomes than the population, so for the sake of this example of building a classifier, we will claim that the prevalence in the population is lower than the prevalence among nurses, say 50%.


## Selecting a threshold

To select a threshold, start with a guess. Using that guess and calculate the classifier's performance using the appropriate loss function. Then try another guess to see if that results in better performance. Continue searching until finding the threshold that optimizes performance.  


Using the training data, we can compute the sensitivity and specificity of the test.


The proper choice of a threshold is based on the false-positive and false-negative rates, as well as the loss function. 





For the purposes given in the example, where the classifier will guide how we search for each person, base the loss function on the difficulty of the different kinds of searching. It is comparatively easy to check the death records; they are public, stored in a central database, and searchable by the nurse's social insurance ID number. It is comparatively difficult to go through employment and pension records; they are private and decentralized. The loss function reflects this difficulty:

- loss_function(false positive) $\equiv$ 10 loss units
- loss_function(false negative) $\equiv$ 1 loss unit

As for the false-positive and false-negative rates themselves, we can compute them from the sensitivity and specificity of the test combined with the **prevalence**.  

For the sake of

are selected with a prevalence higher than that in the background population, because we want to have It might well be that the prevalence among the remaining interviewees from the study is different. 


Having built a classifier, we are ready to check the classifier's performance in terms of false positive and false negative rates. Here, we will treat "employment" as the positive test outcome and "mortality" as the negative outcome. A false-positive will be a classifier output of  "employment" for a person who is, in reality, dead. A false-negative is a classifier output of "mortality" for a person who is alive. 

The (Alive, "mortality") row counts the false-negatives while the (Dead, "Employment records") row counts the false positives. So the false-positive rate---false positives per test---is 5.2%. The false-negative rate is 13.6%.

We still have to explore which choice of the threshold will make the test as good as possible. Our ultimate measure is based on the consequences of the test being wrong. These consequences are encoded into the *loss function*. 



With this loss function, the expected loss per test is 
$$10 \times 0.052 + 1 \times 0.136 = 6.56\  \text{loss units per test}$$
To find the *best* value for the threshold, try other possible values for the threshold. Here, we'll try 0.8, re-apply the test with that threshold, and tabulate the false-positive and false-negative rates.



The 0.8 threshold lowers the false positive rate to 1.6% but raises the false negative rate to 30%. That is a typical behavior for classifiers; changing the threshold to lower the false positive rate will raise the false negative rate, and *vice versa*.

With a threshold of 0.8, the expected loss per test is
$$10 \times 0.16 + 1 \times 0.30 = 1.9\ \text{loss units per test}$$
Better. 

Before we continue on to the find the *best* threshold, we need to do a *reality check.* The loss rates we just calculated are based on the training data. In that training data, there were 945 subjects who survived out of $n=1314$ altogether. That's a prevalence of $\frac{945}{1314} = 72\%$ in the training data. It might well be that the prevalence among the remaining interviewees from the study is different. 

Accordingly, we need to adjust the loss calculations to reflect the actual prevalence. The first step in doing this is to find the *sensitivity* and *specificity*, as we did in Lesson 34.

Sensitivity and specificity are easy to calculate so long as one keeps in mind that sensitivity is only relevant to that part of the population who genuinely have the condition to be marked by an "Employment records" test output. In the Whickham example, the sensitivity is the percentage of correct results **only** among the people whose `outcome` value is "Alive."  There are 395 + 550 such people, of whom 550 had a correct result, so

-sensitivity (using the threshold 0.8) is $550/(395+550) = 58\%$.

Similarly, specificity is only relevant to the people whose `outcome` value is "Dead". There are 348 + 21 such people, of whom 348 got the correct test result.

-specificity (using the threshold 0.8) is $348/(348 + 21) = 94\%$.

Sensitivity and specificity measure the test itself. False positive and false negative rates measure the test as applied to a specific population. The relevant fact of the population is the "**prevalence**" of the condition. 

----------

## Prevalence, sensitivity, specificity

The training data are useless when it comes to representing the prevalence of condition C in the general population. On the other hand, the sensitivity and specificity calculated from the training data are the same in the general population. 

We can put together the prevalence in the general population along with the sensitivity and specificity from the training data, to create an accurate picture of how the test will perform in the general population. Indeed, that is just what we did to draw @fig-divided-by-condition3. First, we divided the general population into C and H groups. The prevalence tells us how many people to put into the C group; the remaining people go into the H group. Then, we apply the specificity to the people in the H group to determine the fraction of $\mathbb{N}$ results in the H group. The remaining people in the H group got a $\Ptest$ test result; these are the false positives.

Similarly, we apply the sensitivity to find the number of $\Ptest$ people in the C group. The remaining C people tested -; there are the false negatives. Lets do this arithmetically so that we can calculate false-positive and false-negative rates. 

 $\ $  | in C group   | in H group
--------|:----------:|:----------:
Overall proportion | 7%        | 93%
Testing positive   | $7\% \times\text{sensitivity}$ | $93\% \times (1 -\text{specificity})$
Testing negative   | $7\% \times (1-\text{sensitivity})$ | $93\% \times \text{specificity}$


The total number of people who test positive is $$7\% \times\text{sensitivity} + 93\% \times (1 -\text{specificity})\ .$$ Of these, $7\% \times\text{sensitivity}$ actually have the condition C. Consequently, the probability that a person with a $\Ptest$ test result has condition C is 

$$\frac{7\% \times\text{sensitivity}}{7\% \times\text{sensitivity} + 93\% \times (1 -\text{specificity})}$$

Similarly, for the people who get a $\mathbb{N}$ test result, the fraction who have condition C is 

$$\frac{7\% \times(1-\text{sensitivity})}{7\% \times(1-\text{sensitivity}) + 93\% \times \text{specificity}}$$
These two formulas are very similar. The difference is only whether the (1 -  x) is applied to specificity (the top formula) or sensitivity (the bottom formula).

$$p(C | \Ptest) = \frac{p(\Ptest | C) \cdot p(C) }{p(\Ptest | C) \cdotp(C) + p(\Ptest|H) \cdotp(H)}$$

$$p(C\ |\!\!|\  \mathbb{N}) = \frac{p(\mathbb{N} | C) \cdot p(C) }{p(\mathbb{N} | C) \cdotp(C) + p(\mathbb{N}|H) \cdotp(H)}$$

-----------


---
title: "A little more detail"
---

```{r include=FALSE}
source("../_startup.R")
```

Put the odds and ends here, then sort them out.

## Standard errors, margins of error, and confidence intervals

## Tiny $n$ (optional)

When you have a very small sample size---say, $n=2$---the values may coincidentally be very close together. Around 1907, William Gosset, a scientist at Guinness, discovered that such coincidences force "twice" to be $> 2$ in order to produce confidence intervals that reliably cover the mean of the data-generating process. Gosset's particular interest was in making sense of Guinness's standard testing protocols, which involve averaging the results from three small batches of beer ingredients. Contacting the leading statisticians of the day, Gosset was told that such small $n$ is "brewing, not statistics." Nonetheless, Gosset had to work within Guinness's testing protocols, which were indeed brewing but still needed statistical interpretation. 

Gosset carried out trials by hand, a large number of measurements from a study of criminals' hand sizes. (They did this kind of thing in 1900.)  Each measurement was written on a card. A trial consisted of drawing $n$ cards from the deck and calculating the mean and standard deviation of the measurements. Using computers, we can simulate the calculation of results from a Gosset-like trials using a simple function that calculates the mean and standard deviation of data from a Gaussian distribution.

```{r}
one_trial <- function(n=2) {
  vals <- rnorm(n)
  tibble(m = mean(vals), s = sd(vals))
}
```

We can pick a small $n$ and running many trials using a candidate value for "twice."


::: {.callout-warning}
## IN DRAFT

CONVERT the `beta` to something named `twice`.

```{r}
n=10
beta <- 2 / sqrt(n)
Trials <- do(100) * one_trial(n=n) %>% 
  mutate(left = m - beta*s, right = m + beta*s) 
gf_errorbarh(.index ~ left + right, data = Trials, alpha=0.5) %>%
  gf_errorbarh(.index ~ left + right, 
               data = Trials %>% filter(left > 0 | right < 0)) %>%
  gf_vline(xintercept = ~ 0, color="blue", inherit=FALSE)
```
Gosset effectively tabulated the $\beta$ multipliers

n  | $\beta$ | $t = \beta / \sqrt{\strut n}$
---|---------|------------------
2  | 8.98    | 12.7
3  | 2.48    | 4.30
4  | 1.59    | 3.18
5  | 1.24    | 2.78
6  | 1.04    | 2.57
7  | 0.92    | 2.44
$\vdots$ |  |
10 | 0.72    | 2.26
15 | 0.55    | 2.14
20 | 0.47    | 2.09
50 | 0.28    | 2.01
100 | 0.20   | 1.98
500 | 0.088  | 1.96
1000 | 0.062 | 1.96

You can see that for $n$ bigger than 10 or 20, the $t$ multiplier is 2. But for very small $n$, the t-multiplier can be considerably larger. 

You can see the wisdom of brewers here. They made tests by averaging measurements from three small batches of beer. If they had used only two batches, the confidence interval would be almost three times larger than for $n=3$, making it very hard to conclude anything about whether the tests show the ingredients to be within the quality-control standards.

Gosset's work was published under the pseudonym "Student," since Guinness forbade employees to publish under their own names. Statisticians, recognizing the value of the work (and knowing the name behind the pseudonym), came to use the name $t$, perhaps because tea was considered more refined than "beer." In many statistics texts, you will see the phrase "Student t" to refer to how Gosset's work is used.

## Interaction

RETURN TO THIS MODEL, but add an interaction.

```{r}
Mod3 <- lm(list_price ~ hard_paper + num_pages, data = amazon_books)
model_eval(Mod3, hard_paper = c("P", "H"),  num_pages=c(200, 400))
```

## Prediction interval

::: {.callout-warning}
## Demonstration: Simple-minded data analysis shows why

We constructed our model of running time using the "linear least-squares" modeling methodology implemented by the `lm()` model-training function. For the purpose of demonstration, we'll show you a simple-minded method to make a prediction. This simple-minded method would give results more or less equivalent to the least squares method if we had an almost infinite amount of data. Since we don't, the simple-minded method is not as reliable as the least-squares method.

Remember our goal: to predict the running time for a 10km race with a 500m climb. All that we have to inform the prediction is the historical data contained in the `Hill_racing` data frame. The simple-minded method is ... well ... simple to understand. We will pull out from the `Hill_racing` data frame those rows where the race distance is close to 10km and the race climb is close to 500m. For example:

```{r}
close_rows <- Hill_racing %>%
  filter(9 <= distance, distance <= 11,
         450 <= climb, climb <= 550)
## the prediction
close_rows %>% summarize(sample_size=n(), pred = mean(time))
```
You might not agree with our definition of "close to 10km" as "between 9 and 11", and similarly for `climb`.

To get the confidence interval on this simple-minded prediction, we point out that the value of the mean time is the same as the coefficient from the model `time ~ 1`. Let's fit that model formula to the `close_rows` and look at the coefficient and confidence interval. 

```{r}
simple_mod <- lm(time ~ 1, data = close_rows)
simple_mod %>% coefficients()
simple_mod %>% confint()
```

To interpret this confidence interval, we can plot the actual running times and compare them to the interval, as in @fig-running-prediction-conf

```{r}
#| label: fig-running-prediction-conf
#| fig-cap: "The rows from `Hill_racing` with a distance close to 10km and climb close to 500m. The confidence interval on the mean time (shown in red) is narrow compared to the prediction interval (blue) calculated from the whole data frame and the model `time ~ distance + climb`."
ggplot(close_rows, aes(y=time, x=1)) +
  geom_jitter(width=0.2) + xlim(0,2) +
  geom_violin(fill="blue", alpha=0.2, color=NA) +
  geom_errorbar(aes(ymin=3364, ymax=3683), color="red") +
  geom_errorbar(aes(ymin=1665, ymax=5082, x=1.5), color="blue")
```
:::

::: {.callout-note}
## Example: A missed historical opportunity

Sewall Wright (1889-1988) was an American geneticist and statistical pioneer. One of his statistical inventions is the "coefficient of determination" now universally called R^2^ and a widely used, basic summary of statistical models. In 1921, he invented "path analysis." One of his "path diagrams" is shown in @fig-path-guinea-pig.

```{r echo=FALSE}
#| label: fig-path-guinea-pig
#| fig-cap: "Sewall Wright's path diagram describing the the mixing of environmental, developmental, and genetic factors contributing to the color patterns on guinea pigs."

"GET THIS FROM p.75 of the *Book of Why*"
```

His path diagrams are directed acyclic graphs, DAGs, augmented with coefficients representing the relative strength of each contributor to a node. He worked out the algebra of the correlation induced by the graph between any two nodes. Then, by measuring the R^2^ between pairs of nodes, he was able in some cases to work backwards to numerical values for the coefficients.

Wright's path diagrams are the historically earliest form of our DAGs. In his honor, we've constructed a DAG to represent one of his calculations, how much the body weight at birth of a guinea pig increases due to one day longer in the womb. The path diagram Wright imagined is drawn below, though we have left out the coefficients from the display.

```{r echo=FALSE}
dag_pigs <- dag_make(
  liter_size ~ 5 + as.numeric(exo() > 1),
  .gestation ~ 24 - liter_size + exo(),
  growth_rate ~ 10 - liter_size + exo(.3),
  weight ~ growth_rate*.gestation + liter_size + exo(),
  gestation ~ round(.gestation)
)
dag_pigs_for_drawing <- dag_make(
  Liter_size ~ 5 + as.numeric(exo() > 1),
  Gestation ~ 24 - Liter_size + exo(),
  growth_rate ~ 10 - Liter_size + exo(.3),
  Weight ~ growth_rate*Gestation + Liter_size + exo()
)
dag_draw(dag_pigs_for_drawing, vertex.label.cex=1,
         vertex.size=60, seed=103)
```

We can't measure the growth rate directly, but we can measure liter size, gestation length, and birth weight. But how can we estimate the direct effect of growth rate when it is confounded with the other causal pathways?

Sewall's breeding experiments would have provided data like this:

```{r echo=FALSE}
set.seed(104)
Pigs <- sample(dag_pigs, size=1000)
knitr::kable(head(round(Pigs %>% select(-gestation))))
```

You might think that weight gain per day of gestation can be simply calculated as `weight/gestation`, but this ignores the fact that weight gain is slow early in gestation and faster as the cubs develop. Instead, using a model `weight ~ gestation` lets us look at the marginal impact of an extra day of gestation. The coefficients from this model indicates that weight increases by 6.8 grams per extra day of gestation.

```{r}
lm(weight ~ gestation, data = Pigs) %>% confint()
```

But Wright knew that this number was misleading. Larger liters tend to have shorter gestation times. And larger liters produce cubs that weigh less. With more computational power available to us, we can use a simpler calculation to incorporate these facts into the estimation of weight gain per day of gestation:

```{r}
lm(weight ~ gestation + liter_size, data = Pigs) %>% confint()
```

This model pegs the growth rate at about 4.5 grams per day. 

Since we generated the data from a DAG, we have the luxury of measuring the actual growth rate used for each liter. 

```{r}
Pigs %>% summarize(rate = mean(growth_rate))
```

Covariates help us deal with confounding! Unfortunately, the statistical bigwigs of the 1920s through 1950s poo-pooed Wright's ideas about path analysis. They were rehabilitated only the the 1980s.

:::



## DAGs and covariates

WHAT CHAPTER DID THE SPENDING and aprotonin examples COME FROM?


The argument, "reduce spending by reducing spending" is very compelling, common sense even. It's harder to see how reducing spending in one area---the cash payment to people not on the insurance plan---can increase spending overall. I might have been more successful convincing the college budget committee not to eliminate the cash payment if they had understood the language of DAGs. @fig-insurance-two-dags shows two competing DAGs for the situation:

```{r echo=FALSE}
#| label: fig-insurance-two-dags
#| fig-cap: "Two different DAGs relevant to the debate about eliminating the cash payment to employees not on the college's health care plan."
#| layout-ncol: 2
#| fig-subcap: 
#|   - "One path from cash to expenditures"
#|   - "Two paths from cash to expenditures"
dagA <- dag_make(`Cash payment` ~ 1, 
                 Enrollment ~ 1,
                 `Expenditures` ~ `Cash payment` + Enrollment)
dagB <- dag_make(`Cash payment` ~ 1, 
                 `Enrollment` ~ `Cash payment`,
                 `Expenditures` ~ `Cash payment` + `Enrollment`)
set.seed(107); dag_draw(dagA, seed=107, vertex.label.cex=1);
dag_draw(dagB, vertex.label.cex=1, seed=109, vertex.size=60)
```

The people on the budget committee saw clearly the direct link between the cash payment and total expenditures and likely would not have disputed a direct link between enrollment and expenditures. But they didn't imagine a link between the cash payment and enrollment. I did, because I knew of several colleagues who used their spouse's companies insurance plan, even though it was identical to the college's plan.

The situation with the drug aprotinin is similar. 

```{r echo=FALSE}
#| label: fig-drugs-two-dags
#| fig-cap: "Two different DAGs relevant to the link between aprontinin and mortality."
#| layout-ncol: 2
#| fig-subcap: 
#|   - "One path from cash to expenditures"
#|   - "Two paths from cash to expenditures"
dagA <- dag_make(`Health condition` ~ 1, 
                 Aprontinin ~ 1,
                 Mortality ~ `Health condition` + Aprontinin)
dagB <- dag_make(`Health condition` ~ 1, 
                 Aprontinin ~ `Health condition`,
                 Mortality ~ `Health condition` + Aprontinin)
dag_draw(dagA, seed=107, vertex.label.cex=1); 
dag_draw(dagB, seed=107, vertex.label.cex=1)
```


------


::: {.callout-note}
## Example: A bookies' calculations [NEEDS FIXING]

The most familiar use of "odds" is in gambling. For instance, a famous song lyric puts the odds of Valentine winning the horse rate "at five to nine." Less musically, this odds is $5/9 = 0.5555$, but the two-number format makes particular sense for keeping track of bets. Five-to-nine describes a bet of one unit. The second number, 9, specifies the amount the gambler is staking on the outcome. On a loss, the gambler loses that stake. On a win, the gambler gets back the stake and, in addition, gets the amount specified by the first number. So a winner at five to nine would leave the racetrack with an extra $5. But on a loss, the gambler leaves $9 behind.

A "bookie" is someone who provides a service. You can go to a bookie to lay a bit. In drama, this might be done by telephone: "Lay $90 on Valentine" is all the gambler needs to communicate. No money has to change hands. On a win, the bookie will return $50 to the gambler. On a loss, the gambler has a debt of $90.

A bookie is not a gambler; he's an accountant who records numbers. The bookie arranges these numbers so that he makes money. To see this, imagine a horse race including Valentine, Paul Revere, and Epitaph. To start, the bookie specifies odds on each possible outcome, say 5:9 for Valentine, 1:3 for Paul Revere (a favorite!), and 1:2 on Epitaph. 

If the bookie has a good nose, about a third of the stakes will be bet on each outcome. If not, as new bets come in the bookie raises or lowers the odds to encourage or discourage bets so that the roughly one-third of stakes are placed on each outcome. Suppose at the end of the day that $500 is staked on each of the three outcomes. 

WRONG WRONG WRONG. It needs to work that the winning returned for Valentine has to be less than the stakes on the other horses, and similarly for all horses. So if $100 is bet on Valentine we need $100 staked on the other horses.  





Added up, these odds are $5+1+1=7$ on the top and $9+2+1=12$ on the bottom. It's important---for the bookie---that the odds are arranged so that the bottom number is larger than the top number: 12 is larger than 11. Note that this method of adding is simpler than combining fractions. To add the fractions $1/2$ and $1/3$ gives $5/6$. But to combine the odds $1:2$ and $1:3$ gives $2:5$. One more detail is needed for a real-life bookies, taking into account the size of each bet. For instance, a $5 bet at 5:9 would be recorded as 25:45.

Now the race is run. The winner is ... well ... from the bookie's point of view it doesn't matter who wins.
:::




## "Irrationality"

[From *The Model Thinker*, p. 52] 

**Gain Framing**: You have two options

Option A) Win $400 for certain

Option B) Win $1000 if a fair coin comes up heads and $0 if tails

**Loss Framing**: You are given $1000 and have two options:

Option a) Lose $600 for certain

Option b) Lose $0 if a fair coin comes up heads and lose $1000 if tails.


**Hyperbolic discounting**: see pp 52-43

"Prospect theory", Kahneman and Tversky (1979) "Prospect theory: an analysis of decisions under risk," *Econometrica* 47(2):263-291 [link to paper](www/kahneman-tversky-79.pdf)


::: {.callout-note}
## Example

A subtle modification to the linear model architecture allows the modeller to guarantee that the output will be between zero and one. The modified architecture, called "**logistic regression**", is therefore well suited to modeling categorical response variables, where the model output will be interpreted as a probability.

@fig-w-logistic shows a logistic model of survival as a function of age and smoking status. Notice that in the logistic model, the effect of smoking on survival is negative, particularly for people around age 50. The logistic architecture provides an intrinsic flexibility which avoids the undue influence of the very young and very old, for whom survival is close to 100% or 0 respectively *regardless* of smoking status.

::: {.content-visible when-format="html"}
```{r echo = FALSE, warning=FALSE}
#| label: fig-w-logistic
#| fig-cap: A simple logistic model of survival versus age and smoking status. 
wmod_glm1 <- glm(outcome=="Alive" ~ age + smoker, data = mosaicData::Whickham, family = binomial)
mod_plot(wmod_glm1) %>%
  gf_lims(y = c(-.1,1.1)) %>%
  gf_refine(scale_y_continuous(breaks=c(0,.2,.4,.6,.8,1.0))) %>%
  gf_labs(y = "Probability of survival for 20 years") %>%
  gf_jitter(as.numeric(outcome=="Alive") ~ age, color = ~ smoker, 
            data = mosaicData::Whickham, alpha = 0.2, height = 0.1)
```
:::


:::: {.content-visible when-format="pdf"}

::: {.callout-warning}
The figure fig-w-logistic is not compiling in PDF mode
:::
::::




# Confidence intervals from a single sample {#sec-lesson-23}

---
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r include=FALSE}
lesson <- 23
source("../_startup.R")
```

[Lesson -@sec-lesson-21] introduced separating data into components: signal and noise. The *signal* is a summary of the data that tells us something we want to know. Often, the signal will be one or more coefficients from a regression report, but it might be something as simple as the mean, median, or standard deviation of a variable in a data frame.

The *noise* comes into the data from various sources: e.g., error in measurement or a data-entry blunder. Another source of noise is omnipresent (except in a perfect census): sampling variation as discussed in [Lesson -@sec-lesson-22]. Sampling variation arises because the particular sample we happen to be working with reflects, to some extent, the play of luck. If we had happened to select another sample, the results would be different.

In general, whenever we measure something, say the altitude of a plane or the fuel economy of a car, it is helpful to know the "**precision**" of that estimate. It would be disastrous if the measured difference in altitude of two planes flying toward a common point were imprecise to the extent that the difference might actually be zero! Knowing the precision of the altitude measurement enables us to space the place in a safe way. 

One way to think about precision is in terms of repeatability. If we make multiple measurements of the same object in the same manner, the precision is the degree to which those measurements vary from one another. With the instrumentation used for physical measurements, the precision of individual measurements is estimated by repeated measuring the same thing. Likewise, in statistical summaries, the precision is related to sampling variation.

In [Lesson -@sec-lesson-22], we repeated trials over and over again to gain some feeling for sampling variation. We quantified the repeatability in any of several closely related ways: the sampling variance or its square root (the "standard error") or a "margin of error" or a "confidence interval." Our experiments with simulations demonstrated an important property of sampling variation: the amount of sampling variation depends on the sample size $n$. In particular, the sampling variance gets smaller as $n$ increases in proportion to $1/n$. (Consequently, the standard error gets smaller in proportion to $1/\sqrt{n}$.)

It is time to take off the DAG simulation training wheels and measure sampling variation from a *single* data frame. Our first approach will be to turn the single sample into several smaller samples: subsampling. Later, we will turn to another technique, resampling, which draws a sample of full size from the data frame. Sometimes, in particular with regression models, it is possible to calculate the sampling variation from a formula, allowing software to carry out and report the calculations automatically.

## Subsampling {#sec-subsampling}

To "subsample" means to draw a smaller sample from a large one. "Small" and "large" are relative. For our example, we turn to the `TenMileRace` data frame containing the record of thousands of runners' times in a race, along with basic information about each runner. There are many ways we could summarize `TenMileRace.` Any summary would do for the example. We will summarize the relationship between the runners' ages and their start-to-finish times (variable `net`), that is, `net ~ age`. To avoid the complexity of a runner's improvement with age followed by a decline, we will limit the study to people over 40.

```{r}
TenMileRace %>% filter(age > 40) %>%
  lm(net ~ age, data = .) %>% coef()
```

The units of `net` are seconds, and the units of `age` are years. The model coefficient on `age` tells us how the `net` time changes for each additional year of `age`: seconds per year. Using the entire data frame, we see that the time to run the race gets longer by about 28 seconds per year. So a 45-year-old runner who completed this year's 10-mile race in 3900 seconds (about 9.2 mph, a pretty good pace!) might expect that, in ten years, when she is 55 years old, her time will be longer by 280 seconds.

It would be asinine to report the ten-year change as 281.3517 seconds. The runner's time ten years from now will be influenced by the weather, crowding, the course conditions, whether she finds a good pace runner, the training regime, improvements in shoe technology, injuries, and illnesses, among other factors. There is little or nothing we can say from the `TenMileRace` data about such factors.

There's also sampling variation. There are 2898 people older than 40 in the `TenMileRace` data frame. The way the data was collected (radio-frequency interrogation of a dongle on the runner's shoe) suggests that the data is a census of finishers. However, it is also fair to treat it as a sample of the kind of people who run such races. People might have been interested in running but had a schedule conflict, lived too far away, or missed their train to the start line in the city.

We see sampling variation by comparing multiple samples. To create those multiple samples from `TenMileRace`, we will draw, at random, subsamples of, say, one-tenth the size of the whole, that is, $n=290$

```{r}
Over40 <- TenMileRace %>% filter(age > 40)
lm(time ~ age, data = Over40 %>% sample(size=290)) %>% coef()
lm(time ~ age, data = Over40 %>% sample(size=290)) %>% coef()
```

The age coefficients from these two subsampling trials differ one from the other by about 0.5 seconds. To get a more systematic view, run more trials:

```{r}
# a sample of summaries
Trials <- do(1000) * {
  lm(time ~ age, data = sample(Over40, size=290)) %>% coef()
}
# a summary of the sample of summaries
Trials %>%
  dplyr::summarize(se = sd(age))
```

We used the name `se` for the summary of samples of summaries because what we have calculated is the standard error of the age coefficient from samples of size $n=290$. 

In [Lesson -@sec-lesson-22] we saw that the standard error is proportional to $1/\sqrt{\strut n}$, where $n$ is the sample size. From the subsamples, know that the SE for $n=290$ is about 9.0 seconds. This tells us that the SE for the full $n=2898$ samples would be about $9.0 \frac{\sqrt{290}}{\sqrt{2898}} = 2.85$. 

So the interval summary of the `age` coefficient---the *confidence interval*--- is $$\underbrace{28.1}_\text{age coef.} \pm 2\times\!\!\!\!\!\!\! \underbrace{2.85}_\text{standard error} =\ \ \ \  28.1 \pm\!\!\!\!\!\!\!\! \underbrace{5.6}_\text{margin of error}\ \  \text{or, equivalently, 22.6 to 33.6}$$


## Bootstrapping

There is a trick, called "**resampling**," to generate a random subsample of a data frame with the same $n$ as the data frame: draw the new sample randomly from the original sample **with replacement**. An example will suffice to show what the "with replacement" does: `r set.seed(102)`

```{r}
example <- c(1,2,3,4,5)
# without replacement
sample(example)
# now, with replacement
sample(example, replace=TRUE)
sample(example, replace=TRUE)
sample(example, replace=TRUE)
sample(example, replace=TRUE)
```
The "with replacement" leads to the possibility that some values will be repeated two or more times and other values will be left out entirely.

The calculation of the SE using resampling is called "**bootstrapping**." 

::: {.callout-warning}
## Demonstration: Bootstrapping the standard error 

We will apply bootstrapping to find the standard error of the `age` coefficient from the model `time ~ age` fit to the `Over40` data frame.

There are two steps:

1. Run many trials, each of which fits the model `time ~ age` using `lm()`. From trial to trial, the data used for fitting is a resampling of the `Over40` data frame. The result of each trial is the coefficients from the model. 

2. Summarize the trials with the standard deviation of the `age` coefficients.

`r set.seed(207)`

```{r}
# run many trials
Trials <- do(1000) * {
  lm(time ~ age, data = sample(Over40, replace=TRUE)) %>% 
       coef()
}
# summarize the trials to find the SE
Trials %>% summarize(se = sd(age))
```

:::

## Confidence intervals from software

The same mathematical process that powers regression modeling software such as `lm()` can be used to compute standard errors for model coefficients as part of the fitting process. So, for regression models, finding a confidence interval is just a matter of asking for it. 

[Note: Experienced R users will know that the "standard" function for calculating confidence intervals is `confint()`, which is used in exactly the same manner as `conf_interval()`. Regrettably, `confint()` does not create a data frame. In keeping with these Lessons use of data wrangling, the `conf_interval()` from the `{math300}` package reformats the output of `confint()` into a data frame.]

There are several ways to do the asking. In R, the `conf_interval()` function makes it easy to extract the confidence intervals on each coefficient from a model. For example:

```{r}
lm(net ~ age + sex, data = TenMileRace) |> conf_interval() 
```

Each row of the result reports the confidence interval for one coefficient from the model.













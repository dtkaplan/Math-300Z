---
title: "Math 300R NTI Lesson `r (lesson <- 37)`"
subtitle: "False discovery"
author: "Prof. Danny Kaplan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
source("../_startup.R")
```


## Objectives

#. Identify signs of false discovery in a research paper.

#. Estimate how overall p-value should change when study is replicated.


## Reading

One or more of these articles:

- [A review of false discovery](www/false-discovery-significance.pdf)
- [Diet and sex determination](www/cereal_and_sex_determination.pdf)
- [Most research findings false](www/most-publication-findings-false.pdf)


## Lesson

Discussion of article(s).


### What should the p-value become

Consider `dag07`

```{r echo=FALSE}
dag_draw(dag07)
```

Node `d` is not connected to any of the other nodes. There should accordingly be a "null" relationship between `d` and the others. On the other hand, `b` and `c` are connected (although the connection is confounded with `a`).

Let's model `d` by `b` and look at the p-value: `r set.seed(101)`

```{r}
Sample <- sample(dag07, size=50)
lm(d ~ b, data=Sample) %>% broom::tidy()
```

The p-value on the `b` coefficient is large, greater than the usual threshold of 0.05.

On the other hand, `b` and `c` are connected and the p-value (with this much data) is tiny.

```{r}
lm(c ~ b, data = Sample) %>% broom::tidy()
```

Imagine a setting where a popular (but unproven!) hypothesis has emerged: that `b` and `d` are really related. 100 different research teams rush in to be the first to demonstrate, each generating their own experimental data. We'll simulate this and collect the summary of the `b` coefficient w.r.t. `d`. [First show the statement without the `do()` to show what each row looks like. Then run the 100 trials and look for small p-values]

```{r}
All_groups <- do(100) * {
  lm(d ~ b, data=sample(dag07, size=50)) %>% 
  broom::tidy() %>%
  filter(term == 'b')
  }
```

Did any of the groups get a "significant" result?

```{r}
All_groups %>% 
  filter(p.value < 0.05)
```

In the context of 100 trials being done, it's understandable that some of the groups happened to get a p-value < 0.05. But suppose that only the groups with small p-values publish their results? Then it looks as if they found a "significant" result.

How can we guard against this accidental generation of significant results? The standard answer in scientific work is to **replicate** the result: the labs should try again to confirm the result they got in the first study. (In practice, there are strong social/financial/career pressures *against* conducting such replications. These need to be overcome to guard against false discovery.)

Here's a simulation where each lab group runs the study twice. Do any get small p-values both times?

```{r}
Replicated_groups <- do(100) * {
  do(2) * {
    lm(d ~ b, data=sample(dag07, size=50)) %>% 
      broom::tidy() %>%
      filter(term == 'b')
    } %>% .$p.value
} 
Pairs <- Replicated_groups %>% 
  tidyr::pivot_wider(names_from = .row, values_from = result)
Pairs %>% filter(`1` < 0.05, `2` < 0.05)
```

A better approach. As a rule of thumb, once you have a sample size $n$ that gives a genuine  p $\approx 0.05$, doubling $n$ should reduce p by a factor of about 10. But if p is merely accidentally small, doubling the sample size won't have any effect. 

A demonstration when there is a genuine relationship:

```{r}
lm(c ~ a, data = sample(dag07, size=5)) %>% broom::tidy() %>%
  filter(term == 'a')
lm(c ~ a, data = sample(dag07, size=10)) %>% broom::tidy() %>%
  filter(term == 'a')
```

Let's re-run the simulation with $n$ doubled, that is, `size=100` compared to the previous `size=50`

```{r}
Bigger_n <- do(100) * {
  lm(d ~ b, data=sample(dag07, size=100)) %>% 
    broom::tidy() %>%
    filter(term == 'b')
  }
```

Are these p-values smaller than in the trials with `size=50`?


## Learning Challenges



## Documenting software

  


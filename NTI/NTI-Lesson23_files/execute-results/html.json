{
  "hash": "59d500bc3e23a5cb471489e8de1051f7",
  "result": {
    "markdown": "---\ntitle: \"Math 300R NTI Lesson 23\"\nsubtitle: \"sampling variation from a single sample\"\nauthor: \"Prof. Danny Kaplan\"\ndate: \"November 04, 2022\"\noutput:\n  html_document:\n    theme: lumen\n    toc: yes\n    toc_float: yes\n    css: NTI.css\n  pdf_document:\n    toc: yes\n---\n\n\n\n\n## Objectives\n\n\n\n\n\n\n\n\n23.1 Use bootstrapping to estimate sampling variation.\n\n23.2 Infer sampling variation from a regression table: \"standard error\" of a model coefficient.\n\n23.3 Construct and interpret confidence intervals on a model coefficient and relate the interval to the sampling distribution.\n\n23.4. Understand and use scaling of confidence interval length as a function of $n$.\n\n\n## Reading\n\nTBD\n\n\n## Lesson\n\n::: {.callout-tip icon=FALSE}\n## Setup \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../_startup.R\")\n```\n:::\n\n:::\n\n\n\nThe point of today's lesson is to show how some properties of the sampling distribution can be estimated from a single sample, rather than the many trials of sample-then-summarize that we used in Lesson 22.\n\n## Bootstrapping\n\nOne approach is to mimic what we did when we had a DAG available: run many trials. But all we have is a sample, not the DAG. Can we run many trials from the sample itself?\n\nLet's use the `moderndive::amazon_books` data set and the model tilde `list_price ~ num_pages`\n\nHere's the results on the actual data:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ num_pages, data = moderndive::amazon_books) %>% coefficients()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)   num_pages \n 11.8438357   0.0198815 \n```\n:::\n:::\n\nInterpretation: books cost about $12 plus 2 cents per page.\n\nHere is our first stab at sampling from the data. Note that when sampling from a data set, `sample()` sets the sample size to be the same as the number of rows in the data set.\n\n::: {.cell}\n\n```{.r .cell-code}\ntrial <- function() {\n  lm(list_price ~ num_pages, data = sample(moderndive::amazon_books)) %>% \n    coefficients()\n}\n{do(100) * trial()} %>% summarize(sd(Intercept), sd(num_pages))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  sd(Intercept) sd(num_pages)\n1  1.299845e-14  2.193162e-17\n```\n:::\n:::\n\n::: {.callout-note icon=false}\n## Questions\n\n- How to interpret these numbers? What are they telling us about the sampling distribution on the regression coefficients?\n- What's wrong?\n- How to fix it?\n:::\n\nIntroduce the `replace=TRUE` argument for `sample()`. Explain what it does and show the result. \nIs the result correct? We can't know, because we don't have access to the data-generating DAG in order to run many trials. But we can test the method on data generated from a DAG and confirm that it gives a reasonable result.\n\n::: {.callout-important icon=false}\n## Activity\n\nEach student should construct one from a dag of his or her choice.\n:::\n\n### Regression table\n\nIn the readings, we mentioned that there are formulas for the standard deviation of the sampling distribution. [NEEDED TO GIVE THIS A NAME EARLIER: \"SD of sampling variability\", \"standard error\"] All you need to know about the formulas for the standard error is that i) they exist for linear regression and ii) the standard error is inversely proportional to $\\sqrt{n}$.\n\nShow how to construct the regression table and where to find the relevant standard error\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndo(3) * {lm(list_price ~ num_pages, data = moderndive::amazon_books) %>% broom::tidy()}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  term        estimate std.error statistic  p.value  .row .index\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl> <int>  <dbl>\n1 (Intercept)  11.8      1.79         6.63 1.40e-10     1      1\n2 num_pages     0.0199   0.00479      4.15 4.24e- 5     2      1\n3 (Intercept)  11.8      1.79         6.63 1.40e-10     1      2\n4 num_pages     0.0199   0.00479      4.15 4.24e- 5     2      2\n5 (Intercept)  11.8      1.79         6.63 1.40e-10     1      3\n6 num_pages     0.0199   0.00479      4.15 4.24e- 5     2      3\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ num_pages, data = moderndive::amazon_books) %>% broom::glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0511  0.0481  13.9    17.2 4.24e-5     1 -1304. 2614. 2625.  61999.     320\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n```\n:::\n:::\n\n\n\n## Margin of error\n\nConfidence interval as estimate $\\pm 2\\times$ standard error.\n\nGraphics for confidence interval. They need to know how to read them, not to make the graphics.\n\n## Learning Checks\n\n\n\n\n\n\n\n\n\n## 23.1\n\nVocabulary: Sampling distribution, standard error, sampling variability, sample size\n\n::: {.callout-note}\n## Solution\n\n:::\n\n--------\n\n\n## 23.2\n\nIn LC 22.2, using `do(100)`, you displayed the sampling distribution on the `x` coefficient of the model `y ~ x` applied to data simulated from `dag01`. Among other things, you calculated the standard deviation of the sampling distribution. Copy over the `proc1()` you wrote for LC 22.2 into your Rmd document for this lesson.\n\nCalculate the standard deviation of the sampling distribution in each of these situations.\n\nNumber of trials  |  Sample size\n------------------|---------------\n`do(100)`         | `size=25`\n`do(100)`         | `size=100`\n`do(100)`         | `size=400`\n`do(500)`         | `size=25`\n`do(500)`         | `size=100`\n`do(500)`         | `size=400`\n\nIn each case, the standard deviation is somewhat random, since new simulated data is collected from `dag01` each time. Nonetheless, there is a systematic pattern to how the standard deviation varies with the number of trials and with the sample size.\n\n1. Describe how the standard deviation of the sampling distribution of the `x` coefficient varies with sample size. The general trend should be easy to see.\n\n2. Does the standard deviation of the sampling distribution depend on the number of trials?\n\n3. Going back to your results from (1), try to find a simple quantitative relationship that describes how the standard deviation depends on sample size. State that relationship in words.\n\n-----------\n\n## 23.3\n\nWe're going to build models of prices of books based on the `moderndive::amazon_books` data frame. For each model, you will calculate the confidence interval of one or more coefficients in two ways:\n\ni. Directly, using `confint()`.\nii. Indirectly, using `broom::tidy()`\n\n**Model 1**. \n\n1. Model `list_price` versus `amazon_price`. Calculate the confidence intervals on the intercept and on the `amazon_price` coefficient.\n\n2. Interpret the `amazon_price` coefficient in everyday words.\n\n::: {.callout-note}\n## Solution to part 1.\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ amazon_price, data = amazon_books) %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                2.5 %   97.5 %\n(Intercept)  3.716532 5.122737\namazon_price 1.049308 1.127471\n```\n:::\n\n```{.r .cell-code}\nlm(list_price ~ amazon_price, data = amazon_books) %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      4.42    0.357       12.4 5.22e- 29\n2 amazon_price     1.09    0.0199      54.8 2.82e-165\n```\n:::\n:::\n\n1. You can read the confidence interval directly from the `confint()` report. For the regression report, calculate the confidence interval as the estimate $\\pm 2$ times the \"standard error.\"\n\n2. Just to look at the `amazon_price()` coefficient, the list price is about 8% higher than the Amazon price. Here, \"about\" means 5% to 13%. But don't forget the intercept. The list price is, on average, about $4.50 higher than the 1.08 multiplier on the Amazon price.\n:::\n\n**Model 2**.\n\n3. Model `list_price` versus `amazon_price`, including `hard_paper` as a covariate.\n\n::: {.callout-note}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                2.5 %   97.5 %\n(Intercept)  3.028400 4.466996\namazon_price 1.042536 1.117826\nhard_paperH  1.786977 3.882884\n```\n:::\n\n```{.r .cell-code}\nlm(list_price ~ amazon_price + hard_paper, data = amazon_books) %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)      3.75    0.366      10.3  1.62e- 21\n2 amazon_price     1.08    0.0191     56.5  9.65e-169\n3 hard_paperH      2.83    0.533       5.32 1.93e-  7\n```\n:::\n:::\n\nA hardcover costs about $2 to $4 more than a paperback.\n:::\n\n::: {.callout-warning}\n## Note in draft\nReturn to this example in the prediction lesson, to show how the confidence interval and the prediction interval are different.\n\nMaybe also use it in one of the side-exercises on interaction terms. (The plan is not to strongly emphasize interaction terms but to refer to them in the occasional exercise.)\n:::\n\n-------------\n\n## 23.4 (Objective 23.1)\n\nWe're going to work with a very short dataset so that you can see directly what **resampling** a data frame does. (Ordinarily, you use resampling on an entire dataset, but here we are trying to make a point about the mechanism of resampling.)\n\n1. Create a data frame `Five` that consists of the first five rows of `moderndive::mythbusters_yawn`. (Hint: Use `head()`.) Put the code for doing this into your Rmd homework paper. Note that `Five` contains the data from subjects 1 through 5.\n\n::: {.callout-note}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nFive <- mythbusters_yawn %>% head(5)\n```\n:::\n:::\n\n2. Use `resample()` to generate a new data frame from `Five`. At this point, you are just going to look at the result, processing it \"by eye.\" How many distinct human subjects are reported in the resampled data? (Your answer will likely differ from your classmates', since resampling is done at random.)\n\n3. Repeat (2) ten times. Each time, count the number of distinct human subjects. \n\n    i. Report those ten numbers on your write-up.\n    ii. There will usually be one or more subjects repeated in the output. Look at these repeats carefully to check whether the variables have the same value for all the repeats or whether sometimes a repeated subject has different values for `group` or `yawn`. \n    \n::: {.callout-note}\n## Solution\n\nMost of the time there will be 2, 3, or 4 distinct subject. The balance of the five rows will be repeats of other subjects. When a subject is repeated, the entire row is identical for all instances of that subject.\n:::\n\n**Going further** (optional). It's pretty easy to automate the process of generating the resample and counting the number of distinct human subjects. Like this:\n\n::: {.cell}\n\n```{.r .cell-code}\n{resample(Five) %>% unique() %>% nrow()}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\nUsing `do(1000)`, carry out 1000 trials of this process, saving the overall results in a data frame named `Trials`. What is the mean number of unique human subjects across the 1000 trials? What fraction is this of the five subjects.\n\nDo the same again, but instead of using `Five`, use the whole `mythbusters_yawn` data frame (which has 50 rows).  What fraction of the 50 human subjects, on average, shows up in the resamples?\n\n::: {.callout-note}\n## Solution\n\n::: {.cell}\n\n```{.r .cell-code}\nTrials <- do(1000) * {resample(mythbusters_yawn) %>% unique() %>% nrow()}\nTrials %>% summarize(mn = mean(result)/nrow(mythbusters_yawn))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mn\n1 0.6352\n```\n:::\n:::\n:::\n\n\n-------------\n\n## 23.5 (Objective 23.1)\n\nReturn to the `amazon_books` data frame and the model `list_price ~ amazon_price`. In Exercise 23.3 you used the regression report to calculate the confidence intervals on the intercept and on the `amazon_price` coefficient. Now you are going to repeat the calculation in a different way, using randomization, a process called \"**bootstrapping**\".\n\nThe basic process is to train a model using *resampled* data, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept) amazon_price \n    5.041221     1.060036 \n```\n:::\n:::\n\nThen, using `do(500)`, carry out 500 trials, saving the result in a data frame named `Trials`.\n\nProcess `Trials` to calculate both the mean and the standard deviation of the intercept and `amazon_price` columns. How do those results compare to the \"standard error\" results from the same model (without resampling) as you found in LC 23.3?\n\n::: {.callout-note}\n## Solution\n\nSOMETHING IS WRONG HERE. The means are about the same as from the regression report (as they should be) but the standard deviations are 3-4 times larger. WHAT GIVES?\n\nTHE PROBLEM IS a handful of books where the Amazon price is very different from the list price, because the book itself is very expensive (e.g. $100). Remedy\n\n1. Switch to another data example, maybe doing both the regression report and the bootstrapping in one exercise.\n2. This is an object lesson in outliers. Since the dollar discount is presumably *proportional* to the price, we should have used log transforms.\n\n::: {.cell}\n\n```{.r .cell-code}\nTrials <- do(500) * {lm(list_price ~ amazon_price, data = resample(amazon_books)) %>% coefficients()}\nTrials %>% summarize(m1 = mean(Intercept), \n                     m2 = mean(amazon_price), \n                     sintercept = sd(Intercept), \n                     samazon_price = sd(amazon_price))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        m1       m2 sintercept samazon_price\n1 4.171633 1.109313  0.9593141    0.08356072\n```\n:::\n:::\n\n:::\n\n-------------\n\n\n\n\n\n## Documenting software\n\n* File creation date: 2022-11-04\n* R version 4.2.1 (2022-06-23)\n* `tidyverse` package version: 1.3.2\n* `mosaic` package version: 1.8.4\n* `math300` package version: 0.1.0.9000\n\n  \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
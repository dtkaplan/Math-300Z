---
title: "Math 300R NTI Lesson `r (lesson <- 19)`"
subtitle: "Statistical thinking"
author: "Prof. Danny Kaplan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
source("../_startup.R")
library(mosaicData)
```

In the first half of the course, we examined *data wrangling* methods---`group_by()`, `summarize()` and `tally()` in particular---for summarizing variables and breaking up the summaries by groups. 

In this second half of the course, we will be emphasizing the **relationships between variables**. The conceptual framework we've used for describing such relationships involves choosing a **response variable** and selecting one or more **explanatory variables**. Within this framework, you've already seen regression techniques for describing how a numerical response variable can be related to numerical explanatory variables. Something new you will see in the second half of the course is using regression techniques to handle *categorical* response variables. This will allow us to extend regression to cover such things as proportions and probabilities.

Graphics can be a powerful way to perceive relationships between variables. In keeping with the response/explanatory framework, our go-to graphical technique will be to plot data as a "**point plot**" absolutely sticking to the convention that the response variable will be assigned to the *vertical axis*. Note that we'll use the term "point plot" rather than the "scatter plot" that was used in the early graphics chapters. That's partly because we're going to use the idea of "scattering" in a different way.

We're also going to adopt a new graphical convention: that statistical summaries should always be graphed as a layer on top of the raw data from which the summaries are derived. This means that we'll switch away from histograms as a way of displaying distributions. Why? Because the vertical axis on a histogram does not correspond to the values of a response variable. Similarly, barplots won't be appropriate, because the response variable in a barplot is represented by color rather than position on a vertical axis.

Overall, both the graphical display of data and summaries and the calculation of numerical summaries like means and proportions will become much simpler to accomplish, since relationship summaries will be produced using using `lm()` and closely related model types. Data graphics will be made with `geom_point()` and closely related geoms.

## Graphing data and statistical annotations




## Categorical response variables

To get started, we'll work with categorical response variables that are "**dicotomous**" or, equivalently, "**binomial**," that is, with only two methods. This covers a large fraction of the situations where categorical response values are needed. We'll leave those situations where there are lots of levels for the response variable to courses on "machine learning," a topic we can only scratch the surface of in this introductory course.

The important insight into using categorical variables in regression models is encapsulated in the idea of the 0-1 ("zero-one") encoding. To illustrate, consider the `mosaicData::Whickham` data frame that records the age, smoking status, and survival of 1000 or so nurses in the UK. The relationship that motivated the collection of these data is between smoking and survival. Taking a few rows from the data frame let's us easily see the types of the variables involved:

```{r echo=FALSE}
set.seed(101)
sample_n(Whickham, size=10) %>% knitr::kable()
```

HOW TO USE `math300::zero_one()`

Plotting a zero-one response variable with correctly labelled axes.

```{r}
P <- Whickham %>% 
  gf_jitter(zero_one(outcome) ~ age, color=~smoker, height=0.1, alpha=0.5) %>% 
  label_zero_one()
P
```

We might equally well plot age as a function of outcome:

```{r}
gf_jitter(age ~ outcome, data = Whickham, width=0.2) %>% gf_violin(color=NA, fill="blue", alpha=0.5)
```

PLOT OUT A MODEL

```{r}
mod <- model_train(zero_one(outcome) ~ age + smoker, data = Whickham)
# fun <- mod_fun(mod) 
# P %>% mosaicCalc::slice_plot(fun(age, smoker="Yes") ~ age, color="red")
```

## Intervals

Once you have FIXES MOSAICMODEL, Generate a "prediction" interval and use gf_errorbar() to plot it over the model.

## Objectives


```{r child=Obj_file(lesson)}
```

## Reading

TBD


## Lesson

*This lesson marks the beginning of a new phase of the course. Thus far, we've worked with *techniques* for data wrangling, graphics, and regression modeling. Now we address the question of what a regression model (and other information that we might have) can tell us about the real world.*

::: {.callout-tip icon=false}
## Setup

```r
library(mosaicData)
```
:::

:::: {.callout-note icon=false}
## Guided activity: House prices

*Have students do the calculations for the first model, answering the questions that follow. After this is complete, have students do the calculations for the second model and answer those questions.*

The `mosaicData::SaratogaHouses` data frame contains information about the sales price and various attributes of about 1700 houses. (See `?SaratogaHouses` for a detailed description.)

Do a regression of `price ~ bedrooms` and explain what the regression coefficients mean.

```{r}
lm(price ~ bedrooms, data=SaratogaHouses) |> coefficients()
```

1. What are the units of the intercept and of the `bedrooms` coefficient? *Intercept in dollars, `bedrooms` in dollars per bedroom.

2. Interpret what the coefficients indicate about the price of houses and bedrooms. *Each additional bedroom adds about $50000 to the value of a house.*

3. According to the model, predict what would be the sales price (at the time the data was collected, 2006) of a house with two bedrooms? *$59863 + 2\times 48218$*

Of course, bedrooms are not the only important thing about a house. Let's include `livingArea` along with `bedrooms` in the model.

```{r}
lm(price ~ livingArea + bedrooms, data=SaratogaHouses) |> coefficients()
```

1. What are the units of the `livingArea` coefficient? *dollars per square foot*
2. What does this model say about the value of adding a bedroom? *It seems to reduce the value of the house by about $15,000.*

Based on exactly the same data, the two models seem to give contradictory statements about the value of an additional bedroom. 

i. Is one model right and the other wrong? If so, which one is right? (Explain your reasoning.) *Both models are mathematically correct. But they need to be interpreted in different ways. Each is telling us something different about the real world.*
ii. Could both models be right? If so, explain why the `bedroom` coefficients have opposite signs. *We will need to develop some additional tools and concepts before we can take on this question.*

Learning how to interpret models in terms of what they say about the world is a major theme of this second half of Math 300. 

Another, more technical question that we will address has to do with the *precision* of coefficients like 125.40 dollars per square foot. Might it actually be $200/ft^2^? How about $500/ft^2^? And how seriously should we take the sales value that we calculate by setting numbers for `bedrooms` and `livingArea` into the model?

::::

## Learning Checks

```{r child=LC_file(lesson)}
```






## Documenting software

  * File creation date: `r Sys.Date()`
  * `r R.version.string`
  * `tidyverse` package version: `r packageVersion("tidyverse")`


::: {.callout-note}
## Violins versus boxes

All of the graphical statistical annotations are human inventions. Each invention attempts to meet a need, but usually, the invention is a compromise between the statistical objective and the computational and graphical resources available. The **box plot** format is a case in point. The statistical goal of a box plot is to display the distribution of values of a variable. It was invented in a time when graphics were mostly drawn by hand and computers were not widely available. The computations behind a box plot produce a five-number summary: min, first quartile, median, third quartile, max. It's straightforward (but tedious!) to do these by hand since they are based on sorting and counting. The drawing itself uses only straight lines, which are easy to draw by hand with only a pencil and a straightedge.

:::: {.content-visible when-format="pdf"}

\includegraphics[width=0.3\columnwidth]{www/cigar-box-guitar.png} A cigar-box guitar

::::

:::: {.column-margin .content-visible when-format="html"}
![](www/cigar-box-guitar.png)

A cigar-box guitar

::::


A violin plot requires hundreds or thousands of evaluations of the gaussian function along with post-processing. They are not feasible for a human; a computer is required. Similarly, drawing the detailed shape of the violin (@fig-violin-intro) requires a computer. 

The box plot has important deficiencies. It is appropriate only for uni-modal distributions and doesn't give even a hint of possible bi-modality. The sharp boundaries of the box and endpoints of the whiskers suggest that even smooth density shapes have abrupt transitions. Points are marked as "outliers" in order to keep the whiskers from becoming absurdly long, but box plots of data with a normal (gaussian) distribution will produce such "outliers" whenever the sample size is large. 

When it comes to computing power, we are today unimaginably rich compared to the generation that introduced box plots. In a sense, we are so rich we can use expensive, well made products such as a violin. The box-plot generation was living in computational poverty. Not having the (computational) funds to buy a violin, they had to make do with primitive instruments they had to make do with the materials at hand, just as early blues mucisians, coming out of poverty, often had to build instruments such as a cigar-box guitar. 
:::

---
title: "Math 300R NTI Lesson `r (lesson <- 29)`"
subtitle: "Covariates eat variance"
author: "Prof. Danny Kaplan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
source("../_startup.R")
```

## Objectives

```{r child=Obj_file(lesson)}
```

## Reading

TBD


## Lesson


::: {.callout-note icon=false}
## Summary

Including covariates in a model can help, or can hurt. These are the conclusions we're working toward:

i. In-sample, including covariates in a model always reduces the (in-sample) prediction error. The pattern is stronger the smaller the training data set.

ii. Out-of-sample, including covariates may or may not reduce prediction error. It depends on whether the covariates are genuinely connected to the response variable.

iii. Irrelevant covariates make (out-of-sample) prediction worse. This effect is strongest for small training data sets.

Remember, since some of the conclusions depend on what variables are connected to what, we need to demonstrate the phenomena using a system where we know the structure.

We'll come back to this topic, as a basis for ANOVA, when we do hypothesis testing. There we'll look at the sum of squares, mean square, F and such.

:::


Today, we'll work mostly with **in-sample** modeling. This reflects the case in the real world, where you have a data set but not usually an easy way to collect more data for testing.^[There is a technique that let's you get many of the benefits of out-of-sample testing with only one dataset. It's called **cross-validation**. Perhaps later in the course, but we have bigger fish to fry right now.]

Let's work with `dag04`,`dag05`, and `dag07` to illustrate some points about covariates.

```{r}
dag_draw(dag04)
dag_draw(dag05)
dag_draw(dag07)
```

Start with `dag04`, where variables `a`, `b`, and `c` all contribute to the formation of `d`. 

```{r}
compare_rms_error(dag04, d ~ c, d~ b + c, d ~ a + b + c, n=50, in_sample = TRUE)
```

Prediction error gets smaller, the more covariates are included.

The situation can be different. In `dag05`, `a`, `b`, and `c` all contribute to `d`, but **not separately**. `a` and `b` communicate with `d`  only via `c`. If `c` is in the model, `a` and `b` contribute nothing to reducing prediction error.

```{r}
compare_rms_error(dag05, d ~ c, d~ b + c, d ~ a + b + c, n=50, in_sample = TRUE)
```

`dag07` is a case where we have covariates, but they aren't actually connected to `d`. Will they reduce prediction error? We'll use a very small sample size, $n=4$, to make the situation obvious.

```{r}
compare_rms_error(dag07, d ~ 1, d ~ c, d~ b + c, d ~ a + b + c, n=4, in_sample = TRUE)
```

::: {.callout-note icon=false}
## Pattern shown by `dag07` model

Confirm these by running many simulations.

1. The prediction error gets smaller the more covariates are included in the model.
2. The last prediction error, with 4 terms in the model (don't forget `1`!) is zero. A perfect model?

:::

### Alternative accountings

We've been using RMS prediction error to quantify how well the response variable has been accounted for by the explanatory variable(s). RMS prediction error is a convenient summary of the size of the typical prediction error because 1) it is an average over all the cases in the testing data and 2) it has the same units as the response variable. But RMS is not the only the only such accounting. In this section, we'll look at two others that are widely used in statistical reports: R^2^ and the "**sum of squares**".

We'll start with the *sum of squares* accounting. Recall that the letters in RMS each stand for a specific step:

- **S**: square the each of the values
- **M**: average over the (squared) values
- **R**: take the square root of the (average squared) values.

The *order* of the steps is important; S first, M next, then finally R. 

The *sum of squares*, often written SS, is a two-step process.

- **S**: square each of the values
- **S**: sum (not average) the (squared) values.

Again, the *order* of the steps is important: square first then finally sum. (The notation SS doesn't make the order clear, but the name "sum of squares" does. So remember that SS stands for "sum of squares".)

::: {.callout-warning}
## Note in draft

WHEN YOU GET TO THE ANOVA REPORT, make sure to point out that the so-called "mean square" is a confusing name because it wrongly brings to mind the MS in RMS.  The quantity is really the sum of squares divided by the degrees of freedom.
:::






## Learning Checks

```{r child=LC_file(lesson)}
```



## Documenting software

  


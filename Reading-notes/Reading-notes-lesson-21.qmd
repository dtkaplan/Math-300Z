---
title: "Math 300R Lesson `r (lesson <- 21)` Reading Notes"
subtitle: "DAGs, noise, and simulation"
author: "Prof. Danny Kaplan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r include=FALSE}
source("../_startup.R")
```


The theme of this second half of the course is one word: "inference." To illustrate how inference is distinct from the data science methods---wrangling, visualization, modeling---you have been learning, consider @fig-hadley-r4ds-diagram, copied from a well-regarded data-science text.

```{r echo=FALSE, out.width="60%"}
#| label: fig-hadley-r4ds-diagram
#| fig.cap: "Schematic diagram illustrating a path for data exploration. From Wickham & Grolemund, [*R for Data Science*](https://r4ds.had.co.nz/explore-intro.html)"
knitr::include_graphics("www/data-science-explore.png")
```
You've been to all the stepping stones on this path (with "transform" meaning "wrangling"), except the one on the far right: "Communicate." I'd prefer if "communicate" were replaced with "inform." One of the main uses of data science is to extract from data information that is useful for decision-making. The best information is that which reflects the true state of the world. 

**Statistical inference** is the body of concepts and techniques that help us be careful and responsible that our statistical results do actually reflect the true state of the world insofar as the data can tell us about it.

Example: Imagine a salesperson bargaining with a customer. Successful salespeople want to have some idea about how much money the customer would be willing to spend. They try to predict this based on observations they can make about the customer. These might include the kind of car the customer drives, how the customer is dressed, age of the customer, etc. You might think that the prediction takes the form of a **quantity**: the amount of money. But it's better if the prediction is in the form of a **range**. After all, the relationship between willingness to pay and the observed customer attributes is weak and uncertain. It's not good if the prediction suggests more precision than is actually warranted.

Example: Nutritionists are interested in helping people to make diet choices that will increase health. Often they do this by collecting data on what people eat and their health outcomes. A regression model might indicate, for instance, that "organic" food is associated with a reduction in risk of an illness by, say, 20%. Skeptics could point out that "correlation is not causation," and that other factors, say family income, account for the association. To inform the consumers decision about spending on food, it's helpful if the data modeling is structured in a way that can provide confidence that the link between food and health is causal. 

Since inference involves the relationship between the resulted gleaned from models and the state of the world, it's helpful to have situations where the state of the world is well known. We are going to provide such situations in a simple, sure way: by simulation.

To emphasize that the simulation is only for developing an understanding of inference, and **not for depicting the real world**, we'll use the word "gaming" to refer to using such simulations. The full process of gaming has four stages.

::: {.callout-note icon=false}
## The Four Stages 
i. building the deck: Instructors provide a simulation of a mechanism that generates rows of a data frame.
ii. the deal: Some of these rows will be dealt to you, constituting the data you have to work with. [real-world]
iii. the play: Build models and extract results. [real-world]
iv. the reveal: Compare your results from (iii) either to the mechanism given in (i) or to more data generated by the simulation.
:::
    

WE'RE GOING TO USE SIMPLE NAMES like X, Y, A, B, C just to remind you that this is a simulation. Real-world data will usually have descriptive variable names, like `height`, `age`, `score`.

## Signal and noise {#sec-signal-and-noise}

A useful distinction is made in statistics---as it is in engineering---between **signal** and **noise**. For instance, it is usually assumed that response variables consist of a part that stems deterministically from the rest of the system ("signal") and a random part ("noise").

An important task in statistical modeling is to estimate the **size** of the noise. We'll need to develop some statistical techniques to do this with real world data, but we can illustrate the ideas with DAG games.  

Consider the mechanism represented by `dag01`:

```{r}
dag01
dag_draw(dag01)
```

`dag01` has two variables, `x` and `y`. The tilde expression `x ~ eps()` means that `x` is generated entirely as random noise; that's what the `eps()` function does.  In contrast, the tilde expression `y ~ 1.5 * x + 4 + eps()` means that variable `y` will be composed of a part that depends on `x` and another part that is random noise. 

- "depends on `x`": `1.5*x + 4`
- "random noise": `eps()`

In contrast, the `dag00` system involves two variables that are not connected in any way.

```{r}
dag00
dag_draw(dag00)
```

::: {.callout-tip}
## `eps()` is a process
The idea of randomness is central to statistics. In simulating data from a DAG, we use a variety of **random number generators**. One of the most often used generators appears in the tilde expressions as `eps()`.

From the notation, you can see that `eps()` is an R function. By default, it is used without an input, just the bare `()` with nothing inside. So what could the output be? 

A random number generator, like `eps()`, produces its output as if from nothing. (There's a lot of sophisticated mathematics behind this "nothing" but that doesn't concern us here.) Each time `eps()` is used, it makes a new set of random numbers that are not connected to any previous output created by `eps()`. It's best to think about random number generators like `eps()` as a process, for instance, rolls of dice or spins of a lottery wheel.

By default, the "size" of the random noise produced by `eps()` is 1. Some DAGs, such as `dag01`, explicitly specify the size of the noise. For instance, `eps(2)` means the noise will have size 2, `eps(0.17)` means the noise will have size 0.17.
:::

## Simple DAGs

A DAG is a **hypothesis**, a statement that might or might not be true. DAGs are part of the statistical apparatus for thinking responsibly about **causality**. You use a DAG---or, potentially, multiple DAGs---when the issue of what causes what is relevant to your work.

When there are only two variables involved in the system under consideration---we'll call them X and Y for simplicity---there are only two possible DAGs:

$$X \rightarrow Y\ \ \ \ \ \text{and}\ \ \ \ \ X \leftarrow Y$$

But there are additional DAG possibilities once we accept that there might be other factors that positioned in the middle of the relationship between X and Y, for instance a variable C.

$$X \rightarrow C \rightarrow Y \ \ \ \ \ \text{and}\ \ \ \ \
X \leftarrow C \leftarrow Y \ \ \ \ \ \text{and}\ \ \ \ \
X \leftarrow C \rightarrow Y \ \ \ \ \ \text{and}\ \ \ \ \
X \rightarrow C \leftarrow Y$$

Actually, there are many other configurations of DAGs involving three variables. To keep things simple, we'll restrict things to DAGs where X might or might not cause Y, but Y never causes X. (We don't lose anything from this restriction because you get to make the choice of what real-world variable correspond to X and which one to Y.)  @fig-ten-triples shows the 10 configurations of 3-variable DAGs where Y doesn't cause X.

```{r echo=FALSE}
#| label: fig-ten-triples
#| fig-cap: "Ten DAG configurations involving three variables X, Y, and C and where there is no causal arrow between Y and X."
knitr::include_graphics("www/ten-triples.png")
```

This enumeration of the ten configurations is useful for two reasons. First, the appropriate ways of analyzing data to quantify the causal links depends on which DAGs are hypothesized to be relevant. (We'll get to this matter in Lesson 30.) Second, however strong may be your conviction that a particular DAG describes the way the world works, it's worthwhile to look at each of the other DAGs as possibilities and try to imagine scenarios where they might be an appropriate choice. This exercise can reveal alternative reasonable hypotheses about the real-world connections among variables.

The behavior of such three-variable DAGs is sufficiently rich that we won't have to systematically enumerate the possibilities for four- and higher-variable DAGs. On the other hand, in framing hypotheses about connections in the real world, you will often have occasion to connect multiple factors.

Always keep in mind that DAGs never involve loops ("cycles"). @fig-two-cycles shows examples of connections that have cycles. The middle A or DAG stands for "acyclic," that is, without cycles.

```{r echo=FALSE}
#| label: fig-two-cycles
#| fig-cap: "Examples of networks with cycles. These are **not** DAGs."
#| layout-ncol: 2
knitr::include_graphics("www/three-RLR.png")
knitr::include_graphics("www/three-CX-loop.png")
```

What about systems that involve feedback, for instance an economic model where today's prices determine tomorrow's supply which in turn shapes the prices on the next day? Keep in mind that "today's prices" and "the next day's prices" are different variables and therefore not the same node in a directed graph.


---
title: "Math 300R Lesson `r (lesson <- 20)` Reading Notes"
subtitle: "Measuring and simulating variation"
author: "Prof. Danny Kaplan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r include=FALSE}
source("../_startup.R")
```

This Lesson introduces two ideas. The first is how to measure variation. This is important, as you can see from the definition of statistical thinking given in the previous Lesson:

> *Statistic thinking is the explanation or description of measured variation* in the context of *what remains unexplained or undescribed.*

Variation is what we're trying to explain/describe. To do this, it helps to be able to measure variation.

The second idea is also fundamental to statistical thinking. Often, but not always, our interest in studying data is to reveal the causal connections between variables. This is important, for instance, if we are planning to make an intervention in the world and want to anticipate the consequences. Interventions are things like "increase the dose of medicine," "stop smoking!", "lower the budget," "add more cargo to a plane (which will increase fuel consumption and reduce the range)."

Historically, statisticians were hostile to the idea of using data to explore causal relationships. The one exception was **experiment**, where the data come from an actual intervention in the world. (See Lesson 32.) Statistics teachers encouraged students to use phrases like "associated with" or "correlated with" and reminded them that "correlation is not causation."

Regretably, this attitude made statistics irrelevant to that part of the real world where intervention was the matter of interest and experiment was not feasible. A tragic episode of this sort likely caused millions of unnecessary deaths. Starting in the 1940s, doctors and epidemiologists were seeing evidence that smoking causes lung cancer. In stepped the most famous statistician of the age, Ronald Fisher, to insist that the statement should be, "smoking is associated with lung cancer." He speculated that smoking and lung cancer might have a common cause, perhaps genetic. To establish causation, it would be necessary to run an experiment where people are randomly assigned to smoke or not smoke and then observed for decades to see if they developed lung cancer. Such an experiment is unfeasible and unethical, to say nothing of the need to wait decades to get a result. 

Fortunately, around 1960 a researcher at the US National Institutes of Health, Jerome Cornfield, was able to show mathematically that the strength of the association between smoking and cancer ruled out any genetic mechanism. This was one of the first developments in a field called "causal inference" 






## Measuring variation {#sec-size-of-variable}

> *Variation itself is nature’s only irreducible essence. Variation is the hard reality, not a set of imperfect measures for a central tendency. Means and medians are the abstractions.* —-- Stephen Jay Gould (1941- 2002), paleontologist and historian of science


A common task in statistical modeling is to break down a variable into components. For instance, a person's height or intelligence or charm is presumably a combination of genetics and environment. In doing this breaking down, it's convenient to be able to characterize the **size** of each component.

There are many possible ways to measure "size." In this course, we will emphasize two, intimately related measures:

i. *variance*
ii. *standard deviation*, which is simply the square root of variance.

The *OpenIntro* text introduced the standard deviation in [Chapter 3](https://moderndive.com/3-wrangling.html) where it was described as a measure of "spread." In Chapter 6, *OpenIntro* introduced the variance as the square of the variance. All this is right, so far as it goes, but it dramatically understates the importance of the two measures. These measures are as important to statistical thinking as the Pythagorean Theorem is to geometry.

You remember the Pythagorean Theorem: $A^2 + B^2 = C^2$, where $C$ is the length of the hypothenuse of a right triangle, and $A$ and $B$ are the lengths of the other two sides of the triangle. Surprisingly, the Pythagorean Theorem is highly relevant to statistical models. 

Recall from *OpenIntro* chapters 5 & 6 that the linear modeling technique produces two columns of numbers: the **fitted values** and the **residuals**. These columns have the same number of rows as the data frame used for training.
The fitted values are the output from the model when the explanatory variables from the training data are given as inputs to the model. The residuals are the row-by-row numerical *difference* betwen the response variable and the fitted values. 

These three columns of numbers---the response variable, the fitted model values, and the residuals---are exactly analogous to the three sides of a right triangle. (This is not an obvious fact, but it is an important one to keep in mind.) In particular, the following numerical relationship is as true for linear models as it is for triangles:
$$\text{sd(fitted)}^2 + \text{sd(residuals)}^2 = \text{sd(response)}^2$$ where sd() refers to the standard deviation. Consequently, sd()^2 is the variance.

The variance and the standard deviation are defined mathematically in a special way that makes the Pythagorean relationship always describe models constructed the the `lm()` technique, that is "least squares" models.

The antique name "standard deviation" hardly hints at this, largely because it was invented before the least squares technique was invented. For thinking about data, it can be helpful to have in mind a modernization of "standard deviation."

Step 1 in the modernization is to make clear what "standard" means:
$$\text{standard deviation} = \text{accepted}\ \ \textit{measure of} \
\ \text{deviation}$$

Step 2 in the modernization replaces the archaic word "deviation" with something more descriptive:
$$\text{standard deviation} = \text{accepted}\ \ \textit{measure of} \
\ \text{variation in the variable}$$

We won't explain here *why* the standard deviation became the go-to, accepted, standard measure of
variation, but it did and for excellent reasons. 

:::



Both variance and standard deviation are **quantities**, that is, a single number with associated units. The standard deviation of any variable has units that are exactly the same as the variable itself. For instance, the measured heights of a group of people is often measured in cm. So the units of the standard deviation of height will also be in cm.

In contrast, the variance, being the square of standard deviation, has units of the square of the units of the variable. The variance of height, for instance, will be measured in cm^2^. This will seem odd at first glance, but you have to get used to it: the variance of a variable has units that are the square of the units of the variable itself.

Keep in mind also that variance and standard deviation are *summaries* of a variable. A variable in a data frame consists of multiple values, one for each row of the data frame. The variance or standard deviation of that variable will be just a single number (and units), summarizing all of the values in the variable. 

::: {.callout-note}
## Calculating variance

Almost always, people use software to do the calculations. The relevant R functions are `sd()` and `var()`. You can use these functions in a `summarize()` statement, for instance

```{r}
mtcars %>%
  summarize(v = var(hp))
mtcars %>% 
  summarize(s = sd(hp))
```
Regrettably, the software does not indicate the units of the quantity. For that, you need to determine the units of the variable itself, typically by reading the documentation for the data frame.

To understand *what* is being calculated by `var()`, we will describe an algorithm. There are more efficient algorithms than the one described here, but this one is easy to understand.

Starting material: 

- A single column of numbers creating by pulling out from the data frame the variable whose variance is to be calculated. 

- A long roll of paper on which you can write numbers, one after the other.

Basic calculation: You are going to repeat a calculation for each and every row in the column of numbers. To illustrate, suppose you are doing the calculation for the k^th^ row. Take the data value from the k^th^ row, and call it the "reference value."  Then subtract the reference value from each and every other value in the column and square the results. Write those numbers, all of them, on the roll.

Using the same roll of paper for all, carry out the basic calculation starting at each of the rows in the single column of data.  Now your roll of paper has many numbers on each, each of which is the square difference between the values from two rows of the table. If you are mathematically inclined, you might like to know that there will be exactly $n(n-1)$ numbers written on the roll. If you are a statistical instructor, your ears might perk up when you notice the $n-1$ in that count. 

The final result---the variance---will be the mean of the numbers on the roll. Since each of the numbers on the roll is the square difference between two values of the variable, the mean will be the average square difference.

::: {.callout-warning}
## For the statistically experienced reader ...

**Warning!** This box contains mathematical formulas that are **not needed for the course**. The formulas might be interested to mathematically inclined statistics instructors. If that's not you, skip this material.

I realize that the algorithm described above is probably not used by any statistical software package. It's really inefficient numerically. 

You can see this by comparing the traditional formula for the variance to the formula version of the above algorithm:

$$\underbrace{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2}_\text{traditional} \ \ \text{versus} \ \ \underbrace{\frac{1}{n} \sum_{i=1}^n \left[\frac{1}{n-1}\sum_{j\neq i} (x_i - x_j)^2\right]}_\text{our algorithm}$$
The inefficiency of the algorithm stems from the double sum. The advantage of the algorithm is conceptual and two-fold:

i. You can see where the $n-1$ in the formula for the variance comes from: the inner sum involves $(n-1)$ numbers. No hand waving needed to explain the $n-1$. (What might need explaining is the $j \neq i$ in the inner sum. Why not $\sum_j=1^n$? Because that would put $n$ zeros on the roll and bias the result downward. We want to average the square distance between each value and *every other value*.)

ii. There is no need to introduce the mean $\bar{x}$ of the values. Of course, $\bar{x}$ is easy and fast to calculate so there is no numerical reason to avoid using it in the calculation. There is, however, a philosophical reason  based on Stephen J. Gould's observation, quoted at the start of this lesson: "Variation is the hard reality. Means ... are the abstractions." 

Here's a traditional-minded definition: "Variance is equal to the average squared deviations from the mean." This definition makes it seem like the mean has some special status and that variations from it are "deviations."
:::

## Causality

The introduction to this chapter contained a very brief mention of a causal relationship: changing the dose of a medication to, say, lower a patient's blood pressure. Assuming that the drug is effective, it's common sense that the change in dose had a causal influence on the patients' condition. A natural belief that one thing can cause another is the entire basis for medical and other interventions.

The historical statisticians who insisted that data alone cannot establish a causal connection would also, nonetheless, go to the doctor for treatment. Without an experiment, professional pride would lead them to stipulate that data can only establish "correlation" or "association" and that it's impossible to say from data what causes what. But in their everyday lives they believed that medication has a causal influence. How could they justify this belief given their professional attitudes toward causality? Because they have common sense and know something about how the world works. 

This section is about formal ways to say "something about how the world works" that can be used, along with data, to make responsible conclusions about causal relationships.




## Using do( )

THERE's a matter of the order of precedence of the operations in a pipe. Use the CURLY BRACES in the same way you would use parentheses in an arithmetic expression.

WHY CURLY BRACES? Because parentheses mean something else in R: either application of a function to arguments or the usual arithmetic meaning.

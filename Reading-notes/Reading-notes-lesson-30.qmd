---
title: "Math 300R Lesson `r (lesson <- 30)` Reading Notes"
subtitle: "Confounding"
author: "Prof. Danny Kaplan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    css: NTI.css
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
source("../_startup.R")
```

Interest in data often stems from a desire to anticipate the consequences of an intervention. Is a new polio vaccine effective? Will increasing the consumption of organic food improve health generally? Does giving bed nets to poor people in malaria-prone regions reduce the incidence of malaria?  

The previous chapters introduced techniques for modeling a response variable as a function of explanatory variables. Each model is a machine for turning inputs into outputs. Change the input and the output will change correspondingly. But this does not mean that nature works in the same way. Changing an input in the real world -- administering polio vaccine, eating organic food, providing bed nets to the poor -- may not *cause* the same change in the response variable as happens when you change the input to a model.

*The key word here is "**cause**."*

Statisticians are careful to distinguish between two different interpretations of relationship: **"correlation"** and **"causal."** Every successful prediction model `Y ~ X` is a demonstration that there is a correlation between the response Y and the explanatory variable X.^["Successful" means that the prediction performance of the model is better than the performance of a no-input model.] But the performance of the model does not itself tell us that X *causes* Y in the real world. There are other possible configurations that will produce a correlation between X and Y. For instance, both X and Y may themselves have a common cause C without X being otherwise related to Y. In such a circumstance, a real-world intervention to change X will have no effect on Y. To put this in the form of a story, consider that the start of the school year and leaves changing color are correlated. But an intervention to start the school year in mid-winter will not result in  leaves changing color. There's a common cause for the school year and colorful folliage that produces the relationship: the end of summer.

This chapter considers simple networks of causality involving three variables, generically called X, Y, and C. Always, we'll imagine that the modeler's interest is in anticipating how an intervention to change X will create to a change in Y. To accomplish this, the modeler has two basic choices for structuring a model, either

1. `Y ~ X`, or
2. `Y ~ X + C`.

It's surprising to many people that models (1) and (2) can have utterly different, even contradictory implications for how a change in model input X will produce a change in the model output Y. To the modeler trying to capture how the real world works, there's a fundamental choice to be made between using model (1) or model (2) to anticipate the consequences of a real-world intervention on X.

Consider this hypothesis: "It's harder to learn to drive as you move into your 20s." The hypothesis might or might not be true. The way such hypotheses are formed is often by anecdote. Say, you're having dinner when the conversation turns to a friend who has been learning to drive in her 30s. She explains that even after taking many lessons last year, she failed her driving test twice. Others at the table, who started driving in their teens, learned in a much shorter amount of time.

The hypothesis suggests a practical recommendation: It will be easier to learn to drive when younger, so better to start young. Such recommendations to take an action are always rooted in causality: starting young will *cause* you to have less difficulty learning to drive. A diagram, or *graphical causal model*, representing the causal hypothesis is seen in @fig-learn-to-drive-1.

```{r echo=FALSE, out.width = "40%"}
#| label: fig-learn-to-drive-1
#| fig-cap: "A simple graphical causal model expressing the hypothesis that age when learning to drive is the causal factor for the difficulty of learning to drive."
knitr::include_graphics("www/anxiety-drive-1.png")
```

The direction of the arrow in fig-learn-to-drive-1 is a crucial feature. The diagram asserts that a person's age when learning to drive *causes* difficulty, as opposed to the other way around. 

So, does learning to drive when young make it easier to succeed? You might collect some data, perhaps a survey asking people at what age (if ever) they learned to drive (X) and how difficult it was (Y). Suppose you build a successful model Y ~ X. This establishes a correlation X and Y (and vice versa), confirming the dinner table anecdote.  

If you were undertaking serious study of the hypothesis, you should consider how other factors might influence the situation. For instance, it might be that people who learn to drive in their 30s were more anxious about driving when in their teens. That anxiety is why they didn't learn in their teens.  The anxiety might also influence the drivers perception of the difficulty learning. Such a situation is expressed in the graphical causal models in @fig-learn-to-drive-2.


```{r echo=FALSE}
#| label: fig-learn-to-drive-2
#| fig-cap: 'Two possible graphical causal models of the hypothesis, "Age causes difficulty learning to drive," which do not incorporate a direct link from age to difficulty learning.  Left: Anxiety itself leads to people deferring learning to drive, and to increased difficulty when learning. Right: Age is causally linked to difficulty, but the issue is really the diminished support available to older learners.'
knitr::include_graphics("www/anxiety-drive-3.png")
```


Another possibility is that older learners have busier lives and less support for learning to drive. (@fig-learn-to-drive-2(right)) It's harder for older learners to schedule opportunities to practice or to find car owners who can help them learn. 

There is nothing inevitable about graphical causal networks. We can, and often do, intervene in ways that alter the causal flow. For example, @fig-friend-drive shows the network when a later-in-life friend steps in to support a mature student.

```{r echo=FALSE}
#| label: fig-friend-drive
#| fig-cap: "Intervening in a system can change the structure of the graphical causal network. Here, a friend has stepped in to provide the support needed to learn to drive, severing the link that previously connected age when learning to support. (That link -- now severed -- reflects the kind of learning support often available to teenage students of driving, but not to older learners.)"
knitr::include_graphics("www/anxiety-drive-4.png")
```



::: {.callout-note icon=false}
## Causal Caution

Just because you've calculated an effect size doesn't mean that you have captured any sort of causal relationship between the variables. To illustrate, use `dag01` and fit two different models: `y ~ x` and `x ~ y`.

```{r}
Sample <- sample(dag01, size=500)
lm(y ~ x, data = Sample)
lm(x ~ y, data = Sample)
```

You can't tell from these coefficients whether `x` causes `y` or vice versa (or something entirely different). The words "correlation" or "association" are used when we don't want to claim that there is a causal connection. Many statisticians will only use those words unless the data come from an **experiment**. 

We're going to use causal language ("relationship", "effect," etc.) because that is often the matter of concern to decision making. But using language doesn't doesn't make the connection causal.

:::
